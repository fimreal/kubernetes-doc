<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Production-Grade Container Orchestration</title>
    <link>https://kubernetes.io/</link>
    <description>The Kubernetes project blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <image>
      <url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url>
      <title>Kubernetes.io</title>
      <link>https://kubernetes.io/</link>
    </image>
    
	<atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: Introducing Single Pod Access Mode for PersistentVolumes</title>
      <link>https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/</link>
      <pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Chris Henzie (Google)&lt;/p&gt;
&lt;p&gt;Last month&#39;s release of Kubernetes v1.22 introduced a new ReadWriteOncePod access mode for &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes&#34;&gt;PersistentVolumes&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims&#34;&gt;PersistentVolumeClaims&lt;/a&gt;.
With this alpha feature, Kubernetes allows you to restrict volume access to a single pod in the cluster.&lt;/p&gt;
&lt;h2 id=&#34;what-are-access-modes-and-why-are-they-important&#34;&gt;What are access modes and why are they important?&lt;/h2&gt;
&lt;p&gt;When using storage, there are different ways to model how that storage is consumed.&lt;/p&gt;
&lt;p&gt;For example, a storage system like a network file share can have many users all reading and writing data simultaneously.
In other cases maybe everyone is allowed to read data but not write it.
For highly sensitive data, maybe only one user is allowed to read and write data but nobody else.&lt;/p&gt;
&lt;p&gt;In the world of Kubernetes, &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34;&gt;access modes&lt;/a&gt; are the way you can define how durable storage is consumed.
These access modes are a part of the spec for PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;shared-cache&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;accessModes&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- ReadWriteMany&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Allow many pods to access shared-cache simultaneously.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;resources&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;requests&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;storage&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;1Gi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before v1.22, Kubernetes offered three access modes for PVs and PVCs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce – the volume can be mounted as read-write by a single node&lt;/li&gt;
&lt;li&gt;ReadOnlyMany – the volume can be mounted read-only by many nodes&lt;/li&gt;
&lt;li&gt;ReadWriteMany – the volume can be mounted as read-write by many nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These access modes are enforced by Kubernetes components like the &lt;code&gt;kube-controller-manager&lt;/code&gt; and &lt;code&gt;kubelet&lt;/code&gt; to ensure only certain pods are allowed to access a given PersistentVolume.&lt;/p&gt;
&lt;h2 id=&#34;what-is-this-new-access-mode-and-how-does-it-work&#34;&gt;What is this new access mode and how does it work?&lt;/h2&gt;
&lt;p&gt;Kubernetes v1.22 introduced a fourth access mode for PVs and PVCs, that you can use for CSI volumes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOncePod – the volume can be mounted as read-write by a single pod&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you create a pod with a PVC that uses the ReadWriteOncePod access mode, Kubernetes ensures that pod is the only pod across your whole cluster that can read that PVC or write to it.&lt;/p&gt;
&lt;p&gt;If you create another pod that references the same PVC with this access mode, the pod will fail to start because the PVC is already in use by another pod.
For example:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  1s    default-scheduler  0/1 nodes are available: 1 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode.
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;how-is-this-different-than-the-readwriteonce-access-mode&#34;&gt;How is this different than the ReadWriteOnce access mode?&lt;/h3&gt;
&lt;p&gt;The ReadWriteOnce access mode restricts volume access to a single &lt;em&gt;node&lt;/em&gt;, which means it is possible for multiple pods on the same node to read from and write to the same volume.
This could potentially be a major problem for some applications, especially if they require at most one writer for data safety guarantees.&lt;/p&gt;
&lt;p&gt;With ReadWriteOncePod these issues go away.
Set the access mode on your PVC, and Kubernetes guarantees that only a single pod has access.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-use-it&#34;&gt;How do I use it?&lt;/h2&gt;
&lt;p&gt;The ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes.
As a first step you need to enable the ReadWriteOncePod &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates&#34;&gt;feature gate&lt;/a&gt; for &lt;code&gt;kube-apiserver&lt;/code&gt;, &lt;code&gt;kube-scheduler&lt;/code&gt;, and &lt;code&gt;kubelet&lt;/code&gt;.
You can enable the feature by setting command line arguments:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;--feature-gates=&amp;quot;...,ReadWriteOncePod=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You also need to update the following CSI sidecars to these versions or greater:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0&#34;&gt;csi-provisioner:v3.0.0+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0&#34;&gt;csi-attacher:v3.3.0+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0&#34;&gt;csi-resizer:v1.3.0+&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;creating-a-persistentvolumeclaim&#34;&gt;Creating a PersistentVolumeClaim&lt;/h3&gt;
&lt;p&gt;In order to use the ReadWriteOncePod access mode for your PVs and PVCs, you will need to create a new PVC with the access mode:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;single-writer-only&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;accessModes&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- ReadWriteOncePod&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Allow only a single pod to access single-writer-only.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;resources&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;requests&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;storage&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;1Gi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If your storage plugin supports &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/&#34;&gt;dynamic provisioning&lt;/a&gt;, new PersistentVolumes will be created with the ReadWriteOncePod access mode applied.&lt;/p&gt;
&lt;h4 id=&#34;migrating-existing-persistentvolumes&#34;&gt;Migrating existing PersistentVolumes&lt;/h4&gt;
&lt;p&gt;If you have existing PersistentVolumes, they can be migrated to use ReadWriteOncePod.&lt;/p&gt;
&lt;p&gt;In this example, we already have a &amp;quot;cat-pictures-pvc&amp;quot; PersistentVolumeClaim that is bound to a &amp;quot;cat-pictures-pv&amp;quot; PersistentVolume, and a &amp;quot;cat-pictures-writer&amp;quot; Deployment that uses this PersistentVolumeClaim.&lt;/p&gt;
&lt;p&gt;As a first step, you need to edit your PersistentVolume&#39;s &lt;code&gt;spec.persistentVolumeReclaimPolicy&lt;/code&gt; and set it to &lt;code&gt;Retain&lt;/code&gt;.
This ensures your PersistentVolume will not be deleted when we delete the corresponding PersistentVolumeClaim:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl patch pv cat-pictures-pv -p &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;persistentVolumeReclaimPolicy&amp;#34;:&amp;#34;Retain&amp;#34;}}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next you need to stop any workloads that are using the PersistentVolumeClaim bound to the PersistentVolume you want to migrate, and then delete the PersistentVolumeClaim.&lt;/p&gt;
&lt;p&gt;Once that is done, you need to clear your PersistentVolume&#39;s &lt;code&gt;spec.claimRef.uid&lt;/code&gt; to ensure PersistentVolumeClaims can bind to it upon recreation:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl scale --replicas&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt; deployment cat-pictures-writer
kubectl delete pvc cat-pictures-pvc
kubectl patch pv cat-pictures-pv -p &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;claimRef&amp;#34;:{&amp;#34;uid&amp;#34;:&amp;#34;&amp;#34;}}}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After that you need to replace the PersistentVolume&#39;s access modes with ReadWriteOncePod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl patch pv cat-pictures-pv -p &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;accessModes&amp;#34;:[&amp;#34;ReadWriteOncePod&amp;#34;]}}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note callout&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; The ReadWriteOncePod access mode cannot be combined with other access modes.
Make sure ReadWriteOncePod is the only access mode on the PersistentVolume when updating, otherwise the request will fail.&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;Next you need to modify your PersistentVolumeClaim to set ReadWriteOncePod as the only access mode.
You should also set your PersistentVolumeClaim&#39;s &lt;code&gt;spec.volumeName&lt;/code&gt; to the name of your PersistentVolume.&lt;/p&gt;
&lt;p&gt;Once this is done, you can recreate your PersistentVolumeClaim and start up your workloads:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# IMPORTANT: Make sure to edit your PVC in cat-pictures-pvc.yaml before applying. You need to:&lt;/span&gt;
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# - Set ReadWriteOncePod as the only access mode&lt;/span&gt;
&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# - Set spec.volumeName to &amp;#34;cat-pictures-pv&amp;#34;&lt;/span&gt;

kubectl apply -f cat-pictures-pvc.yaml
kubectl apply -f cat-pictures-writer-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Lastly you may edit your PersistentVolume&#39;s &lt;code&gt;spec.persistentVolumeReclaimPolicy&lt;/code&gt; and set to it back to &lt;code&gt;Delete&lt;/code&gt; if you previously changed it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl patch pv cat-pictures-pv -p &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;persistentVolumeReclaimPolicy&amp;#34;:&amp;#34;Delete&amp;#34;}}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can read &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/&#34;&gt;Configure a Pod to Use a PersistentVolume for Storage&lt;/a&gt; for more details on working with PersistentVolumes and PersistentVolumeClaims.&lt;/p&gt;
&lt;h2 id=&#34;what-volume-plugins-support-this&#34;&gt;What volume plugins support this?&lt;/h2&gt;
&lt;p&gt;The only volume plugins that support this are CSI drivers.
SIG Storage does not plan to support this for in-tree plugins because they are being deprecated as part of &lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#what-is-the-timeline-status&#34;&gt;CSI migration&lt;/a&gt;.
Support may be considered for beta for users that prefer to use the legacy in-tree volume APIs with CSI migration enabled.&lt;/p&gt;
&lt;h2 id=&#34;as-a-storage-vendor-how-do-i-add-support-for-this-access-mode-to-my-csi-driver&#34;&gt;As a storage vendor, how do I add support for this access mode to my CSI driver?&lt;/h2&gt;
&lt;p&gt;The ReadWriteOncePod access mode will work out of the box without any required updates to CSI drivers, but &lt;a href=&#34;#update-your-csi-sidecars&#34;&gt;does require updates to CSI sidecars&lt;/a&gt;.
With that being said, if you would like to stay up to date with the latest changes to the CSI specification (v1.5.0+), read on.&lt;/p&gt;
&lt;p&gt;Two new access modes were introduced to the CSI specification in order to disambiguate the legacy &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L418-L420&#34;&gt;&lt;code&gt;SINGLE_NODE_WRITER&lt;/code&gt;&lt;/a&gt; access mode.
They are &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L437-L447&#34;&gt;&lt;code&gt;SINGLE_NODE_SINGLE_WRITER&lt;/code&gt; and &lt;code&gt;SINGLE_NODE_MULTI_WRITER&lt;/code&gt;&lt;/a&gt;.
In order to communicate to sidecars (like the &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34;&gt;external-provisioner&lt;/a&gt;) that your driver understands and accepts these two new CSI access modes, your driver will also need to advertise the &lt;code&gt;SINGLE_NODE_MULTI_WRITER&lt;/code&gt; capability for the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1073-L1081&#34;&gt;controller service&lt;/a&gt; and &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1515-L1524&#34;&gt;node service&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you&#39;d like to read up on the motivation for these access modes and capability bits, you can also read the &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md#csi-specification-changes-volume-capabilities&#34;&gt;CSI Specification Changes, Volume Capabilities&lt;/a&gt; section of KEP-2485 (ReadWriteOncePod PersistentVolume Access Mode).&lt;/p&gt;
&lt;h3 id=&#34;update-your-csi-driver-to-use-the-new-interface&#34;&gt;Update your CSI driver to use the new interface&lt;/h3&gt;
&lt;p&gt;As a first step you will need to update your driver&#39;s &lt;code&gt;container-storage-interface&lt;/code&gt; dependency to v1.5.0+, which contains support for these new access modes and capabilities.&lt;/p&gt;
&lt;h3 id=&#34;accept-new-csi-access-modes&#34;&gt;Accept new CSI access modes&lt;/h3&gt;
&lt;p&gt;If your CSI driver contains logic for validating CSI access modes for requests , it may need updating.
If it currently accepts &lt;code&gt;SINGLE_NODE_WRITER&lt;/code&gt;, it should be updated to also accept &lt;code&gt;SINGLE_NODE_SINGLE_WRITER&lt;/code&gt; and &lt;code&gt;SINGLE_NODE_MULTI_WRITER&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Using the &lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/utils.go#L116-L130&#34;&gt;GCP PD CSI driver validation logic&lt;/a&gt; as an example, here is how it can be extended:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;diff --git a/pkg/gce-pd-csi-driver/utils.go b/pkg/gce-pd-csi-driver/utils.go
&lt;/span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;index 281242c..b6c5229 100644
&lt;/span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a00000&#34;&gt;--- a/pkg/gce-pd-csi-driver/utils.go
&lt;/span&gt;&lt;span style=&#34;color:#a00000&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;+++ b/pkg/gce-pd-csi-driver/utils.go
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#800080;font-weight:bold&#34;&gt;@@ -123,6 +123,8 @@ func validateAccessMode(am *csi.VolumeCapability_AccessMode) error {
&lt;/span&gt;&lt;span style=&#34;color:#800080;font-weight:bold&#34;&gt;&lt;/span&gt;        case csi.VolumeCapability_AccessMode_SINGLE_NODE_READER_ONLY:
        case csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY:
        case csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER:
&lt;span style=&#34;color:#00a000&#34;&gt;+       case csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER:
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;+       case csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER:
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;&lt;/span&gt;        default:
                return fmt.Errorf(&amp;#34;%v access mode is not supported for for PD&amp;#34;, am.GetMode())
        }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;advertise-new-csi-controller-and-node-service-capabilities&#34;&gt;Advertise new CSI controller and node service capabilities&lt;/h3&gt;
&lt;p&gt;Your CSI driver will also need to return the new &lt;code&gt;SINGLE_NODE_MULTI_WRITER&lt;/code&gt; capability as part of the &lt;code&gt;ControllerGetCapabilities&lt;/code&gt; and &lt;code&gt;NodeGetCapabilities&lt;/code&gt; RPCs.&lt;/p&gt;
&lt;p&gt;Using the &lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/gce-pd-driver.go#L54-L77&#34;&gt;GCP PD CSI driver capability advertisement logic&lt;/a&gt; as an example, here is how it can be extended:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;diff --git a/pkg/gce-pd-csi-driver/gce-pd-driver.go b/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;index 45903f3..0d7ea26 100644
&lt;/span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a00000&#34;&gt;--- a/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span&gt;&lt;span style=&#34;color:#a00000&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;+++ b/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#800080;font-weight:bold&#34;&gt;@@ -56,6 +56,8 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu
&lt;/span&gt;&lt;span style=&#34;color:#800080;font-weight:bold&#34;&gt;&lt;/span&gt;                csi.VolumeCapability_AccessMode_SINGLE_NODE_WRITER,
                csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY,
                csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER,
&lt;span style=&#34;color:#00a000&#34;&gt;+               csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER,
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;+               csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER,
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;&lt;/span&gt;        }
        gceDriver.AddVolumeCapabilityAccessModes(vcam)
        csc := []csi.ControllerServiceCapability_RPC_Type{
&lt;span style=&#34;color:#800080;font-weight:bold&#34;&gt;@@ -67,12 +69,14 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu
&lt;/span&gt;&lt;span style=&#34;color:#800080;font-weight:bold&#34;&gt;&lt;/span&gt;                csi.ControllerServiceCapability_RPC_EXPAND_VOLUME,
                csi.ControllerServiceCapability_RPC_LIST_VOLUMES,
                csi.ControllerServiceCapability_RPC_LIST_VOLUMES_PUBLISHED_NODES,
&lt;span style=&#34;color:#00a000&#34;&gt;+               csi.ControllerServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;&lt;/span&gt;        }
        gceDriver.AddControllerServiceCapabilities(csc)
        ns := []csi.NodeServiceCapability_RPC_Type{
                csi.NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME,
                csi.NodeServiceCapability_RPC_EXPAND_VOLUME,
                csi.NodeServiceCapability_RPC_GET_VOLUME_STATS,
&lt;span style=&#34;color:#00a000&#34;&gt;+               csi.NodeServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;&lt;/span&gt;        }
        gceDriver.AddNodeServiceCapabilities(ns)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;implement-nodepublishvolume-behavior&#34;&gt;Implement &lt;code&gt;NodePublishVolume&lt;/code&gt; behavior&lt;/h3&gt;
&lt;p&gt;The CSI spec outlines expected behavior for the &lt;code&gt;NodePublishVolume&lt;/code&gt; RPC when called more than once for the same volume but with different arguments (like the target path).
Please refer to &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/v1.5.0/spec.md#nodepublishvolume&#34;&gt;the second table in the NodePublishVolume section of the CSI spec&lt;/a&gt; for more details on expected behavior when implementing in your driver.&lt;/p&gt;
&lt;h3 id=&#34;update-your-csi-sidecars&#34;&gt;Update your CSI sidecars&lt;/h3&gt;
&lt;p&gt;When deploying your CSI drivers, you must update the following CSI sidecars to versions that depend on CSI spec v1.5.0+ and the Kubernetes v1.22 API.
The minimum required versions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0&#34;&gt;csi-provisioner:v3.0.0+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0&#34;&gt;csi-attacher:v3.3.0+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0&#34;&gt;csi-resizer:v1.3.0+&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;As part of the beta graduation for this feature, SIG Storage plans to update the Kubenetes scheduler to support pod preemption in relation to ReadWriteOncePod storage.
This means if two pods request a PersistentVolumeClaim with ReadWriteOncePod, the pod with highest priority will gain access to the PersistentVolumeClaim and any pod with lower priority will be preempted from the node and be unable to access the PersistentVolumeClaim.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;
&lt;p&gt;Please see &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md&#34;&gt;KEP-2485&lt;/a&gt; for more details on the ReadWriteOncePod access mode and motivations for CSI spec changes.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://kubernetes.slack.com/messages/csi&#34;&gt;Kubernetes #csi Slack channel&lt;/a&gt; and any of the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact&#34;&gt;standard SIG Storage communication channels&lt;/a&gt; are great mediums to reach out to the SIG Storage and the CSI teams.&lt;/p&gt;
&lt;p&gt;Special thanks to the following people for their insightful reviews and design considerations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Abdullah Gharaibeh (ahg-g)&lt;/li&gt;
&lt;li&gt;Aldo Culquicondor (alculquicondor)&lt;/li&gt;
&lt;li&gt;Ben Swartzlander (bswartz)&lt;/li&gt;
&lt;li&gt;Deep Debroy (ddebroy)&lt;/li&gt;
&lt;li&gt;Hemant Kumar (gnufied)&lt;/li&gt;
&lt;li&gt;Humble Devassy Chirammal (humblec)&lt;/li&gt;
&lt;li&gt;James DeFelice (jdef)&lt;/li&gt;
&lt;li&gt;Jan Šafránek (jsafrane)&lt;/li&gt;
&lt;li&gt;Jing Xu (jingxu97)&lt;/li&gt;
&lt;li&gt;Jordan Liggitt (liggitt)&lt;/li&gt;
&lt;li&gt;Michelle Au (msau42)&lt;/li&gt;
&lt;li&gt;Saad Ali (saad-ali)&lt;/li&gt;
&lt;li&gt;Tim Hockin (thockin)&lt;/li&gt;
&lt;li&gt;Xing Yang (xing-yang)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG).
We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Alpha in Kubernetes v1.22: API Server Tracing</title>
      <link>https://kubernetes.io/blog/2021/09/03/api-server-tracing/</link>
      <pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/09/03/api-server-tracing/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David Ashpole (Google)&lt;/p&gt;
&lt;p&gt;In distributed systems, it can be hard to figure out where problems are. You grep through one component&#39;s logs just to discover that the source of your problem is in another component.  You search there only to discover that you need to enable debug logs to figure out what really went wrong... And it goes on. The more complex the path your request takes, the harder it is to answer questions about where it went.  I&#39;ve personally spent many hours doing this dance with a variety of Kubernetes components. Distributed tracing is a tool which is designed to help in these situations, and the Kubernetes API Server is, perhaps, the most important Kubernetes component to be able to debug. At Kubernetes&#39; Sig Instrumentation, our mission is to make it easier to understand what&#39;s going on in your cluster, and we are happy to announce that distributed tracing in the Kubernetes API Server reached alpha in 1.22.&lt;/p&gt;
&lt;h2 id=&#34;what-is-tracing&#34;&gt;What is Tracing?&lt;/h2&gt;
&lt;p&gt;Distributed tracing links together a bunch of super-detailed information from multiple different sources, and structures that telemetry into a single tree for that request.  Unlike logging, which limits the quantity of data ingested by using log levels, tracing collects all of the details and uses sampling to collect only a small percentage of requests.  This means that once you have a trace which demonstrates an issue, you should have all the information you need to root-cause the problem--no grepping for object UID required! My favorite aspect, though, is how useful the visualizations of traces are.  Even if you don&#39;t understand the inner workings of the API Server, or don&#39;t have a clue what an etcd &amp;quot;Transaction&amp;quot; is, I&#39;d wager you (yes, you!) could tell me roughly what the order of events was, and which components were involved in the request.  If some step takes a long time, it is easy to tell where the problem is.&lt;/p&gt;
&lt;h2 id=&#34;why-opentelemetry&#34;&gt;Why OpenTelemetry?&lt;/h2&gt;
&lt;p&gt;It&#39;s important that Kubernetes works well for everyone, regardless of who manages your infrastructure, or which vendors you choose to integrate with.  That is particularly true for Kubernetes&#39; integrations with telemetry solutions.  OpenTelemetry, being a CNCF project, shares these core values, and is creating exactly what we need in Kubernetes: A set of open standards for Tracing client library APIs and a standard trace format. By using OpenTelemetry, we can ensure users have the freedom to choose their backend, and ensure vendors have a level playing field. The timing couldn&#39;t be better: the OpenTelemetry golang API and SDK are very close to their 1.0 release, and will soon offer backwards-compatibility for these open standards.&lt;/p&gt;
&lt;h2 id=&#34;why-instrument-the-api-server&#34;&gt;Why instrument the API Server?&lt;/h2&gt;
&lt;p&gt;The Kubernetes API Server is a great candidate for tracing for a few reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It follows the standard &amp;quot;RPC&amp;quot; model (serve a request by making requests to downstream components), which makes it easy to instrument.&lt;/li&gt;
&lt;li&gt;Users are latency-sensitive: If a request takes more than 10 seconds to complete, many clients will time-out.&lt;/li&gt;
&lt;li&gt;It has a complex service topology: A single request could require consulting a dozen webhooks, or involve multiple requests to etcd.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;trying-out-apiserver-tracing-with-a-webhook&#34;&gt;Trying out APIServer Tracing with a webhook&lt;/h2&gt;
&lt;h3 id=&#34;enabling-api-server-tracing&#34;&gt;Enabling API Server Tracing&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Enable the APIServerTracing &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature-gate&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set our configuration for tracing by pointing the &lt;code&gt;--tracing-config-file&lt;/code&gt; flag on the kube-apiserver at our config file, which contains:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiserver.config.k8s.io/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;TracingConfiguration&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# 1% sampling rate&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;samplingRatePerMillion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;enabling-etcd-tracing&#34;&gt;Enabling Etcd Tracing&lt;/h3&gt;
&lt;p&gt;Add &lt;code&gt;--experimental-enable-distributed-tracing&lt;/code&gt;,  &lt;code&gt;--experimental-distributed-tracing-address=0.0.0.0:4317&lt;/code&gt;, &lt;code&gt;--experimental-distributed-tracing-service-name=etcd&lt;/code&gt; flags to etcd to enable tracing.  Note that this traces every request, so it will probably generate a lot of traces if you enable it.&lt;/p&gt;
&lt;h3 id=&#34;example-trace-list-nodes&#34;&gt;Example Trace: List Nodes&lt;/h3&gt;
&lt;p&gt;I could&#39;ve used any trace backend, but decided to use Jaeger, since it is one of the most popular open-source tracing projects.  I deployed &lt;a href=&#34;https://hub.docker.com/r/jaegertracing/all-in-one&#34;&gt;the Jaeger All-in-one container&lt;/a&gt; in my cluster, deployed &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector&#34;&gt;the OpenTelemetry collector&lt;/a&gt; on my control-plane node (&lt;a href=&#34;https://github.com/dashpole/dashpole_demos/tree/master/otel/controlplane&#34;&gt;example&lt;/a&gt;), and captured traces like this one:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-1.png&#34; alt=&#34;Jaeger screenshot showing API server and etcd trace&#34; title=&#34;Jaeger screenshot showing API server and etcd trace&#34;&gt;&lt;/p&gt;
&lt;p&gt;The teal lines are from the API Server, and includes it serving a request to &lt;code&gt;/api/v1/nodes&lt;/code&gt;, and issuing a grpc &lt;code&gt;Range&lt;/code&gt; RPC to ETCD.  The yellow-ish line is from ETCD handling the &lt;code&gt;Range&lt;/code&gt; RPC.&lt;/p&gt;
&lt;h3 id=&#34;example-trace-create-pod-with-mutating-webhook&#34;&gt;Example Trace: Create Pod with Mutating Webhook&lt;/h3&gt;
&lt;p&gt;I instrumented the &lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime/tree/master/examples/builtins&#34;&gt;example webhook&lt;/a&gt; with OpenTelemetry (I had to &lt;a href=&#34;https://github.com/dashpole/controller-runtime/commit/85fdda7ba03dd2c22ef62c1a3dbdf5aa651f90da&#34;&gt;patch&lt;/a&gt; controller-runtime, but it makes a neat demo), and routed traces to Jaeger as well.  I collected traces like this one:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-2.png&#34; alt=&#34;Jaeger screenshot showing API server, admission webhook, and etcd trace&#34; title=&#34;Jaeger screenshot showing API server, admission webhook, and etcd trace&#34;&gt;&lt;/p&gt;
&lt;p&gt;Compared with the previous trace, there are two new spans: A teal span from the API Server making a request to the admission webhook, and a brown span from the admission webhook serving the request.  Even if you didn&#39;t instrument your webhook, you would still get the span from the API Server making the request to the webhook.&lt;/p&gt;
&lt;h2 id=&#34;get-involved&#34;&gt;Get involved!&lt;/h2&gt;
&lt;p&gt;As this is our first attempt at adding distributed tracing to a Kubernetes component, there is probably a lot we can improve! If my struggles resonated with you, or if you just want to try out the latest Kubernetes has to offer, please give the feature a try and open issues with any problem you encountered and ways you think the feature could be improved.&lt;/p&gt;
&lt;p&gt;This is just the very beginning of what we can do with distributed tracing in Kubernetes. If there are other components you think would benefit from distributed tracing, or want to help bring API Server Tracing to GA, join sig-instrumentation at our &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-instrumentation#instrumentation-special-interest-group&#34;&gt;regular meetings&lt;/a&gt; and get involved!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.22: A New Design for Volume Populators</title>
      <link>https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;
Ben Swartzlander (NetApp)&lt;/p&gt;
&lt;p&gt;Kubernetes v1.22, released earlier this month, introduced a redesigned approach for volume
populators. Originally implemented
in v1.18, the API suffered from backwards compatibility issues. Kubernetes v1.22 includes a new API
field called &lt;code&gt;dataSourceRef&lt;/code&gt; that fixes these problems.&lt;/p&gt;
&lt;h2 id=&#34;data-sources&#34;&gt;Data sources&lt;/h2&gt;
&lt;p&gt;Earlier Kubernetes releases already added a &lt;code&gt;dataSource&lt;/code&gt; field into the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims&#34;&gt;PersistentVolumeClaim&lt;/a&gt; API,
used for cloning volumes and creating volumes from snapshots. You could use the &lt;code&gt;dataSource&lt;/code&gt; field when
creating a new PVC, referencing either an existing PVC or a VolumeSnapshot in the same namespace.
That also modified the normal provisioning process so that instead of yielding an empty volume, the
new PVC contained the same data as either the cloned PVC or the cloned VolumeSnapshot.&lt;/p&gt;
&lt;p&gt;Volume populators embrace the same design idea, but extend it to any type of object, as long
as there exists a &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;custom resource&lt;/a&gt;
to define the data source, and a populator controller to implement the logic. Initially,
the &lt;code&gt;dataSource&lt;/code&gt; field was directly extended to allow arbitrary objects, if the &lt;code&gt;AnyVolumeDataSource&lt;/code&gt;
feature gate was enabled on a cluster. That change unfortunately caused backwards compatibility
problems, and so the new &lt;code&gt;dataSourceRef&lt;/code&gt; field was born.&lt;/p&gt;
&lt;p&gt;In v1.22 if the &lt;code&gt;AnyVolumeDataSource&lt;/code&gt; feature gate is enabled, the &lt;code&gt;dataSourceRef&lt;/code&gt; field is
added, which behaves similarly to the &lt;code&gt;dataSource&lt;/code&gt; field except that it allows arbitrary
objects to be specified. The API server ensures that the two fields always have the same
contents, and neither of them are mutable. The differences is that at creation time
&lt;code&gt;dataSource&lt;/code&gt; allows only PVCs or VolumeSnapshots, and ignores all other values, while
&lt;code&gt;dataSourceRef&lt;/code&gt; allows most types of objects, and in the few cases it doesn&#39;t allow an
object (core objects other than PVCs) a validation error occurs.&lt;/p&gt;
&lt;p&gt;When this API change graduates to stable, we would deprecate using &lt;code&gt;dataSource&lt;/code&gt; and recommend
using &lt;code&gt;dataSourceRef&lt;/code&gt; field for all use cases.
In the v1.22 release, &lt;code&gt;dataSourceRef&lt;/code&gt; is available (as an alpha feature) specifically for cases
where you want to use for custom volume populators.&lt;/p&gt;
&lt;h2 id=&#34;using-populators&#34;&gt;Using populators&lt;/h2&gt;
&lt;p&gt;Every volume populator must have one or more CRDs that it supports. Administrators may
install the CRD and the populator controller and then PVCs with a &lt;code&gt;dataSourceRef&lt;/code&gt; specifies
a CR of the type that the populator supports will be handled by the populator controller
instead of the CSI driver directly.&lt;/p&gt;
&lt;p&gt;Underneath the covers, the CSI driver is still invoked to create an empty volume, which
the populator controller fills with the appropriate data. The PVC doesn&#39;t bind to the PV
until it&#39;s fully populated, so it&#39;s safe to define a whole application manifest including
pod and PVC specs and the pods won&#39;t begin running until everything is ready, just as if
the PVC was a clone of another PVC or VolumeSnapshot.&lt;/p&gt;
&lt;h2 id=&#34;how-it-works&#34;&gt;How it works&lt;/h2&gt;
&lt;p&gt;PVCs with data sources are still noticed by the external-provisioner sidecar for the
related storage class (assuming a CSI provisioner is used), but because the sidecar
doesn&#39;t understand the data source kind, it doesn&#39;t do anything. The populator controller
is also watching for PVCs with data sources of a kind that it understands and when it
sees one, it creates a temporary PVC of the same size, volume mode, storage class,
and even on the same topology (if topology is used) as the original PVC. The populator
controller creates a worker pod that attaches to the volume and writes the necessary
data to it, then detaches from the volume and the populator controller rebinds the PV
from the temporary PVC to the orignal PVC.&lt;/p&gt;
&lt;h2 id=&#34;trying-it-out&#34;&gt;Trying it out&lt;/h2&gt;
&lt;p&gt;The following things are required to use volume populators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enable the &lt;code&gt;AnyVolumeDataSource&lt;/code&gt; feature gate&lt;/li&gt;
&lt;li&gt;Install a CRD for the specific data source / populator&lt;/li&gt;
&lt;li&gt;Install the populator controller itself&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Populator controllers may use the &lt;a href=&#34;https://github.com/kubernetes-csi/lib-volume-populator&#34;&gt;lib-volume-populator&lt;/a&gt;
library to do most of the Kubernetes API level work. Individual populators only need to
provide logic for actually writing data into the volume based on a particular CR
instance. This library provides a sample populator implementation.&lt;/p&gt;
&lt;p&gt;These optional components improve user experience:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install the VolumePopulator CRD&lt;/li&gt;
&lt;li&gt;Create a VolumePopulator custom respource for each specific data source&lt;/li&gt;
&lt;li&gt;Install the &lt;a href=&#34;https://github.com/kubernetes-csi/volume-data-source-validator&#34;&gt;volume data source validator&lt;/a&gt;
controller (alpha)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The purpose of these components is to generate warning events on PVCs with data sources
for which there is no populator.&lt;/p&gt;
&lt;h2 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;To see how this works, you can install the sample &amp;quot;hello&amp;quot; populator and try it
out.&lt;/p&gt;
&lt;p&gt;First install the volume-data-source-validator controller.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-terminal&#34; data-lang=&#34;terminal&#34;&gt;kubectl apply -f https://github.com/kubernetes-csi/volume-data-source-validator/blob/master/deploy/kubernetes/rbac-data-source-validator.yaml
kubectl apply -f https://github.com/kubernetes-csi/volume-data-source-validator/blob/master/deploy/kubernetes/setup-data-source-validator.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next install the example populator.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-terminal&#34; data-lang=&#34;terminal&#34;&gt;kubectl apply -f https://github.com/kubernetes-csi/lib-volume-populator/blob/master/example/hello-populator/crd.yaml
kubectl apply -f https://github.com/kubernetes-csi/lib-volume-populator/blob/master/example/hello-populator/deploy.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Create an instance of the &lt;code&gt;Hello&lt;/code&gt; CR, with some text.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello.k8s.io/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Hello&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;example-hello&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;fileName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;example.txt&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;fileContents&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Hello, world!&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create a PVC that refers to that CR as its data source.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;example-pvc&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;accessModes&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- ReadWriteOnce&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;resources&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;requests&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;storage&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;10Mi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;dataSourceRef&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiGroup&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;hello.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Hello&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;example-hello&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;volumeMode&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Filesystem&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, run a job that reads the file in the PVC.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;batch/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Job&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;example-job&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;template&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;containers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;example-container&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;busybox:latest&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;command&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;- cat&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;- /mnt/example.txt&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;volumeMounts&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vol&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;mountPath&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/mnt&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;restartPolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Never&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;volumes&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;vol&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;persistentVolumeClaim&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;claimName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;example-pvc&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wait for the job to complete (including all of its dependencies).&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-terminal&#34; data-lang=&#34;terminal&#34;&gt;kubectl wait --for=condition=Complete job/example-job
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And last examine the log from the job.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-terminal&#34; data-lang=&#34;terminal&#34;&gt;kubectl logs job/example-job
Hello, world!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that the volume already contained a text file with the string contents from
the CR. This is only the simplest example. Actual populators can set up the volume
to contain arbitrary contents.&lt;/p&gt;
&lt;h2 id=&#34;how-to-write-your-own-volume-populator&#34;&gt;How to write your own volume populator&lt;/h2&gt;
&lt;p&gt;Developers interested in writing new poplators are encouraged to use the
&lt;a href=&#34;https://github.com/kubernetes-csi/lib-volume-populator&#34;&gt;lib-volume-populator&lt;/a&gt; library
and to only supply a small controller wrapper around the library, and a pod image
capable of attaching to volumes and writing the appropriate data to the volume.&lt;/p&gt;
&lt;p&gt;Individual populators can be extremely generic such that they work with every type
of PVC, or they can do vendor specific things to rapidly fill a volume with data
if the volume was provisioned by a specific CSI driver from the same vendor, for
example, by communicating directly with the storage for that volume.&lt;/p&gt;
&lt;h2 id=&#34;the-future&#34;&gt;The future&lt;/h2&gt;
&lt;p&gt;As this feature is still in alpha, we expect to update the out of tree controllers
with more tests and documentation. The community plans to eventually re-implement
the populator library as a sidecar, for ease of operations.&lt;/p&gt;
&lt;p&gt;We hope to see some official community-supported populators for some widely-shared
use cases. Also, we expect that volume populators will be used by backup vendors
as a way to &amp;quot;restore&amp;quot; backups to volumes, and possibly a standardized API to do
this will evolve.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;
&lt;p&gt;The enhancement proposal,
&lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators&#34;&gt;Volume Populators&lt;/a&gt;, includes lots of detail about the history and technical implementation
of this feature.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources&#34;&gt;Volume populators and data sources&lt;/a&gt;, within the documentation topic about persistent volumes,
explains how to use this feature in your cluster.&lt;/p&gt;
&lt;p&gt;Please get involved by joining the Kubernetes storage SIG to help us enhance this
feature. There are a lot of good ideas already and we&#39;d be thrilled to have more!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Minimum Ready Seconds for StatefulSets</title>
      <link>https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ravi Gudimetla (Red Hat), Maciej Szulik (Red Hat)&lt;/p&gt;
&lt;p&gt;This blog describes the notion of Availability for &lt;code&gt;StatefulSet&lt;/code&gt; workloads, and a new alpha feature in Kubernetes 1.22 which adds &lt;code&gt;minReadySeconds&lt;/code&gt; configuration for &lt;code&gt;StatefulSets&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-problems-does-this-solve&#34;&gt;What problems does this solve?&lt;/h2&gt;
&lt;p&gt;Prior to Kubernetes 1.22 release, once a &lt;code&gt;StatefulSet&lt;/code&gt; &lt;code&gt;Pod&lt;/code&gt; is in the &lt;code&gt;Ready&lt;/code&gt; state it is considered &lt;code&gt;Available&lt;/code&gt; to receive traffic. For some of the &lt;code&gt;StatefulSet&lt;/code&gt; workloads, it may not be the case. For example, a workload like Prometheus with multiple instances of Alertmanager, it should be considered &lt;code&gt;Available&lt;/code&gt; only when Alertmanager&#39;s state transfer is complete, not when the &lt;code&gt;Pod&lt;/code&gt; is in &lt;code&gt;Ready&lt;/code&gt; state. Since &lt;code&gt;minReadySeconds&lt;/code&gt; adds buffer, the state transfer may be complete before the &lt;code&gt;Pod&lt;/code&gt; becomes &lt;code&gt;Available&lt;/code&gt;. While this is not a fool proof way of identifying if the state transfer is complete or not, it gives a way to the end user to express their intention of waiting for sometime before the &lt;code&gt;Pod&lt;/code&gt; is considered &lt;code&gt;Available&lt;/code&gt; and it is ready to serve requests.&lt;/p&gt;
&lt;p&gt;Another case, where &lt;code&gt;minReadySeconds&lt;/code&gt; helps is when using &lt;code&gt;LoadBalancer&lt;/code&gt; &lt;code&gt;Services&lt;/code&gt; with cloud providers. Since &lt;code&gt;minReadySeconds&lt;/code&gt; adds latency after a &lt;code&gt;Pod&lt;/code&gt; is &lt;code&gt;Ready&lt;/code&gt;, it provides buffer time to prevent killing pods in rotation before new pods show up. Imagine a load balancer in unhappy path taking 10-15s to propagate. If you have 2 replicas then, you&#39;d kill the second replica only after the first one is up but in reality, first replica cannot be seen because it is not yet ready to serve requests.&lt;/p&gt;
&lt;p&gt;So, in general, the notion of &lt;code&gt;Availability&lt;/code&gt; in &lt;code&gt;StatefulSets&lt;/code&gt; is pretty useful and this feature helps in solving the above problems. This is a feature that already exists for &lt;code&gt;Deployments&lt;/code&gt; and &lt;code&gt;DaemonSets&lt;/code&gt; and we now have them for &lt;code&gt;StatefulSets&lt;/code&gt; too to give users consistent workload experience.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;The statefulSet controller watches for both &lt;code&gt;StatefulSets&lt;/code&gt; and the &lt;code&gt;Pods&lt;/code&gt; associated with them. When the feature gate associated with this feature is enabled, the statefulSet controller identifies how long a particular &lt;code&gt;Pod&lt;/code&gt; associated with a &lt;code&gt;StatefulSet&lt;/code&gt; has been in the &lt;code&gt;Running&lt;/code&gt; state.&lt;/p&gt;
&lt;p&gt;If this value is greater than or equal to the time specified by the end user in &lt;code&gt;.spec.minReadySeconds&lt;/code&gt; field, the statefulSet controller updates a field called &lt;code&gt;availableReplicas&lt;/code&gt; in the &lt;code&gt;StatefulSet&lt;/code&gt;&#39;s status subresource to include this &lt;code&gt;Pod&lt;/code&gt;. The &lt;code&gt;status.availableReplicas&lt;/code&gt; in &lt;code&gt;StatefulSet&lt;/code&gt;&#39;s status is an integer field which tracks the number of pods that are &lt;code&gt;Available&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-use-it&#34;&gt;How do I use it?&lt;/h2&gt;
&lt;p&gt;You are required to prepare the following things in order to try out the feature:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download and install a kubectl greater than v1.22.0 version&lt;/li&gt;
&lt;li&gt;Switch on the feature gate with the command line flag &lt;code&gt;--feature-gates=StatefulSetMinReadySeconds=true&lt;/code&gt; on &lt;code&gt;kube-apiserver&lt;/code&gt; and &lt;code&gt;kube-controller-manager&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After successfully starting &lt;code&gt;kube-apiserver&lt;/code&gt; and &lt;code&gt;kube-controller-manager&lt;/code&gt;, you will see &lt;code&gt;AvailableReplicas&lt;/code&gt; in the status and &lt;code&gt;minReadySeconds&lt;/code&gt; of spec (with a default value of 0).&lt;/p&gt;
&lt;p&gt;Specify a value for &lt;code&gt;minReadySeconds&lt;/code&gt; for any StatefulSet and you can check if &lt;code&gt;Pods&lt;/code&gt; are available or not by checking &lt;code&gt;AvailableReplicas&lt;/code&gt; field using:
&lt;code&gt;kubectl get statefulset/&amp;lt;name_of_the_statefulset&amp;gt; -o yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read the KEP: &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2599-minreadyseconds-for-statefulsets#readme&#34;&gt;minReadySeconds for StatefulSets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read the documentation: &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds&#34;&gt;Minimum ready seconds&lt;/a&gt; for StatefulSet&lt;/li&gt;
&lt;li&gt;Review the &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/&#34;&gt;API definition&lt;/a&gt; for StatefulSet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;
&lt;p&gt;Please reach out to us in the &lt;a href=&#34;https://kubernetes.slack.com/archives/C18NZM5K9&#34;&gt;#sig-apps&lt;/a&gt; channel on Slack (visit &lt;a href=&#34;https://slack.k8s.io/&#34;&gt;https://slack.k8s.io/&lt;/a&gt; for an invitation if you need one), or on the SIG Apps mailing list: &lt;a href=&#34;mailto:kubernetes-sig-apps@googlegroups.com&#34;&gt;kubernetes-sig-apps@googlegroups.com&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Enable seccomp for all workloads with a new v1.22 alpha feature</title>
      <link>https://kubernetes.io/blog/2021/08/25/seccomp-default/</link>
      <pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/25/seccomp-default/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Sascha Grunert, Red Hat&lt;/p&gt;
&lt;p&gt;This blog post is about a new Kubernetes feature introduced in v1.22, which adds
an additional security layer on top of the existing seccomp support. Seccomp is
a security mechanism for Linux processes to filter system calls (syscalls) based
on a set of defined rules. Applying seccomp profiles to containerized workloads
is one of the key tasks when it comes to enhancing the security of the
application deployment. Developers, site reliability engineers and
infrastructure administrators have to work hand in hand to create, distribute
and maintain the profiles over the applications life-cycle.&lt;/p&gt;
&lt;p&gt;You can use the &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1&#34;&gt;&lt;code&gt;securityContext&lt;/code&gt;&lt;/a&gt; field of Pods and their
containers can be used to adjust security related configurations of the
workload. Kubernetes introduced dedicated &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1&#34;&gt;seccomp related API
fields&lt;/a&gt; in this &lt;code&gt;SecurityContext&lt;/code&gt; with the &lt;a href=&#34;https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#graduated-to-stable&#34;&gt;graduation of seccomp to
General Availability (GA)&lt;/a&gt; in v1.19.0. This enhancement allowed an easier
way to specify if the whole pod or a specific container should run as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Unconfined&lt;/code&gt;: seccomp will not be enabled&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RuntimeDefault&lt;/code&gt;: the container runtimes default profile will be used&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Localhost&lt;/code&gt;: a node local profile will be applied, which is being referenced
by a relative path to the seccomp profile root (&lt;code&gt;&amp;lt;kubelet-root-dir&amp;gt;/seccomp&lt;/code&gt;)
of the kubelet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the graduation of seccomp, nothing has changed from an overall security
perspective, because &lt;code&gt;Unconfined&lt;/code&gt; is still the default. This is totally fine if
you consider this from the upgrade path and backwards compatibility perspective of
Kubernetes releases. But it also means that it is more likely that a workload
runs without seccomp at all, which should be fixed in the long term.&lt;/p&gt;
&lt;h2 id=&#34;seccompdefault-to-the-rescue&#34;&gt;&lt;code&gt;SeccompDefault&lt;/code&gt; to the rescue&lt;/h2&gt;
&lt;p&gt;Kubernetes v1.22.0 introduces a new kubelet &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates&#34;&gt;feature gate&lt;/a&gt;
&lt;code&gt;SeccompDefault&lt;/code&gt;, which has been added in &lt;code&gt;alpha&lt;/code&gt; state as every other new
feature. This means that it is disabled by default and can be enabled manually
for every single Kubernetes node.&lt;/p&gt;
&lt;p&gt;What does the feature do? Well, it just changes the default seccomp profile from
&lt;code&gt;Unconfined&lt;/code&gt; to &lt;code&gt;RuntimeDefault&lt;/code&gt;. If not specified differently in the pod
manifest, then the feature will add a higher set of security constraints by
using the default profile of the container runtime. These profiles may differ
between runtimes like &lt;a href=&#34;https://github.com/cri-o/cri-o/blob/fe30d62/vendor/github.com/containers/common/pkg/seccomp/default_linux.go#L45&#34;&gt;CRI-O&lt;/a&gt; or &lt;a href=&#34;https://github.com/containerd/containerd/blob/e1445df/contrib/seccomp/seccomp_default.go#L51&#34;&gt;containerd&lt;/a&gt;. They also differ for
its used hardware architectures. But generally speaking, those default profiles
allow a common amount of syscalls while blocking the more dangerous ones, which
are unlikely or unsafe to be used in a containerized application.&lt;/p&gt;
&lt;h3 id=&#34;enabling-the-feature&#34;&gt;Enabling the feature&lt;/h3&gt;
&lt;p&gt;Two kubelet configuration changes have to be made to enable the feature:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Enable the feature&lt;/strong&gt; gate by setting the &lt;code&gt;SeccompDefault=true&lt;/code&gt; via the command
line (&lt;code&gt;--feature-gates&lt;/code&gt;) or the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file&#34;&gt;kubelet configuration&lt;/a&gt; file.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Turn on the feature&lt;/strong&gt; by enabling the feature by adding the
&lt;code&gt;--seccomp-default&lt;/code&gt; command line flag or via the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file&#34;&gt;kubelet
configuration&lt;/a&gt; file (&lt;code&gt;seccompDefault: true&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The kubelet will error on startup if only one of the above steps have been done.&lt;/p&gt;
&lt;h3 id=&#34;trying-it-out&#34;&gt;Trying it out&lt;/h3&gt;
&lt;p&gt;If the feature is enabled on a node, then you can create a new workload like
this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;containers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-container&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nginx:1.21&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now it is possible to inspect the used seccomp profile by using
&lt;a href=&#34;https://github.com/kubernetes-sigs/cri-tools&#34;&gt;&lt;code&gt;crictl&lt;/code&gt;&lt;/a&gt; while investigating the containers &lt;a href=&#34;https://github.com/opencontainers/runtime-spec/blob/0c021c1/config-linux.md#seccomp&#34;&gt;runtime
specification&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#b8860b&#34;&gt;CONTAINER_ID&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;sudo crictl ps -q --name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;test-container&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;
sudo crictl inspect &lt;span style=&#34;color:#b8860b&#34;&gt;$CONTAINER_ID&lt;/span&gt; | jq .info.runtimeSpec.linux.seccomp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;{&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;defaultAction&amp;#34;: &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ACT_ERRNO&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;architectures&amp;#34;: &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ARCH_X86_64&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ARCH_X86&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ARCH_X32&amp;#34;&lt;/span&gt;],&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;syscalls&amp;#34;: &lt;/span&gt;[&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;{&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;names&amp;#34;: &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;_llseek&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;_newselect&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;accept&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;…, &amp;#34;write&amp;#34;, &amp;#34;writev&amp;#34;],&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;action&amp;#34;: &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ACT_ALLOW&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;},&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;…&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can see that the lower level container runtime (&lt;a href=&#34;https://github.com/cri-o/cri-o&#34;&gt;CRI-O&lt;/a&gt; and
&lt;a href=&#34;https://github.com/opencontainers/runc&#34;&gt;runc&lt;/a&gt; in our case), successfully applied the default seccomp profile.
This profile denies all syscalls per default, while allowing commonly used ones
like &lt;a href=&#34;https://man7.org/linux/man-pages/man2/accept.2.html&#34;&gt;&lt;code&gt;accept&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://man7.org/linux/man-pages/man2/write.2.html&#34;&gt;&lt;code&gt;write&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please note that the feature will not influence any Kubernetes API for now.
Therefore, it is not possible to retrieve the used seccomp profile via &lt;code&gt;kubectl&lt;/code&gt;
&lt;code&gt;get&lt;/code&gt; or &lt;code&gt;describe&lt;/code&gt; if the &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1&#34;&gt;&lt;code&gt;SeccompProfile&lt;/code&gt;&lt;/a&gt; field is unset within the
&lt;code&gt;SecurityContext&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The feature also works when using multiple containers within a pod, for example
if you create a pod like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;containers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-container-nginx&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nginx:1.21&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;securityContext&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;seccompProfile&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;type&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Unconfined&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-container-redis&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;redis:6.2&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;then you should see that the &lt;code&gt;test-container-nginx&lt;/code&gt; runs without a seccomp profile:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sudo crictl inspect &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;sudo crictl ps -q --name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;test-container-nginx&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt; |
    jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;.info.runtimeSpec.linux.seccomp == null&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#a2f&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Whereas the container &lt;code&gt;test-container-redis&lt;/code&gt; runs with &lt;code&gt;RuntimeDefault&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sudo crictl inspect &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;sudo crictl ps -q --name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;test-container-redis&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt; |
    jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;.info.runtimeSpec.linux.seccomp != null&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#a2f&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The same applies to the pod itself, which also runs with the default profile:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sudo crictl inspectp &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;sudo crictl pods -q --name test-pod&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; |
    jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;.info.runtimeSpec.linux.seccomp != null&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#a2f&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;upgrade-strategy&#34;&gt;Upgrade strategy&lt;/h3&gt;
&lt;p&gt;It is recommended to enable the feature in multiple steps, whereas different
risks and mitigations exist for each one.&lt;/p&gt;
&lt;h4 id=&#34;feature-gate-enabling&#34;&gt;Feature gate enabling&lt;/h4&gt;
&lt;p&gt;Enabling the feature gate at the kubelet level will not turn on the feature, but
will make it possible by using the &lt;code&gt;SeccompDefault&lt;/code&gt; kubelet configuration or the
&lt;code&gt;--seccomp-default&lt;/code&gt; CLI flag. This can be done by an administrator for the whole
cluster or only a set of nodes.&lt;/p&gt;
&lt;h4 id=&#34;testing-the-application&#34;&gt;Testing the Application&lt;/h4&gt;
&lt;p&gt;If you&#39;re trying this within a dedicated test environment, you have to ensure
that the application code does not trigger syscalls blocked by the
&lt;code&gt;RuntimeDefault&lt;/code&gt; profile before enabling the feature on a node. This can be done
by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Recommended&lt;/em&gt;: Analyzing the code (manually or by running the application with
&lt;a href=&#34;https://man7.org/linux/man-pages/man1/strace.1.html&#34;&gt;strace&lt;/a&gt;) for any executed syscalls which may be blocked by the
default profiles. If that&#39;s the case, then you can override the default by
explicitly setting the pod or container to run as &lt;code&gt;Unconfined&lt;/code&gt;. Alternatively,
you can create a custom seccomp profile (see optional step below).
profile based on the default by adding the additional syscalls to the
&lt;code&gt;&amp;quot;action&amp;quot;: &amp;quot;SCMP_ACT_ALLOW&amp;quot;&lt;/code&gt; section.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Recommended&lt;/em&gt;: Manually set the profile to the target workload and use a
rolling upgrade to deploy into production. Rollback the deployment if the
application does not work as intended.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Optional&lt;/em&gt;: Run the application against an end-to-end test suite to trigger
all relevant code paths with &lt;code&gt;RuntimeDefault&lt;/code&gt; enabled. If a test fails, use
the same mitigation as mentioned above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Optional&lt;/em&gt;: Create a custom seccomp profile based on the default and change
its default action from &lt;code&gt;SCMP_ACT_ERRNO&lt;/code&gt; to &lt;code&gt;SCMP_ACT_LOG&lt;/code&gt;. This means that
the seccomp filter for unknown syscalls will have no effect on the application
at all, but the system logs will now indicate which syscalls may be blocked.
This requires at least a Kernel version 4.14 as well as a recent &lt;a href=&#34;https://github.com/opencontainers/runc&#34;&gt;runc&lt;/a&gt;
release. Monitor the application hosts audit logs (defaults to
&lt;code&gt;/var/log/audit/audit.log&lt;/code&gt;) or syslog entries (defaults to &lt;code&gt;/var/log/syslog&lt;/code&gt;)
for syscalls via &lt;code&gt;type=SECCOMP&lt;/code&gt; (for audit) or &lt;code&gt;type=1326&lt;/code&gt; (for syslog).
Compare the syscall ID with those &lt;a href=&#34;https://github.com/torvalds/linux/blob/7bb7f2a/arch/x86/entry/syscalls/syscall_64.tbl&#34;&gt;listed in the Linux Kernel
sources&lt;/a&gt; and add them to the custom profile. Be aware that custom
audit policies may lead into missing syscalls, depending on the configuration
of auditd.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Optional&lt;/em&gt;: Use cluster additions like the &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator&#34;&gt;Security Profiles Operator&lt;/a&gt;
for profiling the application via its &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#record-profiles-from-workloads-with-profilerecordings&#34;&gt;log enrichment&lt;/a&gt; capabilities or
recording a profile by using its &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#using-the-log-enricher&#34;&gt;recording feature&lt;/a&gt;. This makes the
above mentioned manual log investigation obsolete.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;deploying-the-modified-application&#34;&gt;Deploying the modified application&lt;/h4&gt;
&lt;p&gt;Based on the outcome of the application tests, it may be required to change the
application deployment by either specifying &lt;code&gt;Unconfined&lt;/code&gt; or a custom seccomp
profile. This is not the case if the application works as intended with
&lt;code&gt;RuntimeDefault&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;enable-the-kubelet-configuration&#34;&gt;Enable the kubelet configuration&lt;/h4&gt;
&lt;p&gt;If everything went well, then the feature is ready to be enabled by the kubelet
configuration or its corresponding CLI flag. This should be done on a per-node
basis to reduce the overall risk of missing a syscall during the investigations
when running the application tests. If it&#39;s possible to monitor audit logs
within the cluster, then it&#39;s recommended to do this for eventually missed
seccomp events. If the application works as intended then the feature can be
enabled for further nodes within the cluster.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Thank you for reading this blog post! I hope you enjoyed to see how the usage of
seccomp profiles has been evolved in Kubernetes over the past releases as much
as I do. On your own cluster, change the default seccomp profile to
&lt;code&gt;RuntimeDefault&lt;/code&gt; (using this new feature) and see the security benefits, and, of
course, feel free to reach out any time for feedback or questions.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Editor&#39;s note: If you have any questions or feedback about this blog post, feel
free to reach out via the &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-node&#34;&gt;Kubernetes slack in #sig-node&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Alpha in v1.22: Windows HostProcess Containers</title>
      <link>https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/</link>
      <pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Brandon Smith (Microsoft)&lt;/p&gt;
&lt;p&gt;Kubernetes v1.22 introduced a new alpha feature for clusters that
include Windows nodes: HostProcess containers.&lt;/p&gt;
&lt;p&gt;HostProcess containers aim to extend the Windows container model to enable a wider
range of Kubernetes cluster management scenarios. HostProcess containers run
directly on the host and maintain behavior and access similar to that of a regular
process. With HostProcess containers, users can package and distribute management
operations and functionalities that require host access while retaining versioning
and deployment methods provided by containers. This allows Windows containers to
be used for a variety of device plugin, storage, and networking management scenarios
in Kubernetes. With this comes the enablement of host network mode—allowing
HostProcess containers to be created within the host&#39;s network namespace instead of
their own. HostProcess containers can also be built on top of existing Windows server
2019 (or later) base images, managed through the Windows container runtime, and run
as any user that is available on or in the domain of the host machine.&lt;/p&gt;
&lt;p&gt;Linux privileged containers are currently used for a variety of key scenarios in
Kubernetes, including kube-proxy (via kubeadm), storage, and networking scenarios.
Support for these scenarios in Windows previously required workarounds via proxies
or other implementations. Using HostProcess containers, cluster operators no longer
need to log onto and individually configure each Windows node for administrative
tasks and management of Windows services. Operators can now utilize the container
model to deploy management logic to as many clusters as needed with ease.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;Windows HostProcess containers are implemented with Windows &lt;em&gt;Job Objects&lt;/em&gt;, a break from the
previous container model using server silos. Job objects are components of the Windows OS which offer the ability to
manage a group of processes as a group (a.k.a. &lt;em&gt;jobs&lt;/em&gt;) and assign resource constraints to the
group as a whole. Job objects are specific to the Windows OS and are not associated with the Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job/&#34;&gt;Job API&lt;/a&gt;. They have no process or file system isolation,
enabling the privileged payload to view and edit the host file system with the
correct permissions, among other host resources. The init process, and any processes
it launches or that are explicitly launched by the user, are all assigned to the
job object of that container. When the init process exits or is signaled to exit,
all the processes in the job will be signaled to exit, the job handle will be
closed and the storage will be unmounted.&lt;/p&gt;
&lt;p&gt;HostProcess and Linux privileged containers enable similar scenarios but differ
greatly in their implementation (hence the naming difference). HostProcess containers
have their own pod security policies. Those used to configure Linux privileged
containers &lt;strong&gt;do not&lt;/strong&gt; apply. Enabling privileged access to a Windows host is a
fundamentally different process than with Linux so the configuration and
capabilities of each differ significantly. Below is a diagram detailing the
overall architecture of Windows HostProcess containers:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/hostprocess-architecture.png&#34;
         alt=&#34;HostProcess Architecture&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;how-do-i-use-it&#34;&gt;How do I use it?&lt;/h2&gt;
&lt;p&gt;HostProcess containers can be run from within a
&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod&#34;&gt;HostProcess Pod&lt;/a&gt;.
With the feature enabled on Kubernetes version 1.22, a containerd container runtime of
1.5.4 or higher, and the latest version of hcsshim, deploying a pod spec with the
&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/#before-you-begin&#34;&gt;correct HostProcess configuration&lt;/a&gt;
will enable you to run HostProcess containers. To get started with running
Windows containers see the general guidance for &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/windows/&#34;&gt;Windows in Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Work through &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/&#34;&gt;Create a Windows HostProcess Pod&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Read about Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/security/pod-security-standards/&#34;&gt;Pod Security Standards&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Read the enhancement proposal &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-windows/1981-windows-privileged-container-support&#34;&gt;Windows Privileged Containers and Host Networking Mode&lt;/a&gt; (KEP-1981)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;
&lt;p&gt;HostProcess containers are in active development. SIG Windows welcomes suggestions from the community.
Get involved with &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-windows&#34;&gt;SIG Windows&lt;/a&gt;
to contribute!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Memory Manager moves to beta</title>
      <link>https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/</link>
      <pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Artyom Lukianov (Red Hat), Cezary Zukowski (Samsung)&lt;/p&gt;
&lt;p&gt;The blog post explains some of the internals of the &lt;em&gt;Memory manager&lt;/em&gt;, a beta feature
of Kubernetes 1.22. In Kubernetes, the Memory Manager is a
&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/#kubelet&#34;&gt;kubelet&lt;/a&gt; subcomponent.
The memory manage provides guaranteed memory (and hugepages)
allocation for pods in the &lt;code&gt;Guaranteed&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes&#34;&gt;QoS class&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This blog post covers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#Why-do-you-need-it?&#34;&gt;Why do you need it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#How-does-it-work?&#34;&gt;The internal details of how the &lt;strong&gt;MemoryManager&lt;/strong&gt; works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Current-limitations&#34;&gt;Current limitations of the &lt;strong&gt;MemoryManager&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Future-work-for-the-Memory-Manager&#34;&gt;Future work for the &lt;strong&gt;MemoryManager&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;why-do-you-need-it&#34;&gt;Why do you need it?&lt;/h2&gt;
&lt;p&gt;Some Kubernetes workloads run on nodes with
&lt;a href=&#34;https://en.wikipedia.org/wiki/Non-uniform_memory_access&#34;&gt;non-uniform memory access&lt;/a&gt; (NUMA).
Suppose you have NUMA nodes in your cluster. In that case, you&#39;ll know about the potential for extra latency when
compute resources need to access memory on the different NUMA locality.&lt;/p&gt;
&lt;p&gt;To get the best performance and latency for your workload, container CPUs,
peripheral devices, and memory should all be aligned to the same NUMA
locality.
Before Kubernetes v1.22, the kubelet already provided a set of managers to
align CPUs and PCI devices, but you did not have a way to align memory.
The Linux kernel was able to make best-effort attempts to allocate
memory for tasks from the same NUMA node where the container is
executing are placed, but without any guarantee about that placement.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;The memory manager is doing two main things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;provides the topology hint to the Topology Manager&lt;/li&gt;
&lt;li&gt;allocates the memory for containers and updates the state&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The overall sequence of the Memory Manager under the Kubelet&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/MemoryManagerDiagram.svg&#34; alt=&#34;MemoryManagerDiagram&#34; title=&#34;MemoryManagerDiagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;During the Admission phase:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When first handling a new pod, the kubelet calls the TopologyManager&#39;s &lt;code&gt;Admit()&lt;/code&gt; method.&lt;/li&gt;
&lt;li&gt;The Topology Manager is calling &lt;code&gt;GetTopologyHints()&lt;/code&gt; for every hint provider including the Memory Manager.&lt;/li&gt;
&lt;li&gt;The Memory Manager calculates all possible NUMA nodes combinations for every container inside the pod and returns hints to the Topology Manager.&lt;/li&gt;
&lt;li&gt;The Topology Manager calls to &lt;code&gt;Allocate()&lt;/code&gt; for every hint provider including the Memory Manager.&lt;/li&gt;
&lt;li&gt;The Memory Manager allocates the memory under the state according to the hint that the Topology Manager chose.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;During Pod creation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The kubelet calls &lt;code&gt;PreCreateContainer()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For each container, the Memory Manager looks the NUMA nodes where it allocated the
memory for the container and then returns that information to the kubelet.&lt;/li&gt;
&lt;li&gt;The kubelet creates the container, via CRI, using a container specification
that incorporates information from the Memory Manager information.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;let-s-talk-about-the-configuration&#34;&gt;Let&#39;s talk about the configuration&lt;/h3&gt;
&lt;p&gt;By default, the Memory Manager runs with the &lt;code&gt;None&lt;/code&gt; policy, meaning it will just
relax and not do anything. To make use of the Memory Manager, you should set
two command line options for the kubelet:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--memory-manager-policy=Static&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--reserved-memory=&amp;quot;&amp;lt;numaNodeID&amp;gt;:&amp;lt;resourceName&amp;gt;=&amp;lt;quantity&amp;gt;&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The value for &lt;code&gt;--memory-manager-policy&lt;/code&gt; is straightforward: &lt;code&gt;Static&lt;/code&gt;. Deciding what to specify for &lt;code&gt;--reserved-memory&lt;/code&gt; takes more thought. To configure it correctly, you should follow two main rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The amount of reserved memory for the &lt;code&gt;memory&lt;/code&gt; resource must be greater than zero.&lt;/li&gt;
&lt;li&gt;The amount of reserved memory for the resource type must be equal
to &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable&#34;&gt;NodeAllocatable&lt;/a&gt;
(&lt;code&gt;kube-reserved + system-reserved + eviction-hard&lt;/code&gt;) for the resource.
You can read more about memory reservations in &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/&#34;&gt;Reserve Compute Resources for System Daemons&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/ReservedMemory.svg&#34; alt=&#34;Reserved memory&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;current-limitations&#34;&gt;Current limitations&lt;/h2&gt;
&lt;p&gt;The 1.22 release and promotion to beta brings along enhancements and fixes, but the Memory Manager still has several limitations.&lt;/p&gt;
&lt;h3 id=&#34;single-vs-cross-numa-node-allocation&#34;&gt;Single vs Cross NUMA node allocation&lt;/h3&gt;
&lt;p&gt;The NUMA node can not have both single and cross NUMA node allocations. When the container memory is pinned to two or more NUMA nodes, we can not know from which NUMA node the container will consume the memory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/SingleCrossNUMAAllocation.svg&#34; alt=&#34;Single vs Cross NUMA allocation&#34; title=&#34;SingleCrossNUMAAllocation&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;container1&lt;/code&gt; started on the NUMA node 0 and requests &lt;em&gt;5Gi&lt;/em&gt; of the memory but currently is consuming only &lt;em&gt;3Gi&lt;/em&gt; of the memory.&lt;/li&gt;
&lt;li&gt;For container2 the memory request is 10Gi, and no single NUMA node can satisfy it.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;container2&lt;/code&gt; consumes &lt;em&gt;3.5Gi&lt;/em&gt; of the memory from the NUMA node 0, but once the &lt;code&gt;container1&lt;/code&gt; will require more memory, it will not have it, and the kernel will kill one of the containers with the &lt;em&gt;OOM&lt;/em&gt; error.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To prevent such issues, the Memory Manager will fail the admission of the &lt;code&gt;container2&lt;/code&gt; until the machine has two NUMA nodes without a single NUMA node allocation.&lt;/p&gt;
&lt;h3 id=&#34;works-only-for-guaranteed-pods&#34;&gt;Works only for Guaranteed pods&lt;/h3&gt;
&lt;p&gt;The Memory Manager can not guarantee memory allocation for Burstable pods,
also when the Burstable pod has specified equal memory limit and request.&lt;/p&gt;
&lt;p&gt;Let&#39;s assume you have two Burstable pods: &lt;code&gt;pod1&lt;/code&gt; has containers with
equal memory request and limits, and &lt;code&gt;pod2&lt;/code&gt; has containers only with a
memory request set. You want to guarantee memory allocation for the &lt;code&gt;pod1&lt;/code&gt;.
To the Linux kernel, processes in either pod have the same &lt;em&gt;OOM score&lt;/em&gt;,
once the kernel finds that it does not have enough memory, it can kill
processes that belong to pod &lt;code&gt;pod1&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;memory-fragmentation&#34;&gt;Memory fragmentation&lt;/h3&gt;
&lt;p&gt;The sequence of Pods and containers that start and stop can fragment the memory on NUMA nodes.
The alpha implementation of the Memory Manager does not have any mechanism to balance pods and defragment memory back.&lt;/p&gt;
&lt;h2 id=&#34;future-work-for-the-memory-manager&#34;&gt;Future work for the Memory Manager&lt;/h2&gt;
&lt;p&gt;We do not want to stop with the current state of the Memory Manager and are looking to
make improvements, including in the following areas.&lt;/p&gt;
&lt;h3 id=&#34;make-the-memory-manager-allocation-algorithm-smarter&#34;&gt;Make the Memory Manager allocation algorithm smarter&lt;/h3&gt;
&lt;p&gt;The current algorithm ignores distances between NUMA nodes during the
calculation of the allocation. If same-node placement isn&#39;t available, we can still
provide better performance compared to the current implementation, by changing the
Memory Manager to prefer the closest NUMA nodes for cross-node allocation.&lt;/p&gt;
&lt;h3 id=&#34;reduce-the-number-of-admission-errors&#34;&gt;Reduce the number of admission errors&lt;/h3&gt;
&lt;p&gt;The default Kubernetes scheduler is not aware of the node&#39;s NUMA topology, and it can be a reason for many admission errors during the pod start.
We&#39;re hoping to add a KEP (Kubernetes Enhancement Proposal) to cover improvements in this area.
Follow &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2044&#34;&gt;Topology aware scheduler plugin in kube-scheduler&lt;/a&gt; to see how this idea progresses.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With the promotion of the Memory Manager to beta in 1.22, we encourage everyone to give it a try and look forward to any feedback you may have. While there are still several limitations, we have a set of enhancements planned to address them and look forward to providing you with many new features in upcoming releases.
If you have ideas for additional enhancements or a desire for certain features, please let us know. The team is always open to suggestions to enhance and improve the Memory Manager.
We hope you have found this blog informative and helpful! Let us know if you have any questions or comments.&lt;/p&gt;
&lt;p&gt;You can contact us via:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Kubernetes &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-node&#34;&gt;#sig-node &lt;/a&gt;
channel in Slack (visit &lt;a href=&#34;https://slack.k8s.io/&#34;&gt;https://slack.k8s.io/&lt;/a&gt; for an invitation if you need one)&lt;/li&gt;
&lt;li&gt;The SIG Node mailing list, &lt;a href=&#34;https://groups.google.com/g/kubernetes-sig-node&#34;&gt;kubernetes-sig-node@googlegroups.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.22: CSI Windows Support (with CSI Proxy) reaches GA</title>
      <link>https://kubernetes.io/blog/2021/08/09/csi-windows-support-with-csi-proxy-reaches-ga/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/09/csi-windows-support-with-csi-proxy-reaches-ga/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mauricio Poppe (Google), Jing Xu (Google), and Deep Debroy (Apple)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The stable version of CSI Proxy for Windows has been released alongside Kubernetes 1.22.  CSI Proxy enables CSI Drivers running on Windows nodes to perform privileged storage operations.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Container Storage Interface (CSI) for Kubernetes went GA in the Kubernetes 1.13 release. CSI has become the standard for exposing block and file storage to containerized workloads on Container Orchestration systems (COs) like Kubernetes. It enables third-party storage providers to write and deploy plugins without the need to alter the core Kubernetes codebase. Legacy in-tree drivers are deprecated and new storage features are introduced in CSI, therefore it is important to get CSI Drivers to work on Windows.&lt;/p&gt;
&lt;p&gt;A CSI Driver in Kubernetes has two main components: a controller plugin which runs in the control plane and a node plugin which runs on every node.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The controller plugin generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The node plugin, however, requires direct access to the host for making block devices and/or file systems available to the Kubernetes kubelet. Due to the missing capability of running privileged operations from containers on Windows nodes &lt;a href=&#34;https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/&#34;&gt;CSI Proxy was introduced as alpha in Kubernetes 1.18&lt;/a&gt; as a way to enable containers to perform privileged storage operations. This enables containerized CSI Drivers to run on Windows nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-s-csi-proxy-and-how-do-csi-drivers-interact-with-it&#34;&gt;What&#39;s CSI Proxy and how do CSI drivers interact with it?&lt;/h2&gt;
&lt;p&gt;When a workload that uses persistent volumes is scheduled, it&#39;ll go through a sequence of steps defined in the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;CSI Spec&lt;/a&gt;. First, the workload will be scheduled to run on a node. Then the controller component of a CSI Driver will attach the persistent volume to the node. Finally the node component of a CSI Driver will mount the persistent volume on the node.&lt;/p&gt;
&lt;p&gt;The node component of a CSI Driver needs to run on Windows nodes to support Windows workloads. Various privileged operations like scanning of disk devices, mounting of file systems, etc. cannot be done from a containerized application running on Windows nodes yet (&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1981&#34;&gt;Windows HostProcess containers&lt;/a&gt; introduced in Kubernetes 1.22 as alpha enable functionalities that require host access like the operations mentioned before). However, we can perform these operations through a binary (CSI Proxy) that&#39;s pre-installed on the Window nodes. CSI Proxy has a client-server architecture and allows CSI drivers to issue privileged storage operations through a gRPC interface exposed over named pipes created during the startup of CSI Proxy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-08-09-csi-windows-support-with-csi-proxy-reaches-ga/csi-proxy.png&#34; alt=&#34;CSI Proxy Architecture&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;csi-proxy-reaches-ga&#34;&gt;CSI Proxy reaches GA&lt;/h2&gt;
&lt;p&gt;The CSI Proxy development team has worked closely with storage vendors, many of whom started integrating CSI Proxy into their CSI Drivers and provided feedback as early as CSI Proxy design proposal. This cooperation uncovered use cases where additional APIs were needed, found bugs, and identified areas for documentation improvement.&lt;/p&gt;
&lt;p&gt;The CSI Proxy design &lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/2737&#34;&gt;KEP&lt;/a&gt; has been updated to reflect the current CSI Proxy architecture. Additional &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/DEVELOPMENT.md&#34;&gt;development documentation&lt;/a&gt; is included for contributors interested in helping with new features or bug fixes.&lt;/p&gt;
&lt;p&gt;Before we reached GA we wanted to make sure that our API is simple and consistent. We went through an extensive API review of the v1beta API groups where we made sure that the CSI Proxy API methods and messages are consistent with the naming conventions defined in the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;CSI Spec&lt;/a&gt;. As part of this effort we&#39;re graduating the &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/disk_v1.md&#34;&gt;Disk&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/filesystem_v1.md&#34;&gt;Filesystem&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/smb_v1.md&#34;&gt;SMB&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/volume_v1.md&#34;&gt;Volume&lt;/a&gt; API groups to v1.&lt;/p&gt;
&lt;p&gt;Additional Windows system APIs to get information from the Windows nodes and support to mount iSCSI targets in Windows nodes, are available as alpha APIs in the &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/client/api/system/v1alpha1&#34;&gt;System API&lt;/a&gt; and the &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/client/api/iscsi/v1alpha2&#34;&gt;iSCSI API&lt;/a&gt;. These APIs will continue to be improved before we graduate them to v1.&lt;/p&gt;
&lt;p&gt;CSI Proxy v1 is compatible with all the previous v1betaX releases. The GA &lt;code&gt;csi-proxy.exe&lt;/code&gt; binary can handle requests from v1betaX clients thanks to the autogenerated conversion layer that transforms any versioned client request to a version-agnostic request that the server can process. Several &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/integrationtests&#34;&gt;integration tests&lt;/a&gt; were added for all the API versions of the API groups that are graduating to v1 to ensure that CSI Proxy is backwards compatible.&lt;/p&gt;
&lt;p&gt;Version drift between CSI Proxy and the CSI Drivers that interact with it was also carefully considered. A &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy/pull/124&#34;&gt;connection fallback mechanism&lt;/a&gt; has been provided for CSI Drivers to handle multiple versions of CSI Proxy for a smooth upgrade to v1. This allows CSI Drivers, like the GCE PD CSI Driver, &lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/pull/738&#34;&gt;to recognize which version of the CSI Proxy binary is running&lt;/a&gt; and handle multiple versions of the CSI Proxy binary deployed on the node.&lt;/p&gt;
&lt;p&gt;CSI Proxy v1 is already being used by many CSI Drivers, including the &lt;a href=&#34;https://github.com/kubernetes-sigs/aws-ebs-csi-driver/pull/966&#34;&gt;AWS EBS CSI Driver&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes-sigs/azuredisk-csi-driver/pull/919&#34;&gt;Azure Disk CSI Driver&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/pull/738&#34;&gt;GCE PD CSI Driver&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-smb/pull/319&#34;&gt;SMB CSI Driver&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;future-plans&#34;&gt;Future plans&lt;/h2&gt;
&lt;p&gt;We&#39;re very excited for the future of CSI Proxy. With the upcoming &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1981&#34;&gt;Windows HostProcess containers&lt;/a&gt;, we are considering converting the CSI Proxy in to a library consumed by CSI Drivers in addition to the current client/server design. This will allow us to iterate faster on new features because the &lt;code&gt;csi-proxy.exe&lt;/code&gt; binary will no longer be needed.&lt;/p&gt;
&lt;h2 id=&#34;how-to-get-involved&#34;&gt;How to get involved?&lt;/h2&gt;
&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. Those interested in getting involved with the design and development of CSI Proxy, or any part of the Kubernetes Storage system, may join the Kubernetes Storage Special Interest Group (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;
&lt;p&gt;For those interested in more details about CSI support in Windows please reach out in the &lt;a href=&#34;https://kubernetes.slack.com/messages/csi-windows&#34;&gt;#csi-windows&lt;/a&gt; Kubernetes slack channel.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;CSI-Proxy received many contributions from members of the Kubernetes community. We thank all of the people that contributed to CSI Proxy with design reviews, bug reports, bug fixes, and for their continuous support in reaching this milestone:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/andyzhangx&#34;&gt;Andy Zhang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jmpfar&#34;&gt;Dan Ilan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ddebroy&#34;&gt;Deep Debroy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/humblec&#34;&gt;Humble Devassy Chirammal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jingxu97&#34;&gt;Jing Xu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/wk8&#34;&gt;Jean Rougé&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/liggitt&#34;&gt;Jordan Liggitt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ksubrmnn&#34;&gt;Kalya Subramanian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kkmsft&#34;&gt;Krishnakumar R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/manueltellez&#34;&gt;Manuel Tellez&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/marosset&#34;&gt;Mark Rossetti&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mauriciopoppe&#34;&gt;Mauricio Poppe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/wongma7&#34;&gt;Matthew Wong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/msau42&#34;&gt;Michelle Au&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/PatrickLang&#34;&gt;Patrick Lang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/saad-ali&#34;&gt;Saad Ali&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yujuhong&#34;&gt;Yuju Hong&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: New in Kubernetes v1.22: alpha support for using swap memory</title>
      <link>https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Elana Hashman (Red Hat)&lt;/p&gt;
&lt;p&gt;The 1.22 release introduced alpha support for configuring swap memory usage for
Kubernetes workloads on a per-node basis.&lt;/p&gt;
&lt;p&gt;In prior releases, Kubernetes did not support the use of swap memory on Linux,
as it is difficult to provide guarantees and account for pod memory utilization
when swap is involved. As part of Kubernetes&#39; earlier design, swap support was
considered out of scope, and a kubelet would by default fail to start if swap
was detected on a node.&lt;/p&gt;
&lt;p&gt;However, there are a number of &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/9d127347773ad19894ca488ee04f1cd3af5774fc/keps/sig-node/2400-node-swap/README.md#user-stories&#34;&gt;use cases&lt;/a&gt;
that would benefit from Kubernetes nodes supporting swap, including improved
node stability, better support for applications with high memory overhead but
smaller working sets, the use of memory-constrained devices, and memory
flexibility.&lt;/p&gt;
&lt;p&gt;Hence, over the past two releases, &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node#readme&#34;&gt;SIG Node&lt;/a&gt; has
been working to gather appropriate use cases and feedback, and propose a design
for adding swap support to nodes in a controlled, predictable manner so that
Kubernetes users can perform testing and provide data to continue building
cluster capabilities on top of swap. The alpha graduation of swap memory
support for nodes is our first milestone towards this goal!&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;There are a number of possible ways that one could envision swap use on a node.
To keep the scope manageable for this initial implementation, when swap is
already provisioned and available on a node, &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/9d127347773ad19894ca488ee04f1cd3af5774fc/keps/sig-node/2400-node-swap/README.md#proposal&#34;&gt;we have proposed&lt;/a&gt;
the kubelet should be able to be configured such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It can start with swap on.&lt;/li&gt;
&lt;li&gt;It will direct the Container Runtime Interface to allocate zero swap memory
to Kubernetes workloads by default.&lt;/li&gt;
&lt;li&gt;You can configure the kubelet to specify swap utilization for the entire
node.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Swap configuration on a node is exposed to a cluster admin via the
&lt;a href=&#34;https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/&#34;&gt;&lt;code&gt;memorySwap&lt;/code&gt; in the KubeletConfiguration&lt;/a&gt;.
As a cluster administrator, you can specify the node&#39;s behaviour in the
presence of swap memory by setting &lt;code&gt;memorySwap.swapBehavior&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is possible through the addition of a &lt;code&gt;memory_swap_limit_in_bytes&lt;/code&gt; field
to the container runtime interface (CRI). The kubelet&#39;s config will control how
much swap memory the kubelet instructs the container runtime to allocate to
each container via the CRI. The container runtime will then write the swap
settings to the container level cgroup.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-use-it&#34;&gt;How do I use it?&lt;/h2&gt;
&lt;p&gt;On a node where swap memory is already provisioned, Kubernetes use of swap on a
node can be enabled by enabling the &lt;code&gt;NodeSwap&lt;/code&gt; feature gate on the kubelet, and
disabling the &lt;code&gt;failSwapOn&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration&#34;&gt;configuration setting&lt;/a&gt;
or the &lt;code&gt;--fail-swap-on&lt;/code&gt; command line flag.&lt;/p&gt;
&lt;p&gt;You can also optionally configure &lt;code&gt;memorySwap.swapBehavior&lt;/code&gt; in order to
specify how a node will use swap memory. For example,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;memorySwap&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;swapBehavior&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;LimitedSwap&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The available configuration options for &lt;code&gt;swapBehavior&lt;/code&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LimitedSwap&lt;/code&gt; (default): Kubernetes workloads are limited in how much swap
they can use. Workloads on the node not managed by Kubernetes can still swap.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UnlimitedSwap&lt;/code&gt;: Kubernetes workloads can use as much swap memory as they
request, up to the system limit.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If configuration for &lt;code&gt;memorySwap&lt;/code&gt; is not specified and the feature gate is
enabled, by default the kubelet will apply the same behaviour as the
&lt;code&gt;LimitedSwap&lt;/code&gt; setting.&lt;/p&gt;
&lt;p&gt;The behaviour of the &lt;code&gt;LimitedSwap&lt;/code&gt; setting depends if the node is running with
v1 or v2 of control groups (also known as &amp;quot;cgroups&amp;quot;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cgroups v1:&lt;/strong&gt; Kubernetes workloads can use any combination of memory and
swap, up to the pod&#39;s memory limit, if set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cgroups v2:&lt;/strong&gt; Kubernetes workloads cannot use swap memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;caveats&#34;&gt;Caveats&lt;/h3&gt;
&lt;p&gt;Having swap available on a system reduces predictability. Swap&#39;s performance is
worse than regular memory, sometimes by many orders of magnitude, which can
cause unexpected performance regressions. Furthermore, swap changes a system&#39;s
behaviour under memory pressure, and applications cannot directly control what
portions of their memory usage are swapped out. Since enabling swap permits
greater memory usage for workloads in Kubernetes that cannot be predictably
accounted for, it also increases the risk of noisy neighbours and unexpected
packing configurations, as the scheduler cannot account for swap memory usage.&lt;/p&gt;
&lt;p&gt;The performance of a node with swap memory enabled depends on the underlying
physical storage. When swap memory is in use, performance will be significantly
worse in an I/O operations per second (IOPS) constrained environment, such as a
cloud VM with I/O throttling, when compared to faster storage mediums like
solid-state drives or NVMe.&lt;/p&gt;
&lt;p&gt;Hence, we do not recommend the use of swap for certain performance-constrained
workloads or environments. Cluster administrators and developers should
benchmark their nodes and applications before using swap in production
scenarios, and &lt;a href=&#34;#how-do-i-get-involved&#34;&gt;we need your help&lt;/a&gt; with that!&lt;/p&gt;
&lt;h2 id=&#34;looking-ahead&#34;&gt;Looking ahead&lt;/h2&gt;
&lt;p&gt;The Kubernetes 1.22 release introduces alpha support for swap memory on nodes,
and we will continue to work towards beta graduation in the 1.23 release. This
will include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adding support for controlling swap consumption at the Pod level via cgroups.
&lt;ul&gt;
&lt;li&gt;This will include the ability to set a system-reserved quantity of swap
from what kubelet detects on the host.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Determining a set of metrics for node QoS in order to evaluate the
performance and stability of nodes with and without swap enabled.&lt;/li&gt;
&lt;li&gt;Collecting feedback from test user cases.
&lt;ul&gt;
&lt;li&gt;We will consider introducing new configuration modes for swap, such as a
node-wide swap limit for workloads.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;
&lt;p&gt;You can review the current &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory&#34;&gt;documentation&lt;/a&gt;
on the Kubernetes website.&lt;/p&gt;
&lt;p&gt;For more information, and to assist with testing and provide feedback, please
see &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2400&#34;&gt;KEP-2400&lt;/a&gt; and its
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md&#34;&gt;design proposal&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;
&lt;p&gt;Your feedback is always welcome! SIG Node &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node#meetings&#34;&gt;meets regularly&lt;/a&gt;
and &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node#contact&#34;&gt;can be reached&lt;/a&gt;
via &lt;a href=&#34;https://slack.k8s.io/&#34;&gt;Slack&lt;/a&gt; (channel &lt;strong&gt;#sig-node&lt;/strong&gt;), or the SIG&#39;s
&lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-node&#34;&gt;mailing list&lt;/a&gt;.
Feel free to reach out to me, Elana Hashman (&lt;strong&gt;@ehashman&lt;/strong&gt; on Slack and GitHub)
if you&#39;d like to help.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.22: Server Side Apply moves to GA</title>
      <link>https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jeffrey Ying, Google &amp;amp; Joe Betz, Google&lt;/p&gt;
&lt;p&gt;Server-side Apply (SSA) has been promoted to GA in the Kubernetes v1.22 release. The GA milestone means you can depend on the feature and its API, without fear of future backwards-incompatible changes. GA features are protected by the Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34;&gt;deprecation policy&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-is-server-side-apply&#34;&gt;What is Server-side Apply?&lt;/h2&gt;
&lt;p&gt;Server-side Apply helps users and controllers manage their resources through declarative configurations. Server-side Apply replaces the client side apply feature implemented by “kubectl apply” with a server-side implementation, permitting use by tools/clients other than kubectl. Server-side Apply is a new merging algorithm, as well as tracking of field ownership, running on the Kubernetes api-server. Server-side Apply enables new features like conflict detection, so the system knows when two actors are trying to edit the same field. Refer to the &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/server-side-apply/&#34;&gt;Server-side Apply Documentation&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/&#34;&gt;Beta 2 release announcement&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2 id=&#34;what-s-new-since-beta&#34;&gt;What’s new since Beta?&lt;/h2&gt;
&lt;p&gt;Since the &lt;a href=&#34;https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/&#34;&gt;Beta 2 release&lt;/a&gt; subresources support has been added, and both client-go and Kubebuilder have added comprehensive support for Server-side Apply. This completes the Server-side Apply functionality required to make controller development practical.&lt;/p&gt;
&lt;h3 id=&#34;support-for-subresources&#34;&gt;Support for subresources&lt;/h3&gt;
&lt;p&gt;Server-side Apply now fully supports subresources like &lt;code&gt;status&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt;. This is particularly important for &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/controller/&#34;&gt;controllers&lt;/a&gt;, which are often responsible for writing to subresources.&lt;/p&gt;
&lt;h2 id=&#34;server-side-apply-support-in-client-go&#34;&gt;Server-side Apply support in client-go&lt;/h2&gt;
&lt;p&gt;Previously, Server-side Apply could only be called from the client-go typed client using the &lt;code&gt;Patch&lt;/code&gt; function, with &lt;code&gt;PatchType&lt;/code&gt; set to &lt;code&gt;ApplyPatchType&lt;/code&gt;.  Now, &lt;code&gt;Apply&lt;/code&gt; functions are included in the client to allow for a more direct and typesafe way of calling Server-side Apply. Each &lt;code&gt;Apply&lt;/code&gt; function takes an &amp;quot;apply configuration&amp;quot; type as an argument, which is a structured representation of an Apply request. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; (
         &lt;span style=&#34;color:#666&#34;&gt;...&lt;/span&gt;
         v1ac &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s.io/client-go/applyconfigurations/autoscaling/v1&amp;#34;&lt;/span&gt;
)

hpaApplyConfig &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; v1ac.&lt;span style=&#34;color:#00a000&#34;&gt;HorizontalPodAutoscaler&lt;/span&gt;(autoscalerName, ns).
         &lt;span style=&#34;color:#00a000&#34;&gt;WithSpec&lt;/span&gt;(v1ac.&lt;span style=&#34;color:#00a000&#34;&gt;HorizontalPodAutoscalerSpec&lt;/span&gt;().
                  &lt;span style=&#34;color:#00a000&#34;&gt;WithMinReplicas&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;)
         )

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; hpav1client.&lt;span style=&#34;color:#00a000&#34;&gt;Apply&lt;/span&gt;(ctx, hpaApplyConfig, metav1.ApplyOptions{FieldManager: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;mycontroller&amp;#34;&lt;/span&gt;, Force: &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note in this example that &lt;code&gt;HorizontalPodAutoscaler&lt;/code&gt; is imported from an &amp;quot;applyconfigurations&amp;quot; package. Each &amp;quot;apply configuration&amp;quot; type represents the same Kubernetes object kind as the corresponding go struct, but where all fields are pointers to make them optional, allowing apply requests to be accurately represented. For example, when the apply configuration in the above example is marshalled to YAML, it produces:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;autoscaling/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HorizontalPodAutoscaler&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myHPA&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myNamespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;minReplicas&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To understand why this is needed, the above YAML cannot be produced by the v1.HorizontalPodAutoscaler go struct. Take for example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;hpa &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; v1.HorizontalPodAutoscaler{
         TypeMeta: metav1.TypeMeta{
                  APIVersion: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;autoscaling/v1&amp;#34;&lt;/span&gt;,
                  Kind:       &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;HorizontalPodAutoscaler&amp;#34;&lt;/span&gt;,
         },
         ObjectMeta: ObjectMeta{
                  Namespace: ns,
                  Name:      autoscalerName,
         },
         Spec: v1.HorizontalPodAutoscalerSpec{
                  MinReplicas: pointer.&lt;span style=&#34;color:#00a000&#34;&gt;Int32Ptr&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;),
         },
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above code attempts to declare the same apply configuration as shown in the previous examples, but when marshalled to YAML, produces:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HorizontalPodAutoscaler&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;autoscaling/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myHPA&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myNamespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;creationTimestamp&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;null&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;scaleTargetRef&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;minReplicas&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;maxReplicas&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Which, among other things, contains &lt;code&gt;spec.maxReplicas&lt;/code&gt; set to &lt;code&gt;0&lt;/code&gt;. This is almost certainly not what the caller intended (the intended apply configuration says nothing about the &lt;code&gt;maxReplicas&lt;/code&gt; field), and could have serious consequences on a production system: it directs the autoscaler to downscale to zero pods. The problem here originates from the fact that the go structs contain required fields that are zero valued if not set explicitly. The go structs work as intended for create and update operations, but are fundamentally incompatible with apply, which is why we have introduced the generated &amp;quot;apply configuration&amp;quot; types.&lt;/p&gt;
&lt;p&gt;The &amp;quot;apply configurations&amp;quot; also have convenience &lt;code&gt;With&amp;lt;FieldName&amp;gt;&lt;/code&gt; functions that make it easier to build apply requests. This allows developers to set fields without having to deal with the fact that all the fields in the &amp;quot;apply configuration&amp;quot; types are pointers, and are inconvenient to set using go. For example &lt;code&gt;MinReplicas: &amp;amp;0&lt;/code&gt; is not legal go code, so without the &lt;code&gt;With&lt;/code&gt; functions, developers would work around this problem by using a library, e.g. &lt;code&gt;MinReplicas: pointer.Int32Ptr(0)&lt;/code&gt;, but string enumerations like &lt;code&gt;corev1.Protocol&lt;/code&gt; are still a problem since they cannot be supported by a general purpose library. In addition to the convenience, the &lt;code&gt;With&lt;/code&gt; functions also isolate developers from the underlying representation, which makes it safer for the underlying representation to be changed to support additional features in the future.&lt;/p&gt;
&lt;h2 id=&#34;using-server-side-apply-in-a-controller&#34;&gt;Using Server-side Apply in a controller&lt;/h2&gt;
&lt;p&gt;You can use the new support for Server-side Apply no matter how you implemented your controller. However, the new client-go support makes it easier to use Server-side Apply in controllers.&lt;/p&gt;
&lt;p&gt;When authoring new controllers to use Server-side Apply, a good approach is to have the controller recreate the apply configuration for an object each time it reconciles that object.  This ensures that the controller fully reconciles all the fields that it is responsible for. Controllers typically should unconditionally set all the fields they own by setting &lt;code&gt;Force: true&lt;/code&gt; in the &lt;code&gt;ApplyOptions&lt;/code&gt;. Controllers must also provide a &lt;code&gt;FieldManager&lt;/code&gt; name that is unique to the reconciliation loop that apply is called from.&lt;/p&gt;
&lt;p&gt;When upgrading existing controllers to use Server-side Apply the same approach often works well--migrate the controllers to recreate the apply configuration each time it reconciles any object. Unfortunately, the controller might have multiple code paths that update different parts of an object depending on various conditions. Migrating a controller like this to Server-side Apply can be risky because if the controller forgets to include any fields in an apply configuration that is included in a previous apply request, a field can be accidently deleted. To ease this type of migration, client-go apply support provides a way to replace any controller reconciliation code that performs a &amp;quot;read/modify-in-place/update&amp;quot; (or patch) workflow with a &amp;quot;extract/modify-in-place/apply&amp;quot; workflow. Here&#39;s an example of the new workflow:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;fieldMgr &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;my-field-manager&amp;#34;&lt;/span&gt;
deploymentClient &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; clientset.&lt;span style=&#34;color:#00a000&#34;&gt;AppsV1&lt;/span&gt;().&lt;span style=&#34;color:#00a000&#34;&gt;Deployments&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// read, could also be read from a shared informer
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;deployment, err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; deploymentClient.&lt;span style=&#34;color:#00a000&#34;&gt;Get&lt;/span&gt;(ctx, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;example-deployment&amp;#34;&lt;/span&gt;, metav1.GetOptions{})
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
  &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// handle error
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;}

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// extract
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;deploymentApplyConfig, err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; appsv1ac.&lt;span style=&#34;color:#00a000&#34;&gt;ExtractDeployment&lt;/span&gt;(deployment, fieldMgr)
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
  &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// handle error
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;}

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// modify-in-place
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;deploymentApplyConfig.Spec.Template.Spec.&lt;span style=&#34;color:#00a000&#34;&gt;WithContainers&lt;/span&gt;(corev1ac.&lt;span style=&#34;color:#00a000&#34;&gt;Container&lt;/span&gt;().
	&lt;span style=&#34;color:#00a000&#34;&gt;WithName&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;modify-slice&amp;#34;&lt;/span&gt;).
	&lt;span style=&#34;color:#00a000&#34;&gt;WithImage&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;nginx:1.14.2&amp;#34;&lt;/span&gt;),
)

&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// apply
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;applied, err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; deploymentClient.&lt;span style=&#34;color:#00a000&#34;&gt;Apply&lt;/span&gt;(ctx, extractedDeployment, metav1.ApplyOptions{FieldManager: fieldMgr})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For developers using Custom Resource Definitions (CRDs), the Kubebuilder apply support will provide the same capabilities. Documentation will be included in the Kubebuilder book when available.&lt;/p&gt;
&lt;h2 id=&#34;server-side-apply-and-customresourcedefinitions&#34;&gt;Server-side Apply and CustomResourceDefinitions&lt;/h2&gt;
&lt;p&gt;It is strongly recommended that all &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;Custom Resource Definitions&lt;/a&gt; (CRDs) have a schema. CRDs without a schema are treated as unstructured data by Server-side Apply. Keys are treated as fields in a struct and lists are assumed to be atomic.&lt;/p&gt;
&lt;p&gt;CRDs that specify a schema are able to specify additional annotations in the schema. Please refer to the documentation on the full list of available annotations.&lt;/p&gt;
&lt;p&gt;New annotations since beta:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defaulting:&lt;/strong&gt; Values for fields that appliers do not express explicit interest in should be defaulted. This prevents an applier from unintentionally owning a defaulted field that might cause conflicts with other appliers. If unspecified, the default value is nil or the nil equivalent for the corresponding type.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usage: see the &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting&#34;&gt;CRD Defaulting&lt;/a&gt; documentation for more details.&lt;/li&gt;
&lt;li&gt;Golang: &lt;code&gt;+default=&amp;lt;value&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;OpenAPI extension: &lt;code&gt;default: &amp;lt;value&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Atomic for maps and structs:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maps:&lt;/strong&gt; By default maps are granular. A different manager is able to manage each map entry. They can also be configured to be atomic such that a single manager owns the entire map.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usage: Refer to &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/server-side-apply/#merge-strategy&#34;&gt;Merge Strategy&lt;/a&gt; for a more detailed overview&lt;/li&gt;
&lt;li&gt;Golang: &lt;code&gt;+mapType=granular/atomic&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;OpenAPI extension: &lt;code&gt;x-kubernetes-map-type: granular/atomic&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Structs:&lt;/strong&gt; By default structs are granular and a separate applier may own each field. For certain kinds of structs, atomicity may be desired. This is most commonly seen in small coordinate-like structs such as Field/Object/Namespace Selectors, Object References, RGB values, Endpoints (Protocol/Port pairs), etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usage: Refer to &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/server-side-apply/#merge-strategy&#34;&gt;Merge Strategy&lt;/a&gt; for a more detailed overview&lt;/li&gt;
&lt;li&gt;Golang: &lt;code&gt;+structType=granular/atomic&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;OpenAPI extension: &lt;code&gt;x-kubernetes-map-type:atomic/granular&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What&#39;s Next?&lt;/h2&gt;
&lt;p&gt;After Server Side Apply, the next focus for the API Expression working-group is around improving the expressiveness and size of the published Kubernetes API schema. To see the full list of items we are working on, please join our working group and refer to the work items document.&lt;/p&gt;
&lt;h2 id=&#34;how-to-get-involved&#34;&gt;How to get involved?&lt;/h2&gt;
&lt;p&gt;The working-group for apply is &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-api-expression&#34;&gt;wg-api-expression&lt;/a&gt;. It is available on slack &lt;a href=&#34;https://kubernetes.slack.com/archives/C0123CNN8F3&#34;&gt;#wg-api-expression&lt;/a&gt;, through the &lt;a href=&#34;https://groups.google.com/g/kubernetes-wg-api-expression&#34;&gt;mailing list&lt;/a&gt; and we also meet every other Tuesday at 9.30 PT on Zoom.&lt;/p&gt;
&lt;p&gt;We would also like to use the opportunity to thank the hard work of all the contributors involved in making this promotion to GA possible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Andrea Nodari&lt;/li&gt;
&lt;li&gt;Antoine Pelisse&lt;/li&gt;
&lt;li&gt;Daniel Smith&lt;/li&gt;
&lt;li&gt;Jeffrey Ying&lt;/li&gt;
&lt;li&gt;Jenny Buckley&lt;/li&gt;
&lt;li&gt;Joe Betz&lt;/li&gt;
&lt;li&gt;Julian Modesto&lt;/li&gt;
&lt;li&gt;Kevin Delgado&lt;/li&gt;
&lt;li&gt;Kevin Wiesmüller&lt;/li&gt;
&lt;li&gt;Maria Ntalla&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.22: Reaching New Peaks</title>
      <link>https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.22/release-team.md&#34;&gt;Kubernetes 1.22 Release Team&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We’re pleased to announce the release of Kubernetes 1.22, the second release of 2021!&lt;/p&gt;
&lt;p&gt;This release consists of 53 enhancements: 13 enhancements have graduated to stable, 24 enhancements are moving to beta, and 16 enhancements are entering alpha. Also, three features have been deprecated.&lt;/p&gt;
&lt;p&gt;In April of this year, the Kubernetes release cadence was officially changed from four to three releases yearly. This is the first longer-cycle release related to that change. As the Kubernetes project matures, the number of enhancements per cycle grows. This means more work, from version to version, for the contributor community and Release Engineering team, and it can put pressure on the end-user community to stay up-to-date with releases containing increasingly more features.&lt;/p&gt;
&lt;p&gt;Changing the release cadence from four to three releases yearly balances many aspects of the project, both in how contributions and releases are managed, and also in the community&#39;s ability to plan for upgrades and stay up to date.&lt;/p&gt;
&lt;p&gt;You can read more in the official blog post &lt;a href=&#34;https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/&#34;&gt;Kubernetes Release Cadence Change: Here’s What You Need To Know&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;major-themes&#34;&gt;Major Themes&lt;/h2&gt;
&lt;h3 id=&#34;server-side-apply-graduates-to-ga&#34;&gt;Server-side Apply graduates to GA&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/server-side-apply/&#34;&gt;Server-side Apply&lt;/a&gt; is a new field ownership and object merge algorithm running on the Kubernetes API server. Server-side Apply helps users and controllers manage their resources via declarative configurations. It allows them to create and/or modify their objects declaratively, simply by sending their fully specified intent. After being in beta for a couple releases, Server-side Apply is now generally available.&lt;/p&gt;
&lt;h3 id=&#34;external-credential-providers-now-stable&#34;&gt;External credential providers now stable&lt;/h3&gt;
&lt;p&gt;Support for Kubernetes client &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins&#34;&gt;credential plugins&lt;/a&gt; has been in beta since 1.11, and with the release of Kubernetes 1.22 now graduates to stable. The GA feature set includes improved support for plugins that provide interactive login flows, as well as a number of bug fixes. Aspiring plugin authors can look at &lt;a href=&#34;https://github.com/ankeesler/sample-exec-plugin&#34;&gt;sample-exec-plugin&lt;/a&gt; to get started.&lt;/p&gt;
&lt;h3 id=&#34;etcd-moves-to-3-5-0&#34;&gt;etcd moves to 3.5.0&lt;/h3&gt;
&lt;p&gt;Kubernetes&#39; default backend storage, etcd, has a new release: 3.5.0. The new release comes with improvements to the security, performance, monitoring, and developer experience. There are numerous bug fixes and some critical new features like the migration to structured logging and built-in log rotation. The release comes with a detailed future roadmap to implement a solution to traffic overload. You can read a full and detailed list of changes in the &lt;a href=&#34;https://etcd.io/blog/2021/announcing-etcd-3.5/&#34;&gt;3.5.0 release announcement&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;quality-of-service-for-memory-resources&#34;&gt;Quality of Service for memory resources&lt;/h3&gt;
&lt;p&gt;Originally, Kubernetes used the v1 cgroups API. With that design, the QoS class for a &lt;code&gt;Pod&lt;/code&gt; only applied to CPU resources (such as &lt;code&gt;cpu_shares&lt;/code&gt;). As an alpha feature, Kubernetes v1.22 can now use the cgroups v2 API to control memory allocation and isolation. This feature is designed to improve workload and node availability when there is contention for memory resources, and to improve the predictability of container lifecycle.&lt;/p&gt;
&lt;h3 id=&#34;node-system-swap-support&#34;&gt;Node system swap support&lt;/h3&gt;
&lt;p&gt;Every system administrator or Kubernetes user has been in the same boat regarding setting up and using Kubernetes: disable swap space. With the release of Kubernetes 1.22, alpha support is available to run nodes with swap memory. This change lets administrators opt in to configuring swap on Linux nodes, treating a portion of block storage as additional virtual memory.&lt;/p&gt;
&lt;h3 id=&#34;windows-enhancements-and-capabilities&#34;&gt;Windows enhancements and capabilities&lt;/h3&gt;
&lt;p&gt;Continuing to support the growing developer community, SIG Windows has released their &lt;a href=&#34;https://github.com/kubernetes-sigs/sig-windows-dev-tools/&#34;&gt;Development Environment&lt;/a&gt;. These new tools support multiple CNI providers and can run on multiple platforms. There is also a new way to run bleeding-edge Windows features from scratch by compiling the Windows kubelet and kube-proxy, then using them along with daily builds of other Kubernetes components.&lt;/p&gt;
&lt;p&gt;CSI support for Windows nodes moves to GA in the 1.22 release. In Kubernetes v1.22, Windows privileged containers are an alpha feature. To allow using CSI storage on Windows nodes, &lt;a href=&#34;https://github.com/kubernetes-csi/csi-proxy&#34;&gt;CSIProxy&lt;/a&gt; enables CSI node plugins to be deployed as unprivileged pods, using the proxy to perform privileged storage operations on the node.&lt;/p&gt;
&lt;h3 id=&#34;default-profiles-for-seccomp&#34;&gt;Default profiles for seccomp&lt;/h3&gt;
&lt;p&gt;An alpha feature for default seccomp profiles has been added to the kubelet, along with a new command line flag and configuration. When in use, this new feature provides cluster-wide seccomp defaults, using the &lt;code&gt;RuntimeDefault&lt;/code&gt; seccomp profile rather than &lt;code&gt;Unconfined&lt;/code&gt; by default. This enhances the default security of the Kubernetes Deployment. Security administrators will now sleep better knowing that workloads are more secure by default. To learn more about the feature, please refer to the official &lt;a href=&#34;https://kubernetes.io/docs/tutorials/clusters/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads&#34;&gt;seccomp tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;more-secure-control-plane-with-kubeadm&#34;&gt;More secure control plane with kubeadm&lt;/h3&gt;
&lt;p&gt;A new alpha feature allows running the &lt;code&gt;kubeadm&lt;/code&gt; control plane components as non-root users. This is a long requested security measure in &lt;code&gt;kubeadm&lt;/code&gt;. To try it you must enable the &lt;code&gt;kubeadm&lt;/code&gt; specific RootlessControlPlane feature gate. When you deploy a cluster using this alpha feature, your control plane runs with lower privileges.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;kubeadm&lt;/code&gt;, Kubernetes 1.22 also brings a new &lt;a href=&#34;https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/&#34;&gt;v1beta3 configuration API&lt;/a&gt;. This iteration adds some long requested features and deprecates some existing ones. The v1beta3 version is now the preferred API version; the v1beta2 API also remains available and is not yet deprecated.&lt;/p&gt;
&lt;h2 id=&#34;major-changes&#34;&gt;Major Changes&lt;/h2&gt;
&lt;h3 id=&#34;removal-of-several-deprecated-beta-apis&#34;&gt;Removal of several deprecated beta APIs&lt;/h3&gt;
&lt;p&gt;A number of deprecated beta APIs have been removed in 1.22 in favor of the GA version of those same APIs. All existing objects can be interacted with via stable APIs. This removal includes beta versions of the &lt;code&gt;Ingress&lt;/code&gt;, &lt;code&gt;IngressClass&lt;/code&gt;, &lt;code&gt;Lease&lt;/code&gt;, &lt;code&gt;APIService&lt;/code&gt;, &lt;code&gt;ValidatingWebhookConfiguration&lt;/code&gt;, &lt;code&gt;MutatingWebhookConfiguration&lt;/code&gt;, &lt;code&gt;CustomResourceDefinition&lt;/code&gt;, &lt;code&gt;TokenReview&lt;/code&gt;, &lt;code&gt;SubjectAccessReview&lt;/code&gt;, and &lt;code&gt;CertificateSigningRequest&lt;/code&gt; APIs.&lt;/p&gt;
&lt;p&gt;For the full list, check out the &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22&#34;&gt;Deprecated API Migration Guide&lt;/a&gt; as well as the blog post &lt;a href=&#34;https://blog.k8s.io/2021/07/14/upcoming-changes-in-kubernetes-1-22/&#34;&gt;Kubernetes API and Feature Removals In 1.22: Here’s What You Need To Know&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;api-changes-and-improvements-for-ephemeral-containers&#34;&gt;API changes and improvements for ephemeral containers&lt;/h3&gt;
&lt;p&gt;The API used to create &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/&#34;&gt;Ephemeral Containers&lt;/a&gt; changes in 1.22. The Ephemeral Containers feature is alpha and disabled by default, and the new API does not work with clients that attempt to use the old API.&lt;/p&gt;
&lt;p&gt;For stable features, the kubectl tool follows the Kubernetes &lt;a href=&#34;https://kubernetes.io/releases/version-skew-policy/&#34;&gt;version skew policy&lt;/a&gt;; however, kubectl v1.21 and older do not support the new API for ephemeral containers. If you plan to use &lt;code&gt;kubectl debug&lt;/code&gt; to create ephemeral containers, and your cluster is running Kubernetes v1.22, you cannot do so with kubectl v1.21 or earlier. Please update kubectl to 1.22 if you wish to use &lt;code&gt;kubectl debug&lt;/code&gt; with a mix of cluster versions.&lt;/p&gt;
&lt;h2 id=&#34;other-updates&#34;&gt;Other Updates&lt;/h2&gt;
&lt;h3 id=&#34;graduated-to-stable&#34;&gt;Graduated to Stable&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/542&#34;&gt;Bound Service Account Token Volumes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2047&#34;&gt;CSI Service Account Token&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1122&#34;&gt;Windows Support for CSI Plugins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1693&#34;&gt;Warning mechanism for deprecated API use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/85&#34;&gt;PodDisruptionBudget Eviction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notable-feature-updates&#34;&gt;Notable Feature Updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A new &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2579&#34;&gt;PodSecurity admission&lt;/a&gt; alpha feature is introduced, intended as a replacement for PodSecurityPolicy&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1769&#34;&gt;The Memory Manager&lt;/a&gt; moves to beta&lt;/li&gt;
&lt;li&gt;A new alpha feature to enable &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/647&#34;&gt;API Server Tracing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A new v1beta3 version of the &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/970&#34;&gt;kubeadm configuration&lt;/a&gt; format&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1495&#34;&gt;Generic data populators&lt;/a&gt; for PersistentVolumes are now available in alpha&lt;/li&gt;
&lt;li&gt;The Kubernetes control plane will now always use the &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/19&#34;&gt;CronJobs v2 controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;As an alpha feature, all Kubernetes node components (including the kubelet, kube-proxy, and container runtime) can be &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2033&#34;&gt;run as a non-root user&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;release-notes&#34;&gt;Release notes&lt;/h1&gt;
&lt;p&gt;You can check out the full details of the 1.22 release in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;availability-of-release&#34;&gt;Availability of release&lt;/h1&gt;
&lt;p&gt;Kubernetes 1.22 is &lt;a href=&#34;https://kubernetes.io/releases/download/&#34;&gt;available for download&lt;/a&gt; and also &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.22.0&#34;&gt;on the GitHub project&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34;&gt;interactive tutorials&lt;/a&gt; on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt;. If you’d like to try building a cluster from scratch, check out the &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;Kubernetes the Hard Way&lt;/a&gt; tutorial by Kelsey Hightower.&lt;/p&gt;
&lt;h1 id=&#34;release-team&#34;&gt;Release Team&lt;/h1&gt;
&lt;p&gt;This release was made possible by a very dedicated group of individuals, who came together as a team to deliver technical content, documentation, code, and a host of other components that go into every Kubernetes release.&lt;/p&gt;
&lt;p&gt;A huge thank you to the release lead Savitha Raghunathan for leading us through a successful release cycle, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.22 release for the community.&lt;/p&gt;
&lt;p&gt;We would also like to take this opportunity to remember Peeyush Gupta, a member of our team that we lost earlier this year. Peeyush was actively involved in SIG ContribEx and the Kubernetes Release Team, most recently serving as the 1.22 Communications lead. His contributions and efforts will continue to reflect in the community he helped build. A &lt;a href=&#34;https://github.com/cncf/memorials/blob/main/peeyush-gupta.md&#34;&gt;CNCF memorial&lt;/a&gt; page has been created where thoughts and memories can be shared by the community.&lt;/p&gt;
&lt;h1 id=&#34;release-logo&#34;&gt;Release Logo&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-08-04-kubernetes-release-1.22/kubernetes-1.22.png&#34; alt=&#34;Kubernetes 1.22 Release Logo&#34;&gt;&lt;/p&gt;
&lt;p&gt;Amidst the ongoing pandemic, natural disasters, and ever-present shadow of burnout, the 1.22 release of Kubernetes includes 53 enhancements. This makes it the largest release to date. This accomplishment was only made possible due to the hard-working and passionate Release Team members and the amazing contributors of the Kubernetes ecosystem. The release logo is our reminder to keep reaching for new milestones and setting new records. And it is dedicated to all the Release Team members, hikers, and stargazers!&lt;/p&gt;
&lt;p&gt;The logo is designed by &lt;a href=&#34;https://www.instagram.com/boris.z.man/&#34;&gt;Boris Zotkin&lt;/a&gt;. Boris is a Mac/Linux Administrator at the MathWorks. He enjoys simple things in life and loves spending time with his family. This tech-savvy individual is always up for a challenge and happy to help a friend!&lt;/p&gt;
&lt;h1 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;In May, the CNCF welcomed 27 new organizations across the globe as members of the diverse cloud native ecosystem. These new &lt;a href=&#34;https://www.cncf.io/announcements/2021/05/05/27-new-members-join-the-cloud-native-computing-foundation/&#34;&gt;members&lt;/a&gt; will participate in CNCF events, including the upcoming &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;KubeCon + CloudNativeCon NA in Los Angeles&lt;/a&gt; from October 12 – 15, 2021.&lt;/li&gt;
&lt;li&gt;The CNCF granted Spotify the &lt;a href=&#34;https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/&#34;&gt;Top End User Award&lt;/a&gt; during &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/&#34;&gt;KubeCon + CloudNativeCon EU – Virtual 2021&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://k8s.devstats.cncf.io/&#34;&gt;CNCF K8s DevStats project&lt;/a&gt; aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p&gt;
&lt;p&gt;In the v1.22 release cycle, which ran for 15 weeks (April 26 to August 4), we saw contributions from &lt;a href=&#34;https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.21.0%20-%20now&amp;amp;var-metric=contributions&#34;&gt;1063 companies&lt;/a&gt; and &lt;a href=&#34;https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.21.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&#34;&gt;2054 individuals&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/&#34;&gt;KubeCon + CloudNativeCon Europe 2021&lt;/a&gt; was held in May, the third such event to be virtual. All talks are &lt;a href=&#34;https://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC&#34;&gt;now available on-demand&lt;/a&gt; for anyone that would like to catch up!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/blog/2021/07/13/spring-term-lfx-program-largest-graduating-class-with-28-successful-cncf-interns&#34;&gt;Spring Term LFX Program&lt;/a&gt; had the largest graduating class with 28 successful CNCF interns!&lt;/li&gt;
&lt;li&gt;CNCF launched &lt;a href=&#34;https://www.cncf.io/blog/2021/06/03/cloud-native-community-goes-live-with-10-shows-on-twitch/&#34;&gt;livestreaming on Twitch&lt;/a&gt; at the beginning of the year targeting definitive interactive media experience for anyone wanting to learn, grow, and collaborate with others in the Cloud Native community from anywhere in the world.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;event-updates&#34;&gt;Event Updates&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;KubeCon + CloudNativeCon North America 2021&lt;/a&gt; will take place in Los Angeles, October 12 – 15, 2021! You can find more information about the conference and registration on the event site.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://community.cncf.io/kubernetes-community-days/about-kcd/&#34;&gt;Kubernetes Community Days&lt;/a&gt; has upcoming events scheduled in Italy, the UK, and in Washington DC.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;upcoming-release-webinar&#34;&gt;Upcoming release webinar&lt;/h1&gt;
&lt;p&gt;Join members of the Kubernetes 1.22 release team on October 5, 2021 to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the &lt;a href=&#34;https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-122-release/&#34;&gt;event page&lt;/a&gt; on the CNCF Online Programs site.&lt;/p&gt;
&lt;h1 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h1&gt;
&lt;p&gt;If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find out more about contributing to Kubernetes at the &lt;a href=&#34;https://www.kubernetes.dev/&#34;&gt;Kubernetes Contributors&lt;/a&gt; website.&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read more about what’s happening with Kubernetes on the &lt;a href=&#34;https://kubernetes.io/blog/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learn more about the &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/release-team&#34;&gt;Kubernetes Release Team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Roorkee robots, releases and racing: the Kubernetes 1.21 release interview</title>
      <link>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</link>
      <pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Craig Box (Google)&lt;/p&gt;
&lt;p&gt;With Kubernetes 1.22 due out next week, now is a great time to look back on 1.21. The release team for that version was led by &lt;a href=&#34;https://twitter.com/theonlynabarun&#34;&gt;Nabarun Pal&lt;/a&gt; from VMware.&lt;/p&gt;
&lt;p&gt;Back in April I &lt;a href=&#34;https://kubernetespodcast.com/episode/146-kubernetes-1.21/&#34;&gt;interviewed Nabarun&lt;/a&gt; on the weekly &lt;a href=&#34;https://kubernetespodcast.com/&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt;; the latest in a series of release lead conversations that started back with 1.11, not long after the show started back in 2018.&lt;/p&gt;
&lt;p&gt;In these interviews we learn a little about the release, but also about the process behind it, and the story behind the person chosen to lead it. Getting to know a community member is my favourite part of the show each week, and so I encourage you to &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34;&gt;subscribe wherever you get your podcasts&lt;/a&gt;. With a release coming next week, you can probably guess what our next topic will be!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This transcript has been edited and condensed for clarity.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: You have a Bachelor of Technology in Metallurgical and Materials Engineering. How are we doing at turning lead into gold?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Well, last I checked, we have yet to find the philosopher&#39;s stone!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: One of the more important parts of the process?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: We&#39;re not doing that well in terms of getting alchemists up and running. There is some improvement in nuclear technology, where you can turn lead into gold, but I would guess buying gold would be much more efficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Or Bitcoin? It depends what you want to do with the gold.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Yeah, seeing the increasing prices of Bitcoin, you&#39;d probably prefer to bet on that. But, don&#39;t take this as a suggestion. I&#39;m not a registered investment advisor, and I don&#39;t give investment advice!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: But you are, of course, a trained materials engineer. How did you get into that line of education?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: We had a graded and equated exam structure, where you sit a single exam, and then based on your performance in that exam, you can try any of the universities which take those scores into account. I went to the Indian Institute of Technology, Roorkee.&lt;/p&gt;
&lt;p&gt;Materials engineering interested me a lot. I had a passion for computer science since childhood, but I also liked material science, so I wanted to explore that field. I did a lot of exploration around material science and metallurgy in my freshman and sophomore years, but then computing, since it was a passion, crept into the picture.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Let&#39;s dig in there a little bit. What did computing look like during your childhood?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: It was a very interesting journey. I started exploring computers back when I was seven or eight. For my first programming language, if you call it a programming language, I explored LOGO.&lt;/p&gt;
&lt;p&gt;You have a turtle on the screen, and you issue commands to it, like move forward or rotate or pen up or pen down. You basically draw geometric figures. I could visually see how I could draw a square and how I could draw a triangle. It was an interesting journey after that. I learned BASIC, then went to some amount of HTML, JavaScript.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: It&#39;s interesting to me because Logo and BASIC were probably my first two programming languages, but I think there was probably quite a gap in terms of when HTML became a thing after those two! Did your love of computing always lead you down the path towards programming, or were you interested as a child in using computers for games or application software? What led you specifically into programming?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Programming came in late. Not just in computing, but in life, I&#39;m curious with things. When my parents got me my first computer, I was curious. I was like, &amp;quot;how does this operating system work?&amp;quot; What even is running it? Using a television and using a computer is a different experience, but usability is kind of the same thing. The HCI device for a television is a remote, whereas with a computer, I had a keyboard and a mouse. I used to tinker with the box and reinstall operating systems.&lt;/p&gt;
&lt;p&gt;We used to get magazines back then. They used to bundle OpenSuse or Debian, and I used to install them. It was an interesting experience, 15 years back, how Linux used to be. I have been a tinkerer all around, and that&#39;s what eventually led me to programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: With an interest in both the physical and ethereal aspects of technology, you did a lot of robotics challenges during university. That&#39;s something that I am not surprised to hear from someone who has a background in Logo, to be honest. There&#39;s Mindstorms, and a lot of other technology that is based around robotics that a lot of LOGO people got into. How was that something that came about for you?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: When I joined my university, apart from studying materials, one of the things they used to really encourage was to get involved in a lot of extracurricular activities. One which interested me was robotics. I joined &lt;a href=&#34;https://github.com/marsiitr&#34;&gt;my college robotics team&lt;/a&gt; and participated in a lot of challenges.&lt;/p&gt;
&lt;p&gt;Predominantly, we used to participate in this competition called &lt;a href=&#34;https://en.wikipedia.org/wiki/ABU_Robocon&#34;&gt;ABU Robocon&lt;/a&gt;, which is an event conducted by the Asia-Pacific Broadcasting Union. What they used to do was, every year, one of the participating countries in the contest would provide a problem statement. For example, one year, they asked us to build a badminton-playing robot.  They asked us to build a rugby playing robot or a Frisbee thrower, and there are some interesting problem statements around the challenge: you can&#39;t do this. You can&#39;t do that. Weight has to be like this. Dimensions have to be like that.&lt;/p&gt;
&lt;p&gt;I got involved in that, and most of my time at university, I used to spend there. Material science became kind of a backburner for me, and my hobby became my full time thing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: And you were not only involved there in terms of the project and contributions to it, but you got involved as a secretary of the team, effectively, doing a lot of the organization, which is a thread that will come up as we speak about Kubernetes.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Over the course of time, when I gained more knowledge into how the team works, it became very natural that I graduated up the ladder and then managed juniors. I became the joint secretary of the robotics club in our college. This was more of a broad, engaging role in evangelizing robotics at the university, to promote events, to help students to see the value in learning robotics - what you gain out of that mechanically or electronically, or how do you develop your logic by programming robots.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Your first job after graduation was working at a company called Algoshelf, but you were also an intern there while you were at school?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Algoshelf was known as Rorodata when I joined them as an intern. This was also an interesting opportunity for me in the sense that I was always interested in writing programs which people would use. One of the things that I did there was  build an open source Function as a Service framework, if I may call it that - it was mostly turning Python functions into web servers without even writing any code. The interesting bit there was that it was targeted toward data scientists, and not towards programmers. We had to understand the pain of data scientists, that they had to learn a lot of programming in order to even deploy their machine learning models, and we wanted to solve that problem.&lt;/p&gt;
&lt;p&gt;They offered me a job after my internship, and I kept on working for them after I graduated from university. There, I got introduced to Kubernetes, so we pivoted into a product structure where the very same thing I told you, the Functions as a Service thing, could be deployed in Kubernetes. I was exploring Kubernetes to use it as a scalable platform. Instead of managing pets, we wanted to manage cattle, as in, we wanted to have a very highly distributed architecture.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Not actual cattle. I&#39;ve been to India. There are a lot of cows around.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Yeah, not actual cattle. That is a bit tough.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: When Algoshelf we&#39;re looking at picking up Kubernetes, what was the evaluation process like? Were you looking at other tools at the time? Or had enough time passed that Kubernetes was clearly the platform that everyone was going to use?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Algoshelf was a natural evolution. Before Kubernetes, we used to deploy everything on a single big AWS server, using systemd. Everything was a systemd service, and everything was deployed using Fabric. Fabric is a Python package which essentially is like Ansible, but much leaner, as it does not have all the shims and things that Ansible has.&lt;/p&gt;
&lt;p&gt;Then we asked &amp;quot;what if we need to scale out to different machines?&amp;quot; Kubernetes was in the hype. We hopped onto the hype train to see whether Kubernetes was worth it for us. And that&#39;s where my journey started, exploring the ecosystem, exploring the community. How can we improve the community in essence?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: A couple of times now you&#39;ve mentioned as you&#39;ve grown in a role, becoming part of the organization and the arranging of the group. You&#39;ve talked about working in Python. You had submitted some talks to Pycon India. And I understand you&#39;re now a tech lead for that conference. What does the tech community look like in India and how do you describe your involvement in it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: My involvement with the community began when I was at university. When I was working as an intern at Algoshelf, I was introduced to this-- I never knew about PyCon India, or tech conferences in general.&lt;/p&gt;
&lt;p&gt;The person that I was working with just asked me, like hey, did you submit a talk to PyCon India? It&#39;s very useful, the library that we were making. So I &lt;a href=&#34;https://www.nabarun.in/talk/2017/pyconindia/#1&#34;&gt;submitted a talk&lt;/a&gt; to PyCon India in 2017. Eventually the talk got selected. That was not my first speaking opportunity, it was my second. I also spoke at PyData Delhi on a similar thing that I worked on in my internship.&lt;/p&gt;
&lt;p&gt;It has been a journey since then. I talked about the same thing at FOSSASIA Summit in Singapore, and got really involved with the Python community because it was what I used to work on back then.&lt;/p&gt;
&lt;p&gt;After giving all those talks at conferences, I got also introduced to this amazing group called &lt;a href=&#34;https://dgplug.org/&#34;&gt;dgplug&lt;/a&gt;, which is an acronym for the Durgapur Linux Users Group. It is a group started in-- I don&#39;t remember the exact year, but it was around 12 to 13 years back, by someone called Kushal Das, with the ideology of &lt;a href=&#34;https://foss.training/&#34;&gt;training students into being better open source contributors&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I liked the idea and got involved with in teaching last year. It is not limited to students. Professionals can also join in. It&#39;s about making anyone better at upstream contributions, making things sustainable. I started training people on Vim, on how to use text editors. so they are more efficient and productive. In general life, text editors are a really good tool.&lt;/p&gt;
&lt;p&gt;The other thing was the shell. How do you navigate around the Linux shell and command line? That has been a fun experience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: It&#39;s very interesting to think about that, because my own involvement with a Linux User Group was probably around the year 2000. And back then we were teaching people how to install things-- Linux on CD was kinda new at that point in time. There was a lot more of, what is this new thing and how do we get involved? When the internet took off around that time, all of that stuff moved online - you no longer needed to go meet a group of people in a room to talk about Linux. And I haven&#39;t really given much thought to the concept of a LUG since then, but it&#39;s great to see it having turned into something that&#39;s now about contributing, rather than just about how you get things going for yourself.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Exactly. So as I mentioned earlier, my journey into Linux was installing SUSE from DVDs that came bundled with magazines. Back then it was a pain installing things because you did not get any instructions. There has certainly been a paradigm shift now. People are more open to reading instructions online, downloading ISOs, and then just installing them. So we really don&#39;t need to do that as part of LUGs.&lt;/p&gt;
&lt;p&gt;We have shifted more towards enabling people to contribute to whichever project that they use. For example, if you&#39;re using Fedora, contribute to Fedora; make things better. It&#39;s just about giving back to the community in any way possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: You&#39;re also involved in the &lt;a href=&#34;https://www.meetup.com/Bangalore-Kubernetes-Meetup/&#34;&gt;Kubernetes Bangalore meetup group&lt;/a&gt;. Does that group have a similar mentality?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: The Kubernetes Bangalore meetup group is essentially focused towards spreading the knowledge of Kubernetes and the aligned products in the ecosystem, whatever there is in the Cloud Native Landscape, in various ways. For example, to evangelize about using them in your company or how people use them in existing ways.&lt;/p&gt;
&lt;p&gt;So a few months back in February, we did something like a &lt;a href=&#34;https://www.youtube.com/watch?v=FgsXbHBRYIc&#34;&gt;Kubernetes contributor workshop&lt;/a&gt;. It was one of its kind in India. It was the first one if I recall correctly. We got a lot of traction and community members interested in contributing to Kubernetes and a lot of other projects. And this is becoming a really valuable thing.&lt;/p&gt;
&lt;p&gt;I&#39;m not much involved in the organization of the group. There are really great people already organizing it. I keep on being around and attending the meetups and trying to answer any questions if people have any.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: One way that it is possible to contribute to the Kubernetes ecosystem is through the release process. You&#39;ve &lt;a href=&#34;https://blog.naba.run/posts/release-enhancements-journey/&#34;&gt;written a blog&lt;/a&gt; which talks about your journey through that. It started in Kubernetes 1.17, where you took a shadow role for that release. Tell me about what it was like to first take that plunge.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Taking the plunge was a big step, I would say. It should not have been that way. After getting into the team, I saw that it is really encouraged that you should just apply to the team - but then write truthfully about yourself. What do you want? Write your passionate goal, why you want to be in the team.&lt;/p&gt;
&lt;p&gt;So even right now the shadow applications are open for the next release. I wanted to give that a small shoutout. If you want to contribute to the Kubernetes release team, please do apply. The form is pretty simple. You just need to say why do you want to contribute to the release team.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: What was your answer to that question?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: It was a bit tricky. I have this philosophy of contributing to projects that I use in my day-to-day life. I use a lot of open source projects daily, and I started contributing to Kubernetes primarily because I was using the Kubernetes Python client. That was one of my first contributions.&lt;/p&gt;
&lt;p&gt;When I was contributing to that, I explored the release team and it interested me a lot, particularly how interesting and varied the mechanics of releasing Kubernetes are. For most software projects, it&#39;s usually whenever you decide that you have made meaningful progress in terms of features, you release it. But Kubernetes is not like that. We follow a regular release cadence. And all those aspects really interested me. I actually applied for the first time in Kubernetes 1.16, but got rejected.&lt;/p&gt;
&lt;p&gt;But I still applied to Kubernetes 1.17, and I got into the enhancements team. That team was led by &lt;a href=&#34;https://kubernetespodcast.com/episode/126-research-steering-honking/&#34;&gt;MrBobbyTables, Bob Killen&lt;/a&gt;, back then, and &lt;a href=&#34;https://kubernetespodcast.com/episode/131-kubernetes-1.20/&#34;&gt;Jeremy Rickard&lt;/a&gt; was one of my co-shadows in the team. I shadowed enhancements again. Then I lead enhancements in 1.19. I then shadowed the lead in 1.20 and eventually led the 1.21 team. That&#39;s what my journey has been.&lt;/p&gt;
&lt;p&gt;My suggestion to people is don&#39;t be afraid of failure. Even if you don&#39;t get selected, it&#39;s perfectly fine. You can still contribute to the release team. Just hop on the release calls, raise your hand, and introduce yourself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Between the 1.20 and 1.21 releases, you moved to work on the upstream contribution team at VMware. I&#39;ve noticed that VMware is hiring a lot of great upstream contributors at the moment. Is this something that &lt;a href=&#34;https://kubernetespodcast.com/episode/130-kubecon-na-2020/&#34;&gt;Stephen Augustus&lt;/a&gt; had his fingerprints all over? Is there something in the water?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: A lot of people have fingerprints on this process. Stephen certainly had his fingerprints on it, I would say. We are expanding the team of upstream contributors primarily because the product that we are working for is based on Kubernetes. It helps us a lot in driving processes upstream and helping out the community as a whole, because everyone then gets enabled and benefits from what we contribute to the community.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: I understand that the Tanzu team is being built out in India at the moment, but I guess you probably haven&#39;t been able to meet them in person yet?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Yes and no. I did not meet any of them after joining VMware, but I met a lot of my teammates, before I joined VMware, at KubeCons. For example, I met Nikhita, I met Dims, I met Stephen at KubeCon. I am yet to meet other members of the team and I&#39;m really excited to catch up with them once everything comes out of lockdown and we go back to our normal lives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Yes, everyone that I speak to who has changed jobs in the pandemic says it&#39;s a very odd experience, just nothing really being different. And the same perhaps for people who are working on open source moving companies as well. They&#39;re doing the same thing, perhaps just for a different employer.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: As we say in the community, see you in another Slack in some time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: We now turn to the recent release of Kubernetes 1.21. First of all, congratulations on that.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Thank you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: &lt;a href=&#34;https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/&#34;&gt;The announcement&lt;/a&gt; says the release consists of 51 enhancements, 13 graduating to stable, 16 moving to beta, 20 entering alpha, and then two features that have been deprecated. How would you summarize this release?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: One of the big points for this release is that it is the largest release of all time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Really?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Yep. 1.20 was the largest release back then, but 1.21 got more enhancements, primarily due to a lot of changes that we did to the process.&lt;/p&gt;
&lt;p&gt;In the 1.21 release cycle, we did a few things differently compared to other release cycles-- for example, in the enhancement process. An enhancement, in the Kubernetes context, is basically a feature proposal. You will hear the terminology &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/README.md&#34;&gt;Kubernetes Enhancement Proposals&lt;/a&gt;, or KEP, a lot in the community. An enhancement is a broad thing encapsulated in a specific document.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: I like to think of it as a thing that&#39;s worth having a heading in the release notes.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Indeed. Until the 1.20 release cycle, what we used to do was-- the release team has a vertical called enhancements. The enhancements team members used to ping each of the enhancement issues and ask whether they want to be part of the release cycle or not. The authors would decide, or talk to their SIG, and then come back with the answer, as to whether they wanted to be part of the cycle.&lt;/p&gt;
&lt;p&gt;In this release, what we did was we eliminated that process and asked the SIGs proactively to discuss amongst themselves, what they wanted to pitch in for this release cycle. What set of features did they want to graduate this release? They may introduce things in alpha, graduate things to beta or stable, or they may also deprecate features.&lt;/p&gt;
&lt;p&gt;What this did was promote a lot of async processes, and at the same time, give power back to the community. The community decides what they want in the release and then comes back collectively. It also reduces a lot of stress on the release team who previously had to ask people consistently what they wanted to pitch in for the release. You now have a deadline. You discuss amongst your SIG what your roadmap is and what it looks like for the near future. Maybe this release, and the next two. And you put all of those answers into a Google spreadsheet. Spreadsheets are still a thing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: The Kubernetes ecosystem runs entirely on Google Spreadsheets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: It does, and a lot of Google Docs for meeting notes! We did a lot of process improvements, which essentially led to a better release. This release cycle we had 13 enhancements graduating to stable, 16 which moved to beta, and 20 enhancements which were net new features into the ecosystem, and came in as alpha.&lt;/p&gt;
&lt;p&gt;Along with that are features set for deprecation. One of them was PodSecurityPolicy. That has been a point of discussion in the Kubernetes user base and we also published &lt;a href=&#34;https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/&#34;&gt;a blog post about it&lt;/a&gt;. All credit to SIG Security who have been on top of things as to find a replacement for PodSecurityPolicy even before this release cycle ended, so that they could at least have a proposal of what will happen next.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Let&#39;s talk about some old things and some new things. You mentioned PodSecurityPolicy there. That&#39;s a thing that&#39;s been around a long time and is being deprecated. Two things that have been around a long time and that are now being promoted to stable are CronJobs and PodDisruptionBudgets, both of which were introduced in Kubernetes 1.4, which came out in 2016. Why do you think it took so long for them both to go stable?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: I might not have a definitive answer to your question. One of the things that I feel is they might be already so good that nobody saw that they were beta features, and just kept on using them.&lt;/p&gt;
&lt;p&gt;One of the things that I noticed when reading for the CronJobs graduation from beta to stable was the new controller. Users might not see this, but there has been a drastic change in the CronJob controller v2. What it essentially does is goes from a poll-based method of checking what users have defined as CronJobs to a queue architecture, which is the modern method of defining controllers. That has been one of the really good improvements in the case of CronJobs. Instead of the controller working in O(N) time, you now have constant time complexity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: A lot of these features that have been in beta for a long time, like you say, people have an expectation that they are complete. With PodSecurityPolicy, it&#39;s being deprecated, which is allowed because it&#39;s a feature that never made it out of beta. But how do you think people will react to it going away? And does that say something about the need for the process to make sure that features don&#39;t just languish in beta forever, which has been introduced recently?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: That&#39;s true. One of the driving factors, when contributors are thinking of graduating beta features has been the &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/1635-prevent-permabeta/README.md&#34;&gt;&amp;quot;prevention of perma-beta&amp;quot; KEP&lt;/a&gt;. Back in 1.19 we &lt;a href=&#34;https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/&#34;&gt;introduced this process&lt;/a&gt; where each of the beta resources were marked for deprecation and removal in a certain time frame-- three releases for deprecation and another  release for removal. That&#39;s also a motivating factor for eventually rethinking as to how beta resources work for us in the community. That is also very effective, I would say.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Do remember that Gmail was in beta for eight years.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: I did not know that!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Nothing in Kubernetes is quite that old yet, but we&#39;ll get there. Of the 20 new enhancements, do you have a favorite or any that you&#39;d like to call out?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: There are two specific features in 1.21 that I&#39;m really interested in, and are coming as net new features. One of them is the &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor&#34;&gt;persistent volume health monitor&lt;/a&gt;, which gives the users the capability to actually see whether the backing volumes, which power persistent volumes in Kubernetes, are deleted or not. For example, the volumes may get deleted due to an inadvertent event, or they may get corrupted. That information is basically surfaced out as a field so that the user can leverage it in any way.&lt;/p&gt;
&lt;p&gt;The other feature is the proposal for &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-cli/859-kubectl-headers&#34;&gt;adding headers with the command name to kubectl requests&lt;/a&gt;. We have always set the user-agent information when doing those kind of requests, but the proposal is to add what command the user put in so that we can enable more telemetry, and cluster administrators can determine the usage patterns of how people are using the cluster. I&#39;m really excited about these kind of features coming into play.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: You&#39;re the first release lead from the Asia-Pacific region, or more accurately, outside of the US and Europe. Most meetings in the Kubernetes ecosystem are traditionally in the window of overlap between the US and Europe, in the morning in California and the evening here in the UK. What&#39;s it been like to work outside of the time zones that the community had previously been operating in?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: It has been a fun and a challenging proposition, I would say. In the last two-ish years that I have been contributing to Kubernetes, the community has also transformed from a lot of early morning Pacific calls to more towards async processes. For example, we in the release team have transformed our processes so we don&#39;t do updates in the calls anymore. What we do is ask for updates ahead of time, and then in the call, we just discuss things which need to be discussed synchronously in the team.&lt;/p&gt;
&lt;p&gt;We leverage the meetings right now more for discussions. But we also don&#39;t come to decisions in those discussions, because if any stakeholder is not present on the call, it puts them at a disadvantage. We are trying to talk more on Slack, publicly, or talk on mailing lists. That&#39;s where most of the discussion should happen, and also to gain lazy consensus. What I mean by lazy consensus is come up with a pre-decision kind of thing, but then also invite feedback from the broader community about what people would like them to see about that specific thing being discussed. This is where we as a community are also transforming a lot, but there is a lot more headroom to grow.&lt;/p&gt;
&lt;p&gt;The release team also started to have EU/APAC burndown meetings. In addition to having one meeting focused towards the US and European time zones, we also do a meeting which is more suited towards European and Asia-Pacific time zones. One of the driving factors for those decisions was that the release team is seeing a lot of participation from a variety of time zones. To give you one metric, we had release team members this cycle from UTC+8 all through UTC-8 - 16 hours of span. It&#39;s really difficult to accommodate all of those zones in a single meeting. And it&#39;s not just those 16 hours of span - what about the other eight hours?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Yeah, you&#39;re missing New Zealand. You could add another 5 hours of span right there.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Exactly. So we will always miss people in meetings, and that&#39;s why we should also innovate more, have different kinds of meetings. But that also may not be very sustainable in the future. Will people attend duplicate meetings? Will people follow both of the meetings? More meetings is one of the solutions.&lt;/p&gt;
&lt;p&gt;The other solution is you have threaded discussions on some medium, be it Slack or be it a mailing list. Then, people can just pitch in whenever it is work time for them. Then, at the end of the day, a 24-hour rolling period, you digest it, and then push it out as meeting notes. That&#39;s what the Contributor Experience Special Interest Group is doing - shout-out to them for moving to that process. I may be wrong here, but I think once every two weeks, they do async updates on Slack. And that is a really nice thing to have, improving variety of geographies that people can contribute from.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Once you&#39;ve put everything together that you hope to be in your release, you create a release candidate build. How do you motivate people to test those?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: That&#39;s a very interesting question. It is difficult for us to motivate people into trying out these candidates. It&#39;s mostly people who are passionate about Kubernetes who try out the release candidates and see for themselves what the bugs are. I remember &lt;a href=&#34;https://twitter.com/dims/status/1377272238420934656&#34;&gt;Dims tweeting out a call&lt;/a&gt; that if somebody tries out the release candidate and finds a good bug or caveat, they could get a callout in the KubeCon keynote. That&#39;s one of the incentives - if you want to be called out in a KubeCon keynote, please try our release candidates.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Or get a new pair of Kubernetes socks?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: We would love to give out goodies to people who try out our release candidates and find bugs. For example, if you want the brand new release team logo as a sticker, just hit me up. If you find a bug in a 1.22 release candidate, I would love to be able to send you some coupon codes for the store. Don&#39;t quote me on this, but do reach out.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Now the release is out, is it time for you to put your feet up? What more things do you have to do, and how do you feel about the path ahead for yourself?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: I was discussing this with the team yesterday. Even after the release, we had kind of a water-cooler conversation. I just pasted in a Zoom link to all the release team members and said, hey, do you want to chat? One of the things that I realized that I&#39;m really missing is the daily burndowns right now. I will be around in the release team and the SIG Release meetings, helping out the new lead in transitioning. And even my job, right now, is not over. I&#39;m working with Taylor, who is the emeritus advisor for 1.21, on figuring out some of the mechanics for the next release cycle. I&#39;m also documenting what all we did as part of the process and as part of the process changes, and making sure the next release cycle is up and running.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: We&#39;ve done a lot of these release lead interviews now, and there&#39;s a question which we always like to ask, which is, what will you write down in the transition envelope? Savitha Raghunathan is the release lead for 1.22. What is the advice that you will pass on to her?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Three words-- &lt;strong&gt;Do, Delegate, and Defer&lt;/strong&gt;. Categorize things into those three buckets as to what you should do right away, what you need to defer, and things that you can delegate to your shadows or other release team members. That&#39;s one of the mantras that works really well when leading a team. It is not just in the context of the release team, but it&#39;s in the context of managing any team.&lt;/p&gt;
&lt;p&gt;The other bit is &lt;strong&gt;over-communicate&lt;/strong&gt;. No amount of communication is enough. What I&#39;ve realized is the community is always willing to help you. One of the big examples that I can give is the day before release was supposed to happen, we were seeing a lot of test failures, and then one of the community members had an idea-- why don&#39;t you just send an email? I was like, &amp;quot;that sounds good. We can send an email mentioning all the flakes and call out for help to the broader Kubernetes developer community.&amp;quot; And eventually, once we sent out the email, lots of people came in to help us in de-flaking the tests and trying to find out the root cause as to why those tests were failing so often. Big shout out to Antonio and all the SIG Network folks who came to pitch in.&lt;/p&gt;
&lt;p&gt;No matter how many names I mention, it will never be enough. A lot of people, even outside the release team, have helped us a lot with this release. And that&#39;s where the release theme comes in - &lt;strong&gt;Power to the Community&lt;/strong&gt;. I&#39;m really stoked by how this community behaves and how people are willing to help you all the time. It&#39;s not about what they&#39;re telling you to do, but it&#39;s what they&#39;re also interested in, they&#39;re passionate about.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: One of the things you&#39;re passionate about is Formula One. Do you think Lewis Hamilton is going to take it away this year?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: It&#39;s a fair probability that Lewis will win the title this year as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: Which would take him to eight all time career wins. And thus-- &lt;a href=&#34;https://www.nytimes.com/2020/11/15/sports/autoracing/lewis-hamilton-schumacher-formula-one-record.html&#34;&gt;he&#39;s currently tied with Michael Schumacher&lt;/a&gt;-- would pull him ahead.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: Yes. Michael Schumacher was my first favorite F1 driver, I would say. It feels a bit heartbreaking to see someone break Michael&#39;s record.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CRAIG BOX: How do you feel about &lt;a href=&#34;https://www.formula1.com/en/latest/article.breaking-mick-schumacher-to-race-for-haas-in-2021-as-famous-surname-returns.66XTVfSt80GrZe91lvWVwJ.html&#34;&gt;Michael Schumacher&#39;s son joining the contest?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NABARUN PAL: I feel good. Mick Schumacher is in the fray right now. And I wish we could see him, in a few years, in a Ferrari. The Schumacher family back to Ferrari would be really great to see. But then, my fan favorite has always been McLaren, partly because I like the chemistry of Lando and Carlos over the last two years. It was heartbreaking to see Carlos go to Ferrari. But then we have Lando and Daniel Ricciardo in the team. They&#39;re also fun people.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://twitter.com/theonlynabarun&#34;&gt;Nabarun Pal&lt;/a&gt; is on the Tanzu team at VMware and served as the Kubernetes 1.21 release team lead.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You can find the &lt;a href=&#34;http://www.kubernetespodcast.com/&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/KubernetesPod&#34;&gt;@KubernetesPod&lt;/a&gt; on Twitter, and you can &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34;&gt;subscribe&lt;/a&gt; so you never miss an episode.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Updating NGINX-Ingress to use the stable Ingress API</title>
      <link>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; James Strong, Ricardo Katz&lt;/p&gt;
&lt;p&gt;With all Kubernetes APIs, there is a process to creating, maintaining, and
ultimately deprecating them once they become GA. The networking.k8s.io API group is no
different. The upcoming Kubernetes 1.22 release will remove several deprecated APIs
that are relevant to networking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;networking.k8s.io/v1beta1&lt;/code&gt; API version of &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class&#34;&gt;IngressClass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;all beta versions of &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt;: &lt;code&gt;extensions/v1beta1&lt;/code&gt; and &lt;code&gt;networking.k8s.io/v1beta1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On a v1.22 Kubernetes cluster, you&#39;ll be able to access Ingress and IngressClass
objects through the stable (v1) APIs, but access via their beta APIs won&#39;t be possible.
This change has been in
in discussion since
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43214&#34;&gt;2017&lt;/a&gt;,
&lt;a href=&#34;https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/&#34;&gt;2019&lt;/a&gt; with
1.16 Kubernetes API deprecations, and most recently in
KEP-1453:
&lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1453-ingress-api#122&#34;&gt;Graduate Ingress API to GA&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During community meetings, the networking Special Interest Group has decided to continue
supporting Kubernetes versions older than 1.22 with Ingress-NGINX version 0.47.0.
Support for Ingress-NGINX will continue for six months after Kubernetes 1.22
is released. Any additional bug fixes and CVEs for Ingress-NGINX will be
addressed on a need-by-need basis.&lt;/p&gt;
&lt;p&gt;Ingress-NGINX will have separate branches and releases of Ingress-NGINX to
support this model, mirroring the Kubernetes project process. Future
releases of the Ingress-NGINX project will track and support the latest
versions of Kubernetes.&lt;/p&gt;





&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Ingress NGINX supported version with Kubernetes Versions&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Kubernetes version&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Ingress-NGINX version&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v1.22&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v1.0.0-alpha.2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;New features, plus bug fixes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v1.21&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v0.47.x&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v1.20&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v0.47.x&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Bugfixes only, and just  for security issues or crashes. No end-of-support date announced.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v1.19&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;v0.47.x&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Bugfixes only, and just  for security issues or crashes. Fixes only provided until 6 months after Kubernetes v1.22.0 is released.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Because of the updates in Kubernetes 1.22, &lt;strong&gt;v0.47.0&lt;/strong&gt; will not work with
Kubernetes 1.22.&lt;/p&gt;
&lt;h1 id=&#34;what-you-need-to-do&#34;&gt;What you need to do&lt;/h1&gt;
&lt;p&gt;The team is currently in the process of upgrading ingress-nginx to support
the v1 migration, you can track the progress
&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/pull/7156&#34;&gt;here&lt;/a&gt;.&lt;br&gt;
We&#39;re not making feature improvements to &lt;code&gt;ingress-nginx&lt;/code&gt; until after the support for
Ingress v1 is complete.&lt;/p&gt;
&lt;p&gt;In the meantime to ensure no compatibility issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Update to the latest version of Ingress-NGINX; currently
&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v0.47.0&#34;&gt;v0.47.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;After Kubernetes 1.22 is released, ensure you are using the latest version of
Ingress-NGINX that supports the stable APIs for Ingress and IngressClass.&lt;/li&gt;
&lt;li&gt;Test Ingress-NGINX version v1.0.0-alpha.2 with Cluster versions &amp;gt;= 1.19
and report any issues to the projects Github page.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The community’s feedback and support in this effort is welcome. The
Ingress-NGINX Sub-project regularly holds community meetings where we discuss
this and other issues facing the project. For more information on the sub-project,
please see &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-network&#34;&gt;SIG Network&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Release Cadence Change: Here’s What You Need To Know</title>
      <link>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</link>
      <pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Celeste Horgan, Adolfo García Veytia, James Laverack, Jeremy Rickard&lt;/p&gt;
&lt;p&gt;On April 23, 2021, the Release Team merged a Kubernetes Enhancement Proposal (KEP) changing the Kubernetes release cycle from four releases a year (once a quarter) to three releases a year.&lt;/p&gt;
&lt;p&gt;This blog post provides a high level overview about what this means for the Kubernetes community&#39;s contributors and maintainers.&lt;/p&gt;
&lt;h2 id=&#34;what-s-changing-and-when&#34;&gt;What&#39;s changing and when&lt;/h2&gt;
&lt;p&gt;Starting with the &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/releases/release-1.22&#34;&gt;Kubernetes 1.22 release&lt;/a&gt;, a lightweight policy will drive the creation of each release schedule. This policy states:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first Kubernetes release of a calendar year should start at the second or third
week of January to provide people more time for contributors coming back from the
end of year holidays.&lt;/li&gt;
&lt;li&gt;The last Kubernetes release of a calendar year should be finished by the middle of
December.&lt;/li&gt;
&lt;li&gt;A Kubernetes release cycle has a length of approximately 15 weeks.&lt;/li&gt;
&lt;li&gt;The week of KubeCon + CloudNativeCon is not considered a &#39;working week&#39; for SIG Release. The Release Team will not hold meetings or make decisions in this period.&lt;/li&gt;
&lt;li&gt;An explicit SIG Release break of at least two weeks between each cycle will
be enforced.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, Kubernetes will follow a three releases per year cadence. Kubernetes 1.23 will be the final release of the 2021 calendar year. This new policy results in a very predictable release schedule, allowing us to forecast upcoming release dates:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proposed Kubernetes Release Schedule for the remainder of 2021&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Week Number in Year&lt;/th&gt;
&lt;th&gt;Release Number&lt;/th&gt;
&lt;th&gt;Release Week&lt;/th&gt;
&lt;th&gt;Note&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;td&gt;1.23&lt;/td&gt;
&lt;td&gt;1 (August 23)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;1.23&lt;/td&gt;
&lt;td&gt;16 (December 07)&lt;/td&gt;
&lt;td&gt;KubeCon + CloudNativeCon NA Break (Oct 11-15)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Proposed Kubernetes Release Schedule for 2022&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Week Number in Year&lt;/th&gt;
&lt;th&gt;Release Number&lt;/th&gt;
&lt;th&gt;Release Week&lt;/th&gt;
&lt;th&gt;Note&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1.24&lt;/td&gt;
&lt;td&gt;1 (January 03)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;1.24&lt;/td&gt;
&lt;td&gt;15 (April 12)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;1.25&lt;/td&gt;
&lt;td&gt;1 (April 26)&lt;/td&gt;
&lt;td&gt;KubeCon + CloudNativeCon EU likely to occur&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;1.25&lt;/td&gt;
&lt;td&gt;15 (August 09)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;1.26&lt;/td&gt;
&lt;td&gt;1 (August 22&lt;/td&gt;
&lt;td&gt;KubeCon + CloudNativeCon NA likely to occur&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;td&gt;1.26&lt;/td&gt;
&lt;td&gt;14 (December 06)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These proposed dates reflect only the start and end dates, and they are subject to change. The Release Team will select dates for enhancement freeze, code freeze, and other milestones at the start of each release. For more information on these milestones, please refer to the &lt;a href=&#34;https://www.k8s.dev/resources/release/#phases&#34;&gt;release phases&lt;/a&gt; documentation. Feedback from prior releases will feed into this process.&lt;/p&gt;
&lt;h2 id=&#34;what-this-means-for-end-users&#34;&gt;What this means for end users&lt;/h2&gt;
&lt;p&gt;The major change end users will experience is a slower release cadence and a slower rate of enhancement graduation. Kubernetes release artifacts, release notes, and all other aspects of any given release will stay the same.&lt;/p&gt;
&lt;p&gt;Prior to this change an enhancement could graduate from alpha to stable in 9 months. With the change in cadence, this will stretch to 12 months. Additionally, graduation of features over the last few releases has in some part been driven by release team activities.&lt;/p&gt;
&lt;p&gt;With fewer releases, users can expect to see the rate of feature graduation slow. Users can also expect releases to contain a larger number of enhancements that they need to be aware of during upgrades. However, with fewer releases to consume per year, it&#39;s intended that end user organizations will spend less time on upgrades and gain more time on supporting their Kubernetes clusters. It also means that Kubernetes releases are in support for a slightly longer period of time, so bug fixes and security patches will be available for releases for a longer period of time.&lt;/p&gt;
&lt;h2 id=&#34;what-this-means-for-kubernetes-contributors&#34;&gt;What this means for Kubernetes contributors&lt;/h2&gt;
&lt;p&gt;With a lower release cadence, contributors have more time for project enhancements, feature development, planning, and testing. A slower release cadence also provides more room for maintaining their mental health, preparing for events like KubeCon + CloudNativeCon or work on downstream integrations.&lt;/p&gt;
&lt;h2 id=&#34;why-we-decided-to-change-the-release-cadence&#34;&gt;Why we decided to change the release cadence&lt;/h2&gt;
&lt;p&gt;The Kubernetes 1.19 cycle was far longer than usual. SIG Release extended it to lessen the burden on both Kubernetes contributors and end users due the COVID-19 pandemic. Following this extended release, the Kubernetes 1.20 release became the third, and final, release for 2020.&lt;/p&gt;
&lt;p&gt;As the Kubernetes project matures, the number of enhancements per cycle grows, along with the burden on contributors, the Release Engineering team.  Downstream consumers and integrators also face increased challenges keeping up with &lt;a href=&#34;https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/&#34;&gt;ever more feature-packed releases&lt;/a&gt;. A wider project adoption means the complexity of supporting a rapidly evolving platform affects a bigger downstream chain of consumers.&lt;/p&gt;
&lt;p&gt;Changing the release cadence from four to three releases per year balances a variety of factors for stakeholders: while it&#39;s not strictly an LTS policy, consumers and integrators will get longer support terms for each minor version as the extended release cycles lead to the &lt;a href=&#34;https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/&#34;&gt;previous three releases being supported&lt;/a&gt; for a longer period. Contributors get more time to &lt;a href=&#34;https://www.cncf.io/blog/2021/04/12/enhancing-the-kubernetes-enhancements-process/&#34;&gt;mature enhancements&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md&#34;&gt;get them ready for production&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the management overhead for SIG Release and the Release Engineering team diminishes allowing the team to spend more time on improving the quality of the software releases and the tooling that drives them.&lt;/p&gt;
&lt;h2 id=&#34;how-you-can-help&#34;&gt;How you can help&lt;/h2&gt;
&lt;p&gt;Join the &lt;a href=&#34;https://github.com/kubernetes/sig-release/discussions/1566&#34;&gt;discussion&lt;/a&gt; about communicating future release dates and be sure to be on the lookout for post release surveys.&lt;/p&gt;
&lt;h2 id=&#34;where-you-can-find-out-more&#34;&gt;Where you can find out more&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read the KEP &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-release/2572-release-cadence&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the &lt;a href=&#34;https://groups.google.com/g/kubernetes-dev&#34;&gt;kubernetes-dev&lt;/a&gt; mailing list&lt;/li&gt;
&lt;li&gt;Join &lt;a href=&#34;https://slack.k8s.io&#34;&gt;Kubernetes Slack&lt;/a&gt; and follow the #announcements channel&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Spotlight on SIG Usability</title>
      <link>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</link>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Kunal Kushwaha, Civo&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Are you interested in learning about what &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-usability&#34;&gt;SIG Usability&lt;/a&gt; does and how you can get involved? Well, you&#39;re at the right place. SIG Usability is all about making Kubernetes more accessible to new folks, and its main activity is conducting user research for the community. In this blog, we have summarized our conversation with &lt;a href=&#34;https://twitter.com/morengab&#34;&gt;Gaby Moreno&lt;/a&gt;, who walks us through the various aspects of being a part of the SIG and shares some insights about how others can get involved.&lt;/p&gt;
&lt;p&gt;Gaby is a co-lead for SIG Usability. She works as a Product Designer at IBM and enjoys working on the user experience of open, hybrid cloud technologies like Kubernetes, OpenShift, Terraform, and Cloud Foundry.&lt;/p&gt;
&lt;h2 id=&#34;a-summary-of-our-conversation&#34;&gt;A summary of our conversation&lt;/h2&gt;
&lt;h3 id=&#34;q-could-you-tell-us-a-little-about-what-sig-usability-does&#34;&gt;Q. Could you tell us a little about what SIG Usability does?&lt;/h3&gt;
&lt;p&gt;A. SIG Usability at a high level started because there was no dedicated user experience team for Kubernetes. The extent of SIG Usability is focussed on the end-client ease of use of the Kubernetes project. The main activity is user research for the community, which includes speaking to Kubernetes users.&lt;/p&gt;
&lt;p&gt;This covers points like user experience and accessibility. The objectives of the SIG are to guarantee that the Kubernetes project is maximally usable by people of a wide range of foundations and capacities, such as incorporating internationalization and ensuring the openness of documentation.&lt;/p&gt;
&lt;h3 id=&#34;q-why-should-new-and-existing-contributors-consider-joining-sig-usability&#34;&gt;Q. Why should new and existing contributors consider joining SIG Usability?&lt;/h3&gt;
&lt;p&gt;A. There are plenty of territories where new contributors can begin. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User research projects, where people can help understand the usability of the end-user experiences, including error messages, end-to-end tasks, etc.&lt;/li&gt;
&lt;li&gt;Accessibility guidelines for Kubernetes community artifacts, examples include: internationalization of documentation, color choices for people with color blindness, ensuring compatibility with screen reader technology, user interface design for core components with user interfaces, and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;q-what-do-you-do-to-help-new-contributors-get-started&#34;&gt;Q. What do you do to help new contributors get started?&lt;/h3&gt;
&lt;p&gt;A. New contributors can get started by shadowing one of the user interviews, going through user interview transcripts, analyzing them, and designing surveys.&lt;/p&gt;
&lt;p&gt;SIG Usability is also open to new project ideas. If you have an idea, we’ll do what we can to support it. There are regular SIG Meetings where people can ask their questions live. These meetings are also recorded for those who may not be able to attend. As always, you can reach out to us on Slack as well.&lt;/p&gt;
&lt;h3 id=&#34;q-what-does-the-survey-include&#34;&gt;Q. What does the survey include?&lt;/h3&gt;
&lt;p&gt;A. In simple terms, the survey gathers information about how people use Kubernetes, such as trends in learning to deploy a new system, error messages they receive, and workflows.&lt;/p&gt;
&lt;p&gt;One of our goals is to standardize the responses accordingly. The ultimate goal is to analyze survey responses for important user stories whose needs aren&#39;t being met.&lt;/p&gt;
&lt;h3 id=&#34;q-are-there-any-particular-skills-you-d-like-to-recruit-for-what-skills-are-contributors-to-sig-usability-likely-to-learn&#34;&gt;Q. Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?&lt;/h3&gt;
&lt;p&gt;A. Although contributing to SIG Usability does not have any pre-requisites as such, experience with user research, qualitative research, or prior experience with how to conduct an interview would be great plus points. Quantitative research, like survey design and screening, is also helpful and something that we expect contributors to learn.&lt;/p&gt;
&lt;h3 id=&#34;q-what-are-you-getting-positive-feedback-on-and-what-s-coming-up-next-for-sig-usability&#34;&gt;Q. What are you getting positive feedback on, and what’s coming up next for SIG Usability?&lt;/h3&gt;
&lt;p&gt;A. We have had new members joining and coming to monthly meetings regularly and showing interests in becoming a contributor and helping the community. We have also had a lot of people reach out to us via Slack showcasing their interest in the SIG.&lt;/p&gt;
&lt;p&gt;Currently, we are focused on finishing the study mentioned in our &lt;a href=&#34;https://www.youtube.com/watch?v=Byn0N_ZstE0&#34;&gt;talk&lt;/a&gt;, also our project for this year. We are always happy to have new contributors join us.&lt;/p&gt;
&lt;h3 id=&#34;q-any-closing-thoughts-resources-you-d-like-to-share&#34;&gt;Q: Any closing thoughts/resources you’d like to share?&lt;/h3&gt;
&lt;p&gt;A. We love meeting new contributors and assisting them in investigating different Kubernetes project spaces. We will work with and team up with other SIGs to facilitate engaging with end-users, running studies, and help them integrate accessible design practices into their development practices.&lt;/p&gt;
&lt;p&gt;Here are some resources for you to get started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-usability&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://groups.google.com/g/kubernetes-sig-usability&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/community/labels/sig%2Fusability&#34;&gt;Open Community Issues/PRs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://slack.k8s.io/&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.slack.com/archives/CLC5EF63T&#34;&gt;Slack channel #sig-usability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap Up&lt;/h2&gt;
&lt;p&gt;SIG Usability hosted a &lt;a href=&#34;https://www.youtube.com/watch?v=Byn0N_ZstE0&#34;&gt;KubeCon talk&lt;/a&gt; about studying Kubernetes users&#39; experiences. The talk focuses on updates to the user study projects, understanding who is using Kubernetes, what they are trying to achieve, how the project is addressing their needs, and where we need to improve the project and the client experience. Join the SIG&#39;s update to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream usability team as a contributor!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes API and Feature Removals In 1.22: Here’s What You Need To Know</title>
      <link>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</link>
      <pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Krishna Kilari (Amazon Web Services), Tim Bannister (The Scale Factory)&lt;/p&gt;
&lt;p&gt;As the Kubernetes API evolves, APIs are periodically reorganized or upgraded.
When APIs evolve, the old APIs they replace are deprecated, and eventually removed.
See &lt;a href=&#34;#kubernetes-api-removals&#34;&gt;Kubernetes API removals&lt;/a&gt; to read more about Kubernetes&#39;
policy on removing APIs.&lt;/p&gt;
&lt;p&gt;We want to make sure you&#39;re aware of some upcoming removals. These are
beta APIs that you can use in current, supported Kubernetes versions,
and they are already deprecated. The reason for all of these removals
is that they have been superseded by a newer, stable (“GA”) API.&lt;/p&gt;
&lt;p&gt;Kubernetes 1.22, due for release in August 2021, will remove a number of deprecated
APIs.
&lt;em&gt;Update&lt;/em&gt;:
&lt;a href=&#34;https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/&#34;&gt;Kubernetes 1.22: Reaching New Peaks&lt;/a&gt;
has details on the v1.22 release.&lt;/p&gt;
&lt;h2 id=&#34;api-changes&#34;&gt;API removals for Kubernetes v1.22&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;v1.22&lt;/strong&gt; release will stop serving the API versions we&#39;ve listed immediately below.
These are all beta APIs that were previously deprecated in favor of newer and more stable
API versions.&lt;/p&gt;
&lt;!-- sorted by API group --&gt;
&lt;ul&gt;
&lt;li&gt;Beta versions of the &lt;code&gt;ValidatingWebhookConfiguration&lt;/code&gt; and &lt;code&gt;MutatingWebhookConfiguration&lt;/code&gt; API (the  &lt;strong&gt;admissionregistration.k8s.io/v1beta1&lt;/strong&gt; API versions)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;CustomResourceDefinition&lt;/code&gt; API (&lt;strong&gt;apiextensions.k8s.io/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;APIService&lt;/code&gt; API (&lt;strong&gt;apiregistration.k8s.io/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;TokenReview&lt;/code&gt; API (&lt;strong&gt;authentication.k8s.io/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Beta API versions of &lt;code&gt;SubjectAccessReview&lt;/code&gt;, &lt;code&gt;LocalSubjectAccessReview&lt;/code&gt;, &lt;code&gt;SelfSubjectAccessReview&lt;/code&gt; (API versions from &lt;strong&gt;authorization.k8s.io/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;CertificateSigningRequest&lt;/code&gt; API (&lt;strong&gt;certificates.k8s.io/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;Lease&lt;/code&gt; API (&lt;strong&gt;coordination.k8s.io/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;All beta &lt;code&gt;Ingress&lt;/code&gt; APIs (the &lt;strong&gt;extensions/v1beta1&lt;/strong&gt; and &lt;strong&gt;networking.k8s.io/v1beta1&lt;/strong&gt; API versions)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Kubernetes documentation covers these
&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22&#34;&gt;API removals for v1.22&lt;/a&gt; and explains
how each of those APIs change between beta and stable.&lt;/p&gt;
&lt;h2 id=&#34;what-to-do&#34;&gt;What to do&lt;/h2&gt;
&lt;p&gt;We&#39;re going to run through each of the resources that are affected by these removals
and explain the steps you&#39;ll need to take.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;code&gt;Ingress&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;strong&gt;networking.k8s.io/v1&lt;/strong&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/&#34;&gt;Ingress&lt;/a&gt; API,
&lt;a href=&#34;https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#ingress-graduates-to-general-availability&#34;&gt;available since v1.19&lt;/a&gt;.&lt;br&gt;
The related API &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-class-v1/&#34;&gt;IngressClass&lt;/a&gt;
is designed to complement the &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt;
concept, allowing you to configure multiple kinds of Ingress within one cluster.
If you&#39;re currently using the deprecated
&lt;a href=&#34;https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-ingress-class-deprecated&#34;&gt;&lt;code&gt;kubernetes.io/ingress.class&lt;/code&gt;&lt;/a&gt;
annotation, plan to switch to using the &lt;code&gt;.spec.ingressClassName&lt;/code&gt; field instead.&lt;br&gt;
On any cluster running Kubernetes v1.19 or later, you can use the v1 API to
retrieve or update existing Ingress objects, even if they were created using an
older API version.
&lt;p&gt;When you convert an Ingress to the v1 API, you should review each rule in that Ingress.
Older Ingresses use the legacy &lt;code&gt;ImplementationSpecific&lt;/code&gt; path type. Instead of &lt;code&gt;ImplementationSpecific&lt;/code&gt;, switch &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types&#34;&gt;path matching&lt;/a&gt; to either &lt;code&gt;Prefix&lt;/code&gt; or &lt;code&gt;Exact&lt;/code&gt;. One of the benefits of moving to these alternative path types is that it becomes easier to migrate between different Ingress classes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ⓘ&lt;/strong&gt;  As well as upgrading &lt;em&gt;your&lt;/em&gt; own use of the Ingress API as a client, make sure that
every ingress controller that you use is compatible with the v1 Ingress API.
Read &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#prerequisites&#34;&gt;Ingress Prerequisites&lt;/a&gt;
for more context about Ingress and ingress controllers.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;code&gt;ValidatingWebhookConfiguration&lt;/code&gt; and &lt;code&gt;MutatingWebhookConfiguration&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;strong&gt;admissionregistration.k8s.io/v1&lt;/strong&gt; API versions of
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/validating-webhook-configuration-v1/&#34;&gt;ValidatingWebhookConfiguration&lt;/a&gt;
and &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/mutating-webhook-configuration-v1/&#34;&gt;MutatingWebhookConfiguration&lt;/a&gt;,
available since v1.16.&lt;br&gt;
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.&lt;/dd&gt;
&lt;dt&gt;&lt;code&gt;CustomResourceDefinition&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/&#34;&gt;CustomResourceDefinition&lt;/a&gt;
&lt;strong&gt;apiextensions.k8s.io/v1&lt;/strong&gt; API, available since v1.16.&lt;br&gt;
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. If you defined any custom resources in your cluster, those
are still served after you upgrade.
&lt;p&gt;If you&#39;re using external CustomResourceDefinitions, you can use
&lt;a href=&#34;#kubectl-convert&#34;&gt;&lt;code&gt;kubectl convert&lt;/code&gt;&lt;/a&gt; to translate existing manifests to use the newer API.
Because there are some functional differences between beta and stable CustomResourceDefinitions,
our advice is to test out each one to make sure it works how you expect after the upgrade.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;code&gt;APIService&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;strong&gt;apiregistration.k8s.io/v1&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/api-service-v1/&#34;&gt;APIService&lt;/a&gt;
API, available since v1.10.&lt;br&gt;
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.
If you already have API aggregation using an APIService object, this aggregation continues
to work after you upgrade.&lt;/dd&gt;
&lt;dt&gt;&lt;code&gt;TokenReview&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;strong&gt;authentication.k8s.io/v1&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-review-v1/&#34;&gt;TokenReview&lt;/a&gt;
API, available since v1.10.
&lt;p&gt;As well as serving this API via HTTP, the Kubernetes API server uses the same format to
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication&#34;&gt;send&lt;/a&gt;
TokenReviews to webhooks. The v1.22 release continues to use the v1beta1 API for TokenReviews
sent to webhooks by default. See &lt;a href=&#34;#looking-ahead&#34;&gt;Looking ahead&lt;/a&gt; for some specific tips about
switching to the stable API.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;code&gt;SubjectAccessReview&lt;/code&gt;, &lt;code&gt;SelfSubjectAccessReview&lt;/code&gt; and &lt;code&gt;LocalSubjectAccessReview&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;strong&gt;authorization.k8s.io/v1&lt;/strong&gt; versions of those
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/&#34;&gt;authorization APIs&lt;/a&gt;, available since v1.6.&lt;/dd&gt;
&lt;dt&gt;&lt;code&gt;CertificateSigningRequest&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;strong&gt;certificates.k8s.io/v1&lt;/strong&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/certificate-signing-request-v1/&#34;&gt;CertificateSigningRequest&lt;/a&gt;
API, available since v1.19.&lt;br&gt;
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. Existing issued certificates retain their validity when you upgrade.&lt;/dd&gt;
&lt;dt&gt;&lt;code&gt;Lease&lt;/code&gt;&lt;/dt&gt;
&lt;dd&gt;Migrate to use the &lt;strong&gt;coordination.k8s.io/v1&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/&#34;&gt;Lease&lt;/a&gt;
API, available since v1.14.&lt;br&gt;
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version.&lt;/dd&gt;
&lt;/dl&gt;
&lt;h3 id=&#34;kubectl-convert&#34;&gt;&lt;code&gt;kubectl convert&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;There is a plugin to &lt;code&gt;kubectl&lt;/code&gt; that provides the &lt;code&gt;kubectl convert&lt;/code&gt; subcommand.
It&#39;s an official plugin that you can download as part of Kubernetes.
See &lt;a href=&#34;https://kubernetes.io/releases/download/&#34;&gt;Download Kubernetes&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;You can use &lt;code&gt;kubectl convert&lt;/code&gt; to update manifest files to use a different API
version. For example, if you have a manifest in source control that uses the beta
Ingress API, you can check that definition out,
and run
&lt;code&gt;kubectl convert -f &amp;lt;manifest&amp;gt; --output-version &amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;&lt;/code&gt;.
You can use the &lt;code&gt;kubectl convert&lt;/code&gt; command to automatically convert an
existing manifest.&lt;/p&gt;
&lt;p&gt;For example, to convert an older Ingress definition to
&lt;code&gt;networking.k8s.io/v1&lt;/code&gt;, you can run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl convert -f ./legacy-ingress.yaml --output-version networking.k8s.io/v1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The automatic conversion uses a similar technique to how the Kubernetes control plane
updates objects that were originally created using an older API version. Because it&#39;s
a mechanical conversion, you might need to go in and change the manifest to adjust
defaults etc.&lt;/p&gt;
&lt;h3 id=&#34;rehearse-for-the-upgrade&#34;&gt;Rehearse for the upgrade&lt;/h3&gt;
&lt;p&gt;If you manage your cluster&#39;s API server component, you can try out these API
removals before you upgrade to Kubernetes v1.22.&lt;/p&gt;
&lt;p&gt;To do that, add the following to the kube-apiserver command line arguments:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--runtime-config=admissionregistration.k8s.io/v1beta1=false,apiextensions.k8s.io/v1beta1=false,apiregistration.k8s.io/v1beta1=false,authentication.k8s.io/v1beta1=false,authorization.k8s.io/v1beta1=false,certificates.k8s.io/v1beta1=false,coordination.k8s.io/v1beta1=false,extensions/v1beta1/ingresses=false,networking.k8s.io/v1beta1=false&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(as a side effect, this also turns off v1beta1 of EndpointSlice - watch out for
that when you&#39;re testing).&lt;/p&gt;
&lt;p&gt;Once you&#39;ve switched all the kube-apiservers in your cluster to use that setting,
those beta APIs are removed. You can test that API clients (&lt;code&gt;kubectl&lt;/code&gt;, deployment
tools, custom controllers etc) still work how you expect, and you can revert if
you need to without having to plan a more disruptive downgrade.&lt;/p&gt;
&lt;h3 id=&#34;advice-for-software-authors&#34;&gt;Advice for software authors&lt;/h3&gt;
&lt;p&gt;Maybe you&#39;re reading this because you&#39;re a developer of an addon or other
component that integrates with Kubernetes?&lt;/p&gt;
&lt;p&gt;If you develop an Ingress controller, webhook authenticator, an API aggregation, or
any other tool that relies on these deprecated APIs, you should already have started
to switch your software over.&lt;/p&gt;
&lt;p&gt;You can use the tips in
&lt;a href=&#34;#rehearse-for-the-upgrade&#34;&gt;Rehearse for the upgrade&lt;/a&gt; to run your own Kubernetes
cluster that only uses the new APIs, and make sure that your code works OK.
For your documentation, make sure readers are aware of any steps they should take
for the Kubernetes v1.22 upgrade.&lt;/p&gt;
&lt;p&gt;Where possible, give your users a hand to adopt the new APIs early - perhaps in a
test environment - so they can give you feedback about any problems.&lt;/p&gt;
&lt;p&gt;There are some &lt;a href=&#34;#looking-ahead&#34;&gt;more deprecations&lt;/a&gt; coming in Kubernetes v1.25,
so plan to have those covered too.&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-api-removals&#34;&gt;Kubernetes API removals&lt;/h2&gt;
&lt;p&gt;Here&#39;s some background about why Kubernetes removes some APIs, and also a promise
about &lt;em&gt;stable&lt;/em&gt; APIs in Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes follows a defined
&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34;&gt;deprecation policy&lt;/a&gt; for its
features, including the Kubernetes API. That policy allows for replacing stable
(“GA”) APIs from Kubernetes. Importantly, this policy means that a stable API only
be deprecated when a newer stable version of that same API is available.&lt;/p&gt;
&lt;p&gt;That stability guarantee matters: if you&#39;re using a stable Kubernetes API, there
won&#39;t ever be a new version released that forces you to switch to an alpha or beta
feature.&lt;/p&gt;
&lt;p&gt;Earlier stages are different. Alpha features are under test and potentially
incomplete. Almost always, alpha features are disabled by default.
Kubernetes releases can and do remove alpha features that haven&#39;t worked out.&lt;/p&gt;
&lt;p&gt;After alpha, comes beta. These features are typically enabled by default; if the
testing works out, the feature can graduate to stable. If not, it might need
a redesign.&lt;/p&gt;
&lt;p&gt;Last year, Kubernetes officially
&lt;a href=&#34;https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/#avoiding-permanent-beta&#34;&gt;adopted&lt;/a&gt;
a policy for APIs that have reached their beta phase:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For Kubernetes REST APIs, when a new feature&#39;s API reaches beta, that starts
a countdown. The beta-quality API now has three releases …
to either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reach GA, and deprecate the beta, or&lt;/li&gt;
&lt;li&gt;have a new beta version (and deprecate the previous beta).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;At the time of that article, three Kubernetes releases equated to roughly nine
calendar months. Later that same month, Kubernetes
adopted a new
release cadence of three releases per calendar year, so the countdown period is
now roughly twelve calendar months.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Whether an API removal is because of a beta feature graduating to stable, or
because that API hasn&#39;t proved successful, Kubernetes will continue to remove
APIs by following its deprecation policy and making sure that migration options
are documented.&lt;/p&gt;
&lt;h3 id=&#34;looking-ahead&#34;&gt;Looking ahead&lt;/h3&gt;
&lt;p&gt;There&#39;s a setting that&#39;s relevant if you use webhook authentication checks.
A future Kubernetes release will switch to sending TokenReview objects
to webhooks using the &lt;code&gt;authentication.k8s.io/v1&lt;/code&gt; API by default. At the moment,
the default is to send &lt;code&gt;authentication.k8s.io/v1beta1&lt;/code&gt; TokenReviews to webhooks,
and that&#39;s still the default for Kubernetes v1.22.
However, you can switch over to the stable API right now if you want:
add &lt;code&gt;--authentication-token-webhook-version=v1&lt;/code&gt; to the command line options for
the kube-apiserver, and check that webhooks for authentication still work how you
expected.&lt;/p&gt;
&lt;p&gt;Once you&#39;re happy it works OK, you can leave the &lt;code&gt;--authentication-token-webhook-version=v1&lt;/code&gt;
option set across your control plane.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;v1.25&lt;/strong&gt; release that&#39;s planned for next year will stop serving beta versions of
several Kubernetes APIs that are stable right now and have been for some time.
The same v1.25 release will &lt;strong&gt;remove&lt;/strong&gt; PodSecurityPolicy, which is deprecated and won&#39;t
graduate to stable. See
&lt;a href=&#34;https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/&#34;&gt;PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;p&gt;The official &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25&#34;&gt;list of API removals&lt;/a&gt;
planned for Kubernetes 1.25 is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The beta &lt;code&gt;CronJob&lt;/code&gt; API (&lt;strong&gt;batch/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;EndpointSlice&lt;/code&gt; API (&lt;strong&gt;networking.k8s.io/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;PodDisruptionBudget&lt;/code&gt; API (&lt;strong&gt;policy/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;The beta &lt;code&gt;PodSecurityPolicy&lt;/code&gt; API (&lt;strong&gt;policy/v1beta1&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;want-to-know-more&#34;&gt;Want to know more?&lt;/h2&gt;
&lt;p&gt;Deprecations are announced in the Kubernetes release notes. You can see the announcements
of pending deprecations in the release notes for
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecations&#34;&gt;1.19&lt;/a&gt;,
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation&#34;&gt;1.20&lt;/a&gt;,
and &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation&#34;&gt;1.21&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For information on the process of deprecation and removal, check out the official Kubernetes
&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api&#34;&gt;deprecation policy&lt;/a&gt;
document.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Announcing Kubernetes Community Group Annual Reports</title>
      <link>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</link>
      <pubDate>Mon, 28 Jun 2021 10:00:00 -0800</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Divya Mohan&lt;/p&gt;

&lt;figure&gt;&lt;a href=&#34;https://www.cncf.io/reports/kubernetes-community-annual-report-2020/&#34;&gt;
    &lt;img src=&#34;https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/k8s_annual_report_2020.svg&#34;
         alt=&#34;Community annual report 2020&#34;/&gt; &lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Given the growth and scale of the Kubernetes project, the existing reporting mechanisms were proving to be inadequate and challenging.
Kubernetes is a large open source project. With over 100000 commits just to the main k/kubernetes repository, hundreds of other code
repositories in the project, and thousands of contributors, there&#39;s a lot going on. In fact, there are 37 contributor groups at the time of
writing. We also value all forms of contribution and not just code changes.&lt;/p&gt;
&lt;p&gt;With that context in mind, the challenge of reporting on all this activity was a call to action for exploring better options. Therefore
inspired by the Apache Software Foundation’s &lt;a href=&#34;https://www.apache.org/foundation/board/reporting&#34;&gt;open guide to PMC Reporting&lt;/a&gt; and the
&lt;a href=&#34;https://www.cncf.io/cncf-annual-report-2020/&#34;&gt;CNCF project Annual Reporting&lt;/a&gt;, the Kubernetes project is proud to announce the
&lt;strong&gt;Kubernetes Community Group Annual Reports for Special Interest Groups (SIGs) and Working Groups (WGs)&lt;/strong&gt;. In its flagship edition,
the &lt;a href=&#34;https://www.cncf.io/reports/kubernetes-community-annual-report-2020/&#34;&gt;2020 Summary report&lt;/a&gt; focuses on bettering the
Kubernetes ecosystem by assessing and promoting the healthiness of the groups within the upstream community.&lt;/p&gt;
&lt;p&gt;Previously, the mechanisms for the Kubernetes project overall to report on groups and their activities were
&lt;a href=&#34;https://k8s.devstats.cncf.io/&#34;&gt;devstats&lt;/a&gt;, GitHub data, issues, to measure the healthiness of a given UG/WG/SIG/Committee. As a
project spanning several diverse communities, it was essential to have something that captured the human side of things. With 50,000+
contributors, it’s easy to assume that the project has enough help and this report surfaces more information than /help-wanted and
/good-first-issue for end users. This is how we sustain the project. Paraphrasing one of the Steering Committee members,
&lt;a href=&#34;https://github.com/parispittman&#34;&gt;Paris Pittman&lt;/a&gt;, “There was a requirement for tighter feedback loops - ones that involved more than just
GitHub data and issues. Given that Kubernetes, as a project, has grown in scale and number of contributors over the years, we have
outgrown the existing reporting mechanisms.&amp;quot;&lt;/p&gt;
&lt;p&gt;The existing communication channels between the Steering committee members and the folks leading the groups and committees were also required
to be made as open and as bi-directional as possible. Towards achieving this very purpose, every group and committee has been assigned a
liaison from among the steering committee members for kick off, help, or guidance needed throughout the process. According to
&lt;a href=&#34;https://github.com/dims&#34;&gt;Davanum Srinivas a.k.a. dims&lt;/a&gt;, “... That was one of the main motivations behind this report. People (leading the
groups/committees) know that they can reach out to us and there’s a vehicle for them to reach out to us… This is our way of setting up a
two-way feedback for them.&amp;quot; The progress on these action items would be updated and tracked on the monthly Steering Committee meetings
ensuring that this is not a one-off activity. Quoting &lt;a href=&#34;https://github.com/nikhita&#34;&gt;Nikhita Raghunath&lt;/a&gt;, one of the Steering Committee members,
“... Once we have a base, the liaisons will work with these groups to ensure that the problems are resolved. When we have a report next year,
we’ll have a look at the progress made and how we could still do better. But the idea is definitely to not stop at the report.”&lt;/p&gt;
&lt;p&gt;With this report, we hope to empower our end user communities with information that they can use to identify ways in which they can support
the project as well as a sneak peek into the roadmap for upcoming features. As a community, we thrive on feedback and would love to hear your
views about the report. You can get in touch with the &lt;a href=&#34;https://github.com/kubernetes/steering#contact&#34;&gt;Steering Committee&lt;/a&gt; via
&lt;a href=&#34;https://kubernetes.slack.com/messages/steering-committee&#34;&gt;Slack&lt;/a&gt; or via the &lt;a href=&#34;steering@kubernetes.io&#34;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Writing a Controller for Pod Labels</title>
      <link>https://kubernetes.io/blog/2021/06/21/writing-a-controller-for-pod-labels/</link>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/06/21/writing-a-controller-for-pod-labels/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Arthur Busser (Padok)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Operators&lt;/a&gt; are proving to be an excellent solution to
running stateful distributed applications in Kubernetes. Open source tools like
the &lt;a href=&#34;https://sdk.operatorframework.io/&#34;&gt;Operator SDK&lt;/a&gt; provide ways to build reliable and maintainable
operators, making it easier to extend Kubernetes and implement custom
scheduling.&lt;/p&gt;
&lt;p&gt;Kubernetes operators run complex software inside your cluster. The open source
community has already built &lt;a href=&#34;https://operatorhub.io/&#34;&gt;many operators&lt;/a&gt; for distributed
applications like Prometheus, Elasticsearch, or Argo CD. Even outside of
open source, operators can help to bring new functionality to your Kubernetes
cluster.&lt;/p&gt;
&lt;p&gt;An operator is a set of &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;custom resources&lt;/a&gt; and a
set of &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/controller/&#34;&gt;controllers&lt;/a&gt;. A controller watches for changes to specific
resources in the Kubernetes API and reacts by creating, updating, or deleting
resources.&lt;/p&gt;
&lt;p&gt;The Operator SDK is best suited for building fully-featured operators.
Nonetheless, you can use it to write a single controller. This post will walk
you through writing a Kubernetes controller in Go that will add a &lt;code&gt;pod-name&lt;/code&gt;
label to pods that have a specific annotation.&lt;/p&gt;
&lt;h2 id=&#34;why-do-we-need-a-controller-for-this&#34;&gt;Why do we need a controller for this?&lt;/h2&gt;
&lt;p&gt;I recently worked on a project where we needed to create a Service that routed
traffic to a specific Pod in a ReplicaSet. The problem is that a Service can
only select pods by label, and all pods in a ReplicaSet have the same labels.
There are two ways to solve this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a Service without a selector and manage the Endpoints or
EndpointSlices for that Service directly. We would need to write a custom
controller to insert our Pod&#39;s IP address into those resources.&lt;/li&gt;
&lt;li&gt;Add a label to the Pod with a unique value. We could then use this label in
our Service&#39;s selector. Again, we would need to write a custom controller to
add this label.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A controller is a control loop that tracks one or more Kubernetes resource
types. The controller from option n°2 above only needs to track pods, which
makes it simpler to implement. This is the option we are going to walk through
by writing a Kubernetes controller that adds a &lt;code&gt;pod-name&lt;/code&gt; label to our pods.&lt;/p&gt;
&lt;p&gt;StatefulSets &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-name-label&#34;&gt;do this natively&lt;/a&gt; by adding a
&lt;code&gt;pod-name&lt;/code&gt; label to each Pod in the set. But what if we don&#39;t want to or can&#39;t
use StatefulSets?&lt;/p&gt;
&lt;p&gt;We rarely create pods directly; most often, we use a Deployment, ReplicaSet, or
another high-level resource. We can specify labels to add to each Pod in the
PodSpec, but not with dynamic values, so no way to replicate a StatefulSet&#39;s
&lt;code&gt;pod-name&lt;/code&gt; label.&lt;/p&gt;
&lt;p&gt;We tried using a &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook&#34;&gt;mutating admission webhook&lt;/a&gt;. When
anyone creates a Pod, the webhook patches the Pod with a label containing the
Pod&#39;s name. Disappointingly, this does not work: not all pods have a name before
being created. For instance, when the ReplicaSet controller creates a Pod, it
sends a &lt;code&gt;namePrefix&lt;/code&gt; to the Kubernetes API server and not a &lt;code&gt;name&lt;/code&gt;. The API
server generates a unique name before persisting the new Pod to etcd, but only
after calling our admission webhook. So in most cases, we can&#39;t know a Pod&#39;s
name with a mutating webhook.&lt;/p&gt;
&lt;p&gt;Once a Pod exists in the Kubernetes API, it is mostly immutable, but we can
still add a label. We can even do so from the command line:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl label my-pod my-label-key&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;my-label-value
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We need to watch for changes to any pods in the Kubernetes API and add the label
we want. Rather than do this manually, we are going to write a controller that
does it for us.&lt;/p&gt;
&lt;h2 id=&#34;bootstrapping-a-controller-with-the-operator-sdk&#34;&gt;Bootstrapping a controller with the Operator SDK&lt;/h2&gt;
&lt;p&gt;A controller is a reconciliation loop that reads the desired state of a resource
from the Kubernetes API and takes action to bring the cluster&#39;s actual state
closer to the desired state.&lt;/p&gt;
&lt;p&gt;In order to write this controller as quickly as possible, we are going to use
the Operator SDK. If you don&#39;t have it installed, follow the
&lt;a href=&#34;https://sdk.operatorframework.io/docs/installation/&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-terminal&#34; data-lang=&#34;terminal&#34;&gt;$ operator-sdk version
operator-sdk version: &amp;quot;v1.4.2&amp;quot;, commit: &amp;quot;4b083393be65589358b3e0416573df04f4ae8d9b&amp;quot;, kubernetes version: &amp;quot;v1.19.4&amp;quot;, go version: &amp;quot;go1.15.8&amp;quot;, GOOS: &amp;quot;darwin&amp;quot;, GOARCH: &amp;quot;amd64&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&#39;s create a new directory to write our controller in:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir label-operator &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;cd&lt;/span&gt; label-operator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, let&#39;s initialize a new operator, to which we will add a single controller.
To do this, you will need to specify a domain and a repository. The domain
serves as a prefix for the group your custom Kubernetes resources will belong
to. Because we are not going to be defining custom resources, the domain does
not matter. The repository is going to be the name of the Go module we are going
to write. By convention, this is the repository where you will be storing your
code.&lt;/p&gt;
&lt;p&gt;As an example, here is the command I ran:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Feel free to change the domain and repo values.&lt;/span&gt;
operator-sdk init --domain&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;padok.fr --repo&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;github.com/busser/label-operator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we need a create a new controller. This controller will handle pods and
not a custom resource, so no need to generate the resource code. Let&#39;s run this
command to scaffold the code we need:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;operator-sdk create api --group&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;core --version&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;v1 --kind&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;Pod --controller&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a2f&#34;&gt;true&lt;/span&gt; --resource&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a2f&#34;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We now have a new file: &lt;code&gt;controllers/pod_controller.go&lt;/code&gt;. This file contains a
&lt;code&gt;PodReconciler&lt;/code&gt; type with two methods that we need to implement. The first is
&lt;code&gt;Reconcile&lt;/code&gt;, and it looks like this for now:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; (r &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;PodReconciler) &lt;span style=&#34;color:#00a000&#34;&gt;Reconcile&lt;/span&gt;(ctx context.Context, req ctrl.Request) (ctrl.Result, &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;error&lt;/span&gt;) {
    _ = r.Log.&lt;span style=&#34;color:#00a000&#34;&gt;WithValues&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;pod&amp;#34;&lt;/span&gt;, req.NamespacedName)

    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// your logic here
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;Reconcile&lt;/code&gt; method is called whenever a Pod is created, updated, or deleted.
The name and namespace of the Pod are in the &lt;code&gt;ctrl.Request&lt;/code&gt; the method receives
as a parameter.&lt;/p&gt;
&lt;p&gt;The second method is &lt;code&gt;SetupWithManager&lt;/code&gt; and for now it looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; (r &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;PodReconciler) &lt;span style=&#34;color:#00a000&#34;&gt;SetupWithManager&lt;/span&gt;(mgr ctrl.Manager) &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;error&lt;/span&gt; {
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.&lt;span style=&#34;color:#00a000&#34;&gt;NewControllerManagedBy&lt;/span&gt;(mgr).
        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// Uncomment the following line adding a pointer to an instance of the controlled resource as an argument
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// For().
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#00a000&#34;&gt;Complete&lt;/span&gt;(r)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;SetupWithManager&lt;/code&gt; method is called when the operator starts. It serves to
tell the operator framework what types our &lt;code&gt;PodReconciler&lt;/code&gt; needs to watch. To
use the same &lt;code&gt;Pod&lt;/code&gt; type used by Kubernetes internally, we need to import some of
its code. All of the Kubernetes source code is open source, so you can import
any part you like in your own Go code. You can find a complete list of available
packages in the Kubernetes source code or &lt;a href=&#34;https://pkg.go.dev/k8s.io/api&#34;&gt;here on pkg.go.dev&lt;/a&gt;. To
use pods, we need the &lt;code&gt;k8s.io/api/core/v1&lt;/code&gt; package.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;package&lt;/span&gt; controllers

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; (
    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// other imports...
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;    corev1 &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s.io/api/core/v1&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// other imports...
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Lets use the &lt;code&gt;Pod&lt;/code&gt; type in &lt;code&gt;SetupWithManager&lt;/code&gt; to tell the operator framework we
want to watch pods:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; (r &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;PodReconciler) &lt;span style=&#34;color:#00a000&#34;&gt;SetupWithManager&lt;/span&gt;(mgr ctrl.Manager) &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;error&lt;/span&gt; {
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.&lt;span style=&#34;color:#00a000&#34;&gt;NewControllerManagedBy&lt;/span&gt;(mgr).
        &lt;span style=&#34;color:#00a000&#34;&gt;For&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;corev1.Pod{}).
        &lt;span style=&#34;color:#00a000&#34;&gt;Complete&lt;/span&gt;(r)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before moving on, we should set the RBAC permissions our controller needs. Above
the &lt;code&gt;Reconcile&lt;/code&gt; method, we have some default permissions:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +kubebuilder:rbac:groups=core,resources=pods/status,verbs=get;update;patch
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +kubebuilder:rbac:groups=core,resources=pods/finalizers,verbs=update
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We don&#39;t need all of those. Our controller will never interact with a Pod&#39;s
status or its finalizers. It only needs to read and update pods. Lets remove the
unnecessary permissions and keep only what we need:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;update;patch
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We are now ready to write our controller&#39;s reconciliation logic.&lt;/p&gt;
&lt;h2 id=&#34;implementing-reconciliation&#34;&gt;Implementing reconciliation&lt;/h2&gt;
&lt;p&gt;Here is what we want our &lt;code&gt;Reconcile&lt;/code&gt; method to do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use the Pod&#39;s name and namespace from the &lt;code&gt;ctrl.Request&lt;/code&gt; to fetch the Pod
from the Kubernetes API.&lt;/li&gt;
&lt;li&gt;If the Pod has an &lt;code&gt;add-pod-name-label&lt;/code&gt; annotation, add a &lt;code&gt;pod-name&lt;/code&gt; label to
the Pod; if the annotation is missing, don&#39;t add the label.&lt;/li&gt;
&lt;li&gt;Update the Pod in the Kubernetes API to persist the changes made.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Lets define some constants for the annotation and label:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;const&lt;/span&gt; (
    addPodNameLabelAnnotation = &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;padok.fr/add-pod-name-label&amp;#34;&lt;/span&gt;
    podNameLabel              = &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;padok.fr/pod-name&amp;#34;&lt;/span&gt;
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first step in our reconciliation function is to fetch the Pod we are working
on from the Kubernetes API:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// Reconcile handles a reconciliation request for a Pod.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// If the Pod has the addPodNameLabelAnnotation annotation, then Reconcile
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// will make sure the podNameLabel label is present with the correct value.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// If the annotation is absent, then Reconcile will make sure the label is too.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; (r &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;PodReconciler) &lt;span style=&#34;color:#00a000&#34;&gt;Reconcile&lt;/span&gt;(ctx context.Context, req ctrl.Request) (ctrl.Result, &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;error&lt;/span&gt;) {
    log &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; r.Log.&lt;span style=&#34;color:#00a000&#34;&gt;WithValues&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;pod&amp;#34;&lt;/span&gt;, req.NamespacedName)

    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/*
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;        Step 0: Fetch the Pod from the Kubernetes API.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;    */&lt;/span&gt;

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;var&lt;/span&gt; pod corev1.Pod
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; r.&lt;span style=&#34;color:#00a000&#34;&gt;Get&lt;/span&gt;(ctx, req.NamespacedName, &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;pod); err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
        log.&lt;span style=&#34;color:#00a000&#34;&gt;Error&lt;/span&gt;(err, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;unable to fetch Pod&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, err
    }

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Our &lt;code&gt;Reconcile&lt;/code&gt; method will be called when a Pod is created, updated, or
deleted. In the deletion case, our call to &lt;code&gt;r.Get&lt;/code&gt; will return a specific error.
Let&#39;s import the package that defines this error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;package&lt;/span&gt; controllers

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; (
    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// other imports...
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;    apierrors &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// other imports...
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can now handle this specific error and — since our controller does not care
about deleted pods — explicitly ignore it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/*
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;        Step 0: Fetch the Pod from the Kubernetes API.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;    */&lt;/span&gt;

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;var&lt;/span&gt; pod corev1.Pod
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; r.&lt;span style=&#34;color:#00a000&#34;&gt;Get&lt;/span&gt;(ctx, req.NamespacedName, &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;pod); err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; apierrors.&lt;span style=&#34;color:#00a000&#34;&gt;IsNotFound&lt;/span&gt;(err) {
            &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// we&amp;#39;ll ignore not-found errors, since we can get them on deleted requests.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt;
        }
        log.&lt;span style=&#34;color:#00a000&#34;&gt;Error&lt;/span&gt;(err, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;unable to fetch Pod&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, err
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, lets edit our Pod so that our dynamic label is present if and only if our
annotation is present:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/*
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;        Step 1: Add or remove the label.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;    */&lt;/span&gt;

    labelShouldBePresent &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; pod.Annotations[addPodNameLabelAnnotation] &lt;span style=&#34;color:#666&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
    labelIsPresent &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; pod.Labels[podNameLabel] &lt;span style=&#34;color:#666&#34;&gt;==&lt;/span&gt; pod.Name

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; labelShouldBePresent &lt;span style=&#34;color:#666&#34;&gt;==&lt;/span&gt; labelIsPresent {
        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// The desired state and actual state of the Pod are the same.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// No further action is required by the operator at this moment.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        log.&lt;span style=&#34;color:#00a000&#34;&gt;Info&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;no update required&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt;
    }

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; labelShouldBePresent {
        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// If the label should be set but is not, set it.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; pod.Labels &lt;span style=&#34;color:#666&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
            pod.Labels = &lt;span style=&#34;color:#a2f&#34;&gt;make&lt;/span&gt;(&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;map&lt;/span&gt;[&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;]&lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;)
        }
        pod.Labels[podNameLabel] = pod.Name
        log.&lt;span style=&#34;color:#00a000&#34;&gt;Info&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;adding label&amp;#34;&lt;/span&gt;)
    } &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;else&lt;/span&gt; {
        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// If the label should not be set but is, remove it.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#a2f&#34;&gt;delete&lt;/span&gt;(pod.Labels, podNameLabel)
        log.&lt;span style=&#34;color:#00a000&#34;&gt;Info&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;removing label&amp;#34;&lt;/span&gt;)
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, let&#39;s push our updated Pod to the Kubernetes API:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/*
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;        Step 2: Update the Pod in the Kubernetes API.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;    */&lt;/span&gt;

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; r.&lt;span style=&#34;color:#00a000&#34;&gt;Update&lt;/span&gt;(ctx, &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;pod); err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
        log.&lt;span style=&#34;color:#00a000&#34;&gt;Error&lt;/span&gt;(err, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;unable to update Pod&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, err
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When writing our updated Pod to the Kubernetes API, there is a risk that the Pod
has been updated or deleted since we first read it. When writing a Kubernetes
controller, we should keep in mind that we are not the only actors in the
cluster. When this happens, the best thing to do is start the reconciliation
from scratch, by requeuing the event. Lets do exactly that:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;/*
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;        Step 2: Update the Pod in the Kubernetes API.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;    */&lt;/span&gt;

    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; r.&lt;span style=&#34;color:#00a000&#34;&gt;Update&lt;/span&gt;(ctx, &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;pod); err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; apierrors.&lt;span style=&#34;color:#00a000&#34;&gt;IsConflict&lt;/span&gt;(err) {
            &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// The Pod has been updated since we read it.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// Requeue the Pod to try to reconciliate again.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{Requeue: &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;}, &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt;
        }
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; apierrors.&lt;span style=&#34;color:#00a000&#34;&gt;IsNotFound&lt;/span&gt;(err) {
            &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// The Pod has been deleted since we read it.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// Requeue the Pod to try to reconciliate again.
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;            &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{Requeue: &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;}, &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt;
        }
        log.&lt;span style=&#34;color:#00a000&#34;&gt;Error&lt;/span&gt;(err, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;unable to update Pod&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, err
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&#39;s remember to return successfully at the end of the method:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; ctrl.Result{}, &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And that&#39;s it! We are now ready to run the controller on our cluster.&lt;/p&gt;
&lt;h2 id=&#34;run-the-controller-on-your-cluster&#34;&gt;Run the controller on your cluster&lt;/h2&gt;
&lt;p&gt;To run our controller on your cluster, we need to run the operator. For that,
all you will need is &lt;code&gt;kubectl&lt;/code&gt;. If you don&#39;t have a Kubernetes cluster at hand,
I recommend you start one locally with &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/#installation&#34;&gt;KinD (Kubernetes in Docker)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All it takes to run the operator from your machine is this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;make run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After a few seconds, you should see the operator&#39;s logs. Notice that our
controller&#39;s &lt;code&gt;Reconcile&lt;/code&gt; method was called for all pods already running in the
cluster.&lt;/p&gt;
&lt;p&gt;Let&#39;s keep the operator running and, in another terminal, create a new Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl run --image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;nginx my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The operator should quickly print some logs, indicating that it reacted to the
Pod&#39;s creation and subsequent changes in status:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;INFO    controllers.Pod no update required  {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO    controllers.Pod no update required  {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO    controllers.Pod no update required  {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO    controllers.Pod no update required  {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Lets check the Pod&#39;s labels:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-terminal&#34; data-lang=&#34;terminal&#34;&gt;$ kubectl get pod my-nginx --show-labels
NAME       READY   STATUS    RESTARTS   AGE   LABELS
my-nginx   1/1     Running   0          11m   run=my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&#39;s add an annotation to the Pod so that our controller knows to add our
dynamic label to it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl annotate pod my-nginx padok.fr/add-pod-name-label&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a2f&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice that the controller immediately reacted and produced a new line in its
logs:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;INFO    controllers.Pod adding label    {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-terminal&#34; data-lang=&#34;terminal&#34;&gt;$ kubectl get pod my-nginx --show-labels
NAME       READY   STATUS    RESTARTS   AGE   LABELS
my-nginx   1/1     Running   0          13m   padok.fr/pod-name=my-nginx,run=my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Bravo! You just successfully wrote a Kubernetes controller capable of adding
labels with dynamic values to resources in your cluster.&lt;/p&gt;
&lt;p&gt;Controllers and operators, both big and small, can be an important part of your
Kubernetes journey. Writing operators is easier now than it has ever been. The
possibilities are endless.&lt;/p&gt;
&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;
&lt;p&gt;If you want to go further, I recommend starting by deploying your controller or
operator inside a cluster. The &lt;code&gt;Makefile&lt;/code&gt; generated by the Operator SDK will do
most of the work.&lt;/p&gt;
&lt;p&gt;When deploying an operator to production, it is always a good idea to implement
robust testing. The first step in that direction is to write unit tests.
&lt;a href=&#34;https://sdk.operatorframework.io/docs/building-operators/golang/testing/&#34;&gt;This documentation&lt;/a&gt; will guide you in writing tests for
your operator. I wrote tests for the operator we just wrote; you can find all of
my code in &lt;a href=&#34;https://github.com/busser/label-operator&#34;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-to-learn-more&#34;&gt;How to learn more?&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://sdk.operatorframework.io/docs/&#34;&gt;Operator SDK documentation&lt;/a&gt; goes into detail on how you
can go further and implement more complex operators.&lt;/p&gt;
&lt;p&gt;When modeling a more complex use-case, a single controller acting on built-in
Kubernetes types may not be enough. You may need to build a more complex
operator with &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;Custom Resource Definitions (CRDs)&lt;/a&gt;
and multiple controllers. The Operator SDK is a great tool to help you do this.&lt;/p&gt;
&lt;p&gt;If you want to discuss building an operator, join the &lt;a href=&#34;https://kubernetes.slack.com/messages/kubernetes-operators&#34;&gt;#kubernetes-operator&lt;/a&gt;
channel in the &lt;a href=&#34;https://slack.k8s.io/&#34;&gt;Kubernetes Slack workspace&lt;/a&gt;!&lt;/p&gt;
&lt;!-- Links --&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Using Finalizers to Control Deletion</title>
      <link>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aaron Alpar (Kasten)&lt;/p&gt;
&lt;p&gt;Deleting objects in Kubernetes can be challenging. You may think you’ve deleted something, only to find it still persists. While issuing a &lt;code&gt;kubectl delete&lt;/code&gt; command and hoping for the best might work for day-to-day operations, understanding how Kubernetes &lt;code&gt;delete&lt;/code&gt; commands operate will help you understand why some objects linger after deletion.&lt;/p&gt;
&lt;p&gt;In this post, I’ll look at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What properties of a resource govern deletion&lt;/li&gt;
&lt;li&gt;How finalizers and owner references impact object deletion&lt;/li&gt;
&lt;li&gt;How the propagation policy can be used to change the order of deletions&lt;/li&gt;
&lt;li&gt;How deletion works, with examples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For simplicity, all examples will use ConfigMaps and basic shell commands to demonstrate the process. We’ll explore how the commands work and discuss repercussions and results from using them in practice.&lt;/p&gt;
&lt;h2 id=&#34;the-basic-delete&#34;&gt;The basic &lt;code&gt;delete&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Kubernetes has several different commands you can use that allow you to create, read, update, and delete objects. For the purpose of this blog post, we’ll focus on four &lt;code&gt;kubectl&lt;/code&gt; commands: &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;get&lt;/code&gt;, &lt;code&gt;patch&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here are examples of the basic &lt;code&gt;kubectl delete&lt;/code&gt; command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl create configmap mymap
configmap/mymap created
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get configmap/mymap
NAME    DATA   AGE
mymap   0      12s
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl delete configmap/mymap
configmap &amp;quot;mymap&amp;quot; deleted
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get configmap/mymap
Error from server (NotFound): configmaps &amp;quot;mymap&amp;quot; not found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Shell commands preceded by &lt;code&gt;$&lt;/code&gt; are followed by their output. You can see that we begin with a &lt;code&gt;kubectl create configmap mymap&lt;/code&gt;, which will create the empty configmap &lt;code&gt;mymap&lt;/code&gt;. Next, we need to &lt;code&gt;get&lt;/code&gt; the configmap to prove it exists. We can then delete that configmap. Attempting to &lt;code&gt;get&lt;/code&gt; it again produces an HTTP 404 error, which means the configmap is not found.&lt;/p&gt;
&lt;p&gt;The state diagram for the basic &lt;code&gt;delete&lt;/code&gt; command is very simple:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2021-05-14-using-finalizers-to-control-deletion/state-diagram-delete.png&#34;
         alt=&#34;State diagram for delete&#34; width=&#34;495&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;State diagram for delete&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Although this operation is straightforward, other factors may interfere with the deletion, including finalizers and owner references.&lt;/p&gt;
&lt;h2 id=&#34;understanding-finalizers&#34;&gt;Understanding Finalizers&lt;/h2&gt;
&lt;p&gt;When it comes to understanding resource deletion in Kubernetes, knowledge of how finalizers work is helpful and can help you understand why some objects don’t get deleted.&lt;/p&gt;
&lt;p&gt;Finalizers are keys on resources that signal pre-delete operations. They control the garbage collection on resources, and are designed to alert controllers what cleanup operations to perform prior to removing a resource. However, they don’t necessarily name code that should be executed; finalizers on resources are basically just lists of keys much like annotations. Like annotations, they can be manipulated.&lt;/p&gt;
&lt;p&gt;Some common finalizers you’ve likely encountered are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubernetes.io/pv-protection&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubernetes.io/pvc-protection&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The finalizers above are used on volumes to prevent accidental deletion. Similarly, some finalizers can be used to prevent deletion of any resource but are not managed by any controller.&lt;/p&gt;
&lt;p&gt;Below with a custom configmap, which has no properties but contains a finalizer:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: mymap
  finalizers:
  - kubernetes
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The configmap resource controller doesn&#39;t understand what to do with the &lt;code&gt;kubernetes&lt;/code&gt; finalizer key. I term these “dead” finalizers for configmaps as it is normally used on namespaces. Here’s what happen upon attempting to delete the configmap:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl delete configmap/mymap &amp;amp;
configmap &amp;quot;mymap&amp;quot; deleted
jobs
[1]+  Running kubectl delete configmap/mymap
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Kubernetes will report back that the object has been deleted, however, it hasn’t been deleted in a traditional sense. Rather, it’s in the process of deletion. When we attempt to &lt;code&gt;get&lt;/code&gt; that object again, we discover the object has been modified to include the deletion timestamp.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get configmap/mymap -o yaml
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2020-10-22T21:30:18Z&amp;quot;
  deletionGracePeriodSeconds: 0
  deletionTimestamp: &amp;quot;2020-10-22T21:30:34Z&amp;quot;
  finalizers:
  - kubernetes
  name: mymap
  namespace: default
  resourceVersion: &amp;quot;311456&amp;quot;
  selfLink: /api/v1/namespaces/default/configmaps/mymap
  uid: 93a37fed-23e3-45e8-b6ee-b2521db81638
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In short, what’s happened is that the object was updated, not deleted. That’s because Kubernetes saw that the object contained finalizers and put it into a read-only state. The deletion timestamp signals that the object can only be read, with the exception of removing the finalizer key updates. In other words, the deletion will not be complete until we edit the object and remove the finalizer.&lt;/p&gt;
&lt;p&gt;Here&#39;s a demonstration of using the &lt;code&gt;patch&lt;/code&gt; command to remove finalizers. If we want to delete an object, we can simply patch it on the command line to remove the finalizers. In this way, the deletion that was running in the background will complete and the object will be deleted. When we attempt to &lt;code&gt;get&lt;/code&gt; that configmap, it will be gone.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl patch configmap/mymap \
    --type json \
    --patch=&#39;[ { &amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/metadata/finalizers&amp;quot; } ]&#39;
configmap/mymap patched
[1]+  Done  kubectl delete configmap/mymap

kubectl get configmap/mymap -o yaml
Error from server (NotFound): configmaps &amp;quot;mymap&amp;quot; not found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here&#39;s a state diagram for finalization:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/images/blog/2021-05-14-using-finalizers-to-control-deletion/state-diagram-finalize.png&#34;
         alt=&#34;State diagram for finalize&#34; width=&#34;617&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;State diagram for finalize&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So, if you attempt to delete an object that has a finalizer on it, it will remain in finalization until the controller has removed the finalizer keys or the finalizers are removed using Kubectl. Once that finalizer list is empty, the object can actually be reclaimed by Kubernetes and put into a queue to be deleted from the registry.&lt;/p&gt;
&lt;h2 id=&#34;owner-references&#34;&gt;Owner References&lt;/h2&gt;
&lt;p&gt;Owner references describe how groups of objects are related. They are properties on resources that specify the relationship to one another, so entire trees of resources can be deleted.&lt;/p&gt;
&lt;p&gt;Finalizer rules are processed when there are owner references. An owner reference consists of a name and a UID. Owner references link resources within the same namespace, and it also needs a UID for that reference to work. Pods typically have owner references to the owning replica set. So, when deployments or stateful sets are deleted, then the child replica sets and pods are deleted in the process.&lt;/p&gt;
&lt;p&gt;Here are some examples of owner references and how they work. In the first example, we create a parent object first, then the child. The result is a very simple configmap that contains an owner reference to its parent:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: mymap-parent
EOF
CM_UID=$(kubectl get configmap mymap-parent -o jsonpath=&amp;quot;{.metadata.uid}&amp;quot;)

cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: mymap-child
  ownerReferences:
  - apiVersion: v1
    kind: ConfigMap
    name: mymap-parent
    uid: $CM_UID
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Deleting the child object when an owner reference is involved does not delete the parent:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get configmap
NAME           DATA   AGE
mymap-child    0      12m4s
mymap-parent   0      12m4s

kubectl delete configmap/mymap-child
configmap &amp;quot;mymap-child&amp;quot; deleted

kubectl get configmap
NAME           DATA   AGE
mymap-parent   0      12m10s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this example, we re-created the parent-child configmaps from above. Now, when deleting from the parent (instead of the child) with an owner reference from the child to the parent, when we &lt;code&gt;get&lt;/code&gt; the configmaps, none are in the namespace:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get configmap
NAME           DATA   AGE
mymap-child    0      10m2s
mymap-parent   0      10m2s

kubectl delete configmap/mymap-parent
configmap &amp;quot;mymap-parent&amp;quot; deleted

kubectl get configmap
No resources found in default namespace.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To sum things up, when there&#39;s an override owner reference from a child to a parent, deleting the parent deletes the children automatically. This is called &lt;code&gt;cascade&lt;/code&gt;. The default for cascade is &lt;code&gt;true&lt;/code&gt;, however, you can use the --cascade=orphan option for &lt;code&gt;kubectl delete&lt;/code&gt; to delete an object and orphan its children.&lt;/p&gt;
&lt;p&gt;In the following example, there is a parent and a child. Notice the owner references are still included. If I delete the parent using --cascade=orphan, the parent is deleted but the child still exists:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get configmap
NAME           DATA   AGE
mymap-child    0      13m8s
mymap-parent   0      13m8s

kubectl delete --cascade=orphan configmap/mymap-parent
configmap &amp;quot;mymap-parent&amp;quot; deleted

kubectl get configmap
NAME          DATA   AGE
mymap-child   0      13m21s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The --cascade option links to the propagation policy in the API, which allows you to change the order in which objects are deleted within a tree. In the following example uses API access to craft a custom delete API call with the background propagation policy:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl proxy --port=8080 &amp;amp;
Starting to serve on 127.0.0.1:8080

curl -X DELETE \
  localhost:8080/api/v1/namespaces/default/configmaps/mymap-parent \
  -d &#39;{ &amp;quot;kind&amp;quot;:&amp;quot;DeleteOptions&amp;quot;, &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;, &amp;quot;propagationPolicy&amp;quot;:&amp;quot;Background&amp;quot; }&#39; \
  -H &amp;quot;Content-Type: application/json&amp;quot;
{
  &amp;quot;kind&amp;quot;: &amp;quot;Status&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
  &amp;quot;metadata&amp;quot;: {},
  &amp;quot;status&amp;quot;: &amp;quot;Success&amp;quot;,
  &amp;quot;details&amp;quot;: { ... }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that the propagation policy cannot be specified on the command line using kubectl. You have to specify it using a custom API call. Simply create a proxy, so you have access to the API server from the client, and execute a &lt;code&gt;curl&lt;/code&gt; command with just a URL to execute that &lt;code&gt;delete&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;There are three different options for the propagation policy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Foreground&lt;/code&gt;: Children are deleted before the parent (post-order)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Background&lt;/code&gt;: Parent is deleted before the children (pre-order)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Orphan&lt;/code&gt;: Owner references are ignored&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keep in mind that when you delete an object and owner references have been specified, finalizers will be honored in the process. This can result in trees of objects persisting, and you end up with a partial deletion. At that point, you have to look at any existing owner references on your objects, as well as any finalizers, to understand what’s happening.&lt;/p&gt;
&lt;h2 id=&#34;forcing-a-deletion-of-a-namespace&#34;&gt;Forcing a Deletion of a Namespace&lt;/h2&gt;
&lt;p&gt;There&#39;s one situation that may require forcing finalization for a namespace. If you&#39;ve deleted a namespace and you&#39;ve cleaned out all of the objects under it, but the namespace still exists, deletion can be forced by updating the namespace subresource, &lt;code&gt;finalize&lt;/code&gt;. This informs the namespace controller that it needs to remove the finalizer from the namespace and perform any cleanup:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | curl -X PUT \
  localhost:8080/api/v1/namespaces/test/finalize \
  -H &amp;quot;Content-Type: application/json&amp;quot; \
  --data-binary @-
{
  &amp;quot;kind&amp;quot;: &amp;quot;Namespace&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;name&amp;quot;: &amp;quot;test&amp;quot;
  },
  &amp;quot;spec&amp;quot;: {
    &amp;quot;finalizers&amp;quot;: null
  }
}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This should be done with caution as it may delete the namespace only and leave orphan objects within the, now non-exiting, namespace - a confusing state for Kubernetes. If this happens, the namespace can be re-created manually and sometimes the orphaned objects will re-appear under the just-created namespace which will allow manual cleanup and recovery.&lt;/p&gt;
&lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h2&gt;
&lt;p&gt;As these examples demonstrate, finalizers can get in the way of deleting resources in Kubernetes, especially when there are parent-child relationships between objects. Often, there is a reason for adding a finalizer into the code, so you should always investigate before manually deleting it. Owner references allow you to specify and remove trees of resources, although finalizers will be honored in the process. Finally, the propagation policy can be used to specify the order of deletion via a custom API call, giving you control over how objects are deleted. Now that you know a little more about how deletions work in Kubernetes, we recommend you try it out on your own, using a test cluster.&lt;/p&gt;

&lt;div class=&#34;youtube-quote-sm&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/F7-ZxWwf4sY&#34; allowfullscreen title=&#34;Clean Up Your Room! What Does It Mean to Delete Something in K8s&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.21: Metrics Stability hits GA</title>
      <link>https://kubernetes.io/blog/2021/04/23/kubernetes-release-1.21-metrics-stability-ga/</link>
      <pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/23/kubernetes-release-1.21-metrics-stability-ga/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Han Kang (Google), Elana Hashman (Red Hat)&lt;/p&gt;
&lt;p&gt;Kubernetes 1.21 marks the graduation of the metrics stability framework and along with it, the first officially supported stable metrics. Not only do stable metrics come with supportability guarantees, the metrics stability framework brings escape hatches that you can use if you encounter problematic metrics.&lt;/p&gt;
&lt;p&gt;See the list of &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml&#34;&gt;stable Kubernetes metrics here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-are-stable-metrics-and-why-do-we-need-them&#34;&gt;What are stable metrics and why do we need them?&lt;/h3&gt;
&lt;p&gt;A stable metric is one which, from a consumption point of view, can be reliably consumed across a number of Kubernetes versions without risk of ingestion failure.&lt;/p&gt;
&lt;p&gt;Metrics stability is an ongoing community concern. Cluster monitoring infrastructure often assumes the stability of some control plane metrics, so we have introduced a mechanism for versioning metrics as a proper API, with stability guarantees around a formal metrics deprecation process.&lt;/p&gt;
&lt;h3 id=&#34;what-are-the-stability-levels-for-metrics&#34;&gt;What are the stability levels for metrics?&lt;/h3&gt;
&lt;p&gt;Metrics can currently have one of two stability levels: alpha or stable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Alpha metrics&lt;/em&gt; have no stability guarantees; as such they can be modified or deleted at any time. At this time, all Kubernetes metrics implicitly fall into this category.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Stable metrics&lt;/em&gt; can be guaranteed to not change, except that the metric may become marked deprecated for a future Kubernetes version. By not change, we mean three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the metric itself will not be deleted or renamed&lt;/li&gt;
&lt;li&gt;the type of metric will not be modified&lt;/li&gt;
&lt;li&gt;no labels can be added or removed from this metric&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From an ingestion point of view, it is backwards-compatible to add or remove possible values for labels which already do exist, but not labels themselves. Therefore, adding or removing values from an existing label is permitted. Stable metrics can also be marked as deprecated for a future Kubernetes version, since this is tracked in a metadata field and does not actually change the metric itself.&lt;/p&gt;
&lt;p&gt;Removing or adding labels from stable metrics is not permitted. In order to add or remove a label from an existing stable metric, one would have to introduce a new metric and deprecate the stable one; otherwise this would violate compatibility agreements.&lt;/p&gt;
&lt;h4 id=&#34;how-are-metrics-deprecated&#34;&gt;How are metrics deprecated?&lt;/h4&gt;
&lt;p&gt;While deprecation policies only affect stability guarantees for stable metrics (and not alpha ones), deprecation information may be optionally provided on alpha metrics to help component owners inform users of future intent and assist with transition plans.&lt;/p&gt;
&lt;p&gt;A stable metric undergoing the deprecation process signals that the metric will eventually be deleted. The metrics deprecation lifecycle looks roughly like this (with each stage representing a Kubernetes release):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;lifecycle-metric.png&#34; alt=&#34;Stable metric → Deprecated metric → Hidden metric → Deletion&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Deprecated metrics&lt;/em&gt; have the same stability guarantees of their stable counterparts. If a stable metric is deprecated, then a deprecated stable metric is guaranteed to not change. When deprecating a stable metric, a future Kubernetes release is specified as the point from which the metric will be considered deprecated.&lt;/p&gt;
&lt;p&gt;Deprecated metrics will have their description text prefixed with a deprecation notice string “(Deprecated from x.y)” and a warning log will be emitted during metric registration, in the spirit of the official Kubernetes deprecation policy.&lt;/p&gt;
&lt;p&gt;Like their stable metric counterparts, deprecated metrics will be automatically registered to the metrics endpoint. On a subsequent release (when the metric&#39;s deprecatedVersion is equal to &lt;em&gt;current_kubernetes_version - 4&lt;/em&gt;)), a deprecated metric will become a &lt;em&gt;hidden&lt;/em&gt; metric. &lt;em&gt;Hidden metrics&lt;/em&gt; are not automatically registered, and hence are hidden by default from end users. These hidden metrics can be explicitly re-enabled for one release after they reach the hidden state, to provide a migration path for cluster operators.&lt;/p&gt;
&lt;h4 id=&#34;as-an-owner-of-a-kubernetes-component-how-do-i-add-stable-metrics&#34;&gt;As an owner of a Kubernetes component, how do I add stable metrics?&lt;/h4&gt;
&lt;p&gt;During metric instantiation, stability can be specified by setting the metadata field, StabilityLevel, to “Stable”. When a StabilityLevel is not explicitly set, metrics default to “Alpha” stability. Note that metrics which have fields determined at runtime cannot be marked as Stable. Stable metrics will be detected during static analysis during the pre-commit phase, and must be reviewed by sig-instrumentation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-golang&#34; data-lang=&#34;golang&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;var&lt;/span&gt; metricDefinition = kubemetrics.CounterOpts{
    Name: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;some_metric&amp;#34;&lt;/span&gt;,
    Help: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;some description&amp;#34;&lt;/span&gt;,
    StabilityLevel: kubemetrics.STABLE,
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For more examples of setting metrics stability and deprecation, see the &lt;a href=&#34;http://bit.ly/metrics-stability&#34;&gt;Metrics Stability KEP&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h3&gt;
&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.
We offer a huge thank you to all the contributors in Kubernetes community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Han Kang (logicalhan)&lt;/li&gt;
&lt;li&gt;Frederic Branczyk (brancz)&lt;/li&gt;
&lt;li&gt;Marek Siarkowicz (serathius)&lt;/li&gt;
&lt;li&gt;Elana Hashman (ehashman)&lt;/li&gt;
&lt;li&gt;Solly Ross (DirectXMan12)&lt;/li&gt;
&lt;li&gt;Stefan Schimanski (sttts)&lt;/li&gt;
&lt;li&gt;David Ashpole (dashpole)&lt;/li&gt;
&lt;li&gt;Yuchen Zhou (yoyinzyc)&lt;/li&gt;
&lt;li&gt;Yu Yi (erain)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you’re interested in getting involved with the design and development of instrumentation or any part of the Kubernetes metrics system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-instrumentation&#34;&gt;Kubernetes Instrumentation Special Interest Group (SIG)&lt;/a&gt;. We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Evolving Kubernetes networking with the Gateway API</title>
      <link>https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/</link>
      <pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mark Church (Google), Harry Bagdi (Kong), Daneyon Hanson (Red Hat), Nick Young (VMware), Manuel Zapf (Traefik Labs)&lt;/p&gt;
&lt;p&gt;The Ingress resource is one of the many Kubernetes success stories. It created a &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34;&gt;diverse ecosystem of Ingress controllers&lt;/a&gt; which were used across hundreds of thousands of clusters in a standardized and consistent way. This standardization helped users adopt Kubernetes. However, five years after the creation of Ingress, there are signs of fragmentation into different but &lt;a href=&#34;https://dave.cheney.net/paste/ingress-is-dead-long-live-ingressroute.pdf&#34;&gt;strikingly similar CRDs&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&#34;&gt;overloaded annotations&lt;/a&gt;. The same portability that made Ingress pervasive also limited its future.&lt;/p&gt;
&lt;p&gt;It was at Kubecon 2019 San Diego when a passionate group of contributors gathered to discuss the &lt;a href=&#34;https://static.sched.com/hosted_files/kccncna19/a5/Kubecon%20San%20Diego%202019%20-%20Evolving%20the%20Kubernetes%20Ingress%20APIs%20to%20GA%20and%20Beyond%20%5BPUBLIC%5D.pdf&#34;&gt;evolution of Ingress&lt;/a&gt;. The discussion overflowed to the hotel lobby across the street and what came out of it would later be known as the &lt;a href=&#34;https://gateway-api.sigs.k8s.io&#34;&gt;Gateway API&lt;/a&gt;. This discussion was based on a few key assumptions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The API standards underlying route matching, traffic management, and service exposure are commoditized and provide little value to their implementers and users as custom APIs&lt;/li&gt;
&lt;li&gt;It’s possible to represent L4/L7 routing and traffic management through common core API resources&lt;/li&gt;
&lt;li&gt;It’s possible to provide extensibility for more complex capabilities in a way that does not sacrifice the user experience of the core API&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introducing-the-gateway-api&#34;&gt;Introducing the Gateway API&lt;/h2&gt;
&lt;p&gt;This led to design principles that allow the Gateway API to improve upon Ingress:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Expressiveness&lt;/strong&gt; - In addition to HTTP host/path matching and TLS, Gateway API can express capabilities like HTTP header manipulation, traffic weighting &amp;amp; mirroring, TCP/UDP routing, and other capabilities that were only possible in Ingress through custom annotations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Role-oriented design&lt;/strong&gt; - The API resource model reflects the separation of responsibilities that is common in routing and Kubernetes service networking.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt; - The resources allow arbitrary configuration attachment at various layers within the API. This makes granular customization possible at the most appropriate places.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexible conformance&lt;/strong&gt; - The Gateway API defines varying conformance levels - core (mandatory support), extended (portable if supported), and custom (no portability guarantee), known together as &lt;a href=&#34;https://gateway-api.sigs.k8s.io/concepts/guidelines/#conformance&#34;&gt;flexible conformance&lt;/a&gt;. This promotes a highly portable core API (like Ingress) that still gives flexibility for Gateway controller implementers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-does-the-gateway-api-look-like&#34;&gt;What does the Gateway API look like?&lt;/h3&gt;
&lt;p&gt;The Gateway API introduces a few new resource types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.GatewayClass&#34;&gt;GatewayClasses&lt;/a&gt;&lt;/strong&gt; are cluster-scoped resources that act as templates to explicitly define behavior for Gateways derived from them. This is similar in concept to StorageClasses, but for networking data-planes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.Gateway&#34;&gt;Gateways&lt;/a&gt;&lt;/strong&gt; are the deployed instances of GatewayClasses. They are the logical representation of the data-plane which performs routing, which may be in-cluster proxies, hardware LBs, or cloud LBs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Routes&lt;/strong&gt; are not a single resource, but represent many different protocol-specific Route resources. The &lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.HTTPRoute&#34;&gt;HTTPRoute&lt;/a&gt; has matching, filtering, and routing rules that get applied to Gateways that can process HTTP and HTTPS traffic. Similarly, there are &lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.TCPRoute&#34;&gt;TCPRoutes&lt;/a&gt;, &lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.UDPRoute&#34;&gt;UDPRoutes&lt;/a&gt;, and &lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.TLSRoute&#34;&gt;TLSRoutes&lt;/a&gt; which also have protocol-specific semantics. This model also allows the Gateway API to incrementally expand its protocol support in the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;gateway-api-resources.png&#34; alt=&#34;The resources of the Gateway API&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;gateway-controller-implementations&#34;&gt;Gateway Controller Implementations&lt;/h3&gt;
&lt;p&gt;The good news is that although Gateway is in &lt;a href=&#34;https://github.com/kubernetes-sigs/gateway-api/releases&#34;&gt;Alpha&lt;/a&gt;, there are already several &lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/implementations/&#34;&gt;Gateway controller implementations&lt;/a&gt; that you can run. Since it’s a standardized spec, the following example could be run on any of them and should function the exact same way. Check out &lt;a href=&#34;https://gateway-api.sigs.k8s.io/guides/getting-started/&#34;&gt;getting started&lt;/a&gt; to see how to install and use one of these Gateway controllers.&lt;/p&gt;
&lt;h2 id=&#34;getting-hands-on-with-the-gateway-api&#34;&gt;Getting Hands-on with the Gateway API&lt;/h2&gt;
&lt;p&gt;In the following example, we’ll demonstrate the relationships between the different API Resources and walk you through a common use case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Team foo has their app deployed in the foo Namespace. They need to control the routing logic for the different pages of their app.&lt;/li&gt;
&lt;li&gt;Team bar is running in the bar Namespace. They want to be able to do blue-green rollouts of their application to reduce risk.&lt;/li&gt;
&lt;li&gt;The platform team is responsible for managing the load balancer and network security of all the apps in the Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following foo-route does path matching to various Services in the foo Namespace and also has a default route to a 404 server. This exposes foo-auth and foo-home Services via &lt;code&gt;foo.example.com/login&lt;/code&gt; and &lt;code&gt;foo.example.com/home&lt;/code&gt; respectively.:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HTTPRoute&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;networking.x-k8s.io/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;foo-route&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;foo&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;labels&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;gateway&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;external-https-prod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;hostnames&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo.example.com&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;rules&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;matches&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;path&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;type&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Prefix&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;value&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/login&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;forwardTo&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;serviceName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;foo-auth&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;matches&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;path&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;type&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Prefix&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;value&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/home&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;forwardTo&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;serviceName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;foo-home&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;matches&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;path&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;type&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Prefix&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;value&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;forwardTo&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;serviceName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;foo-404&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The bar team, operating in the bar Namespace of the same Kubernetes cluster, also wishes to expose their application to the internet, but they also want to control their own canary and blue-green rollouts. The following HTTPRoute is configured for the following behavior:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For traffic to &lt;code&gt;bar.example.com&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Send 90% of the traffic to bar-v1&lt;/li&gt;
&lt;li&gt;Send 10% of the traffic to bar-v2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For traffic to &lt;code&gt;bar.example.com&lt;/code&gt; with the HTTP header &lt;code&gt;env: canary&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Send all the traffic to bar-v2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;httproute.png&#34; alt=&#34;The routing rules configured for the bar-v1 and bar-v2 Services&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HTTPRoute&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;networking.x-k8s.io/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;bar-route&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;bar&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;labels&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;gateway&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;external-https-prod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;hostnames&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar.example.com&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;rules&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;forwardTo&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;serviceName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;bar-v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;weight&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;90&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;serviceName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;bar-v2&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;weight&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;matches&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;headers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;values&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;env&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;canary&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;forwardTo&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;serviceName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;bar-v2&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;route-and-gateway-binding&#34;&gt;Route and Gateway Binding&lt;/h3&gt;
&lt;p&gt;So we have two HTTPRoutes matching and routing traffic to different Services. You might be wondering, where are these Services accessible? Through which networks or IPs are they exposed?&lt;/p&gt;
&lt;p&gt;How Routes are exposed to clients is governed by &lt;a href=&#34;https://gateway-api.sigs.k8s.io/concepts/api-overview/#route-binding&#34;&gt;Route binding&lt;/a&gt;, which describes how Routes and Gateways create a bidirectional relationship between each other. When Routes are bound to a Gateway it means their collective routing rules are configured on the underlying load balancers or proxies and the Routes are accessible through the Gateway. Thus, a Gateway is a logical representation of a networking data plane that can be configured through Routes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;route-binding.png&#34; alt=&#34;How Routes bind with Gateways&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;administrative-delegation&#34;&gt;Administrative Delegation&lt;/h3&gt;
&lt;p&gt;The split between Gateway and Route resources allows the cluster administrator to delegate some of the routing configuration to individual teams while still retaining centralized control. The following Gateway resource exposes HTTPS on port 443 and terminates all traffic on the port with a certificate controlled by the cluster administrator.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Gateway&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;networking.x-k8s.io/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;prod-web&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;gatewayClassName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;acme-lb&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;listeners&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;  
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;protocol&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HTTPS&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;443&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;routes&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HTTPRoute&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;selector&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;matchLabels&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;gateway&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;external-https-prod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespaces&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;from&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;All&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;tls&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;certificateRef&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;admin-controlled-cert&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The following HTTPRoute shows how the Route can ensure it matches the Gateway&#39;s selector via it’s &lt;code&gt;kind&lt;/code&gt; (HTTPRoute) and resource labels (&lt;code&gt;gateway=external-https-prod&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Matches the required kind selector on the Gateway&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;HTTPRoute&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;networking.x-k8s.io/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;foo-route&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;foo-ns&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;labels&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Matches the required label selector on the Gateway&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;gateway&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;external-https-prod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;...&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;role-oriented-design&#34;&gt;Role Oriented Design&lt;/h3&gt;
&lt;p&gt;When you put it all together, you have a single load balancing infrastructure that can be safely shared by multiple teams. The Gateway API is not only a more expressive API for advanced routing, but is also a role-oriented API, designed for multi-tenant infrastructure. Its extensibility ensures that it will evolve for future use-cases while preserving portability. Ultimately these characteristics will allow the Gateway API to adapt to different organizational models and implementations well into the future.&lt;/p&gt;
&lt;h3 id=&#34;try-it-out-and-get-involved&#34;&gt;Try it out and get involved&lt;/h3&gt;
&lt;p&gt;There are many resources to check out to learn more.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://gateway-api.sigs.k8s.io/guides/getting-started/&#34;&gt;user guides&lt;/a&gt; to see what use-cases can be addressed.&lt;/li&gt;
&lt;li&gt;Try out one of the &lt;a href=&#34;https://gateway-api.sigs.k8s.io/references/implementations/&#34;&gt;existing Gateway controllers &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Or &lt;a href=&#34;https://gateway-api.sigs.k8s.io/contributing/community/&#34;&gt;get involved&lt;/a&gt; and help design and influence the future of Kubernetes service networking!&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Graceful Node Shutdown Goes Beta</title>
      <link>https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/</link>
      <pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David Porter (Google), Mrunal Patel (Red Hat), and Tim Bannister (The Scale Factory)&lt;/p&gt;
&lt;p&gt;Graceful node shutdown, beta in 1.21, enables kubelet to gracefully evict pods during a node shutdown.&lt;/p&gt;
&lt;p&gt;Kubernetes is a distributed system and as such we need to be prepared for inevitable failures — nodes will fail, containers might crash or be restarted, and - ideally - your workloads will be able to withstand these catastrophic events.&lt;/p&gt;
&lt;p&gt;One of the common classes of issues are workload failures on node shutdown or restart. The best practice prior to bringing your node down is to &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/&#34;&gt;safely drain and cordon your node&lt;/a&gt;. This will ensure that all pods running on this node can safely be evicted. An eviction will ensure your pods can follow the expected &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination&#34;&gt;pod termination lifecycle&lt;/a&gt; meaning receiving a SIGTERM in your container and/or running &lt;code&gt;preStopHooks&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prior to Kubernetes 1.20 (when graceful node shutdown was introduced as an alpha feature), safe node draining was not easy: it required users to manually take action and drain the node beforehand. If someone or something shut down your node without draining it first, most likely your pods would not be safely evicted from your node and shutdown abruptly. Other services talking to those pods might see errors due to the pods exiting abruptly. Some examples of this situation may be caused by a reboot due to security patches or preemption of short lived cloud compute instances.&lt;/p&gt;
&lt;p&gt;Kubernetes 1.21 brings graceful node shutdown to beta. Graceful node shutdown gives you more control over some of those unexpected shutdown situations. With graceful node shutdown, the kubelet is aware of underlying system shutdown events and can propagate these events to pods, ensuring containers can shut down as gracefully as possible. This gives the containers a chance to checkpoint their state or release back any resources they are holding.&lt;/p&gt;
&lt;p&gt;Note, that for the best availability, even with graceful node shutdown, you should still design your deployments to be resilient to node failures.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;On Linux, your system can shut down in many different situations. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A user or script running &lt;code&gt;shutdown -h now&lt;/code&gt; or &lt;code&gt;systemctl poweroff&lt;/code&gt; or &lt;code&gt;systemctl reboot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Physically pressing a power button on the machine.&lt;/li&gt;
&lt;li&gt;Stopping a VM instance on a cloud provider, e.g. &lt;code&gt;gcloud compute instances stop&lt;/code&gt; on GCP.&lt;/li&gt;
&lt;li&gt;A Preemptible VM or Spot Instance that your cloud provider can terminate unexpectedly, but with a brief warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these situations can be unexpected and there is no guarantee that a cluster administrator drained the node prior to these events. With the graceful node shutdown feature, kubelet uses a systemd mechanism called &lt;a href=&#34;https://www.freedesktop.org/wiki/Software/systemd/inhibit&#34;&gt;&amp;quot;Inhibitor Locks&amp;quot;&lt;/a&gt; to allow draining in most cases. Using Inhibitor Locks, kubelet instructs systemd to postpone system shutdown for a specified duration, giving a chance for the node to drain and evict pods on the system.&lt;/p&gt;
&lt;p&gt;Kubelet makes use of this mechanism to ensure your pods will be terminated cleanly. When the kubelet starts, it acquires a systemd delay-type inhibitor lock. When the system is about to shut down, the kubelet can delay that shutdown for a configurable, short duration utilizing the delay-type inhibitor lock it acquired earlier. This gives your pods extra time to  terminate. As a result, even during unexpected shutdowns, your application will receive a SIGTERM, &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks&#34;&gt;preStop hooks&lt;/a&gt; will execute, and kubelet will properly update &lt;code&gt;Ready&lt;/code&gt; node condition and respective pod statuses to the api-server.&lt;/p&gt;
&lt;p&gt;For example, on a node with graceful node shutdown enabled, you can see that the inhibitor lock is taken by the kubelet:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubelet-node ~ # systemd-inhibit --list
    Who: kubelet (UID 0/root, PID 1515/kubelet)
    What: shutdown
    Why: Kubelet needs time to handle node shutdown
    Mode: delay

1 inhibitors listed.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;One important consideration we took when designing this feature is that not all pods are created equal. For example, some of the pods running on a node such as a logging related daemonset should stay running as long as possible to capture important logs during the shutdown itself. As a result, pods are split into two categories: &amp;quot;regular&amp;quot; and &amp;quot;critical&amp;quot;. &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical&#34;&gt;Critical pods&lt;/a&gt; are those that have &lt;code&gt;priorityClassName&lt;/code&gt; set to &lt;code&gt;system-cluster-critical&lt;/code&gt; or &lt;code&gt;system-node-critical&lt;/code&gt;; all other pods are considered regular.&lt;/p&gt;
&lt;p&gt;In our example, the logging DaemonSet would run as a critical pod. During the graceful node shutdown, regular pods are terminated first, followed by critical pods. As an example, this would allow a critical pod associated with a logging daemonset to continue functioning, and collecting logs during the termination of regular pods.&lt;/p&gt;
&lt;p&gt;We will evaluate during the beta phase if we need more flexibility for different pod priority classes and add support if needed, please let us know if you have some scenarios in mind.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-use-it&#34;&gt;How do I use it?&lt;/h2&gt;
&lt;p&gt;Graceful node shutdown is controlled with the &lt;code&gt;GracefulNodeShutdown&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates&#34;&gt;feature gate&lt;/a&gt; and is enabled by default in Kubernetes 1.21.&lt;/p&gt;
&lt;p&gt;You can configure the graceful node shutdown behavior using two kubelet configuration options: &lt;code&gt;ShutdownGracePeriod&lt;/code&gt; and &lt;code&gt;ShutdownGracePeriodCriticalPods&lt;/code&gt;. To configure these options, you edit the kubelet configuration file that is passed to kubelet via the &lt;code&gt;--config&lt;/code&gt; flag; for more details, refer to &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/&#34;&gt;Set kubelet parameters via a configuration file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During a shutdown, kubelet terminates pods in two phases. You can configure how long each of these phases lasts.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Terminate regular pods running on the node.&lt;/li&gt;
&lt;li&gt;Terminate critical pods running on the node.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The settings that control the duration of shutdown are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ShutdownGracePeriod&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Specifies the total duration that the node should delay the shutdown by. This is the total grace period for pod termination for both regular and critical pods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ShutdownGracePeriodCriticalPods&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Specifies the duration used to terminate critical pods during a node shutdown. This should be less than &lt;code&gt;ShutdownGracePeriod&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, if &lt;code&gt;ShutdownGracePeriod=30s&lt;/code&gt;, and &lt;code&gt;ShutdownGracePeriodCriticalPods=10s&lt;/code&gt;, kubelet will delay the node shutdown by 30 seconds. During this time, the first 20 seconds (30-10) would be reserved for gracefully terminating normal pods, and the last 10 seconds would be reserved for terminating critical pods.&lt;/p&gt;
&lt;p&gt;Note that by default, both configuration options described above, &lt;code&gt;ShutdownGracePeriod&lt;/code&gt; and &lt;code&gt;ShutdownGracePeriodCriticalPods&lt;/code&gt; are set to zero, so you will need to configure them as appropriate for your environment to activate graceful node shutdown functionality.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read the &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read the enhancement proposal, &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2000-graceful-node-shutdown&#34;&gt;KEP 2000&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;View the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.21/pkg/kubelet/nodeshutdown&#34;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;
&lt;p&gt;Your feedback is always welcome! SIG Node meets regularly and can be reached via &lt;a href=&#34;https://slack.k8s.io&#34;&gt;Slack&lt;/a&gt; (channel &lt;code&gt;#sig-node&lt;/code&gt;), or the SIG&#39;s &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node#contact&#34;&gt;mailing list&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Annotating Kubernetes Services for Humans</title>
      <link>https://kubernetes.io/blog/2021/04/20/annotating-k8s-for-humans/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/20/annotating-k8s-for-humans/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Richard Li, Ambassador Labs&lt;/p&gt;
&lt;p&gt;Have you ever been asked to troubleshoot a failing Kubernetes service and struggled to find basic information about the service such as the source repository and owner?&lt;/p&gt;
&lt;p&gt;One of the problems as Kubernetes applications grow is the proliferation of services. As the number of services grows, developers start to specialize working with specific services. When it comes to troubleshooting, however, developers need to be able to find the source, understand the service and dependencies, and chat with the owning team for any service.&lt;/p&gt;
&lt;h2 id=&#34;human-service-discovery&#34;&gt;Human service discovery&lt;/h2&gt;
&lt;p&gt;Troubleshooting always begins with information gathering. While much attention has been paid to centralizing machine data (e.g., logs, metrics), much less attention has been given to the human aspect of service discovery. Who owns a particular service? What Slack channel does the team work on? Where is the source for the service? What issues are currently known and being tracked?&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-annotations&#34;&gt;Kubernetes annotations&lt;/h2&gt;
&lt;p&gt;Kubernetes annotations are designed to solve exactly this problem. Oft-overlooked, Kubernetes annotations are designed to add metadata to Kubernetes objects. The Kubernetes documentation says annotations can “attach arbitrary non-identifying metadata to objects.” This means that annotations should be used for attaching metadata that is external to Kubernetes (i.e., metadata that Kubernetes won’t use to identify objects. As such, annotations can contain any type of data. This is a contrast to labels, which are designed for uses internal to Kubernetes. As such, label structure and values are &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set&#34;&gt;constrained&lt;/a&gt; so they can be efficiently used by Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-annotations-in-action&#34;&gt;Kubernetes annotations in action&lt;/h2&gt;
&lt;p&gt;Here is an example. Imagine you have a Kubernetes service for quoting, called the quote service. You can do the following:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl annotate service quote a8r.io/owner=”@sally”
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this example, we&#39;ve just added an annotation called &lt;code&gt;a8r.io/owner&lt;/code&gt; with the value of @sally. Now, we can use &lt;code&gt;kubectl describe&lt;/code&gt; to get the information.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name:              quote
Namespace:         default
Labels:            &amp;lt;none&amp;gt;
Annotations:       a8r.io/owner: @sally
Selector:          app=quote
Type:              ClusterIP
IP:                10.109.142.131
Port:              http  80/TCP
TargetPort:        8080/TCP
Endpoints:         &amp;lt;none&amp;gt;
Session Affinity:  None
Events:            &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you’re practicing GitOps (and you should be!) you’ll want to code these values directly into your Kubernetes manifest, e.g.,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;quote&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;annotations&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;a8r.io/owner&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;“@sally”&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;ports&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;http&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;targetPort&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;selector&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;app&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;quote&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;a-convention-for-annotations&#34;&gt;A Convention for Annotations&lt;/h2&gt;
&lt;p&gt;Adopting a common convention for annotations ensures consistency and understandability. Typically, you’ll want to attach the annotation to the service object, as services are the high-level resource that maps most clearly to a team’s responsibility. Namespacing your annotations is also very important. Here is one set of conventions, documented at &lt;a href=&#34;https://a8r.io&#34;&gt;a8r.io&lt;/a&gt;, and reproduced below:&lt;/p&gt;





&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Annotation convention for human-readable services&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Annotation&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/description&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Unstructured text description of the service for humans.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/owner&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;SSO username (GitHub), email address (linked to GitHub account), or unstructured owner description.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/chat&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Slack channel, or link to external chat system.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/bugs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external bug tracker.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/logs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external log viewer.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/documentation&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external project documentation.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external VCS repository.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/support&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external support center.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/runbook&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external project runbook.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/incidents&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external incident dashboard.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/uptime&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external uptime dashboard.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/performance&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Link to external performance dashboard.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;a8r.io/dependencies&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Unstructured text describing the service dependencies for humans.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;visualizing-annotations-service-catalogs&#34;&gt;Visualizing annotations: Service Catalogs&lt;/h2&gt;
&lt;p&gt;As the number of microservices and annotations proliferate, running &lt;code&gt;kubectl describe&lt;/code&gt; can get tedious. Moreover, using &lt;code&gt;kubectl describe&lt;/code&gt; requires every developer to have some direct access to the Kubernetes cluster. Over the past few years, service catalogs have gained greater visibility in the Kubernetes ecosystem. Popularized by tools such as &lt;a href=&#34;https://shopify.engineering/scaling-mobile-development-by-treating-apps-as-services&#34;&gt;Shopify&#39;s ServicesDB&lt;/a&gt; and &lt;a href=&#34;https://dzone.com/articles/modeling-microservices-at-spotify-with-petter-mari&#34;&gt;Spotify&#39;s System Z&lt;/a&gt;, service catalogs are internally-facing developer portals that present critical information about microservices.&lt;/p&gt;
&lt;p&gt;Note that these service catalogs should not be confused with the &lt;a href=&#34;https://svc-cat.io/&#34;&gt;Kubernetes Service Catalog project&lt;/a&gt;. Built on the Open Service Broker API, the Kubernetes Service Catalog enables Kubernetes operators to plug in different services (e.g., databases) to their cluster.&lt;/p&gt;
&lt;h2 id=&#34;annotate-your-services-now-and-thank-yourself-later&#34;&gt;Annotate your services now and thank yourself later&lt;/h2&gt;
&lt;p&gt;Much like implementing observability within microservice systems, you often don’t realize that you need human service discovery until it’s too late. Don&#39;t wait until something is on fire in production to start wishing you had implemented better metrics and also documented how to get in touch with the part of your organization that looks after it.&lt;/p&gt;
&lt;p&gt;There&#39;s enormous benefits to building an effective “version 0” service: a &lt;a href=&#34;https://containerjournal.com/topics/container-management/dancing-skeleton-apis-and-microservices/&#34;&gt;&lt;em&gt;dancing skeleton&lt;/em&gt;&lt;/a&gt; application with a thin slice of complete functionality that can be deployed to production with a minimal yet effective continuous delivery pipeline.&lt;/p&gt;
&lt;p&gt;Adding service annotations should be an essential part of your “version 0” for all of your services. Add them now, and you’ll thank yourself later.&lt;/p&gt;


      </description>
    </item>
    
    <item>
      <title>Blog: Defining Network Policy Conformance for Container Network Interface (CNI) providers</title>
      <link>https://kubernetes.io/blog/2021/04/20/defining-networkpolicy-conformance-cni-providers/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/20/defining-networkpolicy-conformance-cni-providers/</guid>
      <description>
        
        
        &lt;p&gt;Authors: Matt Fenwick (Synopsys), Jay Vyas (VMWare), Ricardo Katz, Amim Knabben (Loadsmart), Douglas Schilling Landgraf (Red Hat), Christopher Tomkins (Tigera)&lt;/p&gt;
&lt;p&gt;Special thanks to Tim Hockin and Bowie Du (Google), Dan Winship and Antonio Ojea (Red Hat),
Casey Davenport and Shaun Crampton (Tigera), and Abhishek Raut and Antonin Bas (VMware) for
being supportive of this work, and working with us to resolve issues in different Container Network Interfaces (CNIs) over time.&lt;/p&gt;
&lt;p&gt;A brief conversation around &amp;quot;node local&amp;quot; Network Policies in April of 2020 inspired the creation of a NetworkPolicy subproject from SIG Network. It became clear that as a community,
we need a rock-solid story around how to do pod network security on Kubernetes, and this story needed a community around it, so as to grow the cultural adoption of enterprise security patterns in K8s.&lt;/p&gt;
&lt;p&gt;In this post we&#39;ll discuss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why we created a subproject for &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34;&gt;Network Policies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How we changed the Kubernetes e2e framework to &lt;code&gt;visualize&lt;/code&gt; NetworkPolicy implementation of your CNI provider&lt;/li&gt;
&lt;li&gt;The initial results of our comprehensive NetworkPolicy conformance validator, &lt;em&gt;Cyclonus&lt;/em&gt;, built around these principles&lt;/li&gt;
&lt;li&gt;Improvements subproject contributors have made to the NetworkPolicy user experience&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-we-created-a-subproject-for-networkpolicies&#34;&gt;Why we created a subproject for NetworkPolicies&lt;/h2&gt;
&lt;p&gt;In April of 2020 it was becoming clear that many CNIs were emerging, and many vendors
implement these CNIs in subtly different ways.  Users were beginning to express a little bit
of confusion around how to implement policies for different scenarios, and asking for new features.
It was clear that we needed to begin unifying the way we think about Network Policies
in Kubernetes, to avoid API fragmentation and unnecessary complexity.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In order to be flexible to the user’s environment, Calico as a CNI provider can be run using IPIP or VXLAN mode, or without encapsulation overhead.  CNIs such as Antrea
and Cilium offer similar configuration options as well.&lt;/li&gt;
&lt;li&gt;Some CNI plugins offer iptables for NetworkPolicies amongst other options, whereas other CNIs use a completely
different technology stack (for example, the Antrea project uses Open vSwitch rules).&lt;/li&gt;
&lt;li&gt;Some CNI plugins only implement a subset of the Kubernetes NetworkPolicy API, and some a superset. For example, certain plugins don&#39;t support the
ability to target a named port; others don&#39;t work with certain IP address types, and there are diverging semantics for similar policy types.&lt;/li&gt;
&lt;li&gt;Some CNI plugins combine with OTHER CNI plugins in order to implement NetworkPolicies (canal), some CNI&#39;s might mix implementations (multus), and some clouds do routing separately from NetworkPolicy implementation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although this complexity is to some extent necessary to support different environments, end-users find that they need to follow a multistep process to implement Network Policies to secure their applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Confirm that their network plugin supports NetworkPolicies (some don&#39;t, such as Flannel)&lt;/li&gt;
&lt;li&gt;Confirm that their cluster&#39;s network plugin supports the specific NetworkPolicy features that they are interested in (again, the named port or port range examples come to mind here)&lt;/li&gt;
&lt;li&gt;Confirm that their application&#39;s Network Policy definitions are doing the right thing&lt;/li&gt;
&lt;li&gt;Find out the nuances of a vendor&#39;s implementation of policy, and check whether or not that implementation has a CNI neutral implementation (which is sometimes adequate for users)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NetworkPolicy project in upstream Kubernetes aims at providing a community where
people can learn about, and contribute to, the Kubernetes NetworkPolicy API and the surrounding ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;the-first-step-a-validation-framework-for-networkpolicies-that-was-intuitive-to-use-and-understand&#34;&gt;The First step: A validation framework for NetworkPolicies that was intuitive to use and understand&lt;/h2&gt;
&lt;p&gt;The Kubernetes end to end suite has always had NetworkPolicy tests, but these weren&#39;t
run in CI, and the way they were implemented didn&#39;t provide holistic, easily consumable
information about how a policy was working in a cluster.
This is because the original tests didn&#39;t provide any kind of visual summary of connectivity
across a cluster.   We thus initially set out to make it easy to confirm CNI support for NetworkPolicies by
making the end to end tests (which are often used by administrators or users to diagnose cluster conformance) easy to interpret.&lt;/p&gt;
&lt;p&gt;To solve the problem of confirming that CNIs support the basic features most users care about
for a policy, we built a new NetworkPolicy validation tool into the Kubernetes e2e
framework which allows for visual inspection of policies and their effect on a standard set of pods in a cluster.
For example, take the following test output.  We found a bug in
&lt;a href=&#34;https://github.com/ovn-org/ovn-kubernetes/issues/1782&#34;&gt;OVN Kubernetes&lt;/a&gt;. This bug has now been resolved. With this tool the bug was really
easy to characterize, wherein certain policies caused a state-modification that,
later on, caused traffic to incorrectly be blocked (even after all Network Policies were deleted from the cluster).&lt;/p&gt;
&lt;p&gt;This is the network policy for the test in question:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;creationTimestamp&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;null&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;allow-ingress-port-80&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;ingress&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;ports&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;port&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;serve-80-tcp&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;podSelector&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{}&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These are the expected connectivity results.  The test setup is 9 pods (3 namespaces: x, y, and z;
and 3 pods in each namespace: a, b, and c); each pod runs a server on the same port and protocol
that can be reached through HTTP calls in the absence of network policies.  Connectivity is verified
by using the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/test/images/agnhost&#34;&gt;agnhost&lt;/a&gt; network utility to issue HTTP calls on a port and protocol that other pods are
expected to be serving.  A test scenario first
runs a connectivity check to ensure that each pod can reach each other pod, for 81 (= 9 x 9) data
points.  This is the &amp;quot;control&amp;quot;.  Then perturbations are applied, depending on the test scenario:
policies are created, updated, and deleted; labels are added and removed from pods and namespaces,
and so on.  After each change, the connectivity matrix is recollected and compared to the expected
connectivity.&lt;/p&gt;
&lt;p&gt;These results give a visual indication of connectivity in a simple matrix.  Going down the leftmost column is the &amp;quot;source&amp;quot;
pod, or the pod issuing the request; going across the topmost row is the &amp;quot;destination&amp;quot; pod, or the pod
receiving the request.  A &lt;code&gt;.&lt;/code&gt; means that the connection was allowed; an &lt;code&gt;X&lt;/code&gt; means the connection was
blocked. For example:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Nov  4 16:58:43.449: INFO: expected:

-   x/a x/b x/c y/a y/b y/c z/a z/b z/c
x/a .   .   .   .   .   .   .   .   .
x/b .   .   .   .   .   .   .   .   .
x/c .   .   .   .   .   .   .   .   .
y/a .   .   .   .   .   .   .   .   .
y/b .   .   .   .   .   .   .   .   .
y/c .   .   .   .   .   .   .   .   .
z/a .   .   .   .   .   .   .   .   .
z/b .   .   .   .   .   .   .   .   .
z/c .   .   .   .   .   .   .   .   .
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Below are the observed connectivity results in the case of the OVN Kubernetes bug.  Notice how the top three rows indicate that
all requests from namespace x regardless of pod and destination were blocked.  Since these
experimental results do not match the expected results, a failure will be reported.  Note
how the specific pattern of failure provides clear insight into the nature of the problem --
since all requests from a specific namespace fail, we have a clear clue to start our
investigation.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Nov  4 16:58:43.449: INFO: observed:

-   x/a x/b x/c y/a y/b y/c z/a z/b z/c
x/a X   X   X   X   X   X   X   X   X
x/b X   X   X   X   X   X   X   X   X
x/c X   X   X   X   X   X   X   X   X
y/a .   .   .   .   .   .   .   .   .
y/b .   .   .   .   .   .   .   .   .
y/c .   .   .   .   .   .   .   .   .
z/a .   .   .   .   .   .   .   .   .
z/b .   .   .   .   .   .   .   .   .
z/c .   .   .   .   .   .   .   .   .
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This was one of our earliest wins in the Network Policy group, as we were able to
identify and work with the OVN Kubernetes group to fix a bug in egress policy processing.&lt;/p&gt;
&lt;p&gt;However, even though this tool has made it easy to validate roughly 30 common scenarios,
it doesn&#39;t validate &lt;em&gt;all&lt;/em&gt; Network Policy scenarios - because there are an enormous number of possible
permutations that one might create (technically, we might say this number is
infinite given that there&#39;s an infinite number of possible namespace/pod/port/protocol variations one can create).&lt;/p&gt;
&lt;p&gt;Once these tests were in play, we worked with the Upstream SIG Network and SIG Testing communities
(thanks to Antonio Ojea and Ben Elder) to put a testgrid Network Policy job in place.  This job
continuously runs the entire suite of Network Policy tests against
&lt;a href=&#34;https://testgrid.k8s.io/sig-network-gce#presubmit-network-policies,%20google-gce&#34;&gt;GCE with Calico as a Network Policy provider&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Part of our role as a subproject is to help make sure that, when these tests break, we can help triage them effectively.&lt;/p&gt;
&lt;h2 id=&#34;cyclonus&#34;&gt;Cyclonus: The next step towards Network Policy conformance&lt;/h2&gt;
&lt;p&gt;Around the time that we were finishing the validation work, it became clear from the community that,
in general, we needed to solve the overall problem of testing ALL possible Network Policy implementations.
For example, a KEP was recently written which introduced the concept of micro versioning to
Network Policies to accommodate &lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/2137/files&#34;&gt;describing this at the API level&lt;/a&gt;, by Dan Winship.&lt;/p&gt;
&lt;p&gt;In response to this increasingly obvious need to comprehensively evaluate Network
Policy implementations from all vendors, Matt Fenwick decided to evolve our approach to Network Policy validation again by creating Cyclonus.&lt;/p&gt;
&lt;p&gt;Cyclonus is a comprehensive Network Policy fuzzing tool which verifies a CNI provider
against hundreds of different Network Policy scenarios, by defining similar truth table/policy
combinations as demonstrated in the end to end tests, while also providing a hierarchical
representation of policy &amp;quot;categories&amp;quot;.  We&#39;ve found some interesting nuances and issues
in almost every CNI we&#39;ve tested so far, and have even contributed some fixes back.&lt;/p&gt;
&lt;p&gt;To perform a Cyclonus validation run, you create a Job manifest similar to:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;batch/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Job&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cyclonus&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;template&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;restartPolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Never&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;containers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;command&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;- ./cyclonus&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;- generate&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;- --perturbation-wait-seconds=15&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;- --server-protocol=tcp,udp&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cyclonus&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;imagePullPolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;IfNotPresent&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mfenwick100/cyclonus:latest&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;serviceAccount&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;cyclonus&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Cyclonus outputs a report of all the test cases it will run:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;test cases to run by tag:
- target: 6
- peer-ipblock: 4
- udp: 16
- delete-pod: 1
- conflict: 16
- multi-port/protocol: 14
- ingress: 51
- all-pods: 14
- egress: 51
- all-namespaces: 10
- sctp: 10
- port: 56
- miscellaneous: 22
- direction: 100
- multi-peer: 0
- any-port-protocol: 2
- set-namespace-labels: 1
- upstream-e2e: 0
- allow-all: 6
- namespaces-by-label: 6
- deny-all: 10
- pathological: 6
- action: 6
- rule: 30
- policy-namespace: 4
- example: 0
- tcp: 16
- target-namespace: 3
- named-port: 24
- update-policy: 1
- any-peer: 2
- target-pod-selector: 3
- IP-block-with-except: 2
- pods-by-label: 6
- numbered-port: 28
- protocol: 42
- peer-pods: 20
- create-policy: 2
- policy-stack: 0
- any-port: 14
- delete-namespace: 1
- delete-policy: 1
- create-pod: 1
- IP-block-no-except: 2
- create-namespace: 1
- set-pod-labels: 1
testing 112 cases
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that Cyclonus tags its tests based on the type of policy being created, because
the policies themselves are auto-generated, and thus have no meaningful names to be recognized by.&lt;/p&gt;
&lt;p&gt;For each test, Cyclonus outputs a truth table, which is again similar to that of the
E2E tests, along with the policy being validated:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: null
  name: base
  namespace: x
spec:
  egress:
  - ports:
    - port: 81
    to:
    - namespaceSelector:
        matchExpressions:
        - key: ns
          operator: In
          values:
          - &amp;quot;y&amp;quot;
          - z
      podSelector:
        matchExpressions:
        - key: pod
          operator: In
          values:
          - a
          - b
  - ports:
    - port: 53
      protocol: UDP
  ingress:
  - from:
    - namespaceSelector:
        matchExpressions:
        - key: ns
          operator: In
          values:
          - x
          - &amp;quot;y&amp;quot;
      podSelector:
        matchExpressions:
        - key: pod
          operator: In
          values:
          - b
          - c
    ports:
    - port: 80
      protocol: TCP
  podSelector:
    matchLabels:
      pod: a
  policyTypes:
  - Ingress
  - Egress

0 wrong, 0 ignored, 81 correct
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| TCP/80 | X/A | X/B | X/C | Y/A | Y/B | Y/C | Z/A | Z/B | Z/C |
| TCP/81 |     |     |     |     |     |     |     |     |     |
| UDP/80 |     |     |     |     |     |     |     |     |     |
| UDP/81 |     |     |     |     |     |     |     |     |     |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/a    | X   | X   | X   | X   | X   | X   | X   | X   | X   |
|        | X   | X   | X   | .   | .   | X   | .   | .   | X   |
|        | X   | X   | X   | X   | X   | X   | X   | X   | X   |
|        | X   | X   | X   | X   | X   | X   | X   | X   | X   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/b    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/c    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/a    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/b    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/c    | .   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/a    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/b    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/c    | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
|        | X   | .   | .   | .   | .   | .   | .   | .   | .   |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Both Cyclonus and the e2e tests use the same strategy to validate a Network Policy - probing pods over TCP or UDP, with
SCTP support available as well for CNIs that support it (such as Calico).&lt;/p&gt;
&lt;p&gt;As examples of how we use Cyclonus to help make CNI implementations better from a Network Policy perspective, you can see the following issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/antrea/issues/1764&#34;&gt;Antrea: NetworkPolicy: unable to allow ingress by CIDR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/projectcalico/libcalico-go/pull/1373&#34;&gt;Calico: default missing protocol to TCP; don&#39;t let single port overwrite all ports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cilium/cilium/issues/14678&#34;&gt;Cilium: Egress Network Policy allows traffic that should be denied&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The good news is that Antrea and Calico have already merged fixes for all the issues found and other CNI providers are working on it,
with the support of SIG Network and the Network Policy subproject.&lt;/p&gt;
&lt;p&gt;Are you interested in verifying NetworkPolicy functionality on your cluster?
(if you care about security or offer multi-tenant SaaS, you should be)
If so, you can run the upstream end to end tests, or Cyclonus, or both.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you&#39;re just getting started with NetworkPolicies and want to simply
verify the &amp;quot;common&amp;quot; NetworkPolicy cases that most CNIs should be
implementing correctly, in a way that is quick to diagnose, then you&#39;re
better off running the e2e tests only.&lt;/li&gt;
&lt;li&gt;If you are deeply curious about your CNI provider&#39;s NetworkPolicy
implementation, and want to verify it: use Cyclonus.&lt;/li&gt;
&lt;li&gt;If you want to test &lt;em&gt;hundreds&lt;/em&gt; of policies, and evaluate your CNI plugin
for comprehensive functionality, for deep discovery of potential security
holes: use Cyclonus, and also consider running end-to-end cluster tests.&lt;/li&gt;
&lt;li&gt;If you&#39;re thinking of getting involved with the upstream NetworkPolicy efforts:
use Cyclonus, and read at least an outline of which e2e tests are relevant.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;where-to-start-with-networkpolicy-testing&#34;&gt;Where to start with NetworkPolicy testing?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cyclonus is easy to run on your cluster, check out the &lt;a href=&#34;https://github.com/mattfenwick/cyclonus#run-as-a-kubernetes-job&#34;&gt;instructions on github&lt;/a&gt;,
and determine whether &lt;em&gt;your&lt;/em&gt; specific CNI configuration is fully conformant to the hundreds of different
Kubernetes Network Policy API constructs.&lt;/li&gt;
&lt;li&gt;Alternatively, you can use a tool like &lt;a href=&#34;https://github.com/vmware-tanzu/sonobuoy&#34;&gt;sonobuoy&lt;/a&gt;
to run the existing E2E tests in Kubernetes, with the &lt;code&gt;--ginkgo.focus=NetworkPolicy&lt;/code&gt; flag.
Make sure that you use the K8s conformance image for K8s 1.21 or above (for example, by using the &lt;code&gt;--kube-conformance-image-version v1.21.0&lt;/code&gt; flag),
as older images will not have the &lt;em&gt;new&lt;/em&gt; Network Policy tests in them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;improvements-to-the-networkpolicy-api-and-user-experience&#34;&gt;Improvements to the NetworkPolicy API and user experience&lt;/h2&gt;
&lt;p&gt;In addition to cleaning up the validation story for CNI plugins that implement NetworkPolicies,
subproject contributors have also spent some time improving the Kubernetes NetworkPolicy API for a few commonly requested features.
After months of deliberation, we eventually settled on a few core areas for improvement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Port Range policies: We now allow you to specify a &lt;em&gt;range&lt;/em&gt; of ports for a policy.
This allows users interested in scenarios like FTP or virtualization to enable advanced policies.
The port range option for network policies will be available to use in Kubernetes 1.21.
Read more in &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports&#34;&gt;targeting a range of ports&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Namespace as name policies: Allowing users in Kubernetes &amp;gt;= 1.21 to target namespaces using names,
when building Network Policy objects.  This was done in collaboration with Jordan Liggitt and Tim Hockin on the API Machinery side.
This change allowed us to improve the Network Policy user experience without actually
changing the API! For more details, you can read
&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#automatic-labelling&#34;&gt;Automatic labelling&lt;/a&gt; in the page about Namespaces.
The TL,DR; is that for Kubernetes 1.21 and later, &lt;strong&gt;all namespaces&lt;/strong&gt; have the following label added by default:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubernetes.io/metadata.name: &amp;lt;name-of-namespace&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This means you can write a namespace policy against this namespace, even if you can&#39;t edit its labels.
For example, this policy, will &#39;just work&#39;, without needing to run a command such as &lt;code&gt;kubectl edit namespace&lt;/code&gt;.
In fact, it will even work if you can&#39;t edit or view this namespace&#39;s data at all, because of the magic of API server defaulting.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;networking.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NetworkPolicy&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-network-policy&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;podSelector&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;matchLabels&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;role&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;db&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;policyTypes&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- Ingress&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Allow inbound traffic to Pods labelled role=db, in the namespace &amp;#39;default&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# provided that the source is a Pod in the namespace &amp;#39;my-namespace&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;ingress&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;from&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespaceSelector&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;matchLabels&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kubernetes.io/metadata.name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;my-namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;In our tests, we found that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Antrea and Calico are at a point where they support all of cyclonus&#39;s scenarios, modulo a few very minor tweaks which we&#39;ve made.&lt;/li&gt;
&lt;li&gt;Cilium also conformed to the majority of the policies, outside known features that aren&#39;t fully supported (for example, related to the way Cilium deals with pod CIDR policies).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are a CNI provider and interested in helping us to do a better job curating large tests of network policies, please reach out!  We are continuing to curate the Network Policy conformance results from Cyclonus &lt;a href=&#34;https://raw.githubusercontent.com/K8sbykeshed/cyclonus-artifacts/&#34;&gt;here&lt;/a&gt;, but
we are not capable of maintaining all of the subtleties in NetworkPolicy testing data on our own.  For now, we use github actions and Kind to test in CI.&lt;/p&gt;
&lt;h2 id=&#34;the-future&#34;&gt;The Future&lt;/h2&gt;
&lt;p&gt;We&#39;re also working on some improvements for the future of Network Policies, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fully qualified Domain policies: The Google Cloud team created a prototype (which
we are really excited about) of &lt;a href=&#34;https://github.com/GoogleCloudPlatform/gke-fqdnnetworkpolicies-golang&#34;&gt;FQDN policies&lt;/a&gt;.
This tool uses the Network Policy API to enforce policies against L7 URLs, by finding
their IPs and blocking them proactively when requests are made.&lt;/li&gt;
&lt;li&gt;Cluster Administrative policies: We&#39;re working hard at enabling &lt;em&gt;administrative&lt;/em&gt; or
&lt;em&gt;cluster scoped&lt;/em&gt; Network Policies for the future. These are being presented iteratively to the NetworkPolicy subproject.
You can read about them here in &lt;a href=&#34;https://docs.google.com/presentation/d/1Jk86jtS3TcGAugVSM_I4Yds5ukXFJ4F1ZCvxN5v2BaY/&#34;&gt;Cluster Scoped Network Policy&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Network Policy subproject meets on mondays at 4PM EST. For details, check out the
&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-network&#34;&gt;SIG Network community repo&lt;/a&gt;.  We&#39;d love
to hang out with you, hack on stuff, and help you adopt K8s Network Policies for your cluster wherever possible.&lt;/p&gt;
&lt;h3 id=&#34;a-quick-note-on-user-feedback&#34;&gt;A quick note on User Feedback&lt;/h3&gt;
&lt;p&gt;We&#39;ve gotten a lot of ideas and feedback from users on Network Policies.  A lot of people have interesting ideas about Network Policies,
but we&#39;ve found that as a subproject, very few people were deeply interested in implementing these ideas to the full extent.&lt;/p&gt;
&lt;p&gt;Almost every change to the NetworkPolicy API includes weeks or months of discussion to cover different cases, and ensure no CVEs are being introduced.  Thus, long term ownership
is the biggest impediment in improving the NetworkPolicy user experience for us, over time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We&#39;ve documented a lot of the history of the Network Policy dialogue &lt;a href=&#34;https://github.com/jayunit100/network-policy-subproject/blob/master/history.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We&#39;ve also taken a poll of users, for what they&#39;d like to see in the Network Policy API &lt;a href=&#34;https://github.com/jayunit100/network-policy-subproject/blob/master/p0_user_stories.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We encourage anyone to provide us with feedback, but our most pressing issues right now
involve finding &lt;em&gt;long term owners to help us drive changes&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This doesn&#39;t require a lot of technical knowledge, but rather, just a long term commitment to helping us stay organized, do paperwork,
and iterate through the many stages of the K8s feature process.  If you want to help us and get involved, please reach out on the SIG Network mailing list, or in the SIG Network room in the k8s.io slack channel!&lt;/p&gt;
&lt;p&gt;Anyone can put an oar in the water and help make NetworkPolices better!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing Indexed Jobs</title>
      <link>https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Aldo Culquicondor (Google)&lt;/p&gt;
&lt;p&gt;Once you have containerized a non-parallel &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job/&#34;&gt;Job&lt;/a&gt;,
it is quite easy to get it up and running on Kubernetes without modifications to
the binary. In most cases, when running parallel distributed Jobs, you had
to set a separate system to partition the work among the workers. For
example, you could set up a task queue to &lt;a href=&#34;https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/&#34;&gt;assign one work item to each
Pod&lt;/a&gt; or &lt;a href=&#34;https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/&#34;&gt;multiple items
to each Pod until the queue is emptied&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Kubernetes 1.21 release introduces a new field to control Job &lt;em&gt;completion mode&lt;/em&gt;,
a configuration option that allows you to control how Pod completions affect the
overall progress of a Job, with two possible options (for now):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NonIndexed&lt;/code&gt; (default): the Job is considered complete when there has been
a number of successfully completed Pods equal to the specified number in
&lt;code&gt;.spec.completions&lt;/code&gt;. In other words, each Pod completion is homologous to
each other. Any Job you might have created before the introduction of
completion modes is implicitly NonIndexed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Indexed&lt;/code&gt;: the Job is considered complete when there is one successfully
completed Pod associated with each index from 0 to &lt;code&gt;.spec.completions-1&lt;/code&gt;. The
index is exposed to each Pod in the &lt;code&gt;batch.kubernetes.io/job-completion-index&lt;/code&gt;
annotation and the &lt;code&gt;JOB_COMPLETION_INDEX&lt;/code&gt; environment variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can start using Jobs with Indexed completion mode, or Indexed Jobs, for
short, to easily start parallel Jobs. Then, each worker Pod can have a statically
assigned partition of the data based on the index. This saves you from having to
set up a queuing system or even having to modify your binary!&lt;/p&gt;
&lt;h2 id=&#34;creating-an-indexed-job&#34;&gt;Creating an Indexed Job&lt;/h2&gt;
&lt;p&gt;To create an Indexed Job, you just have to add &lt;code&gt;completionMode: Indexed&lt;/code&gt; to the
Job spec and make use of the &lt;code&gt;JOB_COMPLETION_INDEX&lt;/code&gt; environment variable.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;batch/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Job&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;sample-job&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;completions&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;parallelism&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;completionMode&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Indexed&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;template&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;restartPolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Never&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;containers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;command&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;- &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;bash&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;- &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;- &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;echo &amp;#34;My partition: ${JOB_COMPLETION_INDEX}&amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;docker.io/library/bash&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;sample-load&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that completion mode is an alpha feature in the 1.21 release. To be able to
use it in your cluster, make sure to enable the &lt;code&gt;IndexedJob&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature
gate&lt;/a&gt; on the
&lt;a href=&#34;docs/reference/command-line-tools-reference/kube-apiserver/&#34;&gt;API server&lt;/a&gt; and
the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/&#34;&gt;controller manager&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you run the example, you will see that each of the three created Pods gets a
different completion index. For the user&#39;s convenience, the control plane sets the
&lt;code&gt;JOB_COMPLETION_INDEX&lt;/code&gt; environment variable, but you can choose to &lt;a href=&#34;https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/&#34;&gt;set your
own&lt;/a&gt;
or &lt;a href=&#34;https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/&#34;&gt;expose the index as a file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/&#34;&gt;Indexed Job for parallel processing with static work
assignment&lt;/a&gt; for a
step-by-step guide, and a few more examples.&lt;/p&gt;
&lt;h2 id=&#34;future-plans&#34;&gt;Future plans&lt;/h2&gt;
&lt;p&gt;SIG Apps envisions that there might be more completion modes that enable more
use cases for the Job API. We welcome you to open issues in
&lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34;&gt;kubernetes/kubernetes&lt;/a&gt; with your
suggestions.&lt;/p&gt;
&lt;p&gt;In particular, we are considering an &lt;code&gt;IndexedAndUnique&lt;/code&gt; mode where the indexes
are not just available as annotation, but they are part of the Pod names,
similar to &lt;a class=&#39;glossary-tooltip&#39; title=&#39;Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.&#39; data-toggle=&#39;tooltip&#39; data-placement=&#39;top&#39; href=&#39;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#39; target=&#39;_blank&#39; aria-label=&#39;StatefulSet&#39;&gt;StatefulSet&lt;/a&gt;.
This should facilitate inter-Pod communication for tightly coupled Pods.
You can join the discussion in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/99497&#34;&gt;open issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap-up&lt;/h2&gt;
&lt;p&gt;Indexed Jobs allows you to statically partition work among the workers of your
parallel Jobs. SIG Apps hopes that this feature facilitates the migration of
more batch workloads to Kubernetes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Volume Health Monitoring Alpha Update</title>
      <link>https://kubernetes.io/blog/2021/04/16/volume-health-monitoring-alpha-update/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/16/volume-health-monitoring-alpha-update/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Xing Yang (VMware)&lt;/p&gt;
&lt;p&gt;The CSI Volume Health Monitoring feature, originally introduced in 1.19 has undergone a large update for the 1.21 release.&lt;/p&gt;
&lt;h2 id=&#34;why-add-volume-health-monitoring-to-kubernetes&#34;&gt;Why add Volume Health Monitoring to Kubernetes?&lt;/h2&gt;
&lt;p&gt;Without Volume Health Monitoring, Kubernetes has no knowledge of the state of the underlying volumes of a storage system after a PVC is provisioned and used by a Pod. Many things could happen to the underlying storage system after a volume is provisioned in Kubernetes. For example, the volume could be deleted by accident outside of Kubernetes, the disk that the volume resides on could fail, it could be out of capacity, the disk may be degraded which affects its performance, and so on. Even when the volume is mounted on a pod and used by an application, there could be problems later on such as read/write I/O errors, file system corruption, accidental unmounting of the volume outside of Kubernetes, etc. It is very hard to debug and detect root causes when something happened like this.&lt;/p&gt;
&lt;p&gt;Volume health monitoring can be very beneficial to Kubernetes users. It can communicate with the CSI driver to retrieve errors detected by the underlying storage system. PVC events can be reported up to the user to take action. For example, if the volume is out of capacity, they could request a volume expansion to get more space.&lt;/p&gt;
&lt;h2 id=&#34;what-is-volume-health-monitoring&#34;&gt;What is Volume Health Monitoring?&lt;/h2&gt;
&lt;p&gt;CSI Volume Health Monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on PVCs or Pods.&lt;/p&gt;
&lt;p&gt;The Kubernetes components that monitor the volumes and report events with volume health information include the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubelet, in addition to gathering the existing volume stats will watch the volume health of the PVCs on that node. If a PVC has an abnormal health condition, an event will be reported on the pod object using the PVC. If multiple pods are using the same PVC, events will be reported on all pods using that PVC.&lt;/li&gt;
&lt;li&gt;An &lt;a href=&#34;https://github.com/kubernetes-csi/external-health-monitor&#34;&gt;External Volume Health Monitor Controller&lt;/a&gt; watches volume health of the PVCs and reports events on the PVCs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the node side volume health monitoring logic was an external agent when this feature was first introduced in the Kubernetes 1.19 release. In Kubernetes 1.21, the node side volume health monitoring logic was moved from the external agent into the Kubelet, to avoid making duplicate CSI function calls. With this change in 1.21, a new alpha &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt; &lt;code&gt;CSIVolumeHealth&lt;/code&gt; was introduced for the volume health monitoring logic in Kubelet.&lt;/p&gt;
&lt;p&gt;Currently the Volume Health Monitoring feature is informational only as it only reports abnormal volume health events on PVCs or Pods. Users will need to check these events and manually fix the problems. This feature serves as a stepping stone towards programmatic detection and resolution of volume health issues by Kubernetes in the future.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-use-volume-health-on-my-kubernetes-cluster&#34;&gt;How do I use Volume Health on my Kubernetes Cluster?&lt;/h2&gt;
&lt;p&gt;To use the Volume Health feature, first make sure the CSI driver you are using supports this feature. Refer to this &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;CSI drivers doc&lt;/a&gt; to find out which CSI drivers support this feature.&lt;/p&gt;
&lt;p&gt;To enable Volume Health Monitoring from the node side, the alpha feature gate &lt;code&gt;CSIVolumeHealth&lt;/code&gt; needs to be enabled.&lt;/p&gt;
&lt;p&gt;If a CSI driver supports the Volume Health Monitoring feature from the controller side, events regarding abnormal volume conditions will be recorded on PVCs.&lt;/p&gt;
&lt;p&gt;If a CSI driver supports the Volume Health Monitoring feature from the controller side, user can also get events regarding node failures if the &lt;code&gt;enable-node-watcher&lt;/code&gt; flag is set to true when deploying the External Health Monitor Controller. When a node failure event is detected, an event will be reported on the PVC to indicate that pods using this PVC are on a failed node.&lt;/p&gt;
&lt;p&gt;If a CSI driver supports the Volume Health Monitoring feature from the node side, events regarding abnormal volume conditions will be recorded on pods using the PVCs.&lt;/p&gt;
&lt;h2 id=&#34;as-a-storage-vendor-how-do-i-add-support-for-volume-health-to-my-csi-driver&#34;&gt;As a storage vendor, how do I add support for volume health to my CSI driver?&lt;/h2&gt;
&lt;p&gt;Volume Health Monitoring includes two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An External Volume Health Monitoring Controller monitors volume health from the controller side.&lt;/li&gt;
&lt;li&gt;Kubelet monitors volume health from the node side.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For details, see the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;CSI spec&lt;/a&gt; and the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/volume-health-monitor.html&#34;&gt;Kubernetes-CSI Driver Developer Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is a sample implementation for volume health in &lt;a href=&#34;https://github.com/kubernetes-csi/csi-driver-host-path&#34;&gt;CSI host path driver&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;controller-side-volume-health-monitoring&#34;&gt;Controller Side Volume Health Monitoring&lt;/h3&gt;
&lt;p&gt;To learn how to deploy the External Volume Health Monitoring controller, see &lt;a href=&#34;https://kubernetes-csi.github.io/docs/external-health-monitor-controller.html&#34;&gt;CSI external-health-monitor-controller&lt;/a&gt; in the CSI documentation.&lt;/p&gt;
&lt;p&gt;The External Health Monitor Controller calls either &lt;code&gt;ListVolumes&lt;/code&gt; or &lt;code&gt;ControllerGetVolume&lt;/code&gt; CSI RPC and reports VolumeConditionAbnormal events with messages on PVCs if abnormal volume conditions are detected. Only CSI drivers with &lt;code&gt;LIST_VOLUMES&lt;/code&gt; and &lt;code&gt;VOLUME_CONDITION&lt;/code&gt; controller capability or &lt;code&gt;GET_VOLUME&lt;/code&gt; and &lt;code&gt;VOLUME_CONDITION&lt;/code&gt; controller capability support Volume Health Monitoring in the external controller.&lt;/p&gt;
&lt;p&gt;To implement the volume health feature from the controller side, a CSI driver &lt;strong&gt;must&lt;/strong&gt; add support for the new controller capabilities.&lt;/p&gt;
&lt;p&gt;If a CSI driver supports &lt;code&gt;LIST_VOLUMES&lt;/code&gt; and &lt;code&gt;VOLUME_CONDITION&lt;/code&gt; controller capabilities, it &lt;strong&gt;must&lt;/strong&gt; implement controller RPC &lt;code&gt;ListVolumes&lt;/code&gt; and report the volume condition in the response.&lt;/p&gt;
&lt;p&gt;If a CSI driver supports &lt;code&gt;GET_VOLUME&lt;/code&gt; and &lt;code&gt;VOLUME_CONDITION&lt;/code&gt; controller capability, it &lt;strong&gt;must&lt;/strong&gt; implement controller PRC &lt;code&gt;ControllerGetVolume&lt;/code&gt; and report the volume condition in the response.&lt;/p&gt;
&lt;p&gt;If a CSI driver supports &lt;code&gt;LIST_VOLUMES&lt;/code&gt;, &lt;code&gt;GET_VOLUME&lt;/code&gt;, and &lt;code&gt;VOLUME_CONDITION&lt;/code&gt; controller capabilities, only &lt;code&gt;ListVolumes&lt;/code&gt; CSI RPC will be invoked by the External Health Monitor Controller.&lt;/p&gt;
&lt;h3 id=&#34;node-side-volume-health-monitoring&#34;&gt;Node Side Volume Health Monitoring&lt;/h3&gt;
&lt;p&gt;Kubelet calls &lt;code&gt;NodeGetVolumeStats&lt;/code&gt; CSI RPC and reports VolumeConditionAbnormal events with messages on Pods if abnormal volume conditions are detected. Only CSI drivers with &lt;code&gt;VOLUME_CONDITION&lt;/code&gt; node capability support Volume Health Monitoring in Kubelet.&lt;/p&gt;
&lt;p&gt;To implement the volume health feature from the node side, a CSI driver &lt;strong&gt;must&lt;/strong&gt; add support for the new node capabilities.&lt;/p&gt;
&lt;p&gt;If a CSI driver supports &lt;code&gt;VOLUME_CONDITION&lt;/code&gt; node capability, it &lt;strong&gt;must&lt;/strong&gt; report the volume condition in node RPC &lt;code&gt;NodeGetVoumeStats&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI volume health implementation to beta in either 1.22 or 1.23.&lt;/p&gt;
&lt;p&gt;We are also exploring how to use volume health information for programmatic detection and automatic reconcile in Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;
&lt;p&gt;To learn the design details for Volume Health Monitoring, read the &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor&#34;&gt;Volume Health Monitor&lt;/a&gt; enhancement proposal.&lt;/p&gt;
&lt;p&gt;The Volume Health Monitor controller source code is at &lt;a href=&#34;https://github.com/kubernetes-csi/external-health-monitor&#34;&gt;https://github.com/kubernetes-csi/external-health-monitor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are also more details about volume health checks in the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34;&gt;Container Storage Interface Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://kubernetes.slack.com/messages/csi&#34;&gt;Kubernetes Slack channel #csi&lt;/a&gt; and any of the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact&#34;&gt;standard SIG Storage communication channels&lt;/a&gt; are great mediums to reach out to the SIG Storage and the CSI team.&lt;/p&gt;
&lt;p&gt;We offer a huge thank you to the contributors who helped release this feature in 1.21. We want to thank Yuquan Ren (&lt;a href=&#34;https://github.com/nickrenren&#34;&gt;NickrenREN&lt;/a&gt;) who implemented the initial volume health monitor controller and agent in the external health monitor repo, thank Ran Xu (&lt;a href=&#34;https://github.com/fengzixu&#34;&gt;fengzixu&lt;/a&gt;) who moved the volume health monitoring logic from the external agent to Kubelet in 1.21, and we offer special thanks to the following people for their insightful reviews: David Ashpole (&lt;a href=&#34;https://github.com/dashpole&#34;&gt;dashpole&lt;/a&gt;), Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34;&gt;msau42&lt;/a&gt;), David Eads (&lt;a href=&#34;https://github.com/deads2k&#34;&gt;deads2k&lt;/a&gt;), Elana Hashman (&lt;a href=&#34;https://github.com/ehashman&#34;&gt;ehashman&lt;/a&gt;), Seth Jennings (&lt;a href=&#34;https://github.com/sjenning&#34;&gt;sjenning&lt;/a&gt;), and Jiawei Wang (&lt;a href=&#34;https://github.com/Jiawei0227&#34;&gt;Jiawei0227&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Three Tenancy Models For Kubernetes</title>
      <link>https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/</link>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ryan Bezdicek (Medtronic), Jim Bugwadia (Nirmata), Tasha Drew (VMware),  Fei Guo (Alibaba), Adrian Ludwin (Google)&lt;/p&gt;
&lt;p&gt;Kubernetes clusters are typically used by several teams in an organization. In other cases, Kubernetes may be used to deliver applications to end users requiring segmentation and isolation of resources across users from different organizations. Secure sharing of Kubernetes control plane and worker node resources allows maximizing productivity and saving costs in both cases.&lt;/p&gt;
&lt;p&gt;The Kubernetes Multi-Tenancy Working Group is chartered with defining tenancy models for Kubernetes and making it easier to operationalize tenancy related use cases. This blog post, from the working group members, describes three common tenancy models and introduces related working group projects.&lt;/p&gt;
&lt;p&gt;We will also be presenting on this content and discussing different use cases at our Kubecon EU 2021 panel session, &lt;a href=&#34;https://sched.co/iE66&#34;&gt;Multi-tenancy vs. Multi-cluster: When Should you Use What?&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;namespaces-as-a-service&#34;&gt;Namespaces as a Service&lt;/h2&gt;
&lt;p&gt;With the &lt;em&gt;namespaces-as-a-service&lt;/em&gt; model, tenants share a cluster and tenant workloads are restricted to a set of Namespaces assigned to the tenant. The cluster control plane resources like the API server and scheduler, and worker node resources like CPU, memory, etc. are available for use across all tenants.&lt;/p&gt;
&lt;p&gt;To isolate tenant workloads, each namespace must also contain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding&#34;&gt;role bindings&lt;/a&gt;:&lt;/strong&gt; for controlling access to the namespace&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34;&gt;network policies&lt;/a&gt;:&lt;/strong&gt; to prevent network traffic across tenants&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/resource-quotas/&#34;&gt;resource quotas&lt;/a&gt;:&lt;/strong&gt; to limit usage and ensure fairness across tenants&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this model, tenants share cluster-wide resources like ClusterRoles and CustomResourceDefinitions (CRDs) and hence cannot create or update these cluster-wide resources.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/&#34;&gt;Hierarchical Namespace Controller (HNC)&lt;/a&gt; project makes it easier to manage namespace based tenancy by allowing users to create additional namespaces under a namespace, and propagating resources within the namespace hierarchy. This allows self-service namespaces for tenants, without requiring cluster-wide permissions.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/multi-tenancy/tree/master/benchmarks&#34;&gt;Multi-Tenancy Benchmarks (MTB)&lt;/a&gt; project provides benchmarks and a command-line tool that performs several configuration and runtime checks to report if tenant namespaces are properly isolated and the necessary security controls are implemented.&lt;/p&gt;
&lt;h2 id=&#34;clusters-as-a-service&#34;&gt;Clusters as a Service&lt;/h2&gt;
&lt;p&gt;With the &lt;em&gt;clusters-as-a-service&lt;/em&gt; usage model, each tenant gets their own cluster.  This model allows tenants to have different versions of cluster-wide resources such as CRDs, and provides full isolation of the Kubernetes control plane.&lt;/p&gt;
&lt;p&gt;The tenant clusters may be provisioned using projects like &lt;a href=&#34;https://cluster-api.sigs.k8s.io/&#34;&gt;Cluster API (CAPI)&lt;/a&gt; where a management cluster is used to provision multiple workload clusters. A workload cluster is assigned to a tenant and tenants have full control over cluster resources. Note that in most enterprises a central platform team may be responsible for managing required add-on services such as security and monitoring services, and for providing cluster lifecycle management services such as patching and upgrades. A tenant administrator may be restricted from modifying the centrally managed services and other critical cluster information.&lt;/p&gt;
&lt;h2 id=&#34;control-planes-as-a-service&#34;&gt;Control planes as a Service&lt;/h2&gt;
&lt;p&gt;In a variation of the &lt;em&gt;clusters-as-a-service&lt;/em&gt; model, the tenant cluster may be a &lt;strong&gt;virtual cluster&lt;/strong&gt; where each tenant gets their own dedicated Kubernetes control plane but share worker node resources. As with other forms of virtualization, users of a virtual cluster see no significant differences between a virtual cluster and other Kubernetes clusters. This is sometimes referred to as &lt;code&gt;Control Planes as a Service&lt;/code&gt; (CPaaS).&lt;/p&gt;
&lt;p&gt;A virtual cluster of this type shares worker node resources and workload state independent control plane components, like the scheduler. Other workload aware control-plane components, like the API server, are created on a per-tenant basis to allow overlaps, and additional components are used to synchronize and manage state across the per-tenant control plane and the underlying shared cluster resources. With this model users can manage their own cluster-wide resources.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/virtualcluster&#34;&gt;Virtual Cluster&lt;/a&gt; project implements this model, where a &lt;code&gt;supercluster&lt;/code&gt; is shared by multiple &lt;code&gt;virtual clusters&lt;/code&gt;. The &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api-provider-nested&#34;&gt;Cluster API Nested&lt;/a&gt; project is extending this work to conform to the CAPI model, allowing use of familiar API resources to provision and manage virtual clusters.&lt;/p&gt;
&lt;h2 id=&#34;security-considerations&#34;&gt;Security considerations&lt;/h2&gt;
&lt;p&gt;Cloud native security involves different system layers and lifecycle phases as described in the &lt;a href=&#34;https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters&#34;&gt;Cloud Native Security Whitepaper&lt;/a&gt; from CNCF SIG Security.  Without proper security measures implemented across all layers and phases, Kubernetes tenant isolation can be compromised and a security breach with one tenant can threaten other tenants.&lt;/p&gt;
&lt;p&gt;It is important for any new user to Kubernetes to realize that the default installation of a new upstream Kubernetes cluster is not secure, and you are going to need to invest in hardening it in order to avoid security issues.&lt;/p&gt;
&lt;p&gt;At a minimum, the following security measures are required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image scanning: container image vulnerabilities can be exploited to execute commands and access additional resources.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&#34;&gt;RBAC&lt;/a&gt;: for &lt;em&gt;namespaces-as-a-service&lt;/em&gt; user roles and permissions must be properly configured at a per-namespace level; for other models tenants may need to be restricted from accessing centrally managed add-on services and other cluster-wide resources.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34;&gt;network policies&lt;/a&gt;: for &lt;em&gt;namespaces-as-a-service&lt;/em&gt; default network policies that deny all ingress and egress traffic are recommended to prevent cross-tenant network traffic and may also be used as a best practice for other tenancy models.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/security/pod-security-standards/&#34;&gt;Kubernetes Pod Security Standards&lt;/a&gt;: to enforce Pod hardening best practices the &lt;code&gt;Restricted&lt;/code&gt; policy is recommended as the default for tenant workloads with exclusions configured only as needed.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cisecurity.org/benchmark/kubernetes/&#34;&gt;CIS Benchmarks for Kubernetes&lt;/a&gt;: the CIS Benchmarks for Kubernetes guidelines should be used to properly configure Kubernetes control-plane and worker node components.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional recommendations include using:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;policy engines: for configuration security best practices, such as only allowing trusted registries.&lt;/li&gt;
&lt;li&gt;runtime scanners: to detect and report runtime security events.&lt;/li&gt;
&lt;li&gt;VM-based container sandboxing: for stronger data plane isolation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While proper security is required independently of tenancy models, not having essential security controls like &lt;a href=&#34;https://kubernetes.io/docs/concepts/security/pod-security-standards/&#34;&gt;pod security&lt;/a&gt; in a shared cluster provides attackers with means to compromise tenancy models and possibly access sensitive information across tenants increasing the overall risk profile.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;A 2020 CNCF survey showed that production Kubernetes usage has increased by over 300% since 2016. As an increasing number of Kubernetes workloads move to production, organizations are looking for ways to share Kubernetes resources across teams for agility and cost savings.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;namespaces as a service&lt;/strong&gt; tenancy model allows sharing clusters and hence enables resource efficiencies. However, it requires proper security configurations and has limitations as all tenants share the same cluster-wide resources.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;clusters as a service&lt;/strong&gt; tenancy model addresses these limitations, but with higher management and resource overhead.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;control planes as a service&lt;/strong&gt; model provides a way to share resources of a single Kubernetes cluster and also let tenants manage their own cluster-wide resources. Sharing worker node resources increases resource effeciencies, but also exposes cross tenant security and isolation concerns that exist for shared clusters.&lt;/p&gt;
&lt;p&gt;In many cases, organizations will use multiple tenancy models to address different use cases and as different product and development teams will have varying needs. Following security and management best practices, such as applying &lt;a href=&#34;https://kubernetes.io/docs/concepts/security/pod-security-standards/&#34;&gt;Pod Security Standards&lt;/a&gt; and not using the &lt;code&gt;default&lt;/code&gt; namespace, makes it easer to switch from one model to another.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/multi-tenancy&#34;&gt;Kubernetes Multi-Tenancy Working Group&lt;/a&gt; has created several projects like &lt;a href=&#34;https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc&#34;&gt;Hierarchical Namespaces Controller&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/virtualcluster&#34;&gt;Virtual Cluster&lt;/a&gt; / &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api-provider-nested&#34;&gt;CAPI Nested&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kubernetes-sigs/multi-tenancy/tree/master/benchmarks&#34;&gt;Multi-Tenancy Benchmarks&lt;/a&gt; to make it easier to provision and manage multi-tenancy models.&lt;/p&gt;
&lt;p&gt;If you are interested in multi-tenancy topics, or would like to share your use cases, please join us in an upcoming &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/wg-multitenancy/README.md&#34;&gt;community meeting&lt;/a&gt; or reach out on the &lt;em&gt;wg-multitenancy channel&lt;/em&gt; on the &lt;a href=&#34;https://slack.k8s.io/&#34;&gt;Kubernetes slack&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Local Storage: Storage Capacity Tracking, Distributed Provisioning and Generic Ephemeral Volumes hit Beta</title>
      <link>https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Patrick Ohly (Intel)&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes&#34;&gt;&amp;quot;generic ephemeral
volumes&amp;quot;&lt;/a&gt;
and &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-capacity/&#34;&gt;&amp;quot;storage capacity
tracking&amp;quot;&lt;/a&gt;
features in Kubernetes are getting promoted to beta in Kubernetes
1.21. Together with the &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node&#34;&gt;distributed provisioning
support&lt;/a&gt;
in the CSI external-provisioner, development and deployment of
Container Storage Interface (CSI) drivers which manage storage locally
on a node become a lot easier.&lt;/p&gt;
&lt;p&gt;This blog post explains how such drivers worked before and how these
features can be used to make drivers simpler.&lt;/p&gt;
&lt;h2 id=&#34;problems-we-are-solving&#34;&gt;Problems we are solving&lt;/h2&gt;
&lt;p&gt;There are drivers for local storage, like
&lt;a href=&#34;https://github.com/cybozu-go/topolvm&#34;&gt;TopoLVM&lt;/a&gt; for traditional disks
and &lt;a href=&#34;https://intel.github.io/pmem-csi/latest/README.html&#34;&gt;PMEM-CSI&lt;/a&gt;
for &lt;a href=&#34;https://pmem.io/&#34;&gt;persistent memory&lt;/a&gt;. They work and are ready for
usage today also on older Kubernetes releases, but making that possible
was not trivial.&lt;/p&gt;
&lt;h3 id=&#34;central-component-required&#34;&gt;Central component required&lt;/h3&gt;
&lt;p&gt;The first problem is volume provisioning: it is handled through the
Kubernetes control plane. Some component must react to
&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims&#34;&gt;PersistentVolumeClaims&lt;/a&gt;
(PVCs)
and create volumes. Usually, that is handled by a central deployment
of the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/external-provisioner.html&#34;&gt;CSI
external-provisioner&lt;/a&gt;
and a CSI driver component that then connects to the storage
backplane. But for local storage, there is no such backplane.&lt;/p&gt;
&lt;p&gt;TopoLVM solved this by having its different components communicate
with each other through the Kubernetes API server by creating and
reacting to custom resources. So although TopoLVM is based on CSI, a
standard that is independent of a particular container orchestrator,
TopoLVM only works on Kubernetes.&lt;/p&gt;
&lt;p&gt;PMEM-CSI created its own storage backplane with communication through
gRPC calls. Securing that communication depends on TLS certificates,
which made driver deployment more complicated.&lt;/p&gt;
&lt;h3 id=&#34;informing-pod-scheduler-about-capacity&#34;&gt;Informing Pod scheduler about capacity&lt;/h3&gt;
&lt;p&gt;The next problem is scheduling. When volumes get created independently
of pods (&amp;quot;immediate binding&amp;quot;), the CSI driver must pick a node without
knowing anything about the pod(s) that are going to use it. Topology
information then forces those pods to run on the node where the volume
was created. If other resources like RAM or CPU are exhausted there,
the pod cannot start. This can be avoided by configuring in the
StorageClass that volume creation is meant to wait for the first pod
that uses a volume (&lt;code&gt;volumeBinding: WaitForFirstConsumer&lt;/code&gt;). In that
mode, the Kubernetes scheduler tentatively picks a node based on other
constraints and then the external-provisioner is asked to create a
volume such that it is usable there. If local storage is exhausted,
the provisioner &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner/blob/master/doc/design.md&#34;&gt;can
ask&lt;/a&gt;
for another scheduling round. But without information about available
capacity, the scheduler might always pick the same unsuitable node.&lt;/p&gt;
&lt;p&gt;Both TopoLVM and PMEM-CSI solved this with scheduler extenders. This
works, but it is hard to configure when deploying the driver because
communication between kube-scheduler and the driver is very dependent
on how the cluster was set up.&lt;/p&gt;
&lt;h3 id=&#34;rescheduling&#34;&gt;Rescheduling&lt;/h3&gt;
&lt;p&gt;A common use case for local storage is scratch space. A better fit for
that use case than persistent volumes are ephemeral volumes that get
created for a pod and destroyed together with it. The initial API for
supporting ephemeral volumes with CSI drivers (hence called &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes&#34;&gt;&amp;quot;&lt;em&gt;CSI&lt;/em&gt;
ephemeral
volumes&amp;quot;&lt;/a&gt;)
was &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md&#34;&gt;designed for light-weight
volumes&lt;/a&gt;
where volume creation is unlikely to fail. Volume creation happens
after pods have been permanently scheduled onto a node, in contrast to
the traditional provisioning where volume creation is tried before
scheduling a pod onto a node. CSI drivers must be modified to support
&amp;quot;CSI ephemeral volumes&amp;quot;, which was done for TopoLVM and PMEM-CSI. But
due to the design of the feature in Kubernetes, pods can get stuck
permanently if storage capacity runs out on a node. The scheduler
extenders try to avoid that, but cannot be 100% reliable.&lt;/p&gt;
&lt;h2 id=&#34;enhancements-in-kubernetes-1-21&#34;&gt;Enhancements in Kubernetes 1.21&lt;/h2&gt;
&lt;h3 id=&#34;distributed-provisioning&#34;&gt;Distributed provisioning&lt;/h3&gt;
&lt;p&gt;Starting with &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner/releases/tag/v2.1.0&#34;&gt;external-provisioner
v2.1.0&lt;/a&gt;,
released for Kubernetes 1.20, provisioning can be handled by
external-provisioner instances that get &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node&#34;&gt;deployed together with the
CSI driver on each
node&lt;/a&gt;
and then cooperate to provision volumes (&amp;quot;distributed
provisioning&amp;quot;). There is no need any more to have a central component
and thus no need for communication between nodes, at least not for
provisioning.&lt;/p&gt;
&lt;h3 id=&#34;storage-capacity-tracking&#34;&gt;Storage capacity tracking&lt;/h3&gt;
&lt;p&gt;A scheduler extender still needs some way to find out about capacity
on each node. When PMEM-CSI switched to distributed provisioning in
v0.9.0, this was done by querying the metrics data exposed by the
local driver containers. But it is better also for users to eliminate
the need for a scheduler extender completely because the driver
deployment becomes simpler. &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-capacity/&#34;&gt;Storage capacity
tracking&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/&#34;&gt;introduced in
1.19&lt;/a&gt;
and promoted to beta in Kubernetes 1.21, achieves that. It works by
publishing information about capacity in &lt;code&gt;CSIStorageCapacity&lt;/code&gt;
objects. The scheduler itself then uses that information to filter out
unsuitable nodes. Because information might be not quite up-to-date,
pods may still get assigned to nodes with insufficient storage, it&#39;s
just less likely and the next scheduling attempt for a pod should work
better once the information got refreshed.&lt;/p&gt;
&lt;h3 id=&#34;generic-ephemeral-volumes&#34;&gt;Generic ephemeral volumes&lt;/h3&gt;
&lt;p&gt;So CSI drivers still need the ability to recover from a bad scheduling
decision, something that turned out to be impossible to implement for
&amp;quot;CSI ephemeral volumes&amp;quot;. &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes&#34;&gt;&amp;quot;&lt;em&gt;Generic&lt;/em&gt; ephemeral
volumes&amp;quot;&lt;/a&gt;,
another feature that got promoted to beta in 1.21, don&#39;t have that
limitation. This feature adds a controller that will create and manage
PVCs with the lifetime of the Pod and therefore the normal recovery
mechanism also works for them. Existing storage drivers will be able
to process these PVCs without any new logic to handle this new
scenario.&lt;/p&gt;
&lt;h2 id=&#34;known-limitations&#34;&gt;Known limitations&lt;/h2&gt;
&lt;p&gt;Both generic ephemeral volumes and storage capacity tracking increase
the load on the API server. Whether that is a problem depends a lot on
the kind of workload, in particular how many pods have volumes and how
often those need to be created and destroyed.&lt;/p&gt;
&lt;p&gt;No attempt was made to model how scheduling decisions affect storage
capacity. That&#39;s because the effect can vary considerably depending on
how the storage system handles storage. The effect is that multiple
pods with unbound volumes might get assigned to the same node even
though there is only sufficient capacity for one pod. Scheduling
should recover, but it would be more efficient if the scheduler knew
more about storage.&lt;/p&gt;
&lt;p&gt;Because storage capacity gets published by a running CSI driver and
the cluster autoscaler needs information about a node that hasn&#39;t been
created yet, it will currently not scale up a cluster for pods that
need volumes. There is an &lt;a href=&#34;https://github.com/kubernetes/autoscaler/pull/3887&#34;&gt;idea how to provide that
information&lt;/a&gt;, but
more work is needed in that area.&lt;/p&gt;
&lt;p&gt;Distributed snapshotting and resizing are not currently supported. It
should be doable to adapt the respective sidecar and there are
tracking issues for external-snapshotter and external-resizer open
already, they just need some volunteer.&lt;/p&gt;
&lt;p&gt;The recovery from a bad scheduling decising can fail for pods with
multiple volumes, in particular when those volumes are local to nodes:
if one volume can be created and then storage is insufficient for
another volume, the first volume continues to exist and forces the
scheduler to put the pod onto the node of that volume. There is an
idea how do deal with this, &lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/1703&#34;&gt;rolling back the provision of the
volume&lt;/a&gt;, but
this is only in the very early stages of brainstorming and not even a
merged KEP yet. For now it is better to avoid creating pods with more
than one persistent volume.&lt;/p&gt;
&lt;h2 id=&#34;enabling-the-new-features-and-next-steps&#34;&gt;Enabling the new features and next steps&lt;/h2&gt;
&lt;p&gt;With the feature entering beta in the 1.21 release, no additional actions are needed to enable it. Generic
ephemeral volumes also work without changes in CSI drivers. For more
information, see the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes&#34;&gt;documentation&lt;/a&gt;
and the &lt;a href=&#34;https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/&#34;&gt;previous blog
post&lt;/a&gt;
about it. The API has not changed at all between alpha and beta.&lt;/p&gt;
&lt;p&gt;For the other two features, the external-provisioner documentation
explains how CSI driver developers must change how their driver gets
deployed to support &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner#capacity-support&#34;&gt;storage capacity
tracking&lt;/a&gt;
and &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node&#34;&gt;distributed
provisioning&lt;/a&gt;.
These two features are independent, therefore it is okay to enable
only one of them.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34;&gt;SIG
Storage&lt;/a&gt;
would like to hear from you if you are using these new features. We
can be reached through
&lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-storage&#34;&gt;email&lt;/a&gt;,
&lt;a href=&#34;https://slack.k8s.io/&#34;&gt;Slack&lt;/a&gt; (channel &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-storage&#34;&gt;&lt;code&gt;#sig-storage&lt;/code&gt;&lt;/a&gt;) and in the
&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage#meeting&#34;&gt;regular SIG
meeting&lt;/a&gt;.
A description of your workload would be very useful to validate design
decisions, set up performance tests and eventually promote these
features to GA.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks a lot to the members of the community who have contributed to these
features or given feedback including members of SIG Scheduling, SIG Auth,
￼and of course SIG Storage!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: kube-state-metrics goes v2.0</title>
      <link>https://kubernetes.io/blog/2021/04/13/kube-state-metrics-v-2-0/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/13/kube-state-metrics-v-2-0/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lili Cosic (Red Hat), Frederic Branczyk (Polar Signals), Manuel Rüger (Sony Interactive Entertainment), Tariq Ibrahim (Salesforce)&lt;/p&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics&#34;&gt;kube-state-metrics&lt;/a&gt;, a project under the Kubernetes organization, generates Prometheus format metrics based on the current state of the Kubernetes native resources. It does this by listening to the Kubernetes API and gathering information about resources and objects, e.g. Deployments, Pods, Services, and StatefulSets. A full list of resources is available in the &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics/tree/master/docs&#34;&gt;documentation&lt;/a&gt; of kube-state-metrics.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;There are numerous useful metrics and insights provided by &lt;code&gt;kube-state-metrics&lt;/code&gt; right out of the box! These metrics can be used to serve as an insight into your cluster: Either through metrics alone, in the form of dashboards, or through an alerting pipeline. To provide a few examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kube_pod_container_status_restarts_total&lt;/code&gt; can be used to alert on a crashing pod.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube_deployment_status_replicas&lt;/code&gt; which together with &lt;code&gt;kube_deployment_status_replicas_available&lt;/code&gt; can be used to alert on whether a deployment is rolled out successfully or stuck.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube_pod_container_resource_requests&lt;/code&gt; and &lt;code&gt;kube_pod_container_resource_limits&lt;/code&gt; can be used in capacity planning dashboards.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And there are many more metrics available! To learn more about the other metrics and their details, please check out the &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics/tree/master/docs#readme&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-is-new-in-v2-0&#34;&gt;What is new in v2.0?&lt;/h2&gt;
&lt;p&gt;So now that we know what kube-state-metrics is, we are excited to announce the next release: kube-state-metrics v2.0! This release was long-awaited and started with an alpha release in September 2020. To ease maintenance we removed tech debt and also adjusted some confusing wording around user-facing flags and APIs. We also removed some metrics that caused unnecessarily high cardinality in Prometheus! For the 2.0 release, we took the time to set up scale and performance testing. This allows us to better understand if we hit any issues in large clusters and also to document resource request recommendations for your clusters. In this release (and v1.9.8) container builds providing support for multiple architectures were introduced allowing you to run kube-state-metrics on ARM, ARM64, PPC64 and S390x as well!&lt;/p&gt;
&lt;p&gt;So without further ado, here is the list of more noteworthy user-facing breaking changes. A full list of changes, features and bug fixes is available in the changelog at the end of this post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flag &lt;code&gt;--namespace&lt;/code&gt; was renamed to &lt;code&gt;--namespaces&lt;/code&gt;. If you are using the former, please make sure to update the flag before deploying the latest release.&lt;/li&gt;
&lt;li&gt;Flag &lt;code&gt;--collectors&lt;/code&gt; was renamed to &lt;code&gt;--resources&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Flags &lt;code&gt;--metric-blacklist&lt;/code&gt; and &lt;code&gt;--metric-whitelist&lt;/code&gt; were renamed to &lt;code&gt;--metric-denylist&lt;/code&gt; and &lt;code&gt;--metric-allowlist&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Flag &lt;code&gt;--metric-labels-allowlist&lt;/code&gt; allows you to specify a list of Kubernetes labels that get turned into the dimensions of the &lt;code&gt;kube_&amp;lt;resource-name&amp;gt;_labels&lt;/code&gt; metrics. By default, the metric contains only name and namespace labels.&lt;/li&gt;
&lt;li&gt;All metrics with a prefix of &lt;code&gt;kube_hpa_*&lt;/code&gt; were renamed to &lt;code&gt;kube_horizontalpodautoscaler_*&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Metric labels that relate to Kubernetes were converted to snake_case.&lt;/li&gt;
&lt;li&gt;If you are importing kube-state-metrics as a library, we have updated our go module path to &lt;code&gt;k8s.io/kube-state-metrics/v2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All deprecated stable metrics were removed as per the &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics/tree/release-1.9/docs#metrics-deprecation&#34;&gt;notice in the v1.9 release&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;quay.io/coreos/kube-state-metrics&lt;/code&gt; images will no longer be updated. &lt;code&gt;k8s.gcr.io/kube-state-metrics/kube-state-metrics&lt;/code&gt; is the new canonical location.&lt;/li&gt;
&lt;li&gt;The helm chart that is part of the kubernetes/kube-state-metrics repository is deprecated. &lt;a href=&#34;https://github.com/prometheus-community/helm-charts&#34;&gt;https://github.com/prometheus-community/helm-charts&lt;/a&gt; will be its new location.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the full list of v2.0 release changes includes features, bug fixes and other breaking changes see the full &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics/blob/master/CHANGELOG.md&#34;&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;found-a-problem&#34;&gt;Found a problem?&lt;/h2&gt;
&lt;p&gt;Thanks to all our users for testing so far and thank you to all our contributors for your issue reports as well as code and documentation changes! If you find any problems, we the &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics/blob/master/OWNERS&#34;&gt;maintainers&lt;/a&gt; are more than happy to look into them, so please report them by opening a &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics/issues/new/choose&#34;&gt;GitHub issue&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing Suspended Jobs</title>
      <link>https://kubernetes.io/blog/2021/04/12/introducing-suspended-jobs/</link>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/12/introducing-suspended-jobs/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Adhityaa Chandrasekar (Google)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job/&#34;&gt;Jobs&lt;/a&gt; are a crucial part of
Kubernetes&#39; API. While other kinds of workloads such as &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34;&gt;Deployments&lt;/a&gt;,
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&#34;&gt;ReplicaSets&lt;/a&gt;,
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34;&gt;StatefulSets&lt;/a&gt;, and
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34;&gt;DaemonSets&lt;/a&gt;
solve use-cases that require Pods to run forever, Jobs are useful when Pods need
to run to completion. Commonly used in parallel batch processing, Jobs can be
used in a variety of applications ranging from video rendering and database
maintenance to sending bulk emails and scientific computing.&lt;/p&gt;
&lt;p&gt;While the amount of parallelism and the conditions for Job completion are
configurable, the Kubernetes API lacked the ability to suspend and resume Jobs.
This is often desired when cluster resources are limited and a higher priority
Job needs to execute in the place of another Job. Deleting the lower priority
Job is a poor workaround as Pod completion history and other metrics associated
with the Job will be lost.&lt;/p&gt;
&lt;p&gt;With the recent Kubernetes 1.21 release, you will be able to suspend a Job by
updating its spec. The feature is currently in &lt;strong&gt;alpha&lt;/strong&gt; and requires you to
enable the &lt;code&gt;SuspendJob&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt;
on the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&#34;&gt;API server&lt;/a&gt;
and the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/&#34;&gt;controller manager&lt;/a&gt;
in order to use it.&lt;/p&gt;
&lt;h2 id=&#34;api-changes&#34;&gt;API changes&lt;/h2&gt;
&lt;p&gt;We introduced a new boolean field &lt;code&gt;suspend&lt;/code&gt; into the &lt;code&gt;.spec&lt;/code&gt; of Jobs. Let&#39;s say
I create the following Job:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;batch/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Job&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;my-job&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;suspend&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;parallelism&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;completions&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;template&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;containers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;my-container&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;busybox&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;command&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;sleep&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;5&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;restartPolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Never&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Jobs are not suspended by default, so I&#39;m explicitly setting the &lt;code&gt;suspend&lt;/code&gt; field
to &lt;em&gt;true&lt;/em&gt; in the &lt;code&gt;.spec&lt;/code&gt; of the above Job manifest. In the above example, the
Job controller will refrain from creating Pods until I&#39;m ready to start the Job,
which I can do by updating &lt;code&gt;suspend&lt;/code&gt; to false.&lt;/p&gt;
&lt;p&gt;As another example, consider a Job that was created with the &lt;code&gt;suspend&lt;/code&gt; field
omitted. The Job controller will happily create Pods to work towards Job
completion. However, before the Job completes, if I explicitly set the field to
true with a Job update, the Job controller will terminate all active Pods that
are running and will wait indefinitely for the flag to be flipped back to false.
Typically, Pod termination is done by sending a SIGTERM signal to all container
processes in the Pod; the &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination&#34;&gt;graceful termination period&lt;/a&gt;
defined in the Pod spec will be honoured. Pods terminated this way will not be
counted as failures by the Job controller.&lt;/p&gt;
&lt;p&gt;It is important to understand that succeeded and failed Pods from the past will
continue to exist after you suspend a Job. That is, that they will count towards
Job completion once you resume it. You can verify this by looking at Job&#39;s
status before and after suspension.&lt;/p&gt;
&lt;p&gt;Read the &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job#suspending-a-job&#34;&gt;documentation&lt;/a&gt;
for a full overview of this new feature.&lt;/p&gt;
&lt;h2 id=&#34;where-is-this-useful&#34;&gt;Where is this useful?&lt;/h2&gt;
&lt;p&gt;Let&#39;s say I&#39;m the operator of a large cluster. I have many users submitting Jobs
to the cluster, but not all Jobs are created equal — some Jobs are more
important than others. Cluster resources aren&#39;t infinite either, so all users
must share resources. If all Jobs were created in the suspended state and placed
in a pending queue, I can achieve priority-based Job scheduling by resuming Jobs
in the right order.&lt;/p&gt;
&lt;p&gt;As another motivational use-case, consider a cloud provider where compute
resources are cheaper at night than in the morning. If I have a long-running Job
that takes multiple days to complete, being able to suspend the Job in the
morning and then resume it in the evening every day can reduce costs.&lt;/p&gt;
&lt;p&gt;Since this field is a part of the Job spec, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;CronJobs&lt;/a&gt;
automatically get this feature for free too.&lt;/p&gt;
&lt;h2 id=&#34;references-and-next-steps&#34;&gt;References and next steps&lt;/h2&gt;
&lt;p&gt;If you&#39;re interested in a deeper dive into the rationale behind this feature and
the decisions we have taken, consider reading the &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2232-suspend-jobs&#34;&gt;enhancement proposal&lt;/a&gt;.
There&#39;s more detail on suspending and resuming jobs in the documentation for &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job#suspending-a-job&#34;&gt;Job&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As previously mentioned, this feature is currently in alpha and is available
only if you explicitly opt-in through the &lt;code&gt;SuspendJob&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt;.
If this is a feature you&#39;re interested in, please consider testing suspended
Jobs in your cluster and providing feedback. You can discuss this enhancement &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2232&#34;&gt;on GitHub&lt;/a&gt;.
The SIG Apps community also &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-apps#meetings&#34;&gt;meets regularly&lt;/a&gt;
and can be reached through &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-apps#contact&#34;&gt;Slack or the mailing list&lt;/a&gt;.
Barring any unexpected changes to the API, we intend to graduate the feature to
beta in Kubernetes 1.22, so that the feature becomes available by default.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.21: CronJob Reaches GA</title>
      <link>https://kubernetes.io/blog/2021/04/09/kubernetes-release-1.21-cronjob-ga/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/09/kubernetes-release-1.21-cronjob-ga/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alay Patel (Red Hat), and Maciej Szulik (Red Hat)&lt;/p&gt;
&lt;p&gt;In Kubernetes v1.21, the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;CronJob&lt;/a&gt; resource
reached general availability (GA). We&#39;ve also substantially improved the
performance of CronJobs since Kubernetes v1.19, by implementing a new
controller.&lt;/p&gt;
&lt;p&gt;In Kubernetes v1.20 we launched a revised v2 controller for CronJobs,
initially as an alpha feature. Kubernetes 1.21 uses the newer controller by
default, and the CronJob resource itself is now GA (group version: &lt;code&gt;batch/v1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In this article, we&#39;ll take you through the driving forces behind this new
development, give you a brief description of controller design for core
Kubernetes, and we&#39;ll outline what you will gain from this improved controller.&lt;/p&gt;
&lt;p&gt;The driving force behind promoting the API was Kubernetes&#39; policy choice to
&lt;a href=&#34;https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/&#34;&gt;ensure APIs move beyond beta&lt;/a&gt;.
That policy aims to prevent APIs from being stuck in a “permanent beta” state.
Over the years the old CronJob controller implementation had received healthy
feedback from the community, with reports of several widely recognized
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/82659&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If the beta API for CronJob was to be supported as GA, the existing controller
code would need substantial rework. Instead, the SIG Apps community decided
to introduce a new controller and gradually replace the old one.&lt;/p&gt;
&lt;h2 id=&#34;how-do-controllers-work&#34;&gt;How do controllers work?&lt;/h2&gt;
&lt;p&gt;Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/controller/&#34;&gt;controllers&lt;/a&gt; are control
loops that watch the state of resource(s) in your cluster, then make or
request changes where needed. Each controller tries to move part of the
current cluster state closer to the desired state.&lt;/p&gt;
&lt;p&gt;The v1 CronJob controller works by performing a periodic poll and sweep of all
the CronJob objects in your cluster, in order to act on them. It is a single
worker implementation that gets all CronJobs  every 10 seconds, iterates over
each one of them, and syncs them to their desired state. This was the default
way of doing things almost 5 years ago when the controller was initially
written. In hindsight, we can certainly say that such an approach can
overload the API server at scale.&lt;/p&gt;
&lt;p&gt;These days, every core controller in kubernetes must follow the guidelines
described in &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md#readme&#34;&gt;Writing Controllers&lt;/a&gt;.
Among many details, that document prescribes using
&lt;a href=&#34;https://www.cncf.io/blog/2019/10/15/extend-kubernetes-via-a-shared-informer/&#34;&gt;shared informers&lt;/a&gt;
to “receive notifications of adds, updates, and deletes for a particular
resource”. Upon any such events, the related object(s) is placed in a queue.
Workers pull items from the queue and process them one at a time. This
approach ensures consistency and scalability.&lt;/p&gt;
&lt;p&gt;The picture below shows the flow of information from kubernetes API server,
through shared informers and queue, to the main part of a controller - a
reconciliation loop which is responsible for performing the core functionality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;controller-flowchart.svg&#34; alt=&#34;Controller flowchart&#34;&gt;&lt;/p&gt;
&lt;p&gt;The CronJob controller V2 uses a queue that implements the DelayingInterface to
handle the scheduling aspect. This queue allows processing an element after a
specific time interval. Every time there is a change in a CronJob or its related
Jobs, the key that represents the CronJob is pushed to the queue. The main
handler pops the key, processes the CronJob, and after completion
pushes the key back into the queue for the next scheduled time interval. This is
immediately a more performant implementation, as it no longer requires a linear
scan of all the CronJobs. On top of that, this controller can be scaled by
increasing the number of workers processing the CronJobs in parallel.&lt;/p&gt;
&lt;h2 id=&#34;performance-impact&#34;&gt;Performance impact of the new controller&lt;/h2&gt;
&lt;p&gt;In order to test the performance difference of the two controllers a VM instance
with 128 GiB RAM and 64 vCPUs was used to set up a single node Kubernetes cluster.
Initially, a sample workload was created with 20 CronJob instances with a schedule
to run every minute, and 2100 CronJobs running every 20 hours. Additionally,
over the next few minutes we added 1000 CronJobs with a schedule to run every
20 hours, until we reached a total of 5120 CronJobs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;performance-impact-graph.svg&#34; alt=&#34;Visualization of performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;We observed that for every 1000 CronJobs added, the old controller used
around 90 to 120 seconds more wall-clock time to schedule 20 Jobs every cycle.
That is, at 5120 CronJobs, the old controller took approximately 9 minutes
to create 20 Jobs. Hence, during each cycle, about 8 schedules were missed.
The new controller, implemented with architectural change explained above,
created 20 Jobs without any delay, even when we created an additional batch
of 1000 CronJobs reaching a total of 6120.&lt;/p&gt;
&lt;p&gt;As a closing remark, the new controller exposes a histogram metric
&lt;code&gt;cronjob_controller_cronjob_job_creation_skew_duration_seconds&lt;/code&gt; which helps
monitor the time difference between when a CronJob is meant to run and when
the actual Job is created.&lt;/p&gt;
&lt;p&gt;Hopefully the above description is a sufficient argument to follow the
guidelines and standards set in the Kubernetes project, even for your own
controllers. As mentioned before, the new controller is on by default starting
from Kubernetes v1.21; if you want to check it out in the previous release (1.20),
you can enable the &lt;code&gt;CronJobControllerV2&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt;
for the kube-controller-manager: &lt;code&gt;--feature-gate=&amp;quot;CronJobControllerV2=true&amp;quot;&lt;/code&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.21: Power to the Community</title>
      <link>https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/</link>
      <pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.21/release-team.md&#34;&gt;Kubernetes 1.21 Release Team&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We’re pleased to announce the release of Kubernetes 1.21, our first release of 2021! This release consists of 51 enhancements: 13 enhancements have graduated to stable, 16 enhancements are moving to beta, 20 enhancements are entering alpha, and 2 features have been deprecated.&lt;/p&gt;
&lt;p&gt;This release cycle, we saw a major shift in ownership of processes around the release team. We moved from a synchronous mode of communication, where we periodically asked the community for inputs, to a mode where the community opts-in to contribute features and/or blogs to the release. These changes have resulted in an increase in collaboration and teamwork across the community. The result of all that is reflected in Kubernetes 1.21 having the most number of features in the recent times.&lt;/p&gt;
&lt;h2 id=&#34;major-themes&#34;&gt;Major Themes&lt;/h2&gt;
&lt;h3 id=&#34;cronjobs-graduate-to-stable&#34;&gt;CronJobs Graduate to Stable!&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;CronJobs&lt;/a&gt; (previously ScheduledJobs) has been a beta feature since Kubernetes 1.8! With 1.21 we get to finally see this widely used API graduate to stable.&lt;/p&gt;
&lt;p&gt;CronJobs are meant for performing regular scheduled actions such as backups, report generation, and so on. Each of those tasks should be configured to recur indefinitely (for example: once a day / week / month); you can define the point in time within that interval when the job should start.&lt;/p&gt;
&lt;h3 id=&#34;immutable-secrets-and-configmaps&#34;&gt;Immutable Secrets and ConfigMaps&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable&#34;&gt;Immutable Secrets&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable&#34;&gt;ConfigMaps&lt;/a&gt; add a new field to those resource types that will reject changes to those objects if set. Secrets and ConfigMaps by default are mutable which is beneficial for pods that are able to consume changes. Mutating Secrets and ConfigMaps can also cause problems if a bad configuration is pushed for pods that use them.&lt;/p&gt;
&lt;p&gt;By marking Secrets and ConfigMaps as immutable you can be sure your application configuration won&#39;t change. If you want to make changes you&#39;ll need to create a new, uniquly named Secret or ConfigMap and deploy a new pod to consume that resource. Immutable resources also have scaling benefits because controllers do not need to poll the API server to watch for changes.&lt;/p&gt;
&lt;p&gt;This feature has graduated to stable in Kubernetes 1.21.&lt;/p&gt;
&lt;h3 id=&#34;ipv4-ipv6-dual-stack-support&#34;&gt;IPv4/IPv6 dual-stack support&lt;/h3&gt;
&lt;p&gt;IP addresses are a consumable resource that cluster operators and administrators need to make sure are not exhausted. In particular, public IPv4 addresses are now scarce. Having dual-stack support enables native IPv6 routing to pods and services, whilst still allowing your cluster to talk IPv4 where needed. Dual-stack cluster networking also improves a possible scaling limitation for workloads.&lt;/p&gt;
&lt;p&gt;Dual-stack support in Kubernetes means that pods, services, and nodes can get IPv4 addresses and IPv6 addresses. In Kubernetes 1.21 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dual-stack/&#34;&gt;dual-stack networking&lt;/a&gt; has graduated from alpha to beta, and is now enabled by default.&lt;/p&gt;
&lt;h3 id=&#34;graceful-node-shutdown&#34;&gt;Graceful Node Shutdown&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown&#34;&gt;Graceful Node shutdown&lt;/a&gt; also graduated to beta with this release (and will now be available to a much larger group of users)! This is a hugely beneficial feature that allows the kubelet to be aware of node shutdown, and gracefully terminate pods that are scheduled to that node.&lt;/p&gt;
&lt;p&gt;Currently, when a node shuts down, pods do not follow the expected termination lifecycle and are not shut down gracefully. This can introduce problems with a lot of different workloads. Going forward, the kubelet will be able to detect imminent system shutdown through systemd, then inform running pods so they can terminate as gracefully as possible.&lt;/p&gt;
&lt;h3 id=&#34;persistentvolume-health-monitor&#34;&gt;PersistentVolume Health Monitor&lt;/h3&gt;
&lt;p&gt;Persistent Volumes (PV) are commonly used in applications to get local, file-based storage. They can be used in many different ways and help users migrate applications without needing to re-write storage backends.&lt;/p&gt;
&lt;p&gt;Kubernetes 1.21 has a new alpha feature which allows PVs to be monitored for health of the volume and marked accordingly if the volume becomes unhealthy. Workloads will be able to react to the health state to protect data from being written or read from a volume that is unhealthy.&lt;/p&gt;
&lt;h3 id=&#34;reducing-kubernetes-build-maintenance&#34;&gt;Reducing Kubernetes Build Maintenance&lt;/h3&gt;
&lt;p&gt;Previously Kubernetes has maintained multiple build systems. This has often been a source of friction and complexity for new and current contributors.&lt;/p&gt;
&lt;p&gt;Over the last release cycle, a lot of work has been put in to simplify the build process, and standardize on the native Golang build tools. This should empower broader community maintenance, and lower the barrier to entry for new contributors.&lt;/p&gt;
&lt;h2 id=&#34;major-changes&#34;&gt;Major Changes&lt;/h2&gt;
&lt;h3 id=&#34;podsecuritypolicy-deprecation&#34;&gt;PodSecurityPolicy Deprecation&lt;/h3&gt;
&lt;p&gt;In Kubernetes 1.21, PodSecurityPolicy is deprecated. As with all Kubernetes feature deprecations, PodSecurityPolicy will continue to be available and fully-functional for several more releases. PodSecurityPolicy, previously in the beta stage, is planned for removal in Kubernetes 1.25.&lt;/p&gt;
&lt;p&gt;What&#39;s next? We&#39;re developing a new built-in mechanism to help limit Pod privileges, with a working title of “PSP Replacement Policy.” Our plan is for this new mechanism to cover the key PodSecurityPolicy use cases, with greatly improved ergonomics and maintainability. To learn more, read &lt;a href=&#34;https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future&#34;&gt;PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;topologykeys-deprecation&#34;&gt;TopologyKeys Deprecation&lt;/h3&gt;
&lt;p&gt;The Service field &lt;code&gt;topologyKeys&lt;/code&gt; is now deprecated; all the component features that used this field were previously alpha, and are now also deprecated.
We&#39;ve replaced &lt;code&gt;topologyKeys&lt;/code&gt; with a way to implement topology-aware routing, called topology-aware hints.  Topology-aware hints are an alpha feature in Kubernetes 1.21. You can read more details about the replacement feature in &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service-topology/&#34;&gt;Topology Aware Hints&lt;/a&gt;; the related &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/2433-topology-aware-hints/README.md&#34;&gt;KEP&lt;/a&gt; explains the context for why we switched.&lt;/p&gt;
&lt;h2 id=&#34;other-updates&#34;&gt;Other Updates&lt;/h2&gt;
&lt;h3 id=&#34;graduated-to-stable&#34;&gt;Graduated to Stable&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/752&#34;&gt;EndpointSlice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/34&#34;&gt;Add sysctl support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/85&#34;&gt;PodDisruptionBudgets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notable-feature-updates&#34;&gt;Notable Feature Updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/541&#34;&gt;External client-go credential providers&lt;/a&gt; - beta in 1.21&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1602&#34;&gt;Structured logging&lt;/a&gt; - graduating to beta in 1.22&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/592&#34;&gt;TTL after finish cleanup for Jobs and Pods&lt;/a&gt; - graduated to beta&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;release-notes&#34;&gt;Release notes&lt;/h1&gt;
&lt;p&gt;You can check out the full details of the 1.21 release in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;availability-of-release&#34;&gt;Availability of release&lt;/h1&gt;
&lt;p&gt;Kubernetes 1.21 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.21.0&#34;&gt;download on GitHub&lt;/a&gt;. There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34;&gt;interactive tutorials&lt;/a&gt; on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt;. If you’d like to try building a cluster from scratch, check out the &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;Kubernetes the Hard Way&lt;/a&gt; tutorial by Kelsey Hightower.&lt;/p&gt;
&lt;h1 id=&#34;release-team&#34;&gt;Release Team&lt;/h1&gt;
&lt;p&gt;This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Nabarun Pal, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.21 release for the community.&lt;/p&gt;
&lt;h1 id=&#34;release-logo&#34;&gt;Release Logo&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2021-04-08-kubernetes-release-1.21/globe_250px.png&#34; alt=&#34;Kubernetes 1.21 Release Logo&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Kubernetes 1.21 Release Logo portrays the global nature of the Release Team, with release team members residing in timezones from UTC+8 all the way to UTC-8. The diversity of the release team brought in a lot of challenges, but the team tackled them all by adopting more asynchronous communication practices. The heptagonal globe in the release logo signifies the sheer determination of the community to overcome the challenges as they come. It celebrates the amazing teamwork of the release team over the last 3 months to bring in a fun packed Kubernetes release!&lt;/p&gt;
&lt;p&gt;The logo is designed by &lt;a href=&#34;https://www.behance.net/noblebatman&#34;&gt;Aravind Sekar&lt;/a&gt;, an independent designer based out of India. Aravind helps open source communities like PyCon India in their design efforts.&lt;/p&gt;
&lt;h1 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;CNCF welcomes 47 new organizations across the globe as members to advance Cloud Native technology further at the start of 2021! These &lt;a href=&#34;https://www.cncf.io/announcements/2021/02/24/cloud-native-computing-foundation-welcomes-47-new-members-at-the-start-of-2021/&#34;&gt;new members&lt;/a&gt; will join CNCF at the upcoming 2021 KubeCon + CloudNativeCon events, including &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/&#34;&gt;KubeCon + CloudNativeCom EU – Virtual&lt;/a&gt; from May 4 – 7, 2021, and &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;KubeCon + CloudNativeCon NA in Los Angeles&lt;/a&gt; from October 12 – 15, 2021.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://k8s.devstats.cncf.io/&#34;&gt;CNCF K8s DevStats project&lt;/a&gt; aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p&gt;
&lt;p&gt;In the v1.21 release cycle, which ran for 12 weeks (January 11 to April 8), we saw contributions from &lt;a href=&#34;https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.20.0%20-%20now&amp;amp;var-metric=contributions&#34;&gt;999 companies&lt;/a&gt; and &lt;a href=&#34;https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.20.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&#34;&gt;1279 individuals&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;In the wake of rising racism &amp;amp; attacks on global Asian communities, read the statement from CNCF General Priyanka Sharma on the &lt;a href=&#34;https://www.cncf.io/blog/2021/03/18/statement-from-cncf-general-manager-priyanka-sharma-on-the-unacceptable-attacks-against-aapi-and-asian-communities/&#34;&gt;CNCF blog&lt;/a&gt; reinstating the community&#39;s commitment towards inclusive values &amp;amp; diversity-powered resilience.&lt;/li&gt;
&lt;li&gt;We now have a process in place for migration of the default branch from master → main. Learn more about the guidelines &lt;a href=&#34;k8s.dev/rename&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CNCF and the Linux Foundation have announced the availability of their new training course, &lt;a href=&#34;https://training.linuxfoundation.org/training/kubernetes-security-essentials-lfs260/&#34;&gt;LFS260 – Kubernetes Security Essentials&lt;/a&gt;. In addition to providing skills and knowledge on a broad range of best practices for securing container-based applications and Kubernetes platforms, the course is also a great way to prepare for the recently launched &lt;a href=&#34;https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/&#34;&gt;Certified Kubernetes Security Specialist&lt;/a&gt; certification exam.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;event-updates&#34;&gt;Event Updates&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! You can find more information about the conference &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetescommunitydays.org/&#34;&gt;Kubernetes Community Days&lt;/a&gt; are being relaunched! Q2 2021 will start with Africa and Bengaluru.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;upcoming-release-webinar&#34;&gt;Upcoming release webinar&lt;/h1&gt;
&lt;p&gt;Join the members of the Kubernetes 1.21 release team on May 13th, 2021 to learn about the major features in this release including IPv4/IPv6 dual-stack support, PersistentVolume Health Monitor, Immutable Secrets and ConfigMaps, and many more. Register here: &lt;a href=&#34;https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-121-release/&#34;&gt;https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-121-release/&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h1&gt;
&lt;p&gt;If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find out more about contributing to Kubernetes at the &lt;a href=&#34;https://www.kubernetes.dev/&#34;&gt;Kubernetes Contributor website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://github.com/cncf/foundation/blob/master/case-study-guidelines.md&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read more about what’s happening with Kubernetes on the &lt;a href=&#34;https://kubernetes.io/blog/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learn more about the &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/release-team&#34;&gt;Kubernetes Release Team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: PodSecurityPolicy Deprecation: Past, Present, and Future</title>
      <link>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Tabitha Sable (Kubernetes SIG Security)&lt;/p&gt;
&lt;p&gt;PodSecurityPolicy (PSP) is being deprecated in Kubernetes 1.21, to be released later this week. This starts the countdown to its removal, but doesn’t change anything else. PodSecurityPolicy will continue to be fully functional for several more releases before being removed completely. In the meantime, we are developing a replacement for PSP that covers key use cases more easily and sustainably.&lt;/p&gt;
&lt;p&gt;What are Pod Security Policies? Why did we need them? Why are they going away, and what’s next? How does this affect you? These key questions come to mind as we prepare to say goodbye to PSP, so let’s walk through them together. We’ll start with an overview of how features get removed from Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;what-does-deprecation-mean-in-kubernetes&#34;&gt;What does deprecation mean in Kubernetes?&lt;/h2&gt;
&lt;p&gt;Whenever a Kubernetes feature is set to go away, our &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34;&gt;deprecation policy&lt;/a&gt; is our guide. First the feature is marked as deprecated, then after enough time has passed, it can finally be removed.&lt;/p&gt;
&lt;p&gt;Kubernetes 1.21 starts the deprecation process for PodSecurityPolicy. As with all feature deprecations, PodSecurityPolicy will continue to be fully functional for several more releases. The current plan is to remove PSP from Kubernetes in the 1.25 release.&lt;/p&gt;
&lt;p&gt;Until then, PSP is still PSP. There will be at least a year during which the newest Kubernetes releases will still support PSP, and nearly two years until PSP will pass fully out of all supported Kubernetes versions.&lt;/p&gt;
&lt;h2 id=&#34;what-is-podsecuritypolicy&#34;&gt;What is PodSecurityPolicy?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34;&gt;PodSecurityPolicy&lt;/a&gt; is a built-in &lt;a href=&#34;https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/&#34;&gt;admission controller&lt;/a&gt; that allows a cluster administrator to control security-sensitive aspects of the Pod specification.&lt;/p&gt;
&lt;p&gt;First, one or more PodSecurityPolicy resources are created in a cluster to define the requirements Pods must meet. Then, RBAC rules are created to control which PodSecurityPolicy applies to a given pod. If a pod meets the requirements of its PSP, it will be admitted to the cluster as usual. In some cases, PSP can also modify Pod fields, effectively creating new defaults for those fields. If a Pod does not meet the PSP requirements, it is rejected, and cannot run.&lt;/p&gt;
&lt;p&gt;One more important thing to know about PodSecurityPolicy: it’s not the same as &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context&#34;&gt;PodSecurityContext&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A part of the Pod specification, PodSecurityContext (and its per-container counterpart &lt;code&gt;SecurityContext&lt;/code&gt;) is the collection of fields that specify many of the security-relevant settings for a Pod. The security context dictates to the kubelet and container runtime how the Pod should actually be run. In contrast, the PodSecurityPolicy only constrains (or defaults) the values that may be set on the security context.&lt;/p&gt;
&lt;p&gt;The deprecation of PSP does not affect PodSecurityContext in any way.&lt;/p&gt;
&lt;h2 id=&#34;why-did-we-need-podsecuritypolicy&#34;&gt;Why did we need PodSecurityPolicy?&lt;/h2&gt;
&lt;p&gt;In Kubernetes, we define resources such as Deployments, StatefulSets, and Services that represent the building blocks of software applications. The various controllers inside a Kubernetes cluster react to these resources, creating further Kubernetes resources or configuring some software or hardware to accomplish our goals.&lt;/p&gt;
&lt;p&gt;In most Kubernetes clusters, RBAC (Role-Based Access Control) &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole&#34;&gt;rules&lt;/a&gt; control access to these resources. &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;get&lt;/code&gt;, &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;edit&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt; are the sorts of API operations that RBAC cares about, but &lt;em&gt;RBAC does not consider what settings are being put into the resources it controls&lt;/em&gt;. For example, a Pod can be almost anything from a simple webserver to a privileged command prompt offering full access to the underlying server node and all the data. It’s all the same to RBAC: a Pod is a Pod is a Pod.&lt;/p&gt;
&lt;p&gt;To control what sorts of settings are allowed in the resources defined in your cluster, you need Admission Control in addition to RBAC. Since Kubernetes 1.3, PodSecurityPolicy has been the built-in way to do that for security-related Pod fields. Using PodSecurityPolicy, you can prevent “create Pod” from automatically meaning “root on every cluster node,” without needing to deploy additional external admission controllers.&lt;/p&gt;
&lt;h2 id=&#34;why-is-podsecuritypolicy-going-away&#34;&gt;Why is PodSecurityPolicy going away?&lt;/h2&gt;
&lt;p&gt;In the years since PodSecurityPolicy was first introduced, we have realized that PSP has some serious usability problems that can’t be addressed without making breaking changes.&lt;/p&gt;
&lt;p&gt;The way PSPs are applied to Pods has proven confusing to nearly everyone that has attempted to use them. It is easy to accidentally grant broader permissions than intended, and difficult to inspect which PSP(s) apply in a given situation. The “changing Pod defaults” feature can be handy, but is only supported for certain Pod settings and it’s not obvious when they will or will not apply to your Pod. Without a “dry run” or audit mode, it’s impractical to retrofit PSP to existing clusters safely, and it’s impossible for PSP to ever be enabled by default.&lt;/p&gt;
&lt;p&gt;For more information about these and other PSP difficulties, check out SIG Auth’s KubeCon NA 2019 Maintainer Track session video: 
&lt;div class=&#34;youtube-quote-sm&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/SFtHRmPuhEw?start=953&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Today, you’re not limited only to deploying PSP or writing your own custom admission controller. Several external admission controllers are available that incorporate lessons learned from PSP to provide a better user experience. &lt;a href=&#34;https://github.com/cruise-automation/k-rail&#34;&gt;K-Rail&lt;/a&gt;, &lt;a href=&#34;https://github.com/kyverno/kyverno/&#34;&gt;Kyverno&lt;/a&gt;, and &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper/&#34;&gt;OPA/Gatekeeper&lt;/a&gt; are all well-known, and each has its fans.&lt;/p&gt;
&lt;p&gt;Although there are other good options available now, we believe there is still value in having a built-in admission controller available as a choice for users. With this in mind, we turn toward building what’s next, inspired by the lessons learned from PSP.&lt;/p&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;Kubernetes SIG Security, SIG Auth, and a diverse collection of other community members have been working together for months to ensure that what’s coming next is going to be awesome. We have developed a Kubernetes Enhancement Proposal (&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2579&#34;&gt;KEP 2579&lt;/a&gt;) and a prototype for a new feature, currently being called by the temporary name &amp;quot;PSP Replacement Policy.&amp;quot; We are targeting an Alpha release in Kubernetes 1.22.&lt;/p&gt;
&lt;p&gt;PSP Replacement Policy starts with the realization that since there is a robust ecosystem of external admission controllers already available, PSP’s replacement doesn’t need to be all things to all people. Simplicity of deployment and adoption is the key advantage a built-in admission controller has compared to an external webhook, so we have focused on how to best utilize that advantage.&lt;/p&gt;
&lt;p&gt;PSP Replacement Policy is designed to be as simple as practically possible while providing enough flexibility to really be useful in production at scale. It has soft rollout features to enable retrofitting it to existing clusters, and is configurable enough that it can eventually be active by default. It can be deactivated partially or entirely, to coexist with external admission controllers for advanced use cases.&lt;/p&gt;
&lt;h2 id=&#34;what-does-this-mean-for-you&#34;&gt;What does this mean for you?&lt;/h2&gt;
&lt;p&gt;What this all means for you depends on your current PSP situation. If you’re already using PSP, there’s plenty of time to plan your next move. Please review the PSP Replacement Policy KEP and think about how well it will suit your use case.&lt;/p&gt;
&lt;p&gt;If you’re making extensive use of the flexibility of PSP with numerous PSPs and complex binding rules, you will likely find the simplicity of PSP Replacement Policy too limiting. Use the next year to evaluate the other admission controller choices in the ecosystem. There are resources available to ease this transition, such as the &lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper-library&#34;&gt;Gatekeeper Policy Library&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If your use of PSP is relatively simple, with a few policies and straightforward binding to service accounts in each namespace, you will likely find PSP Replacement Policy to be a good match for your needs. Evaluate your PSPs compared to the Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/security/pod-security-standards/&#34;&gt;Pod Security Standards&lt;/a&gt; to get a feel for where you’ll be able to use the Restricted, Baseline, and Privileged policies. Please follow along with or contribute to the KEP and subsequent development, and try out the Alpha release of PSP Replacement Policy when it becomes available.&lt;/p&gt;
&lt;p&gt;If you’re just beginning your PSP journey, you will save time and effort by keeping it simple. You can approximate the functionality of PSP Replacement Policy today by using the Pod Security Standards’ PSPs. If you set the cluster default by binding a Baseline or Restricted policy to the &lt;code&gt;system:serviceaccounts&lt;/code&gt; group, and then make a more-permissive policy available as needed in certain Namespaces &lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod&#34;&gt;using ServiceAccount bindings&lt;/a&gt;, you will avoid many of the PSP pitfalls and have an easy migration to PSP Replacement Policy. If your needs are much more complex than this, your effort is probably better spent adopting one of the more fully-featured external admission controllers mentioned above.&lt;/p&gt;
&lt;p&gt;We’re dedicated to making Kubernetes the best container orchestration tool we can, and sometimes that means we need to remove longstanding features to make space for better things to come. When that happens, the Kubernetes deprecation policy ensures you have plenty of time to plan your next move. In the case of PodSecurityPolicy, several options are available to suit a range of needs and use cases. Start planning ahead now for PSP’s eventual removal, and please consider contributing to its replacement! Happy securing!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Acknowledgment:&lt;/strong&gt; It takes a wonderful group to make wonderful software. Thanks are due to everyone who has contributed to the PSP replacement effort, especially (in alphabetical order) Tim Allclair, Ian Coldwater, and Jordan Liggitt. It’s been a joy to work with y’all on this.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: The Evolution of Kubernetes Dashboard</title>
      <link>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</guid>
      <description>
        
        
        &lt;p&gt;Authors: Marcin Maciaszczyk, Kubermatic &amp;amp; Sebastian Florek, Kubermatic&lt;/p&gt;
&lt;p&gt;In October 2020, the Kubernetes Dashboard officially turned five. As main project maintainers, we can barely believe that so much time has passed since our very first commits to the project. However, looking back with a bit of nostalgia, we realize that quite a lot has happened since then. Now it’s due time to celebrate “our baby” with a short recap.&lt;/p&gt;
&lt;h2 id=&#34;how-it-all-began&#34;&gt;How It All Began&lt;/h2&gt;
&lt;p&gt;The initial idea behind the Kubernetes Dashboard project was to provide a web interface for Kubernetes. We wanted to reflect the kubectl functionality through an intuitive web UI. The main benefit from using the UI is to be able to quickly see things that do not work as expected (monitoring and troubleshooting). Also, the Kubernetes Dashboard is a great starting point for users that are new to the Kubernetes ecosystem.&lt;/p&gt;
&lt;p&gt;The very &lt;a href=&#34;https://github.com/kubernetes/dashboard/commit/5861187fa807ac1cc2d9b2ac786afeced065076c&#34;&gt;first commit&lt;/a&gt; to the Kubernetes Dashboard was made by Filip Grządkowski from Google on 16th October 2015 – just a few months from the initial commit to the Kubernetes repository. Our initial commits go back to November 2015 (&lt;a href=&#34;https://github.com/kubernetes/dashboard/commit/09e65b6bb08c49b926253de3621a73da05e400fd&#34;&gt;Sebastian committed on 16 November 2015&lt;/a&gt;; &lt;a href=&#34;https://github.com/kubernetes/dashboard/commit/1da4b1c25ef040818072c734f71333f9b4733f55&#34;&gt;Marcin committed on 23 November 2015&lt;/a&gt;). Since that time, we’ve become regular contributors to the project. For the next two years, we worked closely with the Googlers, eventually becoming main project maintainers ourselves.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/first-ui.png&#34;
         alt=&#34;The First Version of the User Interface&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;The First Version of the User Interface&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/along-the-way-ui.png&#34;
         alt=&#34;Prototype of the New User Interface&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Prototype of the New User Interface&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;figure&gt;
    &lt;img src=&#34;https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/current-ui.png&#34;
         alt=&#34;The Current User Interface&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;The Current User Interface&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As you can see, the initial look and feel of the project were completely different from the current one. We have changed the design multiple times. The same has happened with the code itself.&lt;/p&gt;
&lt;h2 id=&#34;growing-up-the-big-migration&#34;&gt;Growing Up - The Big Migration&lt;/h2&gt;
&lt;p&gt;At &lt;a href=&#34;https://github.com/kubernetes/dashboard/pull/2727&#34;&gt;the beginning of 2018&lt;/a&gt;, we reached a point where AngularJS was getting closer to the end of its life, while the new Angular versions were published quite often. A lot of the libraries and the modules that we were using were following the trend. That forced us to spend a lot of the time rewriting the frontend part of the project to make it work with newer technologies.&lt;/p&gt;
&lt;p&gt;The migration came with many benefits like being able to refactor a lot of the code, introduce design patterns, reduce code complexity, and benefit from the new modules. However, you can imagine that the scale of the migration was huge. Luckily, there were a number of contributions from the community helping us with the resource support, new Kubernetes version support, i18n, and much more. After many long days and nights, we finally released the &lt;a href=&#34;https://github.com/kubernetes/dashboard/releases/tag/v2.0.0-beta1&#34;&gt;first beta version&lt;/a&gt; in July 2019, followed by the &lt;a href=&#34;https://github.com/kubernetes/dashboard/releases/tag/v2.0.0&#34;&gt;2.0 release&lt;/a&gt; in April 2020 — our baby had grown up.&lt;/p&gt;
&lt;h2 id=&#34;where-are-we-standing-in-2021&#34;&gt;Where Are We Standing in 2021?&lt;/h2&gt;
&lt;p&gt;Due to limited resources, unfortunately, we were not able to offer extensive support for many different Kubernetes versions. So, we’ve decided to always try and support the latest Kubernetes version available at the time of the Kubernetes Dashboard release. The latest release, &lt;a href=&#34;https://github.com/kubernetes/dashboard/releases/tag/v2.2.0&#34;&gt;Dashboard v2.2.0&lt;/a&gt; provides support for Kubernetes v1.20.&lt;/p&gt;
&lt;p&gt;On top of that, we put in a great deal of effort into &lt;a href=&#34;https://github.com/kubernetes/dashboard/issues/5232&#34;&gt;improving resource support&lt;/a&gt;. Meanwhile, we do offer support for most of the Kubernetes resources. Also, the Kubernetes Dashboard supports multiple languages: English, German, French, Japanese, Korean, Chinese (Traditional, Simplified, Traditional Hong Kong). Persian and Russian localizations are currently in progress. Moreover, we are working on the support for 3rd party themes and the design of the app in general. As you can see, quite a lot of things are going on.&lt;/p&gt;
&lt;p&gt;Luckily, we do have regular contributors with domain knowledge who are taking care of the project, updating the Helm charts, translations, Go modules, and more. But as always, there could be many more hands on deck. So if you are thinking about contributing to Kubernetes, keep us in mind ;)&lt;/p&gt;
&lt;h2 id=&#34;what-s-next&#34;&gt;What’s Next&lt;/h2&gt;
&lt;p&gt;The Kubernetes Dashboard has been growing and prospering for more than 5 years now. It provides the community with an intuitive Web UI, thereby decreasing the complexity of Kubernetes and increasing its accessibility to new community members. We are proud of what the project has achieved so far, but this is by far not the end. These are our priorities for the future:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep providing support for the new Kubernetes versions&lt;/li&gt;
&lt;li&gt;Keep improving the support for the existing resources&lt;/li&gt;
&lt;li&gt;Keep working on auth system improvements&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/pull/5449&#34;&gt;Rewrite the API to use gRPC and shared informers&lt;/a&gt;: This will allow us to improve the performance of the application but, most importantly, to support live updates coming from the Kubernetes project. It is one of the most requested features from the community.&lt;/li&gt;
&lt;li&gt;Split the application into two containers, one with the UI and the second with the API running inside.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-kubernetes-dashboard-in-numbers&#34;&gt;The Kubernetes Dashboard in Numbers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Initial commit made on October 16, 2015&lt;/li&gt;
&lt;li&gt;Over 100 million pulls from Dockerhub since the v2 release&lt;/li&gt;
&lt;li&gt;8 supported languages and the next 2 in progress&lt;/li&gt;
&lt;li&gt;Over 3360 closed PRs&lt;/li&gt;
&lt;li&gt;Over 2260 closed issues&lt;/li&gt;
&lt;li&gt;100% coverage of the supported core Kubernetes resources&lt;/li&gt;
&lt;li&gt;Over 9000 stars on GitHub&lt;/li&gt;
&lt;li&gt;Over 237 000 lines of code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;join-us&#34;&gt;Join Us&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, we are currently looking for more people to help us further develop and grow the project. We are open to contributions in multiple areas, i.e., &lt;a href=&#34;https://github.com/kubernetes/dashboard/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;issues with help wanted label&lt;/a&gt;. Please feel free to reach out via GitHub or the #sig-ui channel in the &lt;a href=&#34;https://slack.k8s.io/&#34;&gt;Kubernetes Slack&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: A Custom Kubernetes Scheduler to Orchestrate Highly Available Applications</title>
      <link>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</link>
      <pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Chris Seto (Cockroach Labs)&lt;/p&gt;
&lt;p&gt;As long as you&#39;re willing to follow the rules, deploying on Kubernetes and air travel can be quite pleasant. More often than not, things will &amp;quot;just work&amp;quot;. However, if one is interested in travelling with an alligator that must remain alive or scaling a database that must remain available, the situation is likely to become a bit more complicated. It may even be easier to build one&#39;s own plane or database for that matter. Travelling with reptiles aside, scaling a highly available stateful system is no trivial task.&lt;/p&gt;
&lt;p&gt;Scaling any system has two main components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Adding or removing infrastructure that the system will run on, and&lt;/li&gt;
&lt;li&gt;Ensuring that the system knows how to handle additional instances of itself being added and removed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Most stateless systems, web servers for example, are created without the need to be aware of peers. Stateful systems, which includes databases like CockroachDB, have to coordinate with their peer instances and shuffle around data. As luck would have it, CockroachDB handles data redistribution and replication. The tricky part is being able to tolerate failures during these operations by ensuring that data and instances are distributed across many failure domains (availability zones).&lt;/p&gt;
&lt;p&gt;One of Kubernetes&#39; responsibilities is to place &amp;quot;resources&amp;quot; (e.g, a disk or container) into the cluster and satisfy the constraints they request. For example: &amp;quot;I must be in availability zone &lt;em&gt;A&lt;/em&gt;&amp;quot; (see &lt;a href=&#34;https://kubernetes.io/docs/setup/best-practices/multiple-zones/#nodes-are-labeled&#34;&gt;Running in multiple zones&lt;/a&gt;), or &amp;quot;I can&#39;t be placed onto the same node as this other Pod&amp;quot; (see &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;Affinity and anti-affinity&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;As an addition to those constraints, Kubernetes offers &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34;&gt;Statefulsets&lt;/a&gt; that provide identity to Pods as well as persistent storage that &amp;quot;follows&amp;quot; these identified pods. Identity in a StatefulSet is handled by an increasing integer at the end of a pod&#39;s name. It&#39;s important to note that this integer must always be contiguous: in a StatefulSet, if pods 1 and 3 exist then pod 2 must also exist.&lt;/p&gt;
&lt;p&gt;Under the hood, CockroachCloud deploys each region of CockroachDB as a StatefulSet in its own Kubernetes cluster - see &lt;a href=&#34;https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html&#34;&gt;Orchestrate CockroachDB in a Single Kubernetes Cluster&lt;/a&gt;.
In this article, I&#39;ll be looking at an individual region, one StatefulSet and one Kubernetes cluster which is distributed across at least three availability zones.&lt;/p&gt;
&lt;p&gt;A three-node CockroachCloud cluster would look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image01.png&#34; alt=&#34;3-node, multi-zone cockroachdb cluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;When adding additional resources to the cluster we also distribute them across zones. For the speediest user experience, we add all Kubernetes nodes at the same time and then scale up the StatefulSet.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image02.png&#34; alt=&#34;illustration of phases: adding Kubernetes nodes to the multi-zone cockroachdb cluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note that anti-affinities are satisfied no matter the order in which pods are assigned to Kubernetes nodes. In the example, pods 0, 1 and 2 were assigned to zones A, B, and C respectively, but pods 3 and 4 were assigned in a different order, to zones B and A respectively. The anti-affinity is still satisfied because the pods are still placed in different zones.&lt;/p&gt;
&lt;p&gt;To remove resources from a cluster, we perform these operations in reverse order.&lt;/p&gt;
&lt;p&gt;We first scale down the StatefulSet and then remove from the cluster any nodes lacking a CockroachDB pod.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image03.png&#34; alt=&#34;illustration of phases: scaling down pods in a multi-zone cockroachdb cluster in Kubernetes&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, remember that pods in a StatefulSet of size &lt;em&gt;n&lt;/em&gt; must have ids in the range &lt;code&gt;[0,n)&lt;/code&gt;. When scaling down a StatefulSet by &lt;em&gt;m&lt;/em&gt;, Kubernetes removes &lt;em&gt;m&lt;/em&gt; pods, starting from the highest ordinals and moving towards the lowest, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees&#34;&gt;the reverse in which they were added&lt;/a&gt;.
Consider the cluster topology below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image04.png&#34; alt=&#34;illustration: cockroachdb cluster: 6 nodes distributed across 3 availability zones&#34;&gt;&lt;/p&gt;
&lt;p&gt;As ordinals 5 through 3 are removed from this cluster, the statefulset continues to have a presence across all 3 availability zones.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image05.png&#34; alt=&#34;illustration: removing 3 nodes from a 6-node, 3-zone cockroachdb cluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, Kubernetes&#39; scheduler doesn&#39;t &lt;em&gt;guarantee&lt;/em&gt; the placement above as we expected at first.&lt;/p&gt;
&lt;p&gt;Our combined knowledge of the following is what lead to this misconception.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes&#39; ability to &lt;a href=&#34;https://kubernetes.io/docs/setup/best-practices/multiple-zones/#pods-are-spread-across-zones&#34;&gt;automatically spread Pods across zone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The behavior that a StatefulSet with &lt;em&gt;n&lt;/em&gt; replicas, when Pods are being deployed, they are created sequentially, in order from &lt;code&gt;{0..n-1}&lt;/code&gt;. See &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees&#34;&gt;StatefulSet&lt;/a&gt; for more details.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider the following topology:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image06.png&#34; alt=&#34;illustration: 6-node cockroachdb cluster distributed across 3 availability zones&#34;&gt;&lt;/p&gt;
&lt;p&gt;These pods were created in order and they are spread across all availability zones in the cluster. When ordinals 5 through 3 are terminated, this cluster will lose its presence in zone C!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image07.png&#34; alt=&#34;illustration: terminating 3 nodes in 6-node cluster spread across 3 availability zones, where 2/2 nodes in the same availability zone are terminated, knocking out that AZ&#34;&gt;&lt;/p&gt;
&lt;p&gt;Worse yet, our automation, at the time, would remove Nodes A-2, B-2, and C-2. Leaving CRDB-1 in an unscheduled state as persistent volumes are only available in the zone they are initially created in.&lt;/p&gt;
&lt;p&gt;To correct the latter issue, we now employ a &amp;quot;hunt and peck&amp;quot; approach to removing machines from a cluster. Rather than blindly removing Kubernetes nodes from the cluster, only nodes without a CockroachDB pod would be removed. The much more daunting task was to wrangle the Kubernetes scheduler.&lt;/p&gt;
&lt;h2 id=&#34;a-session-of-brainstorming-left-us-with-3-options&#34;&gt;A session of brainstorming left us with 3 options:&lt;/h2&gt;
&lt;h3 id=&#34;1-upgrade-to-kubernetes-1-18-and-make-use-of-pod-topology-spread-constraints&#34;&gt;1. Upgrade to kubernetes 1.18 and make use of Pod Topology Spread Constraints&lt;/h3&gt;
&lt;p&gt;While this seems like it could have been the perfect solution, at the time of writing Kubernetes 1.18 was unavailable on the two most common managed Kubernetes services in public cloud, EKS and GKE.
Furthermore, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/&#34;&gt;pod topology spread constraints&lt;/a&gt; were still a &lt;a href=&#34;https://v1-18.docs.kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/&#34;&gt;beta feature in 1.18&lt;/a&gt; which meant that it &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#kubernetes_feature_choices&#34;&gt;wasn&#39;t guaranteed to be available in managed clusters&lt;/a&gt; even when v1.18 became available.
The entire endeavour was concerningly reminiscent of checking &lt;a href=&#34;https://caniuse.com/&#34;&gt;caniuse.com&lt;/a&gt; when Internet Explorer 8 was still around.&lt;/p&gt;
&lt;h3 id=&#34;2-deploy-a-statefulset-per-zone&#34;&gt;2. Deploy a statefulset &lt;em&gt;per zone&lt;/em&gt;.&lt;/h3&gt;
&lt;p&gt;Rather than having one StatefulSet distributed across all availability zones, a single StatefulSet with node affinities per zone would allow manual control over our zonal topology.
Our team had considered this as an option in the past which made it particularly appealing.
Ultimately, we decided to forego this option as it would have required a massive overhaul to our codebase and performing the migration on existing customer clusters would have been an equally large undertaking.&lt;/p&gt;
&lt;h3 id=&#34;3-write-a-custom-kubernetes-scheduler&#34;&gt;3. Write a custom Kubernetes scheduler.&lt;/h3&gt;
&lt;p&gt;Thanks to an example from &lt;a href=&#34;https://github.com/kelseyhightower/scheduler&#34;&gt;Kelsey Hightower&lt;/a&gt; and a blog post from &lt;a href=&#34;https://banzaicloud.com/blog/k8s-custom-scheduler/&#34;&gt;Banzai Cloud&lt;/a&gt;, we decided to dive in head first and write our own &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/&#34;&gt;custom Kubernetes scheduler&lt;/a&gt;.
Once our proof-of-concept was deployed and running, we quickly discovered that the Kubernetes&#39; scheduler is also responsible for mapping persistent volumes to the Pods that it schedules.
The output of &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/#verifying-that-the-pods-were-scheduled-using-the-desired-schedulers&#34;&gt;&lt;code&gt;kubectl get events&lt;/code&gt;&lt;/a&gt; had led us to believe there was another system at play.
In our journey to find the component responsible for storage claim mapping, we discovered the &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/&#34;&gt;kube-scheduler plugin system&lt;/a&gt;. Our next POC was a &lt;code&gt;Filter&lt;/code&gt; plugin that determined the appropriate availability zone by pod ordinal, and it worked flawlessly!&lt;/p&gt;
&lt;p&gt;Our &lt;a href=&#34;https://github.com/cockroachlabs/crl-scheduler&#34;&gt;custom scheduler plugin&lt;/a&gt; is open source and runs in all of our CockroachCloud clusters.
Having control over how our StatefulSet pods are being scheduled has let us scale out with confidence.
We may look into retiring our plugin once pod topology spread constraints are available in GKE and EKS, but the maintenance overhead has been surprisingly low.
Better still: the plugin&#39;s implementation is orthogonal to our business logic. Deploying it, or retiring it for that matter, is as simple as changing the &lt;code&gt;schedulerName&lt;/code&gt; field in our StatefulSet definitions.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://twitter.com/_ostriches&#34;&gt;Chris Seto&lt;/a&gt; is a software engineer at Cockroach Labs and works on their Kubernetes automation for &lt;a href=&#34;https://cockroachlabs.cloud&#34;&gt;CockroachCloud&lt;/a&gt;, CockroachDB.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers</title>
      <link>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link>
      <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Shihang Zhang (Google)&lt;/p&gt;
&lt;p&gt;Typically when a &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md&#34;&gt;CSI&lt;/a&gt; driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods&#39; identities rather than the CSI driver&#39;s identity. CSI drivers, therefore, need some way to retrieve pod&#39;s service account token.&lt;/p&gt;
&lt;p&gt;Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.&lt;/p&gt;
&lt;p&gt;Both of them exhibit the following drawbacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Violating the principle of least privilege&lt;/li&gt;
&lt;li&gt;Every CSI driver needs to re-implement the logic of getting the pod’s service account token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second approach is more problematic due to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The audience of the token defaults to the kube-apiserver&lt;/li&gt;
&lt;li&gt;The token is not guaranteed to be available (e.g. &lt;code&gt;AutomountServiceAccountToken=false&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission&#34;&gt;file permission section for service account token&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The token might be legacy Kubernetes service account token which doesn’t expire if &lt;code&gt;BoundServiceAccountTokenVolume=false&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes 1.20 introduces an alpha feature, &lt;code&gt;CSIServiceAccountToken&lt;/code&gt;, to improve the security posture. The new feature allows CSI drivers to receive pods&#39; &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md&#34;&gt;bound service account tokens&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.&lt;/p&gt;
&lt;h2 id=&#34;pod-impersonation&#34;&gt;Pod Impersonation&lt;/h2&gt;
&lt;h3 id=&#34;using-gcp-apis&#34;&gt;Using GCP APIs&lt;/h3&gt;
&lt;p&gt;Using &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity&#34;&gt;Workload Identity&lt;/a&gt;, a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod&#39;s service account token to &lt;a href=&#34;https://cloud.google.com/iam/docs/reference/sts/rest&#34;&gt;exchange for GCP tokens&lt;/a&gt;. The pod&#39;s service account token is plumbed through the volume context in &lt;code&gt;NodePublishVolume&lt;/code&gt; RPC calls when the feature &lt;code&gt;CSIServiceAccountToken&lt;/code&gt; is enabled. For example: accessing &lt;a href=&#34;https://cloud.google.com/secret-manager/&#34;&gt;Google Secret Manager&lt;/a&gt; via a &lt;a href=&#34;https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp&#34;&gt;secret store CSI driver&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;using-vault&#34;&gt;Using Vault&lt;/h3&gt;
&lt;p&gt;If users configure &lt;a href=&#34;https://www.vaultproject.io/docs/auth/kubernetes&#34;&gt;Kubernetes as an auth method&lt;/a&gt;, Vault uses the &lt;code&gt;TokenReview&lt;/code&gt; API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod&#39;s service account to Vault. For example, &lt;a href=&#34;https://github.com/hashicorp/secrets-store-csi-driver-provider-vault&#34;&gt;secrets store CSI driver&lt;/a&gt; and &lt;a href=&#34;https://github.com/jetstack/cert-manager-csi&#34;&gt;cert manager CSI driver&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;short-lived-volumes&#34;&gt;Short-lived Volumes&lt;/h2&gt;
&lt;p&gt;To keep short-lived volumes such as certificates effective, CSI drivers can specify &lt;code&gt;RequiresRepublish=true&lt;/code&gt; in their&lt;code&gt;CSIDriver&lt;/code&gt; object to have the kubelet periodically call &lt;code&gt;NodePublishVolume&lt;/code&gt; on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md&#34;&gt;KEP-1855: Service Account Token for CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes-csi.github.io/docs/token-requests.html&#34;&gt;Token Requests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your feedback is always welcome!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SIG-Auth &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-auth#meetings&#34;&gt;meets regularly&lt;/a&gt; and can be reached via &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-auth#contact&#34;&gt;Slack and the mailing list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SIG-Storage &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage#meetings&#34;&gt;meets regularly&lt;/a&gt; and can be reached via &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage#contact&#34;&gt;Slack and the mailing list&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Third Party Device Metrics Reaches GA</title>
      <link>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Renaud Gaubert (NVIDIA), David Ashpole (Google), and Pramod Ramarao (NVIDIA)&lt;/p&gt;
&lt;p&gt;With Kubernetes 1.20, infrastructure teams who manage large scale Kubernetes clusters, are seeing the graduation of two exciting and long awaited features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Pod Resources API (introduced in 1.13) is finally graduating to GA. This allows Kubernetes plugins to obtain information about the node’s resource usage and assignment; for example: which pod/container consumes which device.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;DisableAcceleratorMetrics&lt;/code&gt; feature (introduced in 1.19) is graduating to beta and will be enabled by default. This removes device metrics reported by the kubelet in favor of the new plugin architecture.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of the features related to fundamental device support (device discovery, plugin, and monitoring) are reaching a strong level of stability.
Kubernetes users should see these features as stepping stones to enable more complex use cases (networking, scheduling, storage, etc.)!&lt;/p&gt;
&lt;p&gt;One such example is Non Uniform Memory Access (NUMA) placement where, when selecting a device, an application typically wants to ensure that data transfer between CPU Memory and Device Memory is as fast as possible. In some cases, incorrect NUMA placement can nullify the benefit of offloading compute to an external device.&lt;/p&gt;
&lt;p&gt;If these are topics of interest to you, consider joining the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node&#34;&gt;Kubernetes Node Special Insterest Group&lt;/a&gt; (SIG) for all topics related to the Kubernetes node, the COD (container orchestrated device) workgroup for topics related to runtimes, or the resource management forum for topics related to resource management!&lt;/p&gt;
&lt;h2 id=&#34;the-pod-resources-api-why-does-it-need-to-exist&#34;&gt;The Pod Resources API - Why does it need to exist?&lt;/h2&gt;
&lt;p&gt;Kubernetes is a vendor neutral platform. If we want it to support device monitoring, adding vendor-specific code in the Kubernetes code base is not an ideal solution. Ultimately, devices are a domain where deep expertise is needed and the best people to add and maintain code in that area are the device vendors themselves.&lt;/p&gt;
&lt;p&gt;The Pod Resources API was built as a solution to this issue. Each vendor can build and maintain their own out-of-tree monitoring plugin. This monitoring plugin, often deployed as a separate pod within a cluster, can then associate the metrics a device emits with the associated pod that&#39;s using it.&lt;/p&gt;
&lt;p&gt;For example, use the NVIDIA GPU dcgm-exporter to scrape metrics in Prometheus format:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl -sL http://127.0.01:8080/metrics


# HELP DCGM_FI_DEV_SM_CLOCK SM clock frequency (in MHz).
# TYPE DCGM_FI_DEV_SM_CLOCK gauge
# HELP DCGM_FI_DEV_MEM_CLOCK Memory clock frequency (in MHz).
# TYPE DCGM_FI_DEV_MEM_CLOCK gauge
# HELP DCGM_FI_DEV_MEMORY_TEMP Memory temperature (in C).
# TYPE DCGM_FI_DEV_MEMORY_TEMP gauge
...
DCGM_FI_DEV_SM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 139
DCGM_FI_DEV_MEM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 405
DCGM_FI_DEV_MEMORY_TEMP{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 9223372036854775794
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Each agent is expected to adhere to the node monitoring guidelines. In other words, plugins are expected to generate metrics in Prometheus format, and new metrics should not have any dependency on the Kubernetes base directly.&lt;/p&gt;
&lt;p&gt;This allows consumers of the metrics to use a compatible monitoring pipeline to collect and analyze metrics from a variety of agents, even if they are maintained by different vendors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/metrics-chart.png&#34; alt=&#34;Device metrics flowchart&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nvidia-gpu-metrics-deprecated&#34;&gt;Disabling the NVIDIA GPU metrics - Warning&lt;/h2&gt;
&lt;p&gt;With the graduation of the plugin monitoring system, Kubernetes is deprecating the NVIDIA GPU metrics that are being reported by the kubelet.&lt;/p&gt;
&lt;p&gt;With the &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/#disable-accelerator-metrics&#34;&gt;DisableAcceleratorMetrics&lt;/a&gt; feature being enabled by default in Kubernetes 1.20, NVIDIA GPUs are no longer special citizens  in Kubernetes. This is a good thing in the spirit of being vendor-neutral, and enables the most suited people to maintain their plugin on their own release schedule!&lt;/p&gt;
&lt;p&gt;Users will now need to either install the &lt;a href=&#34;https://github.com/NVIDIA/gpu-monitoring-tools&#34;&gt;NVIDIA GDGM exporter&lt;/a&gt; or use &lt;a href=&#34;https://github.com/nvidia/go-nvml&#34;&gt;bindings&lt;/a&gt; to gather more accurate and complete metrics about NVIDIA GPUs. This deprecation means that you can no longer rely on metrics that were reported by kubelet, such as &lt;code&gt;container_accelerator_duty_cycle&lt;/code&gt; or &lt;code&gt;container_accelerator_memory_used_bytes&lt;/code&gt; which were used to gather NVIDIA GPU memory utilization.&lt;/p&gt;
&lt;p&gt;This means that users who used to rely on the NVIDIA GPU metrics reported by the kubelet, will need to update their reference and deploy the NVIDIA plugin. Namely the different metrics reported by Kubernetes map to the following metrics:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Kubernetes Metrics&lt;/th&gt;
&lt;th&gt;NVIDIA dcgm-exporter metric&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;container_accelerator_duty_cycle&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;DCGM_FI_DEV_GPU_UTIL&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;container_accelerator_memory_used_bytes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;DCGM_FI_DEV_FB_USED&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;container_accelerator_memory_total_bytes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;DCGM_FI_DEV_FB_FREE + DCGM_FI_DEV_FB_USED&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You might also be interested in other metrics such as &lt;code&gt;DCGM_FI_DEV_GPU_TEMP&lt;/code&gt; (the GPU temperature) or DCGM_FI_DEV_POWER_USAGE (the power usage). The &lt;a href=&#34;https://github.com/NVIDIA/gpu-monitoring-tools/blob/d5c9bb55b4d1529ca07068b7f81e690921ce2b59/etc/dcgm-exporter/default-counters.csv&#34;&gt;default set&lt;/a&gt; is available in Nvidia&#39;s &lt;a href=&#34;https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/group__dcgmFieldIdentifiers.html&#34;&gt;Data Center GPU Manager documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that for this release you can still set the &lt;code&gt;DisableAcceleratorMetrics&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt; to &lt;em&gt;false&lt;/em&gt;, effectively re-enabling the ability for the kubelet to report NVIDIA GPU metrics.&lt;/p&gt;
&lt;p&gt;Paired with the graduation of the Pod Resources API, these tools can be used to generate GPU telemetry &lt;a href=&#34;https://grafana.com/grafana/dashboards/12239&#34;&gt;that can be used in visualization dashboards&lt;/a&gt;, below is an example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/grafana.png&#34; alt=&#34;Grafana visualization of device metrics&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-pod-resources-api-what-can-i-go-on-to-do-with-this&#34;&gt;The Pod Resources API - What can I go on to do with this?&lt;/h2&gt;
&lt;p&gt;As soon as this interface was introduced, many vendors started using it for widely different use cases! To list a few examples:&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/openstack/kuryr-kubernetes&#34;&gt;kuryr-kubernetes&lt;/a&gt; CNI plugin in tandem with &lt;a href=&#34;https://github.com/intel/sriov-network-device-plugin&#34;&gt;intel-sriov-device-plugin&lt;/a&gt;. This allowed the CNI plugin to know which allocation of SR-IOV Virtual Functions (VFs) the kubelet made and use that information to correctly setup the container network namespace and use a device with the appropriate NUMA node. We also expect this interface to be used to track the allocated and available resources with information about the NUMA topology of the worker node.&lt;/p&gt;
&lt;p&gt;Another use-case is GPU telemetry, where GPU metrics can be associated with the containers and pods that the GPU is assigned to. One such example is the NVIDIA &lt;code&gt;dcgm-exporter&lt;/code&gt;, but others can be easily built in the same paradigm.&lt;/p&gt;
&lt;p&gt;The Pod Resources API is a simple gRPC service which informs clients of the pods the kubelet knows. The information concerns the devices assignment the kubelet made and the assignment of CPUs. This information is obtained from the internal state of the kubelet&#39;s Device Manager and CPU Manager respectively.&lt;/p&gt;
&lt;p&gt;You can see below a sample example of the API and how a go client could use that information in a few lines:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;service PodResourcesLister {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}

    // Kubernetes 1.21
    rpc Watch(WatchPodResourcesRequest) returns (stream WatchPodResourcesResponse) {}
}
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;main&lt;/span&gt;() {
	ctx, cancel &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; context.&lt;span style=&#34;color:#00a000&#34;&gt;WithTimeout&lt;/span&gt;(context.&lt;span style=&#34;color:#00a000&#34;&gt;Background&lt;/span&gt;(), connectionTimeout)
	&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;defer&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;cancel&lt;/span&gt;()

	socket &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/var/lib/kubelet/pod-resources/kubelet.sock&amp;#34;&lt;/span&gt;
	conn, err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; grpc.&lt;span style=&#34;color:#00a000&#34;&gt;DialContext&lt;/span&gt;(ctx, socket, grpc.&lt;span style=&#34;color:#00a000&#34;&gt;WithInsecure&lt;/span&gt;(), grpc.&lt;span style=&#34;color:#00a000&#34;&gt;WithBlock&lt;/span&gt;(),
		grpc.&lt;span style=&#34;color:#00a000&#34;&gt;WithDialer&lt;/span&gt;(&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt;(addr &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;string&lt;/span&gt;, timeout time.Duration) (net.Conn, &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;error&lt;/span&gt;) {
			&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; net.&lt;span style=&#34;color:#00a000&#34;&gt;DialTimeout&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;unix&amp;#34;&lt;/span&gt;, addr, timeout)
		}),
	)

	&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
		&lt;span style=&#34;color:#a2f&#34;&gt;panic&lt;/span&gt;(err)
	}

    client &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; podresourcesapi.&lt;span style=&#34;color:#00a000&#34;&gt;NewPodResourcesListerClient&lt;/span&gt;(conn)
    resp, err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; client.&lt;span style=&#34;color:#00a000&#34;&gt;List&lt;/span&gt;(ctx, &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;podresourcesapi.ListPodResourcesRequest{})
	&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {
		&lt;span style=&#34;color:#a2f&#34;&gt;panic&lt;/span&gt;(err)
	}
	net.&lt;span style=&#34;color:#00a000&#34;&gt;Printf&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;%+v\n&amp;#34;&lt;/span&gt;, resp)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, note that you can watch the number of requests made to the Pod Resources endpoint by watching the new kubelet metric called &lt;code&gt;pod_resources_endpoint_requests_total&lt;/code&gt; on the kubelet&#39;s &lt;code&gt;/metrics&lt;/code&gt; endpoint.&lt;/p&gt;
&lt;h2 id=&#34;is-device-monitoring-suitable-for-production-can-i-extend-it-can-i-contribute&#34;&gt;Is device monitoring suitable for production? Can I extend it? Can I contribute?&lt;/h2&gt;
&lt;p&gt;Yes! This feature released in 1.13, almost 2 years ago, has seen broad adoption, is already used by different cloud managed services, and with its graduation to G.A in Kubernetes 1.20 is production ready!&lt;/p&gt;
&lt;p&gt;If you are a device vendor, you can start using it today! If you just want to monitor the devices in your cluster, go get the latest version of your monitoring plugin!&lt;/p&gt;
&lt;p&gt;If you feel passionate about that area, join the kubernetes community, help improve the API or contribute the device monitoring plugins!&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We thank the members of the community who have contributed to this feature or given feedback including members of WG-Resource-Management, SIG-Node and the Resource management forum!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.20: Granular Control of Volume Permission Changes</title>
      <link>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</link>
      <pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Hemant Kumar, Red Hat &amp;amp; Christian Huffman, Red Hat&lt;/p&gt;
&lt;p&gt;Kubernetes 1.20 brings two important beta features, allowing Kubernetes admins and users alike to have more adequate control over how volume permissions are applied when a volume is mounted inside a Pod.&lt;/p&gt;
&lt;h3 id=&#34;allow-users-to-skip-recursive-permission-changes-on-mount&#34;&gt;Allow users to skip recursive permission changes on mount&lt;/h3&gt;
&lt;p&gt;Traditionally if your pod is running as a non-root user (&lt;a href=&#34;https://twitter.com/thockin/status/1333892204490735617&#34;&gt;which you should&lt;/a&gt;), you must specify a &lt;code&gt;fsGroup&lt;/code&gt; inside the pod’s security context so that the volume can be readable and writable by the Pod. This requirement is covered in more detail in &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But one side-effect of setting &lt;code&gt;fsGroup&lt;/code&gt; is that, each time a volume is mounted, Kubernetes must recursively &lt;code&gt;chown()&lt;/code&gt; and &lt;code&gt;chmod()&lt;/code&gt; all the files and directories inside the volume - with a few exceptions noted below. This happens even if group ownership of the volume already matches the requested &lt;code&gt;fsGroup&lt;/code&gt;, and can be pretty expensive for larger volumes with lots of small files, which causes pod startup to take a long time. This scenario has been a &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/69699&#34;&gt;known problem&lt;/a&gt; for a while, and in Kubernetes 1.20 we are providing knobs to opt-out of recursive permission changes if the volume already has the correct permissions.&lt;/p&gt;
&lt;p&gt;When configuring a pod’s security context, set &lt;code&gt;fsGroupChangePolicy&lt;/code&gt; to &amp;quot;OnRootMismatch&amp;quot; so if the root of the volume already has the correct permissions, the recursive permission change can be skipped. Kubernetes ensures that permissions of the top-level directory are changed last the first time it applies permissions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;securityContext&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;runAsUser&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;runAsGroup&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;3000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;fsGroup&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;fsGroupChangePolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;OnRootMismatch&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can learn more about this in &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods&#34;&gt;Configure volume permission and ownership change policy for Pods&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;allow-csi-drivers-to-declare-support-for-fsgroup-based-permissions&#34;&gt;Allow CSI Drivers to declare support for fsGroup based permissions&lt;/h3&gt;
&lt;p&gt;Although the previous section implied that Kubernetes &lt;em&gt;always&lt;/em&gt; recursively changes permissions of a volume if a Pod has a &lt;code&gt;fsGroup&lt;/code&gt;, this is not strictly true. For certain multi-writer volume types, such as NFS or Gluster, the cluster doesn’t perform recursive permission changes even if the pod has a &lt;code&gt;fsGroup&lt;/code&gt;. Other volume types may not even support &lt;code&gt;chown()&lt;/code&gt;/&lt;code&gt;chmod()&lt;/code&gt;, which rely on Unix-style permission control primitives.&lt;/p&gt;
&lt;p&gt;So how do we know when to apply recursive permission changes and when we shouldn&#39;t? For in-tree storage drivers, this was relatively simple. For &lt;a href=&#34;https://kubernetes-csi.github.io/docs/introduction.html#introduction&#34;&gt;CSI&lt;/a&gt; drivers that could span a multitude of platforms and storage types, this problem can be a bigger challenge.&lt;/p&gt;
&lt;p&gt;Previously, whenever a CSI volume was mounted to a Pod, Kubernetes would attempt to automatically determine if the permissions and ownership should be modified. These methods were imprecise and could cause issues as we already mentioned, depending on the storage type.&lt;/p&gt;
&lt;p&gt;The CSIDriver custom resource now has a &lt;code&gt;.spec.fsGroupPolicy&lt;/code&gt; field, allowing storage drivers to explicitly opt in or out of these recursive modifications. By having the CSI driver specify a policy for the backing volumes, Kubernetes can avoid needless modification attempts. This optimization helps to reduce volume mount time and also cuts own reporting errors about modifications that would never succeed.&lt;/p&gt;
&lt;h4 id=&#34;csidriver-fsgrouppolicy-api&#34;&gt;CSIDriver FSGroupPolicy API&lt;/h4&gt;
&lt;p&gt;Three FSGroupPolicy values are available as of Kubernetes 1.20, with more planned for future releases.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ReadWriteOnceWithFSType&lt;/strong&gt; - This is the default policy, applied if no &lt;code&gt;fsGroupPolicy&lt;/code&gt; is defined; this preserves the behavior from previous Kubernetes releases. Each volume is examined at mount time to determine if permissions should be recursively applied.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;File&lt;/strong&gt; - Always attempt to apply permission modifications, regardless of the filesystem type or PersistentVolumeClaim’s access mode.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;None&lt;/strong&gt; - Never apply permission modifications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;how-do-i-use-it&#34;&gt;How do I use it?&lt;/h4&gt;
&lt;p&gt;The only configuration needed is defining &lt;code&gt;fsGroupPolicy&lt;/code&gt; inside of the &lt;code&gt;.spec&lt;/code&gt; for a CSIDriver. Once that element is defined, any subsequently mounted volumes will automatically use the defined policy. There’s no additional deployment required!&lt;/p&gt;
&lt;h4 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h4&gt;
&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push these implementations to GA in either 1.21 or 1.22.&lt;/p&gt;
&lt;h3 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h3&gt;
&lt;p&gt;This feature is explained in more detail in Kubernetes project documentation: &lt;a href=&#34;https://kubernetes-csi.github.io/docs/support-fsgroup.html&#34;&gt;CSI Driver fsGroup Support&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods&#34;&gt;Configure volume permission and ownership change policy for Pods &lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://kubernetes.slack.com/messages/csi&#34;&gt;Kubernetes Slack channel #csi&lt;/a&gt; and any of the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact&#34;&gt;standard SIG Storage communication channels&lt;/a&gt; are great mediums to reach out to the SIG Storage and the CSI team.&lt;/p&gt;
&lt;p&gt;Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34;&gt;Kubernetes Storage Special Interest Group (SIG)&lt;/a&gt;. We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.20: Kubernetes Volume Snapshot Moves to GA</title>
      <link>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Xing Yang, VMware &amp;amp; Xiangqian Yu, Google&lt;/p&gt;
&lt;p&gt;The Kubernetes Volume Snapshot feature is now GA in Kubernetes v1.20. It was introduced as &lt;a href=&#34;https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/&#34;&gt;alpha&lt;/a&gt; in Kubernetes v1.12, followed by a &lt;a href=&#34;https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/&#34;&gt;second alpha&lt;/a&gt; with breaking changes in Kubernetes v1.13, and promotion to &lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/&#34;&gt;beta&lt;/a&gt; in Kubernetes 1.17. This blog post summarizes the changes releasing the feature from beta to GA.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-volume-snapshot&#34;&gt;What is a volume snapshot?&lt;/h2&gt;
&lt;p&gt;Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to rehydrate a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p&gt;
&lt;h2 id=&#34;why-add-volume-snapshots-to-kubernetes&#34;&gt;Why add volume snapshots to Kubernetes?&lt;/h2&gt;
&lt;p&gt;Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster-specific” knowledge.&lt;/p&gt;
&lt;p&gt;The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database’s volumes before starting a database operation.&lt;/p&gt;
&lt;p&gt;By providing a standard way to trigger volume snapshot operations in Kubernetes, this feature allows Kubernetes users to incorporate snapshot operations in a portable manner on any Kubernetes environment regardless of the underlying storage.&lt;/p&gt;
&lt;p&gt;Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced enterprise-grade storage administration features for Kubernetes,  including application or cluster level backup solutions.&lt;/p&gt;
&lt;h2 id=&#34;what-s-new-since-beta&#34;&gt;What’s new since beta?&lt;/h2&gt;
&lt;p&gt;With the promotion of Volume Snapshot to GA, the feature is enabled by default on standard Kubernetes deployments and cannot be turned off.&lt;/p&gt;
&lt;p&gt;Many enhancements have been made to improve the quality of this feature and to make it production-grade.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Volume Snapshot APIs and client library were moved to a separate Go module.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A snapshot validation webhook has been added to perform necessary validation on volume snapshot objects. More details can be found in the &lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1900-volume-snapshot-validation-webhook&#34;&gt;Volume Snapshot Validation Webhook Kubernetes Enhancement Proposal&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Along with the validation webhook, the volume snapshot controller will start labeling invalid snapshot objects that already existed. This allows users to identify, remove any invalid objects, and correct their workflows. Once the API is switched to the v1 type, those invalid objects will not be deletable from the system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To provide better insights into how the snapshot feature is performing, an initial set of operation metrics has been added to the volume snapshot controller.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are more end-to-end tests, running on GCP, that validate the feature in a real Kubernetes cluster. Stress tests (based on Google Persistent Disk and &lt;code&gt;hostPath&lt;/code&gt; CSI Drivers) have been introduced to test the robustness of the system.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other than introducing tightening validation, there is no difference between the v1beta1 and v1 Kubernetes volume snapshot API. In this release (with Kubernetes 1.20), both v1 and v1beta1 are served while the stored API version is still v1beta1. Future releases will switch the stored version to v1 and gradually remove v1beta1 support.&lt;/p&gt;
&lt;h2 id=&#34;which-csi-drivers-support-volume-snapshots&#34;&gt;Which CSI drivers support volume snapshots?&lt;/h2&gt;
&lt;p&gt;Snapshots are only supported for CSI drivers, not for in-tree or FlexVolume drivers. Ensure the deployed CSI driver on your cluster has implemented the snapshot interfaces. For more information, see &lt;a href=&#34;https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/&#34;&gt;Container Storage Interface (CSI) for Kubernetes GA&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Currently more than &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;50 CSI drivers&lt;/a&gt; support the Volume Snapshot feature. The &lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver&#34;&gt;GCE Persistent Disk CSI Driver&lt;/a&gt; has gone through the tests for upgrading from volume snapshots beta to GA. GA level support for other CSI drivers should be available soon.&lt;/p&gt;
&lt;h2 id=&#34;who-builds-products-using-volume-snapshots&#34;&gt;Who builds products using volume snapshots?&lt;/h2&gt;
&lt;p&gt;As of the publishing of this blog, the following participants from the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/wg-data-protection&#34;&gt;Kubernetes Data Protection Working Group&lt;/a&gt; are building products or have already built products using Kubernetes volume snapshots.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.delltechnologies.com/en-us/data-protection/powerprotect-data-manager.htm&#34;&gt;Dell-EMC: PowerProtect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.druva.com/&#34;&gt;Druva&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kasten.io/&#34;&gt;Kasten K10&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.netapp.com/project-astra&#34;&gt;NetApp: Project Astra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://portworx.com/products/px-backup/&#34;&gt;Portworx (PX-Backup)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/purestorage/pso-csi&#34;&gt;Pure Storage (Pure Service Orchestrator)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage&#34;&gt;Red Hat OpenShift Container Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://robin.io/storage/&#34;&gt;Robin Cloud Native Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.trilio.io/kubernetes/&#34;&gt;TrilioVault for Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/velero-plugin-for-csi&#34;&gt;Velero plugin for CSI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-deploy-volume-snapshots&#34;&gt;How to deploy volume snapshots?&lt;/h2&gt;
&lt;p&gt;Volume Snapshot feature contains the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/client/config/crd&#34;&gt;Kubernetes Volume Snapshot CRDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller&#34;&gt;Volume snapshot controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/validation-webhook&#34;&gt;Snapshot validation webhook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CSI Driver along with &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/sidecar-controller&#34;&gt;CSI Snapshotter sidecar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is strongly recommended that Kubernetes distributors bundle and deploy the volume snapshot controller, CRDs, and validation webhook as part of their Kubernetes cluster management process (independent of any CSI Driver).&lt;/p&gt;
&lt;blockquote class=&#34;warning callout&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Warning:&lt;/strong&gt; The snapshot validation webhook serves as a critical component to transition smoothly from using v1beta1 to v1 API. Not installing the snapshot validation webhook makes prevention of invalid volume snapshot objects from creation/updating impossible, which in turn will block deletion of invalid volume snapshot objects in coming upgrades.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;If your cluster does not come pre-installed with the correct components, you may manually install them. See the &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter#readme&#34;&gt;CSI Snapshotter&lt;/a&gt; README for details.&lt;/p&gt;
&lt;h2 id=&#34;how-to-use-volume-snapshots&#34;&gt;How to use volume snapshots?&lt;/h2&gt;
&lt;p&gt;Assuming all the required components (including CSI driver) have been already deployed and running on your cluster, you can create volume snapshots using the &lt;code&gt;VolumeSnapshot&lt;/code&gt; API object, or use an existing &lt;code&gt;VolumeSnapshot&lt;/code&gt; to restore a PVC by specifying the VolumeSnapshot data source on it. For more details, see the &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volume-snapshots/&#34;&gt;volume snapshot documentation&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;note callout&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; The Kubernetes Snapshot API does not provide any application consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using higher level APIs/controllers.&lt;/div&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;dynamically-provision-a-volume-snapshot&#34;&gt;Dynamically provision a volume snapshot&lt;/h3&gt;
&lt;p&gt;To dynamically provision a volume snapshot, create a &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; API object first.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshotClass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapclass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;driver&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;testdriver.csi.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;deletionPolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Delete&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;parameters&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;csi.storage.k8s.io/snapshotter-secret-name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mysecret&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;csi.storage.k8s.io/snapshotter-secret-namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;mysecretnamespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then create a &lt;code&gt;VolumeSnapshot&lt;/code&gt; API object from a PVC by specifying the volume snapshot class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ns1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;volumeSnapshotClassName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapclass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;source&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;persistentVolumeClaimName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-pvc&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;importing-an-existing-volume-snapshot-with-kubernetes&#34;&gt;Importing an existing volume snapshot with Kubernetes&lt;/h3&gt;
&lt;p&gt;To import a pre-existing volume snapshot into Kubernetes, manually create a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object first.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshotContent&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-content&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;deletionPolicy&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Delete&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;driver&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;testdriver.csi.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;source&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;snapshotHandle&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;7bdd0de3-xxx&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;volumeSnapshotRef&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then create a &lt;code&gt;VolumeSnapshot&lt;/code&gt; object pointing to the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;source&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;volumeSnapshotContentName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-content&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;rehydrate-volume-from-snapshot&#34;&gt;Rehydrate volume from snapshot&lt;/h3&gt;
&lt;p&gt;A bound and ready &lt;code&gt;VolumeSnapshot&lt;/code&gt; object can be used to rehydrate a new volume with data pre-populated from snapshotted data as shown here:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pvc-restore&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;namespace&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;demo-namespace&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;storageClassName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-storageclass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;dataSource&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test-snapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;VolumeSnapshot&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiGroup&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;snapshot.storage.k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;accessModes&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;- ReadWriteOnce&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;resources&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;requests&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;storage&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;1Gi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;how-to-add-support-for-snapshots-in-a-csi-driver&#34;&gt;How to add support for snapshots in a CSI driver?&lt;/h2&gt;
&lt;p&gt;See the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;CSI spec&lt;/a&gt; and the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html&#34;&gt;Kubernetes-CSI Driver Developer Guide&lt;/a&gt; for more details on how to implement the snapshot feature in a CSI driver.&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-limitations&#34;&gt;What are the limitations?&lt;/h2&gt;
&lt;p&gt;The GA implementation of volume snapshots for Kubernetes has the following limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does not support reverting an existing PVC to an earlier state represented by a snapshot (only supports provisioning a new volume from a snapshot).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-learn-more&#34;&gt;How to learn more?&lt;/h3&gt;
&lt;p&gt;The code repository for snapshot APIs and controller is here: &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34;&gt;https://github.com/kubernetes-csi/external-snapshotter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Check out additional documentation on the snapshot feature here: &lt;a href=&#34;http://k8s.io/docs/concepts/storage/volume-snapshots&#34;&gt;http://k8s.io/docs/concepts/storage/volume-snapshots&lt;/a&gt; and &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34;&gt;https://kubernetes-csi.github.io/docs/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-to-get-involved&#34;&gt;How to get involved?&lt;/h2&gt;
&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;
&lt;p&gt;We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach GA. We want to thank Saad Ali, Michelle Au, Tim Hockin, and Jordan Liggitt for their insightful reviews and thorough consideration with the design, thank Andi Li for his work on adding the support of the snapshot validation webhook, thank Grant Griffiths on implementing metrics support in the snapshot controller and handling password rotation in the validation webhook, thank Chris Henzie, Raunak Shah, and Manohar Reddy for writing critical e2e tests to meet the scalability and stability requirements for graduation, thank Kartik Sharma for moving snapshot APIs and client lib to a separate go module, and thank Raunak Shah and Prafull Ladha for their help with upgrade testing from beta to GA.&lt;/p&gt;
&lt;p&gt;There are many more people who have helped to move the snapshot feature from beta to GA. We want to thank everyone who has contributed to this effort:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/AndiLi99&#34;&gt;Andi Li&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bswartz&#34;&gt;Ben Swartzlander&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chrishenzie&#34;&gt;Chris Henzie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/huffmanca&#34;&gt;Christian Huffman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ggriffiths&#34;&gt;Grant Griffiths&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/humblec&#34;&gt;Humble Devassy Chirammal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jsafrane&#34;&gt;Jan Šafránek&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Jiawei0227&#34;&gt;Jiawei Wang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jingxu97&#34;&gt;Jing Xu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/liggitt&#34;&gt;Jordan Liggitt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Kartik494&#34;&gt;Kartik Sharma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Madhu-1&#34;&gt;Madhu Rajanna&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/boddumanohar&#34;&gt;Manohar Reddy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/msau42&#34;&gt;Michelle Au&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pohly&#34;&gt;Patrick Ohly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/prafull01&#34;&gt;Prafull Ladha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/prateekpandey14&#34;&gt;Prateek Pandey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/RaunakShah&#34;&gt;Raunak Shah&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/saad-ali&#34;&gt;Saad Ali&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/saikat-royc&#34;&gt;Saikat Roychowdhury&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/thockin&#34;&gt;Tim Hockin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yuxiangqian&#34;&gt;Xiangqian Yu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/xing-yang&#34;&gt;Xing Yang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zhucan&#34;&gt;Zhu Can&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt;  (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;
&lt;p&gt;We also hold regular &lt;a href=&#34;https://docs.google.com/document/d/15tLCV3csvjHbKb16DVk-mfUmFry_Rlwo-2uG6KNGsfw/edit#&#34;&gt;Data Protection Working Group meetings&lt;/a&gt;. New attendees are welcome to join in discussions.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.20: The Raddest Release</title>
      <link>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md&#34;&gt;Kubernetes 1.20 Release Team&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We’re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha.&lt;/p&gt;
&lt;p&gt;The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;major-themes&#34;&gt;Major Themes&lt;/h2&gt;
&lt;h3 id=&#34;volume-snapshot-operations-goes-stable&#34;&gt;Volume Snapshot Operations Goes Stable&lt;/h3&gt;
&lt;p&gt;This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers.&lt;/p&gt;
&lt;p&gt;Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions.&lt;/p&gt;
&lt;p&gt;Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster.&lt;/p&gt;
&lt;h3 id=&#34;kubectl-debug-graduates-to-beta&#34;&gt;Kubectl Debug Graduates to Beta&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;kubectl alpha debug&lt;/code&gt; features graduates to beta in 1.20, becoming &lt;code&gt;kubectl debug&lt;/code&gt;. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.&lt;/li&gt;
&lt;li&gt;Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)&lt;/li&gt;
&lt;li&gt;Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that as a new built-in command, &lt;code&gt;kubectl debug&lt;/code&gt; takes priority over any kubectl plugin named “debug”. You must rename the affected plugin.&lt;/p&gt;
&lt;p&gt;Invocations using &lt;code&gt;kubectl alpha debug&lt;/code&gt; are now deprecated and will be removed in a subsequent release. Update your scripts to use &lt;code&gt;kubectl debug&lt;/code&gt;. For more information about &lt;code&gt;kubectl debug&lt;/code&gt;, see &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/&#34;&gt;Debugging Running Pods&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;beta-api-priority-and-fairness&#34;&gt;Beta: API Priority and Fairness&lt;/h3&gt;
&lt;p&gt;Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows &lt;code&gt;kube-apiserver&lt;/code&gt; to categorize incoming requests by priority levels.&lt;/p&gt;
&lt;h3 id=&#34;alpha-with-updates-ipv4-ipv6&#34;&gt;Alpha with updates: IPV4/IPV6&lt;/h3&gt;
&lt;p&gt;The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa.&lt;/p&gt;
&lt;h3 id=&#34;ga-process-pid-limiting-for-stability&#34;&gt;GA: Process PID Limiting for Stability&lt;/h3&gt;
&lt;p&gt;Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine.&lt;/p&gt;
&lt;p&gt;Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node.
After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both &lt;code&gt;SupportNodePidsLimit&lt;/code&gt; (node-to-pod PID isolation) and &lt;code&gt;SupportPodPidsLimit&lt;/code&gt; (ability to limit PIDs per pod).&lt;/p&gt;
&lt;h3 id=&#34;alpha-graceful-node-shutdown&#34;&gt;Alpha: Graceful node shutdown&lt;/h3&gt;
&lt;p&gt;Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The &lt;code&gt;GracefulNodeShutdown&lt;/code&gt; feature is now in Alpha. &lt;code&gt;GracefulNodeShutdown&lt;/code&gt; makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown.&lt;/p&gt;
&lt;h2 id=&#34;major-changes&#34;&gt;Major Changes&lt;/h2&gt;
&lt;h3 id=&#34;dockershim-deprecation&#34;&gt;Dockershim Deprecation&lt;/h3&gt;
&lt;p&gt;Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a &lt;a href=&#34;https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/&#34;&gt;detailed blog post about deprecation&lt;/a&gt; with &lt;a href=&#34;https://blog.k8s.io/2020/12/02/dockershim-faq/&#34;&gt;a dedicated FAQ page for it&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;exec-probe-timeout-handling&#34;&gt;Exec Probe Timeout Handling&lt;/h3&gt;
&lt;p&gt;A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field &lt;code&gt;timeoutSeconds&lt;/code&gt; was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of &lt;code&gt;1 second&lt;/code&gt; will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called &lt;code&gt;ExecProbeTimeout&lt;/code&gt;, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Please review the updated documentation regarding &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes&#34;&gt;configuring probes&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2 id=&#34;other-updates&#34;&gt;Other Updates&lt;/h2&gt;
&lt;h3 id=&#34;graduated-to-stable&#34;&gt;Graduated to Stable&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/585&#34;&gt;RuntimeClass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1929&#34;&gt;Built-in API Types Defaults&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/950&#34;&gt;Add Pod-Startup Liveness-Probe Holdoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1001&#34;&gt;Support CRI-ContainerD On Windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/614&#34;&gt;SCTP Support for Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1507&#34;&gt;Adding AppProtocol To Services And Endpoints&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notable-feature-updates&#34;&gt;Notable Feature Updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/19&#34;&gt;CronJobs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;release-notes&#34;&gt;Release notes&lt;/h1&gt;
&lt;p&gt;You can check out the full details of the 1.20 release in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;availability-of-release&#34;&gt;Availability of release&lt;/h1&gt;
&lt;p&gt;Kubernetes 1.20 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0&#34;&gt;download on GitHub&lt;/a&gt;. There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34;&gt;interactive tutorials&lt;/a&gt; on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href=&#34;https://kind.sigs.k8s.io&#34;&gt;kind&lt;/a&gt;. If you’d like to try building a cluster from scratch, check out the &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;Kubernetes the Hard Way&lt;/a&gt; tutorial by Kelsey Hightower.&lt;/p&gt;
&lt;h1 id=&#34;release-team&#34;&gt;Release Team&lt;/h1&gt;
&lt;p&gt;This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community.&lt;/p&gt;
&lt;h1 id=&#34;release-logo&#34;&gt;Release Logo&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://kubernetes.io/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png&#34; alt=&#34;Kubernetes 1.20 Release Logo&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dictionary.com/browse/rad&#34;&gt;raddest&lt;/a&gt;: &lt;em&gt;adjective&lt;/em&gt;, Slang. excellent; wonderful; cool:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Kubernetes 1.20 Release has been the raddest release yet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14&#34;&gt;Kubernetes 1.14 - Caturnetes&lt;/a&gt; with a &amp;quot;rad&amp;quot; cat named Humphrey.&lt;/p&gt;
&lt;p&gt;Humphrey is the release lead&#39;s cat and has a permanent &lt;a href=&#34;https://www.inverse.com/article/42316-why-do-cats-blep-science-explains&#34;&gt;&lt;code&gt;blep&lt;/code&gt;&lt;/a&gt;. &lt;em&gt;Rad&lt;/em&gt; was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his &lt;em&gt;blep&lt;/em&gt; bring you a little joy at the end of 2020!&lt;/p&gt;
&lt;p&gt;The release logo was created by &lt;a href=&#34;https://www.instagram.com/robotdancebattle/&#34;&gt;Henry Hsu - @robotdancebattle&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch &lt;a href=&#34;https://youtu.be/Tx8qXC-U3KM&#34;&gt;Alena Prokharchyk&#39;s KubeCon NA Keynote&lt;/a&gt; to learn more about their cloud native journey.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://k8s.devstats.cncf.io/&#34;&gt;CNCF K8s DevStats project&lt;/a&gt; aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p&gt;
&lt;p&gt;In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from &lt;a href=&#34;https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&#34;&gt;967 companies&lt;/a&gt; and &lt;a href=&#34;https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&#34;&gt;1335 individuals&lt;/a&gt; (&lt;a href=&#34;https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-repogroup_name=Kubernetes&#34;&gt;44 of whom&lt;/a&gt; made their first Kubernetes contribution) from &lt;a href=&#34;https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-period_name=Quarter&amp;amp;var-countries=All&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-metric=rcommitters&amp;amp;var-cum=countries&#34;&gt;26 countries&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are &lt;a href=&#34;https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut&#34;&gt;now available to all on-demand&lt;/a&gt; for anyone still needing to catch up!&lt;/li&gt;
&lt;li&gt;In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming&#39;s goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given &lt;a href=&#34;https://sched.co/eukp&#34;&gt;at KubeCon 2020 North America&lt;/a&gt;, and the initial impact of this labor &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2067&#34;&gt;can actually be seen in the v1.20 release&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Previously announced this summer, &lt;a href=&#34;https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/&#34;&gt;The Certified Kubernetes Security Specialist (CKS) Certification&lt;/a&gt; was released during Kubecon NA for immediate scheduling!  Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains.  This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?).&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;event-updates&#34;&gt;Event Updates&lt;/h1&gt;
&lt;p&gt;KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/&#34;&gt;here&lt;/a&gt;. Remember that &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/&#34;&gt;the CFP&lt;/a&gt; closes on Sunday, December 13, 11:59pm PST!&lt;/p&gt;
&lt;h1 id=&#34;upcoming-release-webinar&#34;&gt;Upcoming release webinar&lt;/h1&gt;
&lt;p&gt;Stay tuned for the upcoming release webinar happening this January.&lt;/p&gt;
&lt;h1 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h1&gt;
&lt;p&gt;If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find out more about contributing to Kubernetes at the new &lt;a href=&#34;https://www.kubernetes.dev/&#34;&gt;Kubernetes Contributor website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community on &lt;a href=&#34;http://slack.k8s.io/&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read more about what’s happening with Kubernetes on the &lt;a href=&#34;https://kubernetes.io/blog/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learn more about the &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/release-team&#34;&gt;Kubernetes Release Team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: GSoD 2020: Improving the API Reference Experience</title>
      <link>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;https://github.com/feloy&#34;&gt;Philippe Martin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Editor&#39;s note: Better API references have been my goal since I joined Kubernetes docs three and a half years ago. Philippe has succeeded fantastically. More than a better API reference, though, Philippe embodied the best of the Kubernetes community in this project: excellence through collaboration, and a process that made the community itself better. Thanks, Google Season of Docs, for making Philippe&#39;s work possible. —Zach Corleissen&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://developers.google.com/season-of-docs&#34;&gt;Google Season of Docs&lt;/a&gt; project brings open source organizations and technical writers together to work closely on a specific documentation project.&lt;/p&gt;
&lt;p&gt;I was selected by the CNCF to work on Kubernetes documentation, specifically to make the API Reference documentation more accessible.&lt;/p&gt;
&lt;p&gt;I&#39;m a software developer with a great interest in documentation systems. In the late 90&#39;s I started translating Linux-HOWTO documents into French. From one thing to another, I learned about documentation systems. Eventually, I wrote a Linux-HOWTO to help documentarians learn the language used at that time for writing documents, LinuxDoc/SGML.&lt;/p&gt;
&lt;p&gt;Shortly afterward, Linux documentation adopted the DocBook language. I helped some writers rewrite their documents in this format; for example, the Advanced Bash-Scripting Guide. I also worked on the GNU &lt;code&gt;makeinfo&lt;/code&gt; program to add DocBook output, making it possible to transform &lt;em&gt;GNU Info&lt;/em&gt; documentation into Docbook format.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/docs/home/&#34;&gt;Kubernetes website&lt;/a&gt; is built with Hugo from documentation written in Markdown format in the &lt;a href=&#34;https://github.com/kubernetes/website&#34;&gt;website repository&lt;/a&gt;, using the &lt;a href=&#34;https://www.docsy.dev/about/&#34;&gt;Docsy Hugo theme&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The existing API reference documentation is a large HTML file generated from the Kubernetes OpenAPI specification.&lt;/p&gt;
&lt;p&gt;On my side, I wanted for some time to make the API Reference more accessible, by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;building individual and autonomous pages for each Kubernetes resource&lt;/li&gt;
&lt;li&gt;adapting the format to mobile reading&lt;/li&gt;
&lt;li&gt;reusing the website&#39;s assets and theme to build, integrate, and display the reference pages&lt;/li&gt;
&lt;li&gt;allowing the search engines to reference the content of the pages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Around one year ago, I started to work on the generator building the current unique HTML page, to add a DocBook output, so the API Reference could be generated first in DocBook format, and after that in PDF or other formats supported by DocBook processors. The first result has been some &lt;a href=&#34;https://github.com/feloy/kubernetes-resources-reference/releases&#34;&gt;Ebook files for the API Reference&lt;/a&gt; and an auto-edited paper book.&lt;/p&gt;
&lt;p&gt;I decided later to add another output to this generator, to generate Markdown files and create &lt;a href=&#34;https://web.archive.org/web/20201022201911/https://www.k8sref.io/docs/workloads/&#34;&gt;a website with the API Reference&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When the CNCF proposed a project for the Google Season of Docs to work on the API Reference, I applied, and the match occurred.&lt;/p&gt;
&lt;h2 id=&#34;the-project&#34;&gt;The Project&lt;/h2&gt;
&lt;h3 id=&#34;swagger-ui&#34;&gt;swagger-ui&lt;/h3&gt;
&lt;p&gt;The first idea of the CNCF members that proposed this project was to test the &lt;a href=&#34;https://swagger.io/tools/swagger-ui/&#34;&gt;&lt;code&gt;swagger-ui&lt;/code&gt; tool&lt;/a&gt;, to try and document the Kubernetes API Reference with this standard tool.&lt;/p&gt;
&lt;p&gt;Because the Kubernetes API is much larger than many other APIs, it has been necessary to write a tool to split the complete API Reference by API Groups, and insert in the Documentation website several &lt;code&gt;swagger-ui&lt;/code&gt; components, one for each API Group.&lt;/p&gt;
&lt;p&gt;Generally, APIs are used by developers by calling endpoints with a specific HTTP verb, with specific parameters and waiting for a response. The &lt;code&gt;swagger-ui&lt;/code&gt; interface is built for this usage: the interface displays a list of endpoints and their associated verbs, and for each the parameters and responses formats.&lt;/p&gt;
&lt;p&gt;The Kubernetes API is most of the time used differently: users create manifest files containing resources definitions in YAML format, and use the &lt;code&gt;kubectl&lt;/code&gt; CLI to &lt;em&gt;apply&lt;/em&gt; these manifests to the cluster. In this case, the most important information is the description of the structures used as parameters and responses (the Kubernetes Resources).&lt;/p&gt;
&lt;p&gt;Because of this specificity, we realized that it would be difficult to adapt the &lt;code&gt;swagger-ui&lt;/code&gt; interface to satisfy the users of the Kubernetes API and this direction has been abandoned.&lt;/p&gt;
&lt;h3 id=&#34;markdown-pages&#34;&gt;Markdown pages&lt;/h3&gt;
&lt;p&gt;The second stage of the project has been to adapt the work I had done to create the k8sref.io website, to include it in the official documentation website.&lt;/p&gt;
&lt;p&gt;The main changes have been to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use go-templates to represent the output pages, so non-developers can adapt the generated pages without having to edit the generator code&lt;/li&gt;
&lt;li&gt;create a new custom &lt;a href=&#34;https://gohugo.io/content-management/shortcodes/&#34;&gt;shortcode&lt;/a&gt;, to easily create links from inside the website to specific pages of the API reference&lt;/li&gt;
&lt;li&gt;improve the navigation between the sections of the API reference&lt;/li&gt;
&lt;li&gt;add the code of the generator to the Kubernetes GitHub repository containing the different reference generators&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the discussions and work done can be found in website &lt;a href=&#34;https://github.com/kubernetes/website/pull/23294&#34;&gt;pull request #23294&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Adding the generator code to the Kubernetes project happened in &lt;a href=&#34;https://github.com/kubernetes-sigs/reference-docs/pull/179&#34;&gt;kubernetes-sigs/reference-docs#179&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are the features of the new API Reference to be included in the official documentation website:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the resources are categorized, in the categories Workloads, Services, Config &amp;amp; Storage, Authentication, Authorization, Policies, Extend, Cluster. This structure is configurable with a simple &lt;a href=&#34;https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/toc.yaml&#34;&gt;&lt;code&gt;toc.yaml&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;each page displays associated resources at the first level ; for example: Pod, PodSpec, PodStatus, PodList&lt;/li&gt;
&lt;li&gt;most resource pages inline relevant definitions ; the exceptions are when those definitions are common to several resources, or are too complex to be displayed inline. With the old approach, you had to follow a hyperlink to read each extra detail.&lt;/li&gt;
&lt;li&gt;some widely used definitions, such as &lt;code&gt;ObjectMeta&lt;/code&gt;, are documented in a specific page&lt;/li&gt;
&lt;li&gt;required fields are indicated, and placed first&lt;/li&gt;
&lt;li&gt;fields of a resource can be categorized and ordered, with the help of a &lt;a href=&#34;https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/fields.yaml&#34;&gt;&lt;code&gt;fields.yaml&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;map&lt;/code&gt; fields are indicated. For example the &lt;code&gt;.spec.nodeSelector&lt;/code&gt; for a &lt;code&gt;Pod&lt;/code&gt; is &lt;code&gt;map[string]string&lt;/code&gt;, instead of &lt;code&gt;object&lt;/code&gt;, using the value of &lt;code&gt;x-kubernetes-list-type&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;patch strategies are indicated&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apiVersion&lt;/code&gt; and &lt;code&gt;kind&lt;/code&gt; display the value, not the &lt;code&gt;string&lt;/code&gt; type&lt;/li&gt;
&lt;li&gt;At the top of a reference page, the page displays the Go import necessary to use these resources from a Go program.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The work is currently on hold pending the 1.20 release. When the release finishes and the work is integrated, the API reference will be available at &lt;a href=&#34;https://kubernetes.io/docs/reference/&#34;&gt;https://kubernetes.io/docs/reference/&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;future-work&#34;&gt;Future Work&lt;/h3&gt;
&lt;p&gt;There are points to improve, particularly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some Kubernetes resources are deeply nested. Inlining the definition of these resources makes them difficult to understand.&lt;/li&gt;
&lt;li&gt;The created &lt;code&gt;shortcode&lt;/code&gt; uses the URL of the page to reference a Resource page. It would be easier for documentarians if they could reference a Resource by its group and name.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appreciation&#34;&gt;Appreciation&lt;/h2&gt;
&lt;p&gt;I would like to thank my mentor &lt;a href=&#34;https://github.com/zacharysarah&#34;&gt;Zach Corleissen&lt;/a&gt; and the lead writers &lt;a href=&#34;https://github.com/kbhawkey&#34;&gt;Karen Bradshaw&lt;/a&gt;, &lt;a href=&#34;https://github.com/celestehorgan&#34;&gt;Celeste Horgan&lt;/a&gt;, &lt;a href=&#34;https://github.com/sftim&#34;&gt;Tim Bannister&lt;/a&gt; and &lt;a href=&#34;https://github.com/tengqm&#34;&gt;Qiming Teng&lt;/a&gt; who supervised me during all the season. They all have been very encouraging and gave me tons of great advice.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Dockershim Deprecation FAQ</title>
      <link>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</link>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</guid>
      <description>
        
        
        &lt;p&gt;This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
&lt;a href=&#34;https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/&#34;&gt;Don&#39;t Panic: Kubernetes and Docker&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;why-is-dockershim-being-deprecated&#34;&gt;Why is dockershim being deprecated?&lt;/h3&gt;
&lt;p&gt;Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn&#39;t currently implement CRI,
thus the problem.&lt;/p&gt;
&lt;p&gt;Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
&lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1985-remove-dockershim&#34;&gt;Dockershim Removal Kubernetes Enhancement Proposal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.&lt;/p&gt;
&lt;h3 id=&#34;can-i-still-use-docker-in-kubernetes-1-20&#34;&gt;Can I still use Docker in Kubernetes 1.20?&lt;/h3&gt;
&lt;p&gt;Yes, the only thing changing in 1.20 is a single warning log printed at &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/&#34;&gt;kubelet&lt;/a&gt;
startup if using Docker as the runtime.&lt;/p&gt;
&lt;h3 id=&#34;when-will-dockershim-be-removed&#34;&gt;When will dockershim be removed?&lt;/h3&gt;
&lt;p&gt;Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021. We will be working closely with vendors
and other ecosystem groups to ensure a smooth transition and will evaluate things
as the situation evolves.&lt;/p&gt;
&lt;h3 id=&#34;can-i-still-use-dockershim-after-it-is-removed-from-kubernetes&#34;&gt;Can I still use dockershim after it is removed from Kubernetes?&lt;/h3&gt;
&lt;p&gt;Update:
Mirantis and Docker have &lt;a href=&#34;https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/&#34;&gt;committed&lt;/a&gt; to maintaining the dockershim after
it is removed from Kubernetes.&lt;/p&gt;
&lt;h3 id=&#34;will-my-existing-docker-images-still-work&#34;&gt;Will my existing Docker images still work?&lt;/h3&gt;
&lt;p&gt;Yes, the images produced from &lt;code&gt;docker build&lt;/code&gt; will work with all CRI implementations.
All your existing images will still work exactly the same.&lt;/p&gt;
&lt;h3 id=&#34;what-about-private-images&#34;&gt;What about private images?&lt;/h3&gt;
&lt;p&gt;Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.&lt;/p&gt;
&lt;h3 id=&#34;are-docker-and-containers-the-same-thing&#34;&gt;Are Docker and containers the same thing?&lt;/h3&gt;
&lt;p&gt;Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.&lt;/p&gt;
&lt;h3 id=&#34;are-there-examples-of-folks-using-other-runtimes-in-production-today&#34;&gt;Are there examples of folks using other runtimes in production today?&lt;/h3&gt;
&lt;p&gt;All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.&lt;/p&gt;
&lt;p&gt;Additionally, the &lt;a href=&#34;https://kind.sigs.k8s.io/&#34;&gt;kind&lt;/a&gt; project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the &lt;a href=&#34;https://cri-o.io/&#34;&gt;CRI-O&lt;/a&gt; runtime in production since June 2019.&lt;/p&gt;
&lt;p&gt;For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation (&lt;a href=&#34;https://cncf.io&#34;&gt;CNCF&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/containerd/containerd/blob/master/ADOPTERS.md&#34;&gt;containerd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md&#34;&gt;CRI-O&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;people-keep-referencing-oci-what-is-that&#34;&gt;People keep referencing OCI, what is that?&lt;/h3&gt;
&lt;p&gt;OCI stands for the &lt;a href=&#34;https://opencontainers.org/about/overview/&#34;&gt;Open Container Initiative&lt;/a&gt;, which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of &lt;a href=&#34;https://github.com/opencontainers/runc&#34;&gt;runc&lt;/a&gt;, which is the underlying default runtime for both
&lt;a href=&#34;https://containerd.io/&#34;&gt;containerd&lt;/a&gt; and &lt;a href=&#34;https://cri-o.io/&#34;&gt;CRI-O&lt;/a&gt;. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.&lt;/p&gt;
&lt;h3 id=&#34;which-cri-implementation-should-i-use&#34;&gt;Which CRI implementation should I use?&lt;/h3&gt;
&lt;p&gt;That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the &lt;a href=&#34;https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category&#34;&gt;CNCF landscape&lt;/a&gt; in case another would be an
even better fit for your environment.&lt;/p&gt;
&lt;h3 id=&#34;what-should-i-look-out-for-when-changing-cri-implementations&#34;&gt;What should I look out for when changing CRI implementations?&lt;/h3&gt;
&lt;p&gt;While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logging configuration&lt;/li&gt;
&lt;li&gt;Runtime resource limitations&lt;/li&gt;
&lt;li&gt;Node provisioning scripts that call docker or use docker via it&#39;s control socket&lt;/li&gt;
&lt;li&gt;Kubectl plugins that require docker CLI or the control socket&lt;/li&gt;
&lt;li&gt;Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)&lt;/li&gt;
&lt;li&gt;Configuration of functionality like &lt;code&gt;registry-mirrors&lt;/code&gt; and insecure registries&lt;/li&gt;
&lt;li&gt;Other support scripts or daemons that expect Docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)&lt;/li&gt;
&lt;li&gt;GPUs or special hardware and how they integrate with your runtime and Kubernetes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your dockerd configuration, you’ll need to adapt that for your new container
runtime where possible.&lt;/p&gt;
&lt;p&gt;Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the &lt;a href=&#34;https://github.com/kubernetes-sigs/cri-tools&#34;&gt;&lt;code&gt;crictl&lt;/code&gt;&lt;/a&gt; tool as a drop-in replacement (see &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl&#34;&gt;mapping from docker cli to crictl&lt;/a&gt;) and for the
latter you can use newer container build options like &lt;a href=&#34;https://github.com/genuinetools/img&#34;&gt;img&lt;/a&gt;, &lt;a href=&#34;https://github.com/containers/buildah&#34;&gt;buildah&lt;/a&gt;,
&lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;kaniko&lt;/a&gt;, or &lt;a href=&#34;https://github.com/vmware-tanzu/buildkit-cli-for-kubectl&#34;&gt;buildkit-cli-for-kubectl&lt;/a&gt; that don’t require Docker.&lt;/p&gt;
&lt;p&gt;For containerd, you can start with their &lt;a href=&#34;https://github.com/containerd/cri/blob/master/docs/registry.md&#34;&gt;documentation&lt;/a&gt; to see what configuration
options are available as you migrate things over.&lt;/p&gt;
&lt;p&gt;For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/container-runtimes&#34;&gt;Container Runtimes&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-if-i-have-more-questions&#34;&gt;What if I have more questions?&lt;/h3&gt;
&lt;p&gt;If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: &lt;a href=&#34;https://discuss.kubernetes.io/&#34;&gt;https://discuss.kubernetes.io/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can also check out the excellent blog post
&lt;a href=&#34;https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m&#34;&gt;Wait, Docker is deprecated in Kubernetes now?&lt;/a&gt; a more in-depth technical
discussion of the changes.&lt;/p&gt;
&lt;h3 id=&#34;can-i-have-a-hug&#34;&gt;Can I have a hug?&lt;/h3&gt;
&lt;p&gt;Always and whenever you want!  🤗🤗&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Don&#39;t Panic: Kubernetes and Docker</title>
      <link>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas&lt;/p&gt;
&lt;p&gt;Kubernetes is &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation&#34;&gt;deprecating
Docker&lt;/a&gt;
as a container runtime after v1.20.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You do not need to panic. It’s not as dramatic as it sounds.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TL;DR Docker as an underlying runtime is being deprecated in favor of runtimes
that use the &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&#34;&gt;Container Runtime Interface (CRI)&lt;/a&gt;
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.&lt;/p&gt;
&lt;p&gt;If you’re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesn’t mean the death of Docker, and it doesn’t mean you can’t, or
shouldn’t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running &lt;code&gt;docker build&lt;/code&gt; can still run in your Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;If you’re using a managed Kubernetes service like GKE, EKS, or AKS (which &lt;a href=&#34;https://github.com/Azure/AKS/releases/tag/2020-11-16&#34;&gt;defaults to containerd&lt;/a&gt;) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.&lt;/p&gt;
&lt;p&gt;If you’re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).&lt;/p&gt;
&lt;h2 id=&#34;so-why-the-confusion-and-what-is-everyone-freaking-out-about&#34;&gt;So why the confusion and what is everyone freaking out about?&lt;/h2&gt;
&lt;p&gt;We’re talking about two different environments here, and that’s creating
confusion. Inside of your Kubernetes cluster, there’s a thing called a container
runtime that’s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.&lt;/p&gt;
&lt;p&gt;You see, the thing we call “Docker” isn’t actually one thing—it’s an entire
tech stack, and one part of it is a thing called “containerd,” which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while we’re doing development work, but those UX enhancements aren’t necessary
for Kubernetes, because it isn’t a human.&lt;/p&gt;
&lt;p&gt;As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. That’s not great, because it gives us another thing that has to
be maintained and can possibly break. What’s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?&lt;/p&gt;
&lt;p&gt;Docker isn’t compliant with CRI, the &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&#34;&gt;Container Runtime Interface&lt;/a&gt;.
If it were, we wouldn’t need the shim, and this wouldn’t be a thing. But it’s
not the end of the world, and you don’t need to panic—you just need to change
your container runtime from Docker to another supported container runtime.&lt;/p&gt;
&lt;p&gt;One thing to note: If you are relying on the underlying docker socket
(&lt;code&gt;/var/run/docker.sock&lt;/code&gt;) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
&lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;kaniko&lt;/a&gt;,
&lt;a href=&#34;https://github.com/genuinetools/img&#34;&gt;img&lt;/a&gt;, and
&lt;a href=&#34;https://github.com/containers/buildah&#34;&gt;buildah&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-does-this-change-mean-for-developers-though-do-we-still-write-dockerfiles-do-we-still-build-things-with-docker&#34;&gt;What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?&lt;/h2&gt;
&lt;p&gt;This change addresses a different environment than most folks use to interact
with Docker. The Docker installation you’re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. It’s confusing, we understand.
As a developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isn’t really a
Docker-specific image—it’s an OCI (&lt;a href=&#34;https://opencontainers.org/&#34;&gt;Open Container Initiative&lt;/a&gt;) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both &lt;a href=&#34;https://containerd.io/&#34;&gt;containerd&lt;/a&gt; and
&lt;a href=&#34;https://cri-o.io/&#34;&gt;CRI-O&lt;/a&gt; know how to pull those images and run them. This is
why we have a standard for what containers should look like.&lt;/p&gt;
&lt;p&gt;So, this change is coming. It’s going to cause issues for some, but it isn’t
catastrophic, and generally it’s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, it’s going to make things easier. If this is still confusing
for you, that’s okay—there’s a lot going on here; Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. We hope
this has answered most of your questions and soothed some anxieties! ❤️&lt;/p&gt;
&lt;p&gt;Looking for more answers? Check out our accompanying &lt;a href=&#34;https://kubernetes.io/blog/2020/12/02/dockershim-faq/&#34;&gt;Dockershim Deprecation FAQ&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Cloud native security for your clusters</title>
      <link>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/pudijoglekar&#34;&gt;Pushkar Joglekar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over the last few years a small, security focused community has been working diligently to deepen our understanding of security, given the evolving cloud native infrastructure and corresponding iterative deployment practices. To enable sharing of this knowledge with the rest of the community, members of &lt;a href=&#34;https://github.com/cncf/sig-security&#34;&gt;CNCF SIG Security&lt;/a&gt; (a group which reports into &lt;a href=&#34;https://github.com/cncf/toc#sigs&#34;&gt;CNCF TOC&lt;/a&gt; and who are friends with &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-security&#34;&gt;Kubernetes SIG Security&lt;/a&gt;) led by Emily Fox, collaborated on a whitepaper outlining holistic cloud native security concerns and best practices. After over 1200 comments, changes, and discussions from 35 members across the world, we are proud to share &lt;a href=&#34;https://www.cncf.io/blog/2020/11/18/announcing-the-cloud-native-security-white-paper&#34;&gt;cloud native security whitepaper v1.0&lt;/a&gt; that serves as essential reading for security leadership in enterprises, financial and healthcare industries, academia, government, and non-profit organizations.&lt;/p&gt;
&lt;p&gt;The paper attempts to &lt;em&gt;not&lt;/em&gt; focus on any specific &lt;a href=&#34;https://www.cncf.io/projects/&#34;&gt;cloud native project&lt;/a&gt;. Instead, the intent is to model and inject security into four logical phases of cloud native application lifecycle: &lt;em&gt;Develop, Distribute, Deploy, and Runtime&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Cloud native application lifecycle phases&#34;
src=&#34;cloud-native-app-lifecycle-phases.svg&#34;
style=&#34;width:60em;max-width:100%;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-native-security-controls&#34;&gt;Kubernetes native security controls&lt;/h2&gt;
&lt;p&gt;When using Kubernetes as a workload orchestrator, some of the security controls this version of the whitepaper recommends are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34;&gt;Pod Security Policies&lt;/a&gt;: Implement a single source of truth for “least privilege” workloads across the entire cluster&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits&#34;&gt;Resource requests and limits&lt;/a&gt;: Apply requests (soft constraint) and limits (hard constraint) for shared resources such as memory and CPU&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&#34;&gt;Audit log analysis&lt;/a&gt;: Enable Kubernetes API auditing and filtering for security relevant events&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/&#34;&gt;Control plane authentication and certificate root of trust&lt;/a&gt;: Enable mutual TLS authentication with a trusted CA for communication within the cluster&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34;&gt;Secrets management&lt;/a&gt;: Integrate with a built-in or external secrets store&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cloud-native-complementary-security-controls&#34;&gt;Cloud native complementary security controls&lt;/h2&gt;
&lt;p&gt;Kubernetes has direct involvement in the &lt;em&gt;deploy&lt;/em&gt; phase and to a lesser extent in the &lt;em&gt;runtime&lt;/em&gt; phase. Ensuring the artifacts are securely &lt;em&gt;developed&lt;/em&gt; and &lt;em&gt;distributed&lt;/em&gt; is necessary for, enabling workloads in Kubernetes to run “secure by default”. Throughout all phases of the Cloud native application life cycle, several complementary security controls exist for Kubernetes orchestrated workloads, which includes but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop:
&lt;ul&gt;
&lt;li&gt;Image signing and verification&lt;/li&gt;
&lt;li&gt;Image vulnerability scanners&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Distribute:
&lt;ul&gt;
&lt;li&gt;Pre-deployment checks for detecting excessive privileges&lt;/li&gt;
&lt;li&gt;Enabling observability and logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deploy:
&lt;ul&gt;
&lt;li&gt;Using a service mesh for workload authentication and authorization&lt;/li&gt;
&lt;li&gt;Enforcing “default deny” network policies for inter-workload communication via &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/&#34;&gt;network plugins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Runtime:
&lt;ul&gt;
&lt;li&gt;Deploying security monitoring agents for workloads&lt;/li&gt;
&lt;li&gt;Isolating applications that run on the same node using SELinux, AppArmor, etc.&lt;/li&gt;
&lt;li&gt;Scanning configuration against recognized secure baselines for node, workload and orchestrator&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;understand-first-secure-next&#34;&gt;Understand first, secure next&lt;/h2&gt;
&lt;p&gt;The cloud native way, including containers, provides great security benefits for its users: immutability, modularity, faster upgrades and consistent state across the environment. Realizing this fundamental change in “the way things are done”, motivates us to look at security with a cloud native lens. One of the things that was evident for all the authors of the paper was the fact that it’s tough to make smarter decisions on how and what to secure in a cloud native ecosystem if you do not understand the tools, patterns, and frameworks at hand (in addition to knowing your own critical assets). Hence, for all the security practitioners out there who want to be partners rather than a gatekeeper for your friends in Operations, Product Development, and Compliance, let’s make an attempt to &lt;em&gt;learn more so we can secure better&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We recommend following this &lt;strong&gt;7 step R.U.N.T.I.M.E. path&lt;/strong&gt; to get started on cloud native security:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;R&lt;/b&gt;ead the paper and any linked material in it&lt;/li&gt;
&lt;li&gt;&lt;b&gt;U&lt;/b&gt;nderstand challenges and constraints for your environment&lt;/li&gt;
&lt;li&gt;&lt;b&gt;N&lt;/b&gt;ote the content and controls that apply to your environment&lt;/li&gt;
&lt;li&gt;&lt;b&gt;T&lt;/b&gt;alk about your observations with your peers&lt;/li&gt;
&lt;li&gt;&lt;b&gt;I&lt;/b&gt;nvolve your leadership and ask for help&lt;/li&gt;
&lt;li&gt;&lt;b&gt;M&lt;/b&gt;ake a risk profile based on existing and missing security controls&lt;/li&gt;
&lt;li&gt;&lt;b&gt;E&lt;/b&gt;xpend time, money, and resources that improve security posture and reduce risk where appropriate.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Huge shout out to &lt;em&gt;Emily Fox, Tim Bannister (The Scale Factory), Chase Pettet (Mirantis), and Wayne Haber (GitLab)&lt;/em&gt; for contributing with their wonderful suggestions for this blog post.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Remembering Dan Kohn</title>
      <link>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: The Kubernetes Steering Committee&lt;/p&gt;
&lt;p&gt;Dan Kohn was instrumental in getting Kubernetes and CNCF community to where it is today. He shared our values, motivations, enthusiasm, community spirit, and helped the Kubernetes community to become the best that it could be. Dan loved getting people together to solve problems big and small. He enabled people to grow their individual scope in the community which often helped launch their career in open source software.&lt;/p&gt;
&lt;p&gt;Dan built a coalition around the nascent Kubernetes project and turned that into a cornerstone to build the larger cloud native space. He loved challenges, especially ones where the payoff was great like building worldwide communities, spreading the love of open source, and helping diverse, underprivileged communities and students to get a head start in technology.&lt;/p&gt;
&lt;p&gt;Our heart goes out to his family. Thank you, Dan, for bringing your boys to events in India and elsewhere as we got to know how great you were as a father. Dan, your thoughts and ideas will help us make progress in our journey as a community. Thank you for your life&#39;s work!&lt;/p&gt;
&lt;p&gt;If Dan has made an impact on you in some way, please consider adding a memory of him in his &lt;a href=&#34;https://github.com/cncf/memorials/blob/master/dan-kohn.md&#34;&gt;CNCF memorial&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Announcing the 2020 Steering Committee Election Results</title>
      <link>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Kaslin Fields&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/events/elections/2020&#34;&gt;2020 Steering Committee Election&lt;/a&gt; is now complete. In 2019, the committee arrived at its final allocation of 7 seats, 3 of which were up for election in 2020. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.&lt;/p&gt;
&lt;p&gt;This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/charter.md&#34;&gt;charter&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Davanum Srinivas (&lt;a href=&#34;https://github.com/dims&#34;&gt;@dims&lt;/a&gt;), VMware&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jordan Liggitt (&lt;a href=&#34;https://github.com/liggitt&#34;&gt;@liggitt&lt;/a&gt;), Google&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bob Killen (&lt;a href=&#34;https://github.com/mrbobbytables&#34;&gt;@mrbobbytables&lt;/a&gt;), Google&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They join continuing members Christoph Blecker (&lt;a href=&#34;https://github.com/cblecker&#34;&gt;@cblecker&lt;/a&gt;), Red Hat; Derek Carr (&lt;a href=&#34;https://github.com/derekwaynecarr&#34;&gt;@derekwaynecarr&lt;/a&gt;), Red Hat; Nikhita Raghunath (&lt;a href=&#34;https://github.com/nikhita&#34;&gt;@nikhita&lt;/a&gt;), VMware; and Paris Pittman (&lt;a href=&#34;https://github.com/parispittman&#34;&gt;@parispittman&lt;/a&gt;), Apple. Davanum Srinivas is returning for his second term on the committee.&lt;/p&gt;
&lt;h2 id=&#34;big-thanks&#34;&gt;Big Thanks!&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Thank you and congratulations on a successful election to this round’s election officers:
&lt;ul&gt;
&lt;li&gt;Jaice Singer DuMars (&lt;a href=&#34;https://github.com/jdumars&#34;&gt;@jdumars&lt;/a&gt;), Apple&lt;/li&gt;
&lt;li&gt;Ihor Dvoretskyi (&lt;a href=&#34;https://github.com/idvoretskyi&#34;&gt;@idvoretskyi&lt;/a&gt;), CNCF&lt;/li&gt;
&lt;li&gt;Josh Berkus (&lt;a href=&#34;https://github.com/jberkus&#34;&gt;@jberkus&lt;/a&gt;), Red Hat&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:
&lt;ul&gt;
&lt;li&gt;Aaron Crickenberger (&lt;a href=&#34;https://github.com/spiffxp&#34;&gt;@spiffxp&lt;/a&gt;), Google&lt;/li&gt;
&lt;li&gt;and Lachlan Evenson(&lt;a href=&#34;https://github.com/lachie8e&#34;&gt;@lachie8e)&lt;/a&gt;), Microsoft&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;And thank you to all the candidates who came forward to run for election. As &lt;a href=&#34;https://twitter.com/castrojo/status/1315718627639820288?s=20&#34;&gt;Jorge Castro put it&lt;/a&gt;: we are spoiled with capable, kind, and selfless volunteers who put the needs of the project first.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;get-involved-with-the-steering-committee&#34;&gt;Get Involved with the Steering Committee&lt;/h2&gt;
&lt;p&gt;This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee &lt;a href=&#34;https://github.com/kubernetes/steering/projects/1&#34;&gt;backlog items&lt;/a&gt; and weigh in by filing an issue or creating a PR against their &lt;a href=&#34;https://github.com/kubernetes/steering&#34;&gt;repo&lt;/a&gt;. They have an open meeting on &lt;a href=&#34;https://github.com/kubernetes/steering&#34;&gt;the first Monday of the month at 6pm UTC&lt;/a&gt; and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list &lt;a href=&#34;mailto:steering@kubernetes.io&#34;&gt;steering@kubernetes.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can see what the Steering Committee meetings are all about by watching past meetings on the &lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM&#34;&gt;YouTube Playlist&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;This post was written by the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing&#34;&gt;Upstream Marketing Working Group&lt;/a&gt;. If you want to write stories about the Kubernetes community, learn more about us.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Contributing to the Development Guide</title>
      <link>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</guid>
      <description>
        
        
        &lt;p&gt;When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that&#39;s certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of &amp;quot;client.&amp;quot; I just didn&#39;t know what to expect when Google asked my compatriots and me at
&lt;a href=&#34;https://lionswaycontent.com/&#34;&gt;Lion&#39;s Way&lt;/a&gt; to make much-needed updates to the Kubernetes Development Guide.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article originally appeared on the &lt;a href=&#34;https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/&#34;&gt;Kubernetes Contributor Community blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-delights-of-working-with-a-community&#34;&gt;The Delights of Working With a Community&lt;/h2&gt;
&lt;p&gt;As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn&#39;t be present when working on open source documentation, but I couldn&#39;t
predict how it would change my relationship with the project.&lt;/p&gt;
&lt;p&gt;One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they&#39;re looking for. It can be stressful -- which is why I&#39;m so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor &lt;a href=&#34;https://twitter.com/JoelByronBarker&#34;&gt;Joel&lt;/a&gt;
handles most of the client contact.&lt;/p&gt;
&lt;p&gt;I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.&lt;/p&gt;
&lt;p&gt;&amp;quot;How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?&amp;quot; These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the &lt;code&gt;#sig-contribex&lt;/code&gt; channel on the Kubernetes
Slack and announced that I would be working on the
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/development.md&#34;&gt;Development Guide&lt;/a&gt;.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 810px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg&#34; width=&#34;800&#34; height=&#34;450&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&#34;The Kubernetes Code of Conduct is in effect, so please be excellent to each other.&#34; &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;My fears were unfounded. Immediately, I felt welcome. I like to think this isn&#39;t just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the &lt;a href=&#34;https://www.kubernetes.dev/resources/code-of-conduct/&#34;&gt;Kubernetes Code of Conduct&lt;/a&gt; was in
effect, and that we should, like Bill and Ted, be excellent to each other.&lt;/p&gt;
&lt;h2 id=&#34;this-doesn-t-mean-it-s-all-easy&#34;&gt;This Doesn&#39;t Mean It&#39;s All Easy&lt;/h2&gt;
&lt;p&gt;The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
&lt;a href=&#34;https://github.com/kubernetes/community&#34;&gt;Community repo&lt;/a&gt;: 267 additions and 88 deletions.&lt;/p&gt;
&lt;p&gt;The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, &lt;a href=&#34;https://github.com/kubernetes/community/pull/5003&#34;&gt;it was successful&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since Kubernetes is a pretty fast-moving project, and since developers typically aren&#39;t really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the &lt;a href=&#34;https://github.com/amwat&#34;&gt;labyrinthine mind of
a brilliant engineer&lt;/a&gt;, and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.&lt;/p&gt;
&lt;p&gt;This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
&lt;a href=&#34;https://github.com/kubernetes-sigs/kubetest2&#34;&gt;&lt;code&gt;kubetest2&lt;/code&gt; framework&lt;/a&gt; to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
&lt;a href=&#34;https://github.com/kubernetes/community/pull/5045&#34;&gt;completed pull request&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;nobody-is-the-boss-and-everybody-gives-feedback&#34;&gt;Nobody Is the Boss, and Everybody Gives Feedback&lt;/h2&gt;
&lt;p&gt;But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was &lt;em&gt;enjoyable&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.&lt;/p&gt;
&lt;p&gt;In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.&lt;/p&gt;
&lt;p&gt;Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I&#39;ve made in this vast open source community over the past
few months.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: GSoC 2020 - Building operators for cluster addons</title>
      <link>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</link>
      <pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Somtochi Onyekwere&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://summerofcode.withgoogle.com/&#34;&gt;Google Summer of Code&lt;/a&gt; is a global program that is geared towards introducing students to open source. Students are matched with open-source organizations to work with them for three months during the summer.&lt;/p&gt;
&lt;p&gt;My name is Somtochi Onyekwere from the Federal University of Technology, Owerri (Nigeria) and this year, I was given the opportunity to work with Kubernetes (under the CNCF organization) and this led to an amazing summer spent learning, contributing and interacting with the community.&lt;/p&gt;
&lt;p&gt;Specifically, I worked on the &lt;em&gt;Cluster Addons: Package all the things!&lt;/em&gt; project. The project focused on building operators for better management of various cluster addons, extending the tooling for building these operators and making the creation of these operators a smooth process.&lt;/p&gt;
&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;
&lt;p&gt;Kubernetes has progressed greatly in the past few years with a flourishing community and a large number of contributors. The codebase is gradually moving away from the monolith structure where all the code resides in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes&#34;&gt;kubernetes/kubernetes&lt;/a&gt; repository to being split into multiple sub-projects. Part of the focus of cluster-addons is to make some of these sub-projects work together in an easy to assemble, self-monitoring, self-healing and Kubernetes-native way. It enables them to work seamlessly without human intervention.&lt;/p&gt;
&lt;p&gt;The community is exploring the use of operators as a mechanism to monitor various resources in the cluster and properly manage these resources. In addition to this, it provides self-healing and it is a kubernetes-native pattern that can encode how best these addons work and manage them properly.&lt;/p&gt;
&lt;p&gt;What are cluster addons? Cluster addons are a collection of resources (like Services and deployment) that are used to give a Kubernetes cluster additional functionalities. They range from things as simple as the Kubernetes dashboards (for visualization) to more complex ones like Calico (for networking). These addons are essential to different applications running in the cluster and the cluster itself. The addon operator provides a nicer way of managing these addons and understanding the health and status of the various resources that comprise the addon. You can get a deeper overview in this &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/#addons&#34;&gt;article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Operators are custom controllers with custom resource definitions that encode application-specific knowledge and are used for managing complex stateful applications. It is a widely accepted pattern. Managing addons via operators, with these operators encoding knowledge of how best the addons work, introduces a lot of advantages while setting standards that will be easy to follow and scale. This &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator&#34;&gt;article&lt;/a&gt; does a good job of explaining operators.&lt;/p&gt;
&lt;p&gt;The addon operators can solve a lot of problems, but they have their challenges. Those under the &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons&#34;&gt;cluster-addons project&lt;/a&gt; had missing pieces and were still a proof of concept. Generating the RBAC configuration for the operators was a pain and sometimes the operators were given too much privilege. The operators weren’t very extensible as it only pulled manifests from local filesystems or HTTP(s) servers and a lot of simple addons were generating the same code.
I spent the summer working on these issues, looking at them with fresh eyes and coming up with solutions for both the known and unknown issues.&lt;/p&gt;
&lt;h1 id=&#34;various-additions-to-kubebuilder-declarative-pattern&#34;&gt;Various additions to kubebuilder-declarative-pattern&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern&#34;&gt;kubebuilder-declarative-pattern&lt;/a&gt; (from here on referred to as KDP) repo is an extra layer of addon specific tooling on top of the &lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder&#34;&gt;kubebuilder&lt;/a&gt; SDK that is enabled by passing the experimental &lt;code&gt;--pattern=addon&lt;/code&gt; flag to &lt;code&gt;kubebuilder create&lt;/code&gt; command. Together, they create the base code for the addon operator. During the internship, I worked on a couple of features in KDP and cluster-addons.&lt;/p&gt;
&lt;h2 id=&#34;operator-version-checking&#34;&gt;Operator version checking&lt;/h2&gt;
&lt;p&gt;Enabling version checks for operators helped in making upgrades/downgrades safer to different versions of the addon, even though the operator had complex logic. It is a way of matching the version of an addon to the version of the operator that knows how to manage it well. Most addons have different versions and these versions might need to be managed differently. This feature checks the custom resource for the &lt;code&gt;addons.k8s.io/min-operator-version&lt;/code&gt; annotation which states the minimum operator version that is needed to manage the version against the version of the operator. If the operator version is below the minimum version required, the operator pauses with an error telling the user that the version of the operator is too low. This helps to ensure that the correct operator is being used for the addon.&lt;/p&gt;
&lt;h2 id=&#34;git-repository-for-storing-the-manifests&#34;&gt;Git repository for storing the manifests&lt;/h2&gt;
&lt;p&gt;Previously, there was support for only local file directories and HTTPS repositories for storing manifests. Giving creators of addon operators the ability to store manifest in GitHub repository enables faster development and version control.  When starting the controller, you can pass a flag to specify the location of your channels directory. The channels directory contains the manifests for different versions, the controller pulls the manifest from this directory and applies it to the cluster. During the internship period, I extended it to include Git repositories.&lt;/p&gt;
&lt;h2 id=&#34;annotations-to-temporarily-disable-reconciliation&#34;&gt;Annotations to temporarily disable reconciliation&lt;/h2&gt;
&lt;p&gt;The reconciliation loop that ensures that the desired state matches the actual state prevents modification of objects in the cluster. This makes it hard to experiment or investigate what might be wrong in the cluster as any changes made are promptly reverted. I resolved this by allowing users to place an &lt;code&gt;addons.k8s.io/ignore&lt;/code&gt; annotation on the resource that they don’t want the controller to reconcile. The controller checks for this annotation and doesn’t reconcile that object. To resume reconciliation, the annotation can be removed from the resource.&lt;/p&gt;
&lt;h2 id=&#34;unstructured-support-in-kubebuilder-declarative-pattern&#34;&gt;Unstructured support in kubebuilder-declarative-pattern&lt;/h2&gt;
&lt;p&gt;One of the operators that I worked on is a generic controller that could manage more than one cluster addon that did not require extra configuration. To do this, the operator couldn’t use a particular type and needed the kubebuilder-declarative-repo to support using the &lt;a href=&#34;https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured#Unstructured&#34;&gt;unstructured.Unstructured&lt;/a&gt; type. There were various functions in the kubebuilder-declarative-pattern that couldn’t handle this type and returned an error if the object passed in was not of type &lt;code&gt;addonsv1alpha1.CommonObject&lt;/code&gt;. The functions were modified to handle both &lt;code&gt;unstructured.Unstructured&lt;/code&gt; and &lt;code&gt;addonsv1alpha.CommonObject&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;tools-and-cli-programs&#34;&gt;Tools and CLI programs&lt;/h1&gt;
&lt;p&gt;There were also some command-line programs I wrote that could be used to make working with addon operators easier. Most of them have uses outside the addon operators as they try to solve a specific problem that could surface anywhere while working with Kubernetes. I encourage you to &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools&#34;&gt;check them out&lt;/a&gt; when you have the chance!&lt;/p&gt;
&lt;h2 id=&#34;rbac-generator&#34;&gt;RBAC Generator&lt;/h2&gt;
&lt;p&gt;One of the biggest concerns with the operator was RBAC. You had to manually look through the manifest and add the RBAC rule for each resource as it needs to have RBAC permissions to create, get, update and delete the resources in the manifest when running in-cluster. Building the &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen&#34;&gt;RBAC generator&lt;/a&gt; automated the process of writing the RBAC roles and role bindings. The function of the RBAC generator is simple. It accepts the file name of the manifest as a flag. Then, it parses the manifest and gets the API group and resource name of the resources and adds it to a role. It outputs the role and role binding to stdout or a file if the &lt;code&gt;--out&lt;/code&gt; flag is parsed.&lt;/p&gt;
&lt;p&gt;Additionally, the tool enables you to split the RBAC by separating the cluster roles in the manifest. This lessened the security concern of an operator being over-privileged as it needed to have all the permissions that the clusterrole has. If you want to apply the clusterrole yourself and not give the operator these permissions, you can pass in a &lt;code&gt;--supervisory&lt;/code&gt; boolean flag so that the generator does not add these permissions to the role. The CLI program resides &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kubectl-ownerref&#34;&gt;Kubectl Ownerref&lt;/h2&gt;
&lt;p&gt;It is hard to find out at a glance which objects were created by an addon custom resource. This kubectl plugin alleviates that pain by displaying all the objects in the cluster that a resource has ownerrefs on. You simply pass the kind and the name of the resource as arguments to the program and it checks the cluster for the objects and gives the kind, name, the namespace of such an object. It could be useful to get a general overview of all the objects that the controller is reconciling by passing in the name and kind of custom resource. The CLI program resides &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools/kubectl-ownerref&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;addon-operators&#34;&gt;Addon Operators&lt;/h1&gt;
&lt;p&gt;To fully understand addons operators and make changes to how they are being created, you have to try creating and using them. Part of the summer was spent building operators for some popular addons like the Kubernetes dashboard, flannel, NodeLocalDNS and so on. Please check the &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons&#34;&gt;cluster-addons&lt;/a&gt; repository for the different addon operators. In this section, I will just highlight one that is a little different from the others.&lt;/p&gt;
&lt;h2 id=&#34;generic-controller&#34;&gt;Generic Controller&lt;/h2&gt;
&lt;p&gt;The generic controller can be shared between addons that don’t require much configuration. This minimizes resource consumption on the cluster as it reduces the number of controllers that need to be run. Also instead of building your own operator, you can just use the generic controller and whenever you feel that your needs have grown and you need a more complex operator, you can always scaffold the code with kubebuilder and continue from where the generic operator stopped. To use the generic controller, you can generate the CustomResourceDefinition(CRD) using this tool (&lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md&#34;&gt;generic-addon&lt;/a&gt;). You pass in the kind, group, and the location of your channels directory (it could be a Git repository too!). The tool generates the - CRD, RBAC manifest and two custom resources for you.&lt;/p&gt;
&lt;p&gt;The process is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create the Generic CRD&lt;/li&gt;
&lt;li&gt;Generate all the manifests needed with the &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md&#34;&gt;&lt;code&gt;generic-addon tool&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tool creates:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The CRD for your addon&lt;/li&gt;
&lt;li&gt;The RBAC rules for the CustomResourceDefinitions&lt;/li&gt;
&lt;li&gt;The RBAC rules for applying the manifests&lt;/li&gt;
&lt;li&gt;The custom resource for your addon&lt;/li&gt;
&lt;li&gt;A Generic custom resource&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Generic custom resource looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;addons.x-k8s.io/v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Generic&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; 	&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;generic-sample&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;objectKind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NodeLocalDNS&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;version&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1alpha1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;group&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;addons.x-k8s.io&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;channel&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;../nodelocaldns/channels&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Apply these manifests but ensure to apply the CRD before the CR.
Then, run the Generic controller, either on your machine or in-cluster.&lt;/p&gt;
&lt;p&gt;If you are interested in building an operator, Please check out &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;relevant-links&#34;&gt;Relevant Links&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SomtochiAma/gsoc-2020-meta-k8s&#34;&gt;Detailed breakdown of work done during the internship&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/addons/0035-20190128-addons-via-operators.md&#34;&gt;Addon Operator (KEP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/issues/39&#34;&gt;Original GSoC Issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SomtochiAma/gsoc-2020-meta-k8s/blob/master/GSoC%202020%20PROPOSAL%20-%20PACKAGE%20ALL%20THINGS.pdf&#34;&gt;Proposal Submitted for GSoC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/commits?author=SomtochiAma&#34;&gt;All commits to kubernetes-sigs/cluster-addons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern/commits?author=SomtochiAma&#34;&gt;All commits to kubernetes-sigs/kubebuidler-declarative-pattern&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;further-work&#34;&gt;Further Work&lt;/h1&gt;
&lt;p&gt;A lot of work was definitely done on the cluster addons during the GSoC period. But we need more people building operators and using them in the cluster. We need wider adoption in the community. Build operators for your favourite addons and tell us how it went and if you had any issues. Check out this &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md&#34;&gt;README.md&lt;/a&gt; to get started.&lt;/p&gt;
&lt;h1 id=&#34;appreciation&#34;&gt;Appreciation&lt;/h1&gt;
&lt;p&gt;I really want to appreciate my mentors &lt;a href=&#34;https://github.com/justinsb&#34;&gt;Justin Santa Barbara&lt;/a&gt; (Google) and &lt;a href=&#34;https://github.com/stealthybox&#34;&gt;Leigh Capili&lt;/a&gt; (Weaveworks). My internship was awesome because they were awesome. They set a golden standard for what mentorship should be. They were accessible and always available to clear any confusion. I think what I liked best was that they didn’t just dish out tasks, instead, we had open discussions about what was wrong and what could be improved. They are really the best and I hope I get to work with them again!
Also, I want to say a huge thanks to &lt;a href=&#34;https://github.com/neolit123&#34;&gt;Lubomir I. Ivanov&lt;/a&gt; for reviewing this blog post!&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;So far I have learnt a lot about Go, the internals of Kubernetes, and operators. I want to conclude by encouraging people to contribute to open-source (especially Kubernetes :)) regardless of your level of experience. It has been a well-rounded experience for me and I have come to love the community. It is a great initiative and it is a great way to learn and meet awesome people. Special shoutout to Google for organizing this program.&lt;/p&gt;
&lt;p&gt;If you are interested in cluster addons and finding out more on addon operators, you are welcome to join our slack channel on the Kubernetes &lt;a href=&#34;https://kubernetes.slack.com/messages/cluster-addons&#34;&gt;#cluster-addons&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://twitter.com/SomtochiAma&#34;&gt;Somtochi Onyekwere&lt;/a&gt; is a software engineer that loves contributing to open-source and exploring cloud native solutions.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing Structured Logs</title>
      <link>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Marek Siarkowicz (Google), Nathan Beach (Google)&lt;/p&gt;
&lt;p&gt;Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.&lt;/p&gt;
&lt;p&gt;In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.&lt;/p&gt;
&lt;p&gt;To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those &amp;quot;key&amp;quot;=&amp;quot;value&amp;quot; pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the &lt;code&gt;--logging-format=json&lt;/code&gt; flag.&lt;/p&gt;
&lt;h2 id=&#34;using-structured-logs&#34;&gt;Using Structured Logs&lt;/h2&gt;
&lt;p&gt;We&#39;ve added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-golang&#34; data-lang=&#34;golang&#34;&gt;klog.&lt;span style=&#34;color:#00a000&#34;&gt;InfoS&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Pod status updated&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;pod&amp;#34;&lt;/span&gt;, klog.&lt;span style=&#34;color:#00a000&#34;&gt;KObj&lt;/span&gt;(pod), &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;status&amp;#34;&lt;/span&gt;, status)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;will result in this log:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;I1025 00:15:15.525108       1 controller_utils.go:116] &amp;quot;Pod status updated&amp;quot; pod=&amp;quot;kube-system/kubedns&amp;quot; status=&amp;quot;ready&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or, if the --logging-format=json flag is set, it will result in this output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1580306777.04728&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;msg&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Pod status updated&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;pod&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;coredns&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;namespace&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;kube-system&amp;#34;&lt;/span&gt;
  },
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;status&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ready&amp;#34;&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.&lt;/p&gt;
&lt;p&gt;With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.&lt;/p&gt;
&lt;p&gt;Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.&lt;/p&gt;
&lt;p&gt;Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.&lt;/p&gt;
&lt;p&gt;Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.&lt;/p&gt;
&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md&#34;&gt;migrate existing log calls to use structured logs&lt;/a&gt;. It&#39;s a great and easy way to make your first contribution to Kubernetes!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Warning: Helpful Warnings Ahead</title>
      <link>https://kubernetes.io/blog/2020/09/03/warnings/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/blog/2020/09/03/warnings/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jordan Liggitt (Google)&lt;/p&gt;
&lt;p&gt;As Kubernetes maintainers, we&#39;re always looking for ways to improve usability while preserving compatibility.
As we develop features, triage bugs, and answer support questions, we accumulate information that would be helpful for Kubernetes users to know.
In the past, sharing that information was limited to out-of-band methods like release notes, announcement emails, documentation, and blog posts.
Unless someone knew to seek out that information and managed to find it, they would not benefit from it.&lt;/p&gt;
&lt;p&gt;In Kubernetes v1.19, we added a feature that allows the Kubernetes API server to
&lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1693-warnings&#34;&gt;send warnings to API clients&lt;/a&gt;.
The warning is sent using a &lt;a href=&#34;https://tools.ietf.org/html/rfc7234#section-5.5&#34;&gt;standard &lt;code&gt;Warning&lt;/code&gt; response header&lt;/a&gt;,
so it does not change the status code or response body in any way.
This allows the server to send warnings easily readable by any API client, while remaining compatible with previous client versions.&lt;/p&gt;
&lt;p&gt;Warnings are surfaced by &lt;code&gt;kubectl&lt;/code&gt; v1.19+ in &lt;code&gt;stderr&lt;/code&gt; output, and by the &lt;code&gt;k8s.io/client-go&lt;/code&gt; client library v0.19.0+ in log output.
The &lt;code&gt;k8s.io/client-go&lt;/code&gt; behavior can be &lt;a href=&#34;#customize-client-handling&#34;&gt;overridden per-process or per-client&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;deprecation-warnings&#34;&gt;Deprecation Warnings&lt;/h2&gt;
&lt;p&gt;The first way we are using this new capability is to send warnings for use of deprecated APIs.&lt;/p&gt;
&lt;p&gt;Kubernetes is a &lt;a href=&#34;https://www.cncf.io/cncf-kubernetes-project-journey/#development-velocity&#34;&gt;big, fast-moving project&lt;/a&gt;.
Keeping up with the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#changelog-since-v1180&#34;&gt;changes&lt;/a&gt;
in each release can be daunting, even for people who work on the project full-time. One important type of change is API deprecations.
As APIs in Kubernetes graduate to GA versions, pre-release API versions are deprecated and eventually removed.&lt;/p&gt;
&lt;p&gt;Even though there is an &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34;&gt;extended deprecation period&lt;/a&gt;,
and deprecations are &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecation&#34;&gt;included in release notes&lt;/a&gt;,
they can still be hard to track. During the deprecation period, the pre-release API remains functional,
allowing several releases to transition to the stable API version. However, we have found that users often don&#39;t even realize
they are depending on a deprecated API version until they upgrade to the release that stops serving it.&lt;/p&gt;
&lt;p&gt;Starting in v1.19, whenever a request is made to a deprecated REST API, a warning is returned along with the API response.
This warning includes details about the release in which the API will no longer be available, and the replacement API version.&lt;/p&gt;
&lt;p&gt;Because the warning originates at the server, and is intercepted at the client level, it works for all kubectl commands,
including high-level commands like &lt;code&gt;kubectl apply&lt;/code&gt;, and low-level commands like &lt;code&gt;kubectl get --raw&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;kubectl applying a manifest file, then displaying a warning message &#39;networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress&#39;.&#34;
src=&#34;kubectl-warnings.png&#34;
style=&#34;width:637px;max-width:100%;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This helps people affected by the deprecation to know the request they are making is deprecated,
how long they have to address the issue, and what API they should use instead.
This is especially helpful when the user is applying a manifest they didn&#39;t create,
so they have time to reach out to the authors to ask for an updated version.&lt;/p&gt;
&lt;p&gt;We also realized that the person &lt;em&gt;using&lt;/em&gt; a deprecated API is often not the same person responsible for upgrading the cluster,
so we added two administrator-facing tools to help track use of deprecated APIs and determine when upgrades are safe.&lt;/p&gt;
&lt;h3 id=&#34;metrics&#34;&gt;Metrics&lt;/h3&gt;
&lt;p&gt;Starting in Kubernetes v1.19, when a request is made to a deprecated REST API endpoint,
an &lt;code&gt;apiserver_requested_deprecated_apis&lt;/code&gt; gauge metric is set to &lt;code&gt;1&lt;/code&gt; in the kube-apiserver process.
This metric has labels for the API &lt;code&gt;group&lt;/code&gt;, &lt;code&gt;version&lt;/code&gt;, &lt;code&gt;resource&lt;/code&gt;, and &lt;code&gt;subresource&lt;/code&gt;,
and a &lt;code&gt;removed_version&lt;/code&gt; label that indicates the Kubernetes release in which the API will no longer be served.&lt;/p&gt;
&lt;p&gt;This is an example query using &lt;code&gt;kubectl&lt;/code&gt;, &lt;a href=&#34;https://github.com/prometheus/prom2json&#34;&gt;prom2json&lt;/a&gt;,
and &lt;a href=&#34;https://stedolan.github.io/jq/&#34;&gt;jq&lt;/a&gt; to determine which deprecated APIs have been requested
from the current instance of the API server:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get --raw /metrics | prom2json | jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  .[] | select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;group&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;extensions&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;removed_release&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;1.22&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;resource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ingresses&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;subresource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1beta1&amp;#34;&lt;/span&gt;
}
{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;group&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;removed_release&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;1.22&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;resource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;clusterroles&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;subresource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1beta1&amp;#34;&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This shows the deprecated &lt;code&gt;extensions/v1beta1&lt;/code&gt; Ingress and &lt;code&gt;rbac.authorization.k8s.io/v1beta1&lt;/code&gt; ClusterRole APIs
have been requested on this server, and will be removed in v1.22.&lt;/p&gt;
&lt;p&gt;We can join that information with the &lt;code&gt;apiserver_request_total&lt;/code&gt; metrics to get more details about the requests being made to these APIs:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get --raw /metrics | prom2json | jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  # set $deprecated to a list of deprecated APIs
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  [
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    .[] | 
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels |
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;    {group,version,resource}
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  ] as $deprecated 
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  |
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  # select apiserver_request_total metrics which are deprecated
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  .[] | select(.name==&amp;#34;apiserver_request_total&amp;#34;).metrics[] |
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;  select(.labels | {group,version,resource} as $key | $deprecated | index($key))
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;code&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;0&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;component&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;apiserver&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;contentType&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;application/vnd.kubernetes.protobuf;stream=watch&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;dry_run&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;group&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;extensions&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;resource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ingresses&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;scope&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;cluster&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;subresource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;verb&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;WATCH&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1beta1&amp;#34;&lt;/span&gt;
  },
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;21&amp;#34;&lt;/span&gt;
}
{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;code&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;200&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;component&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;apiserver&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;contentType&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;application/vnd.kubernetes.protobuf&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;dry_run&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;group&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;extensions&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;resource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ingresses&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;scope&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;cluster&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;subresource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;verb&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;LIST&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1beta1&amp;#34;&lt;/span&gt;
  },
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;
}
{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;code&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;200&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;component&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;apiserver&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;contentType&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;application/json&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;dry_run&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;group&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;resource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;clusterroles&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;scope&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;cluster&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;subresource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;verb&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;LIST&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;v1beta1&amp;#34;&lt;/span&gt;
  },
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output shows that only read requests are being made to these APIs, and the most requests have been made to watch the deprecated Ingress API.&lt;/p&gt;
&lt;p&gt;You can also find that information through the following Prometheus query,
which returns information about requests made to deprecated APIs which will be removed in v1.22:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-promql&#34; data-lang=&#34;promql&#34;&gt;&lt;span style=&#34;color:#b8860b&#34;&gt;apiserver_requested_deprecated_apis&lt;/span&gt;{&lt;span style=&#34;color:#a0a000&#34;&gt;removed_version&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&amp;#34;&lt;span style=&#34;color:#b44&#34;&gt;1.22&lt;/span&gt;&amp;#34;}&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;on&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;group&lt;/span&gt;,&lt;span style=&#34;color:#b8860b&#34;&gt;version&lt;/span&gt;,&lt;span style=&#34;color:#b8860b&#34;&gt;resource&lt;/span&gt;,&lt;span style=&#34;color:#b8860b&#34;&gt;subresource&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;group_right&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;()&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b8860b&#34;&gt;apiserver_request_total&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;audit-annotations&#34;&gt;Audit Annotations&lt;/h3&gt;
&lt;p&gt;Metrics are a fast way to check whether deprecated APIs are being used, and at what rate,
but they don&#39;t include enough information to identify particular clients or API objects.
Starting in Kubernetes v1.19, &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&#34;&gt;audit events&lt;/a&gt;
for requests to deprecated APIs include an audit annotation of &lt;code&gt;&amp;quot;k8s.io/deprecated&amp;quot;:&amp;quot;true&amp;quot;&lt;/code&gt;.
Administrators can use those audit events to identify specific clients or objects that need to be updated.&lt;/p&gt;
&lt;h2 id=&#34;custom-resource-definitions&#34;&gt;Custom Resource Definitions&lt;/h2&gt;
&lt;p&gt;Along with the API server ability to warn about deprecated API use, starting in v1.19, a CustomResourceDefinition can indicate a
&lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-deprecation&#34;&gt;particular version of the resource it defines is deprecated&lt;/a&gt;.
When API requests to a deprecated version of a custom resource are made, a warning message is returned, matching the behavior of built-in APIs.&lt;/p&gt;
&lt;p&gt;The author of the CustomResourceDefinition can also customize the warning for each version if they want to.
This allows them to give a pointer to a migration guide or other information if needed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;apiextensions.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;CustomResourceDefinition&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;crontabs.example.com&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;versions&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1alpha1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# This indicates the v1alpha1 version of the custom resource is deprecated.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# API requests to this version receive a warning in the server response.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;deprecated&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# This overrides the default warning returned to clients making v1alpha1 API requests.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;deprecationWarning&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;example.com/v1alpha1 CronTab is deprecated; use example.com/v1 CronTab (see http://example.com/v1alpha1-v1)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# This indicates the v1beta1 version of the custom resource is deprecated.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# API requests to this version receive a warning in the server response.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# A default warning message is returned for this version.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;deprecated&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;...&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;admission-webhooks&#34;&gt;Admission Webhooks&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers&#34;&gt;Admission webhooks&lt;/a&gt;
are the primary way to integrate custom policies or validation with Kubernetes.
Starting in v1.19, admission webhooks can &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#response&#34;&gt;return warning messages&lt;/a&gt;
that are passed along to the requesting API client. Warnings can be returned with allowed or rejected admission responses.&lt;/p&gt;
&lt;p&gt;As an example, to allow a request but warn about a configuration known not to work well, an admission webhook could send this response:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;admission.k8s.io/v1&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;AdmissionReview&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;response&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;uid&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&amp;lt;value from request.uid&amp;gt;&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;allowed&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;warnings&amp;#34;&lt;/span&gt;: [
      &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;.spec.memory: requests &amp;gt;1GB do not work on Fridays&amp;#34;&lt;/span&gt;
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you are implementing a webhook that returns a warning message, here are some tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don&#39;t include a &amp;quot;Warning:&amp;quot; prefix in the message (that is added by clients on output)&lt;/li&gt;
&lt;li&gt;Use warning messages to describe problems the client making the API request should correct or be aware of&lt;/li&gt;
&lt;li&gt;Be brief; limit warnings to 120 characters if possible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many ways admission webhooks could use this new feature, and I&#39;m looking forward to seeing what people come up with.
Here are a couple ideas to get you started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;webhook implementations adding a &amp;quot;complain&amp;quot; mode, where they return warnings instead of rejections,
to allow trying out a policy to verify it is working as expected before starting to enforce it&lt;/li&gt;
&lt;li&gt;&amp;quot;lint&amp;quot; or &amp;quot;vet&amp;quot;-style webhooks, inspecting objects and surfacing warnings when best practices are not followed&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;customize-client-handling&#34;&gt;Customize Client Handling&lt;/h2&gt;
&lt;p&gt;Applications that use the &lt;code&gt;k8s.io/client-go&lt;/code&gt; library to make API requests can customize
how warnings returned from the server are handled. By default, warnings are logged to
stderr as they are received, but this behavior can be customized
&lt;a href=&#34;https://godoc.org/k8s.io/client-go/rest#SetDefaultWarningHandler&#34;&gt;per-process&lt;/a&gt;
or &lt;a href=&#34;https://godoc.org/k8s.io/client-go/rest#Config&#34;&gt;per-client&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This example shows how to make your application behave like &lt;code&gt;kubectl&lt;/code&gt;,
overriding message handling process-wide to deduplicate warnings
and highlighting messages using colored output where supported:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; (
  &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;os&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s.io/kubectl/pkg/util/term&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#666&#34;&gt;...&lt;/span&gt;
)

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;main&lt;/span&gt;() {
  rest.&lt;span style=&#34;color:#00a000&#34;&gt;SetDefaultWarningHandler&lt;/span&gt;(
    rest.&lt;span style=&#34;color:#00a000&#34;&gt;NewWarningWriter&lt;/span&gt;(os.Stderr, rest.WarningWriterOptions{
        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// only print a given warning the first time we receive it
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        Deduplicate: &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;,
        &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// highlight the output with color when the output supports it
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;        Color: term.&lt;span style=&#34;color:#00a000&#34;&gt;AllowsColorOutput&lt;/span&gt;(os.Stderr),
      },
    ),
  )

  &lt;span style=&#34;color:#666&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The next example shows how to construct a client that ignores warnings.
This is useful for clients that operate on metadata for all resource types
(found dynamically at runtime using the discovery API)
and do not benefit from warnings about a particular resource being deprecated.
Suppressing deprecation warnings is not recommended for clients that require use of particular APIs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; (
  &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;k8s.io/client-go/kubernetes&amp;#34;&lt;/span&gt;
)

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;getClientWithoutWarnings&lt;/span&gt;(config &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;rest.Config) (kubernetes.Interface, &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;error&lt;/span&gt;) {
  &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// copy to avoid mutating the passed-in config
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;  config = rest.&lt;span style=&#34;color:#00a000&#34;&gt;CopyConfig&lt;/span&gt;(config)
  &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// set the warning handler for this client to ignore warnings
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;  config.WarningHandler = rest.NoWarnings{}
  &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;// construct and return the client
&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; kubernetes.&lt;span style=&#34;color:#00a000&#34;&gt;NewForConfig&lt;/span&gt;(config)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;kubectl-strict-mode&#34;&gt;Kubectl Strict Mode&lt;/h2&gt;
&lt;p&gt;If you want to be sure you notice deprecations as soon as possible and get a jump start on addressing them,
&lt;code&gt;kubectl&lt;/code&gt; added a &lt;code&gt;--warnings-as-errors&lt;/code&gt; option in v1.19. When invoked with this option,
&lt;code&gt;kubectl&lt;/code&gt; treats any warnings it receives from the server as errors and exits with a non-zero exit code:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;kubectl applying a manifest file with a --warnings-as-errors flag, displaying a warning message and exiting with a non-zero exit code.&#34;
src=&#34;kubectl-warnings-as-errors.png&#34;
style=&#34;width:637px;max-width:100%;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This could be used in a CI job to apply manifests to a current server,
and required to pass with a zero exit code in order for the CI job to succeed.&lt;/p&gt;
&lt;h2 id=&#34;future-possibilities&#34;&gt;Future Possibilities&lt;/h2&gt;
&lt;p&gt;Now that we have a way to communicate helpful information to users in context,
we&#39;re already considering other ways we can use this to improve people&#39;s experience with Kubernetes.
A couple areas we&#39;re looking at next are warning about &lt;a href=&#34;http://issue.k8s.io/64841#issuecomment-395141013&#34;&gt;known problematic values&lt;/a&gt;
we cannot reject outright for compatibility reasons, and warning about use of deprecated fields or field values
(like selectors using beta os/arch node labels, &lt;a href=&#34;https://kubernetes.io/docs/reference/labels-annotations-taints/#beta-kubernetes-io-arch-deprecated&#34;&gt;deprecated in v1.14&lt;/a&gt;).
I&#39;m excited to see progress in this area, continuing to make it easier to use Kubernetes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://twitter.com/liggitt&#34;&gt;Jordan Liggitt&lt;/a&gt; is a software engineer at Google, and helps lead Kubernetes authentication, authorization, and API efforts.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
