<!doctype html>
<html lang="zh" class="no-js">
  <head>
    

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36037335-10"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36037335-10');
</script>


<link rel="alternate" hreflang="en" href="https://kubernetes.io/docs/tasks/">
<link rel="alternate" hreflang="ko" href="https://kubernetes.io/ko/docs/tasks/">
<link rel="alternate" hreflang="ja" href="https://kubernetes.io/ja/docs/tasks/">
<link rel="alternate" hreflang="fr" href="https://kubernetes.io/fr/docs/tasks/">
<link rel="alternate" hreflang="de" href="https://kubernetes.io/de/docs/tasks/">
<link rel="alternate" hreflang="es" href="https://kubernetes.io/es/docs/tasks/">
<link rel="alternate" hreflang="pt-br" href="https://kubernetes.io/pt-br/docs/tasks/">
<link rel="alternate" hreflang="id" href="https://kubernetes.io/id/docs/tasks/">
<link rel="alternate" hreflang="pl" href="https://kubernetes.io/pl/docs/tasks/">

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.88.1" />

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<link rel="canonical" type="text/html" href="https://kubernetes.io/zh/docs/tasks/">
<link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/manifest.webmanifest">
<link rel="apple-touch-icon" href="/images/kubernetes-192x192.png">
<title>任务 | Kubernetes</title><meta property="og:title" content="任务" />
<meta property="og:description" content="生产级别的容器编排系统" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://kubernetes.io/zh/docs/tasks/" /><meta property="og:site_name" content="Kubernetes" />

<meta itemprop="name" content="任务">
<meta itemprop="description" content="生产级别的容器编排系统"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="任务"/>
<meta name="twitter:description" content="生产级别的容器编排系统"/>





<link rel="preload" href="/scss/main.min.0bda2f3af46d1514ab4d3cad78295716cfd9557a35a41868b7db38040502bb54.css" as="style">
<link href="/scss/main.min.0bda2f3af46d1514ab4d3cad78295716cfd9557a35a41868b7db38040502bb54.css" rel="stylesheet" integrity="">


<script
  src="/js/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "https://kubernetes.io/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href="/css/feature-states.css">



<meta name="description" content="Kubernetes 文档这一部分包含的一些页面展示如何去完成单个任务。 每个任务页面是一般通过给出若干步骤展示如何执行完成某事。
如果你希望编写一个任务页面，参考 创建一个文档拉取请求。">
<meta property="og:description" content="Kubernetes 文档这一部分包含的一些页面展示如何去完成单个任务。 每个任务页面是一般通过给出若干步骤展示如何执行完成某事。
如果你希望编写一个任务页面，参考 创建一个文档拉取请求。">
<meta name="twitter:description" content="Kubernetes 文档这一部分包含的一些页面展示如何去完成单个任务。 每个任务页面是一般通过给出若干步骤展示如何执行完成某事。
如果你希望编写一个任务页面，参考 创建一个文档拉取请求。">
<meta property="og:url" content="https://kubernetes.io/zh/docs/tasks/">
<meta property="og:title" content="任务">
<meta name="twitter:title" content="任务">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">



<script src="/js/script.js"></script>


  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/zh/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/zh/docs/" >文档</span></a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/blog/" >Kubernetes 博客</span></a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/training/" >培训</span></a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/partners/" >合作伙伴</span></a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/community/" >社区</span></a>
			</li>
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link" href="/zh/case-studies/" >案例分析</span></a>
			</li>
			
			
			
			<li class="nav-item dropdown">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	版本列表
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/releases">Release Information</a>
	
	<a class="dropdown-item" href="https://kubernetes.io/zh/docs/tasks/">v1.22</a>
	
	<a class="dropdown-item" href="https://v1-21.docs.kubernetes.io/zh/docs/tasks/">v1.21</a>
	
	<a class="dropdown-item" href="https://v1-20.docs.kubernetes.io/zh/docs/tasks/">v1.20</a>
	
	<a class="dropdown-item" href="https://v1-19.docs.kubernetes.io/zh/docs/tasks/">v1.19</a>
	
	<a class="dropdown-item" href="https://v1-18.docs.kubernetes.io/zh/docs/tasks/">v1.18</a>
	
</div>
			</li>
			
			
			<li class="nav-item dropdown">
				

<a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	中文 Chinese
</a>
<div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="/docs/tasks/">English</a>
	
	<a class="dropdown-item" href="/ko/docs/tasks/">한국어 Korean</a>
	
	<a class="dropdown-item" href="/ja/docs/tasks/">日本語 Japanese</a>
	
	<a class="dropdown-item" href="/fr/docs/tasks/">Français</a>
	
	<a class="dropdown-item" href="/de/docs/tasks/">Deutsch</a>
	
	<a class="dropdown-item" href="/es/docs/tasks/">Español</a>
	
	<a class="dropdown-item" href="/pt-br/docs/tasks/">Português</a>
	
	<a class="dropdown-item" href="/id/docs/tasks/">Bahasa Indonesia</a>
	
	<a class="dropdown-item" href="/pl/docs/tasks/">Polski</a>
	
</div>

			</li>
			
		</ul>
	</div>
	<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href="#" onclick="print();return false;">点击此处打印</a>.
</p><p>
<a href="/zh/docs/tasks/">返回本页常规视图</a>.
</p>
</div>



<h1 class="title">任务</h1>





    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-57bf66f59d9a642b82eebeabbc66470b">安装工具</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>1.1: <a href="#pg-37b6179f23c8ad977cb9daa6d2da748a">在 Linux 系统中安装并设置 kubectl</a></li>


    
  
    
    
	
<li>1.2: <a href="#pg-961fc70b732cb8df4fd11a3463b6545c">在 macOS 系统上安装和设置 kubectl</a></li>


    
  
    
    
	
<li>1.3: <a href="#pg-2cc93d3011d707aeb6564bab02048f7a">在 Windows 上安装 kubectl</a></li>


    
  
    
    
	
<li>1.4: <a href="#pg-91639f08dfa86a6c88cf0099b2e097bc">内含的工具</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>1.4.1: <a href="#pg-d4ebbbe0a2ddce15850a36dfede9ba52">kubectl-convert 概述</a></li>


    
  
    
    
	
<li>1.4.2: <a href="#pg-2d600cc8ec4dec69673b5f9577b6da22">Linux 系统中的 bash 自动补全功能</a></li>


    
  
    
    
	
<li>1.4.3: <a href="#pg-68808b3ec5807517ed64fd8ae32a7d1b">macOS 系统上的 bash 自动补全</a></li>


    
  
    
    
	
<li>1.4.4: <a href="#pg-9ab27577326839cb4793fd670a916364">zsh 自动补全</a></li>


    
  
    
    
	
<li>1.4.5: <a href="#pg-99d563e9521796074ba3ca7f15a613ce">后续内容</a></li>


    
  
    
    
	
<li>1.4.6: <a href="#pg-f92adff24b39ef90a481e9aec2b8dd08">通过 gcloud 安装 kubectl</a></li>


    
  
    
    
	
<li>1.4.7: <a href="#pg-b2d43b2ae3e8f26eefa83de2db4ba782">验证 kubectl 的安装效果</a></li>


    
  

    </ul>
    
  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-34a810f1516ad9d99b2697e36e9b0d0f">管理集群</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-adb6c52e773f4d890595e14a9251f59b">从 dockershim 迁移</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1.1: <a href="#pg-58702e4818c09c9b3d574349c1a71cb3">检查弃用 Dockershim 对你的影响</a></li>


    
  
    
    
	
<li>2.1.2: <a href="#pg-eb3e279a6c5e1224e744080a52ee3f28">从 dockershim 迁移遥测和安全代理</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.2: <a href="#pg-8e16d69617b175d61e2e7a6e1642c9d6">用 kubeadm 进行管理</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.2.1: <a href="#pg-f62fba1de4084f3be070785757c8079c">使用 kubeadm 进行证书管理</a></li>


    
  
    
    
	
<li>2.2.2: <a href="#pg-6134c5061298affa145ddb801b5c29da">配置 cgroup 驱动</a></li>


    
  
    
    
	
<li>2.2.3: <a href="#pg-2e173356df5179cab9eec90a606f0aa4">升级 kubeadm 集群</a></li>


    
  
    
    
	
<li>2.2.4: <a href="#pg-9133578f1e75663bb031e5a377ca896d">添加 Windows 节点</a></li>


    
  
    
    
	
<li>2.2.5: <a href="#pg-e805c7d8d4ad6195cb82dbbc843bfc29">升级 Windows 节点</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.3: <a href="#pg-47be5dd51f686017f1766e6ec7aa6f41">管理内存，CPU 和 API 资源</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.3.1: <a href="#pg-337620c76587e4aeb32009cb23be46de">为命名空间配置默认的内存请求和限制</a></li>


    
  
    
    
	
<li>2.3.2: <a href="#pg-320af95e480962c538ebef7ae205845c">为命名空间配置默认的 CPU 请求和限制</a></li>


    
  
    
    
	
<li>2.3.3: <a href="#pg-adb489b1ab985c9215657b0d4c6ae92b">配置命名空间的最小和最大内存约束</a></li>


    
  
    
    
	
<li>2.3.4: <a href="#pg-a87cbd1f9379dac7a48ae320da68a9ad">为命名空间配置 CPU 最小和最大约束</a></li>


    
  
    
    
	
<li>2.3.5: <a href="#pg-fe3283559a3df299aae3ee00ecea2fad">为命名空间配置内存和 CPU 配额</a></li>


    
  
    
    
	
<li>2.3.6: <a href="#pg-40e30a9209e0c9f4153707e43243e9d7">配置命名空间下 Pod 配额</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.4: <a href="#pg-7743f043c43f7b12e8654e2227dbc658">证书</a></li>


    
  
    
    
	
<li>2.5: <a href="#pg-8c31aafd38fad5b0de0bd191758d6f93">安装网络规则驱动</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.5.1: <a href="#pg-1239a77618c6278373832a142cd85519">使用 Calico 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>2.5.2: <a href="#pg-95039241255a31df196beaa405b68eba">使用 Cilium 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>2.5.3: <a href="#pg-505a0a6a7e6eff361bbb3be81c84b2e0">使用 kube-router 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>2.5.4: <a href="#pg-2842eac98aa0e229a5c6755c4c83d2a7">使用 Romana 提供 NetworkPolicy</a></li>


    
  
    
    
	
<li>2.5.5: <a href="#pg-ac075c3fdfd0d41aa753cc70e42be064">使用 Weave Net 提供 NetworkPolicy</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.6: <a href="#pg-b45f024608e1b367cdacb1fd9d77278a">IP Masquerade Agent 用户指南</a></li>


    
  
    
    
	
<li>2.7: <a href="#pg-ce4cd28c8feb9faa783e79b48af37961">Kubernetes 云管理控制器</a></li>


    
  
    
    
	
<li>2.8: <a href="#pg-c4d0832845adc92b7ccd54aed63fc932">为 Kubernetes 运行 etcd 集群</a></li>


    
  
    
    
	
<li>2.9: <a href="#pg-b64a1d2bb3f4ed9f7021134e09a75c36">为系统守护进程预留计算资源</a></li>


    
  
    
    
	
<li>2.10: <a href="#pg-a8f6511197efcd7d0db80ade49620f9d">为节点发布扩展资源</a></li>


    
  
    
    
	
<li>2.11: <a href="#pg-e1afcdac8d5e8458274b3c481c5ebcda">使用 CoreDNS 进行服务发现</a></li>


    
  
    
    
	
<li>2.12: <a href="#pg-669c88964b4a9eb2b040057266e4b60d">使用 KMS 驱动进行数据加密</a></li>


    
  
    
    
	
<li>2.13: <a href="#pg-e77685d5b88d2db5c7631a27b9472eea">使用 Kubernetes API 访问集群</a></li>


    
  
    
    
	
<li>2.14: <a href="#pg-12001be83d15fcd7f3242313a55777df">保护集群安全</a></li>


    
  
    
    
	
<li>2.15: <a href="#pg-4a02bcca41439e16655f43fa37c81da4">关键插件 Pod 的调度保证</a></li>


    
  
    
    
	
<li>2.16: <a href="#pg-fe6b50655c29ab0b7c1ee549ff64c138">升级集群</a></li>


    
  
    
    
	
<li>2.17: <a href="#pg-56de8c25b1486599777034111645b803">名字空间演练</a></li>


    
  
    
    
	
<li>2.18: <a href="#pg-09cc2cf3e0f23a3996e6cb31dc4d867c">启用/禁用 Kubernetes API</a></li>


    
  
    
    
	
<li>2.19: <a href="#pg-c2f73ef872a65be44f4ab1e6511b8eb9">启用拓扑感知提示</a></li>


    
  
    
    
	
<li>2.20: <a href="#pg-9ceed97f912df7289ed8872e290cfbad">在 Kubernetes 集群中使用 NodeLocal DNSCache</a></li>


    
  
    
    
	
<li>2.21: <a href="#pg-fe5ad73163d38596340536ec03a205f0">在 Kubernetes 集群中使用 sysctl</a></li>


    
  
    
    
	
<li>2.22: <a href="#pg-eec61e72c300dbfbf7302400ca966432">在运行中的集群上重新配置节点的 kubelet</a></li>


    
  
    
    
	
<li>2.23: <a href="#pg-a3790dfb57271d13517e549dffa805b9">声明网络策略</a></li>


    
  
    
    
	
<li>2.24: <a href="#pg-b35b8ddb9bbc15620ce9636f4346c05c">安全地清空一个节点</a></li>


    
  
    
    
	
<li>2.25: <a href="#pg-a24171610b6ea75a142cb9c8c7882390">将重复的控制平面迁至云控制器管理器</a></li>


    
  
    
    
	
<li>2.26: <a href="#pg-9585dc0efb0450fd68728e7511754717">开发云控制器管理器</a></li>


    
  
    
    
	
<li>2.27: <a href="#pg-00733cc3747eb3f5fe1c9e0439262967">开启服务拓扑</a></li>


    
  
    
    
	
<li>2.28: <a href="#pg-7127e6b7344b315b30b1ce8c4d8bfc55">控制节点上的 CPU 管理策略</a></li>


    
  
    
    
	
<li>2.29: <a href="#pg-8060aed5bf1172fa62199a4c306a4cd1">控制节点上的拓扑管理策略</a></li>


    
  
    
    
	
<li>2.30: <a href="#pg-0b17e83b6049e53b8ffa864bdfa07c87">搭建高可用的 Kubernetes Masters</a></li>


    
  
    
    
	
<li>2.31: <a href="#pg-2bffd7f3571cdd609bd97fb2e1bdb2fe">改变默认 StorageClass</a></li>


    
  
    
    
	
<li>2.32: <a href="#pg-fbc9136f53eccd6eb8c80f4bbea3b8f4">更改 PersistentVolume 的回收策略</a></li>


    
  
    
    
	
<li>2.33: <a href="#pg-966cd1cc69c69410d8698b3ac74abce2">自动扩缩集群 DNS 服务</a></li>


    
  
    
    
	
<li>2.34: <a href="#pg-3d0cd7d2f13d4759094f281504cf57b8">自定义 DNS 服务</a></li>


    
  
    
    
	
<li>2.35: <a href="#pg-bc6e50c405a620aab43b40d41d6375df">访问集群上运行的服务</a></li>


    
  
    
    
	
<li>2.36: <a href="#pg-8bcf4aeb5bbb6d6969a146e5ab97557b">调试 DNS 问题</a></li>


    
  
    
    
	
<li>2.37: <a href="#pg-1e966f5d0540bbee0876f9d0d08d54dc">通过名字空间共享集群</a></li>


    
  
    
    
	
<li>2.38: <a href="#pg-f58763cc9447491b6c40f939a02d441d">通过配置文件设置 Kubelet 参数</a></li>


    
  
    
    
	
<li>2.39: <a href="#pg-5e59f5575dce11fdaed640afdbeedfc1">配置 API 对象配额</a></li>


    
  
    
    
	
<li>2.40: <a href="#pg-6f3658d05bf8864be1d96b1d1287cffb">配置资源不足时的处理方式</a></li>


    
  
    
    
	
<li>2.41: <a href="#pg-a02f35804917d7a269c38d7e2c475005">限制存储消耗</a></li>


    
  
    
    
	
<li>2.42: <a href="#pg-6b4e7ca6586f448c8533a120c29bdd25">静态加密 Secret 数据</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-f5da33b976758a9183018c421eb83f58">配置 Pods 和容器</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1: <a href="#pg-e6dd9300cf3a955f7cdfe77fb5d15292">为容器和 Pod 分配内存资源</a></li>


    
  
    
    
	
<li>3.2: <a href="#pg-aa522472483f900008124a2809f2114b">为 Windows Pod 和容器配置 GMSA</a></li>


    
  
    
    
	
<li>3.3: <a href="#pg-f5da7517bee8a8807431d9fc65263b39">为 Windows 的 Pod 和容器配置 RunAsUserName</a></li>


    
  
    
    
	
<li>3.4: <a href="#pg-8555af270ae7122cc0464bab3f5d1609">为容器和 Pods 分配 CPU 资源</a></li>


    
  
    
    
	
<li>3.5: <a href="#pg-904cea8c8efd5c0d33adbfe579ec2dd2">配置 Pod 的服务质量</a></li>


    
  
    
    
	
<li>3.6: <a href="#pg-4219ac6ab56a3b88d20305083d57d03c">为容器分派扩展资源</a></li>


    
  
    
    
	
<li>3.7: <a href="#pg-484833fb880d1e179cc2965d15f84da5">配置 Pod 以使用卷进行存储</a></li>


    
  
    
    
	
<li>3.8: <a href="#pg-528d2422215cb9632b7b45e886b023b5">配置 Pod 以使用 PersistentVolume 作为存储</a></li>


    
  
    
    
	
<li>3.9: <a href="#pg-4621938ba53c04a77f51b5938a583439">配置 Pod 使用投射卷作存储</a></li>


    
  
    
    
	
<li>3.10: <a href="#pg-abd895c0803315e9717e6ff9ec4e3d30">为 Pod 或容器配置安全性上下文</a></li>


    
  
    
    
	
<li>3.11: <a href="#pg-2c0d882359718c4c69c67099bed2156c">为 Pod 配置服务账户</a></li>


    
  
    
    
	
<li>3.12: <a href="#pg-d385b86a7cb496d3b1c3b2a47280ca70">从私有仓库拉取镜像</a></li>


    
  
    
    
	
<li>3.13: <a href="#pg-eb54daf87df373096b5e830680194dfc">配置存活、就绪和启动探测器</a></li>


    
  
    
    
	
<li>3.14: <a href="#pg-bbc17480da6d051c696489654c64064a">将 Pod 分配给节点</a></li>


    
  
    
    
	
<li>3.15: <a href="#pg-fc3f4777ae8ea685d2b54e175277ac01">用节点亲和性把 Pods 分配到节点</a></li>


    
  
    
    
	
<li>3.16: <a href="#pg-1e7baac1825631a5af5d2aebcf059249">配置 Pod 初始化</a></li>


    
  
    
    
	
<li>3.17: <a href="#pg-efbc43486296f0439d1a89c12d944d94">为容器的生命周期事件设置处理函数</a></li>


    
  
    
    
	
<li>3.18: <a href="#pg-ed34e761c3dbd00fa79577fa78e30020">配置 Pod 使用 ConfigMap</a></li>


    
  
    
    
	
<li>3.19: <a href="#pg-3d7b9cb24a647c36ba63f7a02ec49010">在 Pod 中的容器之间共享进程命名空间</a></li>


    
  
    
    
	
<li>3.20: <a href="#pg-42a59b878d4c58e5c6f4bb87483dda93">创建静态 Pod</a></li>


    
  
    
    
	
<li>3.21: <a href="#pg-1bb997c61a85de753d9994e7a312a291">将 Docker Compose 文件转换为 Kubernetes 资源</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4: <a href="#pg-aa0731e8aa8e2f6cc9e3c1a5e9895863">管理 Kubernetes 对象</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-df206392be6f4d19bd8da41cee7170fa">使用配置文件对 Kubernetes 对象进行声明式管理</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-11aa6950fcb203094823c8e2cbdd517f">使用 Kustomize 对 Kubernetes 对象进行声明式管理</a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-80c83fe9b80d0fef2681c8d59c0aa197">使用指令式命令管理 Kubernetes 对象</a></li>


    
  
    
    
	
<li>4.4: <a href="#pg-b18886277c410fc6f32ce068e2160537">使用配置文件对 Kubernetes 对象进行命令式管理</a></li>


    
  
    
    
	
<li>4.5: <a href="#pg-d4d4414dc91b63cfe0f65ca4f0c2fe31">使用 kubectl patch 更新 API 对象</a></li>


    
  

    </ul>
    
  
    
    
	
<li>5: <a href="#pg-94f49ece137035764368f22a98942872">管理 Secrets</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.1: <a href="#pg-0ed63ce3c9665aed7ff5a560ff1da843">使用 kubectl 管理 Secret</a></li>


    
  
    
    
	
<li>5.2: <a href="#pg-e841cf91fd3566db1e86143ed7a9e13c">使用配置文件管理 Secret</a></li>


    
  
    
    
	
<li>5.3: <a href="#pg-a0ff2e3ba8af5670d5dc3d94c4bd0a68">使用 Kustomize 管理 Secret</a></li>


    
  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-866924fa095f897ede8dfdcab9e97942">给应用注入数据</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>6.1: <a href="#pg-c9af1e81bb6e109f6c41febe44f0931b">为容器设置启动时要执行的命令和参数</a></li>


    
  
    
    
	
<li>6.2: <a href="#pg-82c93897176489678232542102daea40">为容器设置环境变量</a></li>


    
  
    
    
	
<li>6.3: <a href="#pg-eff97c25c917cdb414eda016df0e2bca">定义相互依赖的环境变量</a></li>


    
  
    
    
	
<li>6.4: <a href="#pg-66c0456fdbef5e5116dd606d1e6f73cc">通过环境变量将 Pod 信息呈现给容器</a></li>


    
  
    
    
	
<li>6.5: <a href="#pg-bcf93d1cd019501fd0b7649e9fbcaf60">通过文件将 Pod 信息呈现给容器</a></li>


    
  
    
    
	
<li>6.6: <a href="#pg-7f9454a1e775548c23ee5b300a9218a3">使用 Secret 安全地分发凭证</a></li>


    
  

    </ul>
    
  
    
    
	
<li>7: <a href="#pg-a78a5e7e765fd8c49c8f7c0d72499f72">运行应用</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>7.1: <a href="#pg-790ea02857492b3a822e981e93e3a98b">使用 Deployment 运行一个无状态应用</a></li>


    
  
    
    
	
<li>7.2: <a href="#pg-43398a6f5dc7ce19df59f5f4c2e7922d">运行一个单实例有状态应用</a></li>


    
  
    
    
	
<li>7.3: <a href="#pg-95b3d561509c573e53bec2368264cf6a">运行一个有状态的应用程序</a></li>


    
  
    
    
	
<li>7.4: <a href="#pg-c43537b0ee1da992ecb7488f87e6c934">删除 StatefulSet</a></li>


    
  
    
    
	
<li>7.5: <a href="#pg-f5f2f7a74377a9d45325c5253353fa8f">强制删除 StatefulSet 中的 Pods</a></li>


    
  
    
    
	
<li>7.6: <a href="#pg-0c0bb1bd76d2a9069e50e2cec6d20c2a">Pod 水平自动扩缩</a></li>


    
  
    
    
	
<li>7.7: <a href="#pg-8138226ce9660ac8e3e82ff86fff8ad2">Horizontal Pod Autoscaler 演练</a></li>


    
  
    
    
	
<li>7.8: <a href="#pg-fbe2744f00d1aa4df4cdf4eea6a082d4">为应用程序设置干扰预算（Disruption Budget）</a></li>


    
  
    
    
	
<li>7.9: <a href="#pg-52cd10ee3fc7c74a6c31043a2d489878">从 Pod 中访问 Kubernetes API</a></li>


    
  
    
    
	
<li>7.10: <a href="#pg-7a9b5779e228083ba3fdeaf414fe704e">扩缩 StatefulSet</a></li>


    
  

    </ul>
    
  
    
    
	
<li>8: <a href="#pg-ca3bc4e31dfe46d5044a3b93eb804ee9">运行 Jobs</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>8.1: <a href="#pg-964bdff888520740e5e221695245678d">使用 CronJob 运行自动化任务</a></li>


    
  
    
    
	
<li>8.2: <a href="#pg-1058efa4d70f13c015e6a2094ff85068">使用工作队列进行粗粒度并行处理</a></li>


    
  
    
    
	
<li>8.3: <a href="#pg-457c9dd93aed2b05615ed28dc38075d3">使用工作队列进行精细的并行处理</a></li>


    
  
    
    
	
<li>8.4: <a href="#pg-da7c2b067953d239eb4457e8978ad8f6">使用展开的方式进行并行处理</a></li>


    
  

    </ul>
    
  
    
    
	
<li>9: <a href="#pg-b74b959f5a531003dd0653dfbfc2e88b">访问集群中的应用程序</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>9.1: <a href="#pg-777447042cd4e81df3fa5beb3357a485">Web 界面 (Dashboard)</a></li>


    
  
    
    
	
<li>9.2: <a href="#pg-6a8d9e9e05f2b6825afbb8889c957370">访问集群</a></li>


    
  
    
    
	
<li>9.3: <a href="#pg-72d3dddbc0c166c9a364e753d2b31ff0">使用端口转发来访问集群中的应用</a></li>


    
  
    
    
	
<li>9.4: <a href="#pg-312f29f850826b74618634cd877aa065">使用服务来访问集群中的应用</a></li>


    
  
    
    
	
<li>9.5: <a href="#pg-f3dac629bea950fc026d920306f09fb4">使用 Service 把前端连接到后端</a></li>


    
  
    
    
	
<li>9.6: <a href="#pg-21cd8f87563675fb0278d3694ba9ecb0">创建外部负载均衡器</a></li>


    
  
    
    
	
<li>9.7: <a href="#pg-48e8f306f919c5b81265e265a2b76ab4">列出集群中所有运行容器的镜像</a></li>


    
  
    
    
	
<li>9.8: <a href="#pg-1839d8468a083839ed1cc8d18fe1142e">在 Minikube 环境中使用 NGINX Ingress 控制器配置 Ingress</a></li>


    
  
    
    
	
<li>9.9: <a href="#pg-322786b38586b210fab68f785259c5f6">为集群配置 DNS</a></li>


    
  
    
    
	
<li>9.10: <a href="#pg-7c319a9981586e5fbcfa21b392720650">同 Pod 内的容器使用共享卷通信</a></li>


    
  
    
    
	
<li>9.11: <a href="#pg-5a233e14205d77fe1294917d2da6f876">配置对多集群的访问</a></li>


    
  

    </ul>
    
  
    
    
	
<li>10: <a href="#pg-f6a755efe831d24956501e4bcd49ff96">监控、日志和排错</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>10.1: <a href="#pg-5e4a82f171ec2c11da7360a67efb4abf">使用 crictl 对 Kubernetes 节点进行调试</a></li>


    
  
    
    
	
<li>10.2: <a href="#pg-d25a16285195bd17d9055b1eb7bc605c">在本地开发和调试服务</a></li>


    
  
    
    
	
<li>10.3: <a href="#pg-cbd33a50cc4779f855318a0dd00d7b06">审计</a></li>


    
  
    
    
	
<li>10.4: <a href="#pg-3556c4dbd027b9e90a5b3d72649003fb">应用故障排查</a></li>


    
  
    
    
	
<li>10.5: <a href="#pg-731bb8b338c16aebfb9590ba2bd3fdd1">应用自测与调试</a></li>


    
  
    
    
	
<li>10.6: <a href="#pg-434e0133d71583a27478b10fc1d3d105">故障诊断</a></li>


    
  
    
    
	
<li>10.7: <a href="#pg-ef360b1f8e65236251826db478cfcab3">确定 Pod 失败的原因</a></li>


    
  
    
    
	
<li>10.8: <a href="#pg-bc729eafe3688124d3a6f1110bd5a89c">节点健康监测</a></li>


    
  
    
    
	
<li>10.9: <a href="#pg-9713ac27b6d9e3034033200d968221f2">获取正在运行容器的 Shell</a></li>


    
  
    
    
	
<li>10.10: <a href="#pg-06bb252f25983de12f635c806d180d30">调试 Init 容器</a></li>


    
  
    
    
	
<li>10.11: <a href="#pg-858517cd46a1b5a1fd2e650edd785cea">调试 Pods 和 ReplicationControllers</a></li>


    
  
    
    
	
<li>10.12: <a href="#pg-f79645981e310858111bd5673614cab6">调试 Service</a></li>


    
  
    
    
	
<li>10.13: <a href="#pg-a070b1250ee142402d492b505a56ca83">调试StatefulSet</a></li>


    
  
    
    
	
<li>10.14: <a href="#pg-c0ec963f381296ca26b839cdf0a6f242">调试运行中的 Pod</a></li>


    
  
    
    
	
<li>10.15: <a href="#pg-96b25d30e732385047272b84d3c4188f">资源指标管道</a></li>


    
  
    
    
	
<li>10.16: <a href="#pg-9e6e1b706f11386fe2c4b4ffda1409e4">资源监控工具</a></li>


    
  
    
    
	
<li>10.17: <a href="#pg-47290c80fb8b00accec6729f3da49734">集群故障排查</a></li>


    
  

    </ul>
    
  
    
    
	
<li>11: <a href="#pg-11a6d16334428909c99e7208ab8fa5e9">扩展 Kubernetes</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>11.1: <a href="#pg-8f1f7f0d3a1cc21537506bd4f9103a29">使用自定义资源</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>11.1.1: <a href="#pg-dc64883f1fd119402b112d3ff6733452">使用 CustomResourceDefinition 扩展 Kubernetes API</a></li>


    
  
    
    
	
<li>11.1.2: <a href="#pg-7d2e2400f208b1637530752794e5a3bd">CustomResourceDefinition 的版本</a></li>


    
  

    </ul>
    
  
    
    
	
<li>11.2: <a href="#pg-2bd28753e62a14a597073fa8ea18a5d8">配置聚合层</a></li>


    
  
    
    
	
<li>11.3: <a href="#pg-c4798e42eaccc051e396542befb3c57b">安装一个扩展的 API server</a></li>


    
  
    
    
	
<li>11.4: <a href="#pg-c00a2767fac9dbfafce583cf489cc423">配置多个调度器</a></li>


    
  
    
    
	
<li>11.5: <a href="#pg-1707517970dd390995f760308c2e2de6">使用 HTTP 代理访问 Kubernetes API</a></li>


    
  
    
    
	
<li>11.6: <a href="#pg-61cf1f2f0fbe98e7635fce65f04a775f">设置 Konnectivity 服务</a></li>


    
  

    </ul>
    
  
    
    
	
<li>12: <a href="#pg-d3c88a8663f58e9ec0bed73faff5b670">TLS</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>12.1: <a href="#pg-1272b18ac0c008f6ffc2c62a29fa929f">为 kubelet 配置证书轮换</a></li>


    
  
    
    
	
<li>12.2: <a href="#pg-43d5e2b1fc2a7e104e66d481d08578dc">手动轮换 CA 证书</a></li>


    
  
    
    
	
<li>12.3: <a href="#pg-9a87de8ee8332cb487f34a05debb1125">管理集群中的 TLS 认证</a></li>


    
  

    </ul>
    
  
    
    
	
<li>13: <a href="#pg-ba58efa15c6d46f10e34d799be220965">管理集群守护进程</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>13.1: <a href="#pg-bcfd795e4b59420f7db275a0482af37c">对 DaemonSet 执行滚动更新</a></li>


    
  
    
    
	
<li>13.2: <a href="#pg-f1bf7e426f482a85e1a417d1fd9ea7b7">对 DaemonSet 执行回滚</a></li>


    
  

    </ul>
    
  
    
    
	
<li>14: <a href="#pg-5266308e17490aeee8b018316bf47e03">安装服务目录</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>14.1: <a href="#pg-f741b6fc36e27a6f79c1c3d02a40d8f9">使用 Helm 安装 Service Catalog</a></li>


    
  
    
    
	
<li>14.2: <a href="#pg-d85a30635b5c3578487b9f6f214c07ea">使用 SC 安装服务目录</a></li>


    
  

    </ul>
    
  
    
    
	
<li>15: <a href="#pg-a701e71f3b32dae474c63ae4c596c856">网络</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>15.1: <a href="#pg-2edb5b02ea1e646c333c9fe4d5f02ff1">使用 HostAliases 向 Pod /etc/hosts 文件添加条目</a></li>


    
  
    
    
	
<li>15.2: <a href="#pg-eebac062766222247063d6513f95c7b2">验证 IPv4/IPv6 双协议栈</a></li>


    
  

    </ul>
    
  
    
    
	
<li>16: <a href="#pg-f34d6e348a8e677d6c6eb155cd1a99aa">用插件扩展 kubectl</a></li>


    
  
    
    
	
<li>17: <a href="#pg-fdfb2a2cba62a1e624897eaebac0168e">管理巨页（HugePages）</a></li>


    
  
    
    
	
<li>18: <a href="#pg-5ab7bc7f14942c5c4b29d19f4a87271c">调度 GPUs</a></li>


    
  
    
    
	
<li>19: <a href="#pg-0c4484d31ad3902880897e694bbd306f">配置 kubelet 镜像凭据提供程序</a></li>


    
  

    </ul>


<div class="content">
      <!--
title: Tasks
main_menu: true
weight: 50
content_type: concept
-->
<!-- overview -->
<!--
This section of the Kubernetes documentation contains pages that
show how to do individual tasks. A task page shows how to do a
single thing, typically by giving a short sequence of steps.
-->
<p>Kubernetes 文档这一部分包含的一些页面展示如何去完成单个任务。
每个任务页面是一般通过给出若干步骤展示如何执行完成某事。</p>
<!--
If you would like to write a task page, see
[Creating a Documentation Pull Request](/docs/contribute/new-content/open-a-pr/).
-->
<p>如果你希望编写一个任务页面，参考
<a href="/zh/docs/contribute/new-content/open-a-pr/">创建一个文档拉取请求</a>。</p>

</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-57bf66f59d9a642b82eebeabbc66470b">1 - 安装工具</h1>
    <div class="lead">在你的计算机上设置 Kubernetes 工具。</div>
	<!--
title: "Install Tools"
description: Set up Kubernetes tools on your computer.
weight: 10
no_list: true
-->
<!--
## kubectl

The Kubernetes command-line tool, [kubectl](/docs/reference/kubectl/kubectl/), allows
you to run commands against Kubernetes clusters.
You can use kubectl to deploy applications, inspect and manage cluster resources,
and view logs. For more information including a complete list of kubectl operations, see the
[`kubectl` reference documentation](/docs/reference/kubectl/).
-->
<h2 id="kubectl">kubectl</h2>
<p>Kubernetes 命令行工具，<a href="/docs/reference/kubectl/kubectl/">kubectl</a>，使得你可以对 Kubernetes 集群运行命令。
你可以使用 kubectl 来部署应用、监测和管理集群资源以及查看日志。</p>
<p>有关更多信息，包括 kubectl 操作的完整列表，请参见<a href="/zh/docs/reference/kubectl/"><code>kubectl</code>
参考文件</a>。</p>
<!--
kubectl is installable on a variety of Linux platforms, macOS and Windows. 
Find your preferred operating system below.

- [Install kubectl on Linux](/docs/tasks/tools/install-kubectl-linux)
- [Install kubectl on macOS](/docs/tasks/tools/install-kubectl-macos)
- [Install kubectl on Windows](/docs/tasks/tools/install-kubectl-windows)
-->
<p>kubectl 可安装在各种 Linux 平台、 macOS 和 Windows 上。
在下面找到你喜欢的操作系统。</p>
<ul>
<li><a href="/zh/docs/tasks/tools/install-kubectl-linux">在 Linux 上安装 kubectl</a></li>
<li><a href="/zh/docs/tasks/tools/install-kubectl-macos">在 macOS 上安装 kubectl</a></li>
<li><a href="/zh/docs/tasks/tools/install-kubectl-windows">在 Windows 上安装 kubectl</a></li>
</ul>
<!--
## kind

[`kind`](https://kind.sigs.k8s.io/docs/) lets you run Kubernetes on
your local computer. This tool requires that you have
[Docker](https://docs.docker.com/get-docker/) installed and configured.

The kind [Quick Start](https://kind.sigs.k8s.io/docs/user/quick-start/) page
shows you what you need to do to get up and running with `kind`.

<a class="btn btn-primary" href="https://kind.sigs.k8s.io/docs/user/quick-start/" role="button" aria-label="View kind Quick Start Guide">View kind Quick Start Guide</a>

-->
<h2 id="kind">kind</h2>
<p><a href="https://kind.sigs.k8s.io/docs/"><code>kind</code></a> 让你能够在本地计算机上运行 Kubernetes。
<code>kind</code> 要求你安装并配置好 <a href="https://docs.docker.com/get-docker/">Docker</a>。</p>
<p>kind <a href="https://kind.sigs.k8s.io/docs/user/quick-start/">快速入门</a>页面展示了
开始使用 <code>kind</code> 所需要完成的操作。</p>
<p><a class="btn btn-primary" href="https://kind.sigs.k8s.io/docs/user/quick-start/"
role="button" aria-label="查看 kind 的快速入门指南">
查看 kind 的快速入门指南
</a></p>
<!--
## minikube

Like `kind`, [`minikube`](https://minikube.sigs.k8s.io/) is a tool that lets you run Kubernetes
locally. `minikube` runs a single-node Kubernetes cluster on your personal
computer (including Windows, macOS and Linux PCs) so that you can try out
Kubernetes, or for daily development work.

You can follow the official
[Get Started!](https://minikube.sigs.k8s.io/docs/start/) guide if your focus is
on getting the tool installed.
-->
<h2 id="minikube">minikube</h2>
<p>与 <code>kind</code> 类似，<a href="https://minikube.sigs.k8s.io/"><code>minikube</code></a> 是一个工具，
能让你在本地运行 Kubernetes。
<code>minikube</code> 在你本地的个人计算机（包括 Windows、macOS 和 Linux PC）运行一个单节点的
Kubernetes 集群，以便你来尝试 Kubernetes 或者开展每天的开发工作。</p>
<p>如果你关注如何安装此工具，可以按官方的
<a href="https://minikube.sigs.k8s.io/docs/start/">Get Started!</a>指南操作。</p>
<!--
a class="btn btn-primary" href="https://minikube.sigs.k8s.io/docs/start/" role="button" aria-label="View minikube Get Started! Guide">View minikube Get Started! Guide</a>

Once you have `minikube` working, you can use it to
[run a sample application](/docs/tutorials/hello-minikube/).
-->
<p><a class="btn btn-primary" href="https://minikube.sigs.k8s.io/docs/start/"
role="button" aria-label="查看 minikube 快速入门指南">
查看 minikube 快速入门指南
</a></p>
<p>当你拥有了可工作的 <code>minikube</code> 时，就可以用它来
<a href="/zh/docs/tutorials/hello-minikube/">运行示例应用</a>了。</p>
<h2 id="kubeadm">kubeadm</h2>
<!--
You can use the <a class='glossary-tooltip' title='用来快速安装 Kubernetes 并搭建安全稳定的集群的工具。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/tools/kubeadm/' target='_blank' aria-label='kubeadm'>kubeadm</a> tool to create and manage Kubernetes clusters.
It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.
-->
<p>你可以使用 <a class='glossary-tooltip' title='用来快速安装 Kubernetes 并搭建安全稳定的集群的工具。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/tools/kubeadm/' target='_blank' aria-label='kubeadm'>kubeadm</a> 工具来
创建和管理 Kubernetes 集群。
该工具能够执行必要的动作并用一种用户友好的方式启动一个可用的、安全的集群。</p>
<!--
[Installing kubeadm](/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) shows you how to install kubeadm.
Once installed, you can use it to [create a cluster](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).
-->
<p><a href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">安装 kubeadm</a>
展示了如何安装 kubeadm 的过程。
一旦安装了 kubeadm，你就可以使用它来
<a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">创建一个集群</a>。</p>
<!-- a class="btn btn-primary" href="/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" role="button" aria-label="View kubeadm Install Guide">View kubeadm Install Guide</a-->
<p><a class="btn btn-primary" href="/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/"
role="button" aria-label="查看 kubeadm 安装指南">查看 kubeadm 安装指南</a></p>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-37b6179f23c8ad977cb9daa6d2da748a">1.1 - 在 Linux 系统中安装并设置 kubectl</h1>
    
	<!-- 
reviewers:
- mikedanese
title: Install and Set Up kubectl on Linux
content_type: task
weight: 10
card:
  name: tasks
  weight: 20
  title: Install kubectl on Linux
-->
<h2 id="准备开始">准备开始</h2>
<!-- 
You must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.22 client can communicate with v1.21, v1.22, and v1.23 control planes.
Using the latest version of kubectl helps avoid unforeseen issues.
-->
<p>kubectl 版本和集群版本之间的差异必须在一个小版本号内。
例如：v1.22 版本的客户端能与 v1.21、
v1.22 和 v1.23 版本的控制面通信。
用最新版的 kubectl 有助于避免不可预见的问题。</p>
<!-- 
## Install kubectl on Linux
-->
<h2 id="install-kubectl-on-linux">在 Linux 系统中安装 kubectl</h2>
<!-- 
The following methods exist for installing kubectl on Linux:
-->
<p>在 Linux 系统中安装 kubectl 有如下几种方法：</p>
<!--
- [Install kubectl binary with curl on Linux](#install-kubectl-binary-with-curl-on-linux)
- [Install using native package management](#install-using-native-package-management)
- [Install using other package management](#install-using-other-package-management)
-->
<ul>
<li><a href="#install-kubectl-binary-with-curl-on-linux">用 curl 在 Linux 系统中安装 kubectl</a></li>
<li><a href="#install-using-native-package-management">用原生包管理工具安装</a></li>
<li><a href="#install-using-other-package-management">用其他包管理工具安装</a></li>
</ul>
<!-- 
### Install kubectl binary with curl on Linux
-->
<h3 id="install-kubectl-binary-with-curl-on-linux">用 curl 在 Linux 系统中安装 kubectl</h3>
<!-- 
1. Download the latest release with the command:
-->
<ol>
<li>
<p>用以下命令下载最新发行版：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/linux/amd64/kubectl&#34;</span>
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
   To download a specific version, replace the `$(curl -L -s https://dl.k8s.io/release/stable.txt)` portion of the command with the specific version.

   For example, to download version v1.22.0 on Linux, type:
   -->
<p>如需下载某个指定的版本，请用指定版本号替换该命令的这一部分：
<code>$(curl -L -s https://dl.k8s.io/release/stable.txt)</code>。</p>
<p>例如，要在 Linux 中下载 v1.22.0 版本，请输入：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -LO https://dl.k8s.io/release/v1.22.0/bin/linux/amd64/kubectl
</code></pre></div></div>
</blockquote>
</li>
</ol>
<!-- 
1. Validate the binary (optional)

   Download the kubectl checksum file:
-->
<ol start="2">
<li>
<p>验证该可执行文件（可选步骤）</p>
<p>下载 kubectl 校验和文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/linux/amd64/kubectl.sha256&#34;</span>
</code></pre></div><!-- 
Validate the kubectl binary against the checksum file:
-->
<p>基于校验和文件，验证 kubectl 的可执行文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>&lt;kubectl.sha256<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44"> kubectl&#34;</span> | sha256sum --check
</code></pre></div><!-- 
If valid, the output is:
-->
<p>验证通过时，输出为：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">kubectl: OK
</code></pre><!-- 
If the check fails, `sha256` exits with nonzero status and prints output similar to:
-->
<p>验证失败时，<code>sha256</code> 将以非零值退出，并打印如下输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl: FAILED
sha256sum: WARNING: <span style="color:#666">1</span> computed checksum did NOT match
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
   Download the same version of the binary and checksum.
   -->
<p>下载的 kubectl 与校验和文件版本必须相同。</div>
</blockquote>
</li>
</ol>
<!-- 
1. Install kubectl
-->
<ol start="3">
<li>
<p>安装 kubectl</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo install -o root -g root -m <span style="color:#666">0755</span> kubectl /usr/local/bin/kubectl
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
   If you do not have root access on the target system, you can still install kubectl to the `~/.local/bin` directory:
   -->
<p>即使你没有目标系统的 root 权限，仍然可以将 kubectl 安装到目录 <code>~/.local/bin</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">chmod +x kubectl
mkdir -p ~/.local/bin/kubectl
mv ./kubectl ~/.local/bin/kubectl
<span style="color:#080;font-style:italic"># 之后将 ~/.local/bin/kubectl 添加到 $PATH</span>
</code></pre></div></div>
</blockquote>
</li>
</ol>
<!-- 
1. Test to ensure the version you installed is up-to-date:
-->
<ol start="4">
<li>
<p>执行测试，以保障你安装的版本是最新的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl version --client
</code></pre></div></li>
</ol>
<!-- 
### Install using native package management
-->
<h3 id="install-using-native-package-management">用原生包管理工具安装</h3>
<ul class="nav nav-tabs" id="kubectl-install" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#kubectl-install-0" role="tab" aria-controls="kubectl-install-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#kubectl-install-1" role="tab" aria-controls="kubectl-install-1">基于 Red Hat 的发行版</a></li></ul>
<div class="tab-content" id="kubectl-install"><div id="kubectl-install-0" class="tab-pane show active" role="tabpanel" aria-labelledby="kubectl-install-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">

&lt;!--
1. Update the <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> package index and install packages needed to use the Kubernetes <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> repository:
--&gt;
1. 更新 <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> 包索引，并安装使用 Kubernetes <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> 仓库锁需要的包：

   <span style="color:#b44">```</span>shell
   sudo apt-get update
   sudo apt-get install -y apt-transport-https ca-certificates curl
   <span style="color:#b44">```</span>

&lt;!--
2. Download the Google Cloud public signing key:
--&gt;
2. 下载 Google Cloud 公开签名秘钥：

   <span style="color:#b44">```</span>shell
   sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
   <span style="color:#b44">```</span>

&lt;!--
3. Add the Kubernetes <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> repository:
--&gt;
3. 添加 Kubernetes <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> 仓库：

   <span style="color:#b44">```</span>shell
   <span style="color:#a2f">echo</span> <span style="color:#b44">&#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
   <span style="color:#b44">```</span>

&lt;!--
4. Update <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> package index with the new repository and install kubectl:
--&gt;
4. 更新 <span style="color:#b44">`</span>apt<span style="color:#b44">`</span> 包索引，使之包含新的仓库并安装 kubectl：

   <span style="color:#b44">```</span>shell
   sudo apt-get update
   sudo apt-get install -y kubectl
   <span style="color:#b44">```</span>

</code></pre></div></div>
  <div id="kubectl-install-1" class="tab-pane" role="tabpanel" aria-labelledby="kubectl-install-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
cat <span style="color:#b44">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#b44">[kubernetes]
</span><span style="color:#b44">name=Kubernetes
</span><span style="color:#b44">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#b44">enabled=1
</span><span style="color:#b44">gpgcheck=1
</span><span style="color:#b44">repo_gpgcheck=1
</span><span style="color:#b44">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#b44">EOF</span>
yum install -y kubectl
</code></pre></div></div></div>

<!-- 
### Install using other package management
-->
<h3 id="install-using-other-package-management">用其他包管理工具安装</h3>
<ul class="nav nav-tabs" id="other-kubectl-install" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#other-kubectl-install-0" role="tab" aria-controls="other-kubectl-install-0" aria-selected="true">Snap</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#other-kubectl-install-1" role="tab" aria-controls="other-kubectl-install-1">Homebrew</a></li></ul>
<div class="tab-content" id="other-kubectl-install"><div id="other-kubectl-install-0" class="tab-pane show active" role="tabpanel" aria-labelledby="other-kubectl-install-0">

<p><!-- 
If you are on Ubuntu or another Linux distribution that support [snap](https://snapcraft.io/docs/core/install) package manager, kubectl is available as a [snap](https://snapcraft.io/) application.
-->
<p>如果你使用的 Ubuntu 或其他 Linux 发行版，内建支持
<a href="https://snapcraft.io/docs/core/install">snap</a> 包管理工具，
则可用 <a href="https://snapcraft.io/">snap</a> 命令安装 kubectl。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">snap install kubectl --classic
kubectl version --client
</code></pre></div></div>
  <div id="other-kubectl-install-1" class="tab-pane" role="tabpanel" aria-labelledby="other-kubectl-install-1">

<p><!-- 
If you are on Linux and using [Homebrew](https://docs.brew.sh/Homebrew-on-Linux) package manager, kubectl is available for [installation](https://docs.brew.sh/Homebrew-on-Linux#install).
-->
<p>如果你使用 Linux 系统，并且装了 <a href="https://docs.brew.sh/Homebrew-on-Linux">Homebrew</a>
包管理工具，
则可以使用这种方式<a href="https://docs.brew.sh/Homebrew-on-Linux#install">安装</a> kubectl。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">brew install kubectl
kubectl version --client
</code></pre></div></div></div>

<!-- 
## Verify kubectl configuration
-->
<h2 id="verify-kubectl-configration">验证 kubectl 配置</h2>

	<!-- 
---
title: "verify kubectl install"
description: "How to verify kubectl."
headless: true
---
 -->
<!-- 
In order for kubectl to find and access a Kubernetes cluster, it needs a
[kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/),
which is created automatically when you create a cluster using
[kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh)
or successfully deploy a Minikube cluster.
By default, kubectl configuration is located at `~/.kube/config`.

Check that kubectl is properly configured by getting the cluster state:
 -->
<p>为了让 kubectl 能发现并访问 Kubernetes 集群，你需要一个
<a href="/docs/zh/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>，
该文件在
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh">kube-up.sh</a>
创建集群时，或成功部署一个 Miniube 集群时，均会自动生成。
通常，kubectl 的配置信息存放于文件 <code>~/.kube/config</code> 中。</p>
<p>通过获取集群状态的方法，检查是否已恰当的配置了 kubectl：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info
</code></pre></div><!-- 
If you see a URL response, kubectl is correctly configured to access your cluster.

If you see a message similar to the following, kubectl is not configured correctly or is not able to connect to a Kubernetes cluster.
 -->
<p>如果返回一个 URL，则意味着 kubectl 成功的访问到了你的集群。</p>
<p>如果你看到如下所示的消息，则代表 kubectl 配置出了问题，或无法连接到 Kubernetes 集群。</p>
<pre tabindex="0"><code>The connection to the server &lt;server-name:port&gt; was refused - did you specify the right host or port?
（访问 &lt;server-name:port&gt; 被拒绝 - 你指定的主机和端口是否有误？）
</code></pre><!-- 
For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like Minikube to be installed first and then re-run the commands stated above.

If kubectl cluster-info returns the url response but you can't access your cluster, to check whether it is configured properly, use:
 -->
<p>例如，如果你想在自己的笔记本上（本地）运行 Kubernetes 集群，你需要先安装一个 Minikube 这样的工具，然后再重新运行上面的命令。</p>
<p>如果命令 <code>kubectl cluster-info</code> 返回了 url，但你还不能访问集群，那可以用以下命令来检查配置是否妥当：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info dump
</code></pre></div>
<!--
## Optional kubectl configurations and plugins

### Enable shell autocompletion
-->
<h2 id="optional-kubectl-configurations">kubectl 的可选配置和插件</h2>
<h3 id="enable-shell-autocompletion">启用 shell 自动补全功能</h3>
<!-- 
kubectl provides autocompletion support for Bash and Zsh, which can save you a lot of typing.

Below are the procedures to set up autocompletion for Bash and Zsh.
-->
<p>kubectl 为 Bash 和 Zsh 提供自动补全功能，可以减轻许多输入的负担。</p>
<p>下面是为 Bash 和 Zsh 设置自动补全功能的操作步骤。</p>
<ul class="nav nav-tabs" id="kubectl-autocompletion" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#kubectl-autocompletion-0" role="tab" aria-controls="kubectl-autocompletion-0" aria-selected="true">Bash</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#kubectl-autocompletion-1" role="tab" aria-controls="kubectl-autocompletion-1">Zsh</a></li></ul>
<div class="tab-content" id="kubectl-autocompletion"><div id="kubectl-autocompletion-0" class="tab-pane show active" role="tabpanel" aria-labelledby="kubectl-autocompletion-0">

<p><!-- 
---
title: "bash auto-completion on Linux"
description: "Some optional configuration for bash auto-completion on Linux."
headless: true
---
-->
<!-- 
### Introduction
-->
<h3 id="introduction">简介</h3>
<!-- 
The kubectl completion script for Bash can be generated with the command `kubectl completion bash`. Sourcing the completion script in your shell enables kubectl autocompletion.

However, the completion script depends on [**bash-completion**](https://github.com/scop/bash-completion), which means that you have to install this software first (you can test if you have bash-completion already installed by running `type _init_completion`).
-->
<p>kubectl 的 Bash 补全脚本可以用命令 <code>kubectl completion bash</code> 生成。
在 shell 中导入（Sourcing）补全脚本，将启用 kubectl 自动补全功能。</p>
<p>然而，补全脚本依赖于工具 <a href="https://github.com/scop/bash-completion"><strong>bash-completion</strong></a>，
所以要先安装它（可以用命令 <code>type _init_completion</code> 检查 bash-completion 是否已安装）。</p>
<!-- 
### Install bash-completion
-->
<h3 id="install-bash-comletion">安装 bash-completion</h3>
<!-- 
bash-completion is provided by many package managers (see [here](https://github.com/scop/bash-completion#installation)). You can install it with `apt-get install bash-completion` or `yum install bash-completion`, etc.

The above commands create `/usr/share/bash-completion/bash_completion`, which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your `~/.bashrc` file.

To find out, reload your shell and run `type _init_completion`. If the command succeeds, you're already set, otherwise add the following to your `~/.bashrc` file:
-->
<p>很多包管理工具均支持 bash-completion（参见<a href="https://github.com/scop/bash-completion#installation">这里</a>）。
可以通过 <code>apt-get install bash-completion</code> 或 <code>yum install bash-completion</code> 等命令来安装它。</p>
<p>上述命令将创建文件 <code>/usr/share/bash-completion/bash_completion</code>，它是 bash-completion 的主脚本。
依据包管理工具的实际情况，你需要在 <code>~/.bashrc</code> 文件中手工导入此文件。</p>
<p>要查看结果，请重新加载你的 shell，并运行命令 <code>type _init_completion</code>。
如果命令执行成功，则设置完成，否则将下面内容添加到文件 <code>~/.bashrc</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">source</span> /usr/share/bash-completion/bash_completion
</code></pre></div><!-- 
Reload your shell and verify that bash-completion is correctly installed by typing `type _init_completion`.
-->
<p>重新加载 shell，再输入命令 <code>type _init_completion</code> 来验证 bash-completion 的安装状态。</p>
<!-- 
### Enable kubectl autocompletion
-->
<h3 id="enable-kubectl-autocompletion">启动 kubectl 自动补全功能</h3>
<!-- 
You now need to ensure that the kubectl completion script gets sourced in all your shell sessions. There are two ways in which you can do this:
-->
<p>你现在需要确保一点：kubectl 补全脚本已经导入（sourced）到 shell 会话中。
这里有两种验证方法：</p>
<!-- 
- Source the completion script in your `~/.bashrc` file:
-->
<ul>
<li>
<p>在文件 <code>~/.bashrc</code> 中导入（source）补全脚本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;source &lt;(kubectl completion bash)&#39;</span> &gt;&gt;~/.bashrc
</code></pre></div></li>
</ul>
<!-- 
- Add the completion script to the `/etc/bash_completion.d` directory:
-->
<ul>
<li>
<p>将补全脚本添加到目录 <code>/etc/bash_completion.d</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl completion bash &gt;/etc/bash_completion.d/kubectl
</code></pre></div></li>
</ul>
<!-- 
If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<p>如果 kubectl 有关联的别名，你可以扩展 shell 补全来适配此别名：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.bashrc
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.bashrc
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
bash-completion sources all completion scripts in `/etc/bash_completion.d`.
-->
<p>bash-completion 负责导入 <code>/etc/bash_completion.d</code> 目录中的所有补全脚本。</div>
</blockquote>
<!-- 
Both approaches are equivalent. After reloading your shell, kubectl autocompletion should be working.
-->
<p>两种方式的效果相同。重新加载 shell 后，kubectl 自动补全功能即可生效。</p>
</div>
  <div id="kubectl-autocompletion-1" class="tab-pane" role="tabpanel" aria-labelledby="kubectl-autocompletion-1">

<p><!-- 
---
title: "zsh auto-completion"
description: "Some optional configuration for zsh auto-completion."
headless: true
---
-->
<!-- 
The kubectl completion script for Zsh can be generated with the command `kubectl completion zsh`. Sourcing the completion script in your shell enables kubectl autocompletion.

To do so in all your shell sessions, add the following to your `~/.zshrc` file:
-->
<p>kubectl 通过命令 <code>kubectl completion zsh</code> 生成 Zsh 自动补全脚本。
在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。</p>
<p>为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 <code>~/.zshrc</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">source</span> &lt;<span style="color:#666">(</span>kubectl completion zsh<span style="color:#666">)</span>
</code></pre></div><!-- 
If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<p>如果你为 kubectl 定义了别名，可以扩展脚本补全，以兼容该别名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.zshrc
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.zshrc
</code></pre></div><!-- 
After reloading your shell, kubectl autocompletion should be working.

If you get an error like `complete:13: command not found: compdef`, then add the following to the beginning of your `~/.zshrc` file:
-->
<p>重新加载 shell 后，kubectl 自动补全功能将立即生效。</p>
<p>如果你收到 <code>complete:13: command not found: compdef</code> 这样的错误提示，那请将下面内容添加到 <code>~/.zshrc</code> 文件的开头：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh">autoload -Uz compinit
compinit
</code></pre></div></div></div>

<!--
### Install `kubectl convert` plugin
-->
<h3 id="安装-kubectl-convert-插件">安装 <code>kubectl convert</code> 插件</h3>

	<!--
---
title: "kubectl-convert overview"
description: >-
  A kubectl plugin that allows you to convert manifests from one version
  of a Kubernetes API to a different version.
headless: true
---
-->
<!--
A plugin for Kubernetes command-line tool `kubectl`, which allows you to convert manifests between different API 
versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.
For more info, visit [migrate to non deprecated apis](/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis)
-->
<p>一个 Kubernetes 命令行工具 <code>kubectl</code> 的插件，允许你将清单在不同 API 版本间转换。
在将清单迁移到具有较新 Kubernetes 版本的未弃用 API 版本时，这个插件特别有用。
更多信息请访问 <a href="/zh/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis">迁移到非弃用 API</a></p>

<!--
1. Download the latest release with the command:
-->
<ol>
<li>
<p>用以下命令下载最新发行版：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -LO https://dl.k8s.io/release/<span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span>/bin/linux/amd64/kubectl-convert
</code></pre></div></li>
</ol>
<!--
1. Validate the binary (optional)

   Download the kubectl-convert checksum file:
-->
<ol>
<li>
<p>验证该可执行文件（可选步骤）</p>
<p>下载 kubectl-convert 校验和文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/linux/amd64/kubectl-convert.sha256&#34;</span>
</code></pre></div><!--
Validate the kubectl-convert binary against the checksum file:
-->
<p>基于校验和，验证 kubectl-convert 的可执行文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>&lt;kubectl-convert.sha256<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44"> kubectl-convert&#34;</span> | sha256sum --check
</code></pre></div><!--
If valid, the output is:
-->
<p>验证通过时，输出为：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">kubectl-convert: OK
</code></pre><!--
If the check fails, `sha256` exits with nonzero status and prints output similar to:
-->
<p>验证失败时，<code>sha256</code> 将以非零值退出，并打印输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl-convert: FAILED
sha256sum: WARNING: <span style="color:#666">1</span> computed checksum did NOT match
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
   Download the same version of the binary and checksum.
   -->
<p>下载相同版本的可执行文件和校验和。</div>
</blockquote>
</li>
</ol>
<!--
1. Install kubectl-convert
-->
<ol>
<li>
<p>安装 kubectl-convert</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo install -o root -g root -m <span style="color:#666">0755</span> kubectl-convert /usr/local/bin/kubectl-convert
</code></pre></div></li>
</ol>
<!--
1. Verify plugin is successfully installed
-->
<ol>
<li>
<p>验证插件是否安装成功</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl convert --help
</code></pre></div><!--
If you do not see an error, it means the plugin is successfully installed.
-->
<p>如果你没有看到任何错误就代表插件安装成功了。</p>
</li>
</ol>
<h2 id="接下来">接下来</h2>

	<!-- 
---
title: "What's next?"
description: "What's next after installing kubectl."
headless: true
---
-->
<!-- 
* [Install Minikube](https://minikube.sigs.k8s.io/docs/start/)
* See the [getting started guides](/docs/setup/) for more about creating clusters.
* [Learn how to launch and expose your application.](/docs/tasks/access-application-cluster/service-access-application-cluster/)
* If you need access to a cluster you didn't create, see the
  [Sharing Cluster Access document](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/).
* Read the [kubectl reference docs](/docs/reference/kubectl/kubectl/)
-->
<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/start/">安装 Minikube</a></li>
<li>有关创建集群的更多信息，请参阅<a href="/zh/docs/setup/">入门指南</a>.</li>
<li><a href="/zh/docs/tasks/access-application-cluster/service-access-application-cluster/">学习如何启动并对外公开你的应用程序。</a></li>
<li>如果你需要访问其他人创建的集群，请参阅
<a href="/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">共享集群接入文档</a>.</li>
<li>阅读 <a href="/zh/docs/reference/kubectl/kubectl/">kubectl 参考文档</a></li>
</ul>


</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-961fc70b732cb8df4fd11a3463b6545c">1.2 - 在 macOS 系统上安装和设置 kubectl</h1>
    
	<!-- 
reviewers:
- mikedanese
title: Install and Set Up kubectl on macOS
content_type: task
weight: 10
card:
  name: tasks
  weight: 20
  title: Install kubectl on macOS
-->
<h2 id="准备开始">准备开始</h2>
<!-- 
You must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.22 client can communicate with v1.21, v1.22, and v1.23 control planes.
Using the latest version of kubectl helps avoid unforeseen issues.
-->
<p>kubectl 版本和集群之间的差异必须在一个小版本号之内。
例如：v1.22 版本的客户端能与 v1.21、
v1.22 和 v1.23 版本的控制面通信。
用最新版本的 kubectl 有助于避免不可预见的问题。</p>
<!-- 
## Install kubectl on macOS
-->
<h2 id="install-kubectl-on-macos">在 macOS 系统上安装 kubectl</h2>
<!-- 
The following methods exist for installing kubectl on macOS:
-->
<p>在 macOS 系统上安装 kubectl 有如下方法：</p>
<!--
- [Install kubectl binary with curl on macOS](#install-kubectl-binary-with-curl-on-macos)
- [Install with Homebrew on macOS](#install-with-homebrew-on-macos)
- [Install with Macports on macOS](#install-with-macports-on-macos)
- [Install on macOS as part of the Google Cloud SDK](#install-on-macos-as-part-of-the-google-cloud-sdk)
-->
<ul>
<li><a href="#install-kubectl-binary-with-curl-on-macos">用 curl 在 macOS 系统上安装 kubectl</a></li>
<li><a href="#install-with-homebrew-on-macos">用 Homebrew 在 macOS 系统上安装</a></li>
<li><a href="#install-with-macports-on-macos">用 Macports 在 macOS 上安装</a></li>
<li><a href="#install-on-macos-as-part-of-the-google-cloud-sdk">作为谷歌云 SDK 的一部分，在 macOS 上安装</a></li>
</ul>
<!-- 
### Install kubectl binary with curl on macOS {#install-kubectl-binary-with-curl-on-macos}
-->
<h3 id="install-kubectl-binary-with-curl-on-macos">用 curl 在 macOS 系统上安装 kubectl</h3>
<!-- 
1. Download the latest release:
-->
<ol>
<li>
<p>下载最新的发行版：</p>
<ul class="nav nav-tabs" id="download-binary-macos" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#download-binary-macos-0" role="tab" aria-controls="download-binary-macos-0" aria-selected="true">Intel</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#download-binary-macos-1" role="tab" aria-controls="download-binary-macos-1">Apple Silicon</a></li></ul>
<div class="tab-content" id="download-binary-macos"><div id="download-binary-macos-0" class="tab-pane show active" role="tabpanel" aria-labelledby="download-binary-macos-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/amd64/kubectl&#34;</span>
   </code></pre></div></div>
  <div id="download-binary-macos-1" class="tab-pane" role="tabpanel" aria-labelledby="download-binary-macos-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/arm64/kubectl&#34;</span>
   </code></pre></div></div></div>

<blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
   To download a specific version, replace the `$(curl -L -s https://dl.k8s.io/release/stable.txt)` portion of the command with the specific version.

   For example, to download version v1.22.0 on Intel macOS, type:

   ```bash
   curl -LO "https://dl.k8s.io/release/v1.22.0/bin/darwin/arm64/kubectl"
   ```

   -->
<p>如果需要下载某个指定的版本，用该指定版本号替换掉命令的这个部分：<code>$(curl -L -s https://dl.k8s.io/release/stable.txt)</code>。
例如：要为 Intel macOS 系统下载 v1.22.0 版本，则输入：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/v1.22.0/bin/darwin/amd64/kubectl&#34;</span>
</code></pre></div>   <!--
   And for macOS on Apple Silicon, type:
   -->
<p>对于 Apple Silicon 版本的 macOS，输入：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/v1.22.0/bin/darwin/arm64/kubectl&#34;</span>
</code></pre></div></div>
</blockquote>
</li>
</ol>
<!-- 
1. Validate the binary (optional)

   Download the kubectl checksum file:
-->
<ol start="2">
<li>
<p>验证可执行文件（可选操作）</p>
<p>下载 kubectl 的校验和文件：</p>
<ul class="nav nav-tabs" id="download-checksum-macos" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#download-checksum-macos-0" role="tab" aria-controls="download-checksum-macos-0" aria-selected="true">Intel</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#download-checksum-macos-1" role="tab" aria-controls="download-checksum-macos-1">Apple Silicon</a></li></ul>
<div class="tab-content" id="download-checksum-macos"><div id="download-checksum-macos-0" class="tab-pane show active" role="tabpanel" aria-labelledby="download-checksum-macos-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/amd64/kubectl.sha256&#34;</span>
   </code></pre></div></div>
  <div id="download-checksum-macos-1" class="tab-pane" role="tabpanel" aria-labelledby="download-checksum-macos-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/arm64/kubectl.sha256&#34;</span>
   </code></pre></div></div></div>

<!-- 
Validate the kubectl binary against the checksum file:
-->
<p>根据校验和文件，验证 kubectl：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>&lt;kubectl.sha256<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">  kubectl&#34;</span> | shasum -a <span style="color:#666">256</span> --check
</code></pre></div><!-- 
If valid, the output is:
-->
<p>验证通过时，输出如下：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">kubectl: OK
</code></pre><!-- 
If the check fails, `shasum` exits with nonzero status and prints output similar to:
-->
<p>验证失败时，<code>shasum</code> 将以非零值退出，并打印如下输出：</p>
<pre tabindex="0"><code>kubectl: FAILED
shasum: WARNING: 1 computed checksum did NOT match
</code></pre><blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
   Download the same version of the binary and checksum.
   -->
<p>下载的 kubectl 与校验和文件版本要相同。</div>
</blockquote>
</li>
</ol>
<!-- 
1. Make the kubectl binary executable.
-->
<ol start="3">
<li>
<p>将 kubectl 置为可执行文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">chmod +x ./kubectl
</code></pre></div></li>
</ol>
<!-- 
1. Move the kubectl binary to a file location on your system `PATH`.
-->
<ol start="4">
<li>
<p>将可执行文件 kubectl 移动到系统可寻址路径 <code>PATH</code> 内的一个位置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo mv ./kubectl /usr/local/bin/kubectl
sudo chown root: /usr/local/bin/kubectl
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
   Make sure `/usr/local/bin` is in your PATH environment variable.
   -->
<p>确保 <code>/usr/local/bin</code> 在你的 PATH 环境变量中。</div>
</blockquote>
</li>
</ol>
<!-- 
1. Test to ensure the version you installed is up-to-date:
-->
<ol start="5">
<li>
<p>测试一下，确保你安装的是最新的版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl version --client
</code></pre></div></li>
</ol>
<!-- 
### Install with Homebrew on macOS {#install-with-homebrew-on-macos}
-->
<h3 id="install-with-homebrew-on-macos">用 Homebrew 在 macOS 系统上安装</h3>
<!-- 
If you are on macOS and using [Homebrew](https://brew.sh/) package manager, you can install kubectl with Homebrew.
-->
<p>如果你是 macOS 系统，且用的是 <a href="https://brew.sh/">Homebrew</a> 包管理工具，
则可以用 Homebrew 安装 kubectl。</p>
<!-- 
1. Run the installation command:
-->
<ol>
<li>
<p>运行安装命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install kubectl 
</code></pre></div><p>或</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install kubernetes-cli
</code></pre></div></li>
</ol>
<!-- 
1. Test to ensure the version you installed is up-to-date:
-->
<ol start="2">
<li>
<p>测试一下，确保你安装的是最新的版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl version --client
</code></pre></div></li>
</ol>
<!-- 
### Install with Macports on macOS {#install-with-macports-on-macos}
-->
<h3 id="install-with-macports-on-macos">用 Macports 在 macOS 上安装</h3>
<!-- 
If you are on macOS and using [Macports](https://macports.org/) package manager, you can install kubectl with Macports.
-->
<p>如果你用的是 macOS，且用 <a href="https://macports.org/">Macports</a> 包管理工具，则你可以用 Macports 安装kubectl。</p>
<!-- 
1. Run the installation command:
-->
<ol>
<li>
<p>运行安装命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo port selfupdate
sudo port install kubectl
</code></pre></div></li>
</ol>
<!-- 
1. Test to ensure the version you installed is up-to-date:
-->
<ol start="2">
<li>
<p>测试一下，确保你安装的是最新的版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl version --client
</code></pre></div></li>
</ol>
<!-- 
### Install on macOS as part of the Google Cloud SDK {#install-on-macos-as-part-of-the-google-cloud-sdk}
-->
<h3 id="install-on-macos-as-part-of-the-google-cloud-sdk">作为谷歌云 SDK 的一部分，在 macOS 上安装</h3>

	<!-- 
---
title: "gcloud kubectl install"
description: "How to install kubectl with gcloud snippet for inclusion in each OS-specific tab."
headless: true
---
-->
<!-- 
You can install kubectl as part of the Google Cloud SDK.
-->
<p>kubectl 可以作为 Google Cloud SDK 的一部分被安装。</p>
<!-- 
1. Install the [Google Cloud SDK](https://cloud.google.com/sdk/).
1. Run the `kubectl` installation command:
-->
<ol>
<li>
<p>安装 <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a>。</p>
</li>
<li>
<p>运行安装 <code>kubectl</code> 的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud components install kubectl
</code></pre></div><!-- 
1. Test to ensure the version you installed is up-to-date:
-->
</li>
<li>
<p>验证一下，确保安装的是最新的版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl version --client
</code></pre></div></li>
</ol>

<!-- 
## Verify kubectl configuration {#verify-kubectl-configuration}
-->
<h2 id="verify-kubectl-configuration">验证 kubectl 配置</h2>

	<!-- 
---
title: "verify kubectl install"
description: "How to verify kubectl."
headless: true
---
 -->
<!-- 
In order for kubectl to find and access a Kubernetes cluster, it needs a
[kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/),
which is created automatically when you create a cluster using
[kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh)
or successfully deploy a Minikube cluster.
By default, kubectl configuration is located at `~/.kube/config`.

Check that kubectl is properly configured by getting the cluster state:
 -->
<p>为了让 kubectl 能发现并访问 Kubernetes 集群，你需要一个
<a href="/docs/zh/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>，
该文件在
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh">kube-up.sh</a>
创建集群时，或成功部署一个 Miniube 集群时，均会自动生成。
通常，kubectl 的配置信息存放于文件 <code>~/.kube/config</code> 中。</p>
<p>通过获取集群状态的方法，检查是否已恰当的配置了 kubectl：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info
</code></pre></div><!-- 
If you see a URL response, kubectl is correctly configured to access your cluster.

If you see a message similar to the following, kubectl is not configured correctly or is not able to connect to a Kubernetes cluster.
 -->
<p>如果返回一个 URL，则意味着 kubectl 成功的访问到了你的集群。</p>
<p>如果你看到如下所示的消息，则代表 kubectl 配置出了问题，或无法连接到 Kubernetes 集群。</p>
<pre tabindex="0"><code>The connection to the server &lt;server-name:port&gt; was refused - did you specify the right host or port?
（访问 &lt;server-name:port&gt; 被拒绝 - 你指定的主机和端口是否有误？）
</code></pre><!-- 
For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like Minikube to be installed first and then re-run the commands stated above.

If kubectl cluster-info returns the url response but you can't access your cluster, to check whether it is configured properly, use:
 -->
<p>例如，如果你想在自己的笔记本上（本地）运行 Kubernetes 集群，你需要先安装一个 Minikube 这样的工具，然后再重新运行上面的命令。</p>
<p>如果命令 <code>kubectl cluster-info</code> 返回了 url，但你还不能访问集群，那可以用以下命令来检查配置是否妥当：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info dump
</code></pre></div>
<!-- 
## Optional kubectl configurations and plugins {#optional-kubectl-configurations}

### Enable shell autocompletion {#enable-shell-autocompletion}
-->
<h2 id="optional-kubectl-configurations-and-plugins">可选的 kubectl 配置和插件</h2>
<h3 id="enable-shell-autocompletion">启用 shell 自动补全功能</h3>
<!-- 
kubectl provides autocompletion support for Bash and Zsh, which can save you a lot of typing.

Below are the procedures to set up autocompletion for Bash and Zsh.
-->
<p>kubectl 为 Bash 和 Zsh 提供自动补全功能，这可以节省许多输入的麻烦。</p>
<p>下面是为 Bash 和 Zsh 设置自动补全功能的操作步骤。</p>
<ul class="nav nav-tabs" id="kubectl-autocompletion" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#kubectl-autocompletion-0" role="tab" aria-controls="kubectl-autocompletion-0" aria-selected="true">Bash</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#kubectl-autocompletion-1" role="tab" aria-controls="kubectl-autocompletion-1">Zsh</a></li></ul>
<div class="tab-content" id="kubectl-autocompletion"><div id="kubectl-autocompletion-0" class="tab-pane show active" role="tabpanel" aria-labelledby="kubectl-autocompletion-0">

<p><!-- 
title: "bash auto-completion on macOS"
description: "Some optional configuration for bash auto-completion on macOS."
headless: true
 -->
<!-- 
### Introduction
-->
<h3 id="简介">简介</h3>
<!-- 
The kubectl completion script for Bash can be generated with `kubectl completion bash`. Sourcing this script in your shell enables kubectl completion.

However, the kubectl completion script depends on [**bash-completion**](https://github.com/scop/bash-completion) which you thus have to previously install.
-->
<p>kubectl 的 Bash 补全脚本可以通过 <code>kubectl completion bash</code> 命令生成。
在你的 shell 中导入（Sourcing）这个脚本即可启用补全功能。</p>
<p>此外，kubectl 补全脚本依赖于工具 <a href="https://github.com/scop/bash-completion"><strong>bash-completion</strong></a>，
所以你必须先安装它。</p>
<blockquote class="warning callout">
  <div><strong>警告：</strong> <!-- 
There are two versions of bash-completion, v1 and v2. V1 is for Bash 3.2 (which is the default on macOS), and v2 is for Bash 4.1+. The kubectl completion script **doesn't work** correctly with bash-completion v1 and Bash 3.2. It requires **bash-completion v2** and **Bash 4.1+**. Thus, to be able to correctly use kubectl completion on macOS, you have to install and use Bash 4.1+ ([*instructions*](https://itnext.io/upgrading-bash-on-macos-7138bd1066ba)). The following instructions assume that you use Bash 4.1+ (that is, any Bash version of 4.1 or newer).
-->
<p>bash-completion 有两个版本：v1 和 v2。v1 对应 Bash3.2（也是 macOS 的默认安装版本），v2 对应 Bash 4.1+。
kubectl 的补全脚本<strong>无法适配</strong> bash-completion v1 和 Bash 3.2。
必须为它配备  <strong>bash-completion v2</strong> 和 <strong>Bash 4.1+</strong>。
有鉴于此，为了在 macOS 上使用 kubectl 补全功能，你必须要安装和使用 Bash 4.1+
(<a href="https://itnext.io/upgrading-bash-on-macos-7138bd1066ba"><em>说明</em></a>)。
后续说明假定你用的是 Bash 4.1+（也就是 Bash 4.1 或更新的版本）</div>
</blockquote>

<!-- 
### Upgrade Bash
-->
<h3 id="升级-bash">升级 Bash</h3>
<!-- 
The instructions here assume you use Bash 4.1+. You can check your Bash's version by running:
-->
<p>后续说明假定你已使用 Bash 4.1+。你可以运行以下命令检查 Bash 版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b8860b">$BASH_VERSION</span>
</code></pre></div><!-- 
If it is too old, you can install/upgrade it using Homebrew:
-->
<p>如果版本太旧，可以用 Homebrew 安装/升级：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install bash
</code></pre></div><!-- 
Reload your shell and verify that the desired version is being used:
-->
<p>重新加载 shell，并验证所需的版本已经生效：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b8860b">$BASH_VERSION</span> <span style="color:#b8860b">$SHELL</span>
</code></pre></div><!-- 
Homebrew usually installs it at `/usr/local/bin/bash`.
-->
<p>Homebrew 通常把它安装为 <code>/usr/local/bin/bash</code>。</p>
<!-- 
### Install bash-completion
-->
<h3 id="安装-bash-completion">安装 bash-completion</h3>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
As mentioned, these instructions assume you use Bash 4.1+, which means you will install bash-completion v2 (in contrast to Bash 3.2 and bash-completion v1, in which case kubectl completion won't work).
-->
<p>如前所述，本说明假定你使用的 Bash 版本为 4.1+，这意味着你要安装 bash-completion v2
（不同于 Bash 3.2 和 bash-completion v1，kubectl 的补全功能在该场景下无法工作）。</div>
</blockquote>
<!-- 
You can test if you have bash-completion v2 already installed with `type _init_completion`. If not, you can install it with Homebrew:
-->
<p>你可以用命令 <code>type _init_completion</code> 测试 bash-completion v2 是否已经安装。
如未安装，用 Homebrew 来安装它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install bash-completion@2
</code></pre></div><!-- 
As stated in the output of this command, add the following to your `~/.bash_profile` file:
-->
<p>如命令的输出信息所显示的，将如下内容添加到文件 <code>~/.bash_profile</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">export</span> <span style="color:#b8860b">BASH_COMPLETION_COMPAT_DIR</span><span style="color:#666">=</span><span style="color:#b44">&#34;/usr/local/etc/bash_completion.d&#34;</span>
<span style="color:#666">[[</span> -r <span style="color:#b44">&#34;/usr/local/etc/profile.d/bash_completion.sh&#34;</span> <span style="color:#666">]]</span> <span style="color:#666">&amp;&amp;</span> . <span style="color:#b44">&#34;/usr/local/etc/profile.d/bash_completion.sh&#34;</span>
</code></pre></div><!-- 
Reload your shell and verify that bash-completion v2 is correctly installed with `type _init_completion`.
-->
<p>重新加载 shell，并用命令 <code>type _init_completion</code> 验证 bash-completion v2 已经恰当的安装。</p>
<!-- 
### Enable kubectl autocompletion
-->
<h3 id="启用-kubectl-自动补全功能">启用 kubectl 自动补全功能</h3>
<!-- 
You now have to ensure that the kubectl completion script gets sourced in all your shell sessions. There are multiple ways to achieve this:

- Source the completion script in your `~/.bash_profile` file:
-->
<p>你现在需要确保在所有的 shell 环境中均已导入（sourced） kubectl 的补全脚本，
有若干种方法可以实现这一点：</p>
<ul>
<li>
<p>在文件 <code>~/.bash_profile</code> 中导入（Source）补全脚本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;source &lt;(kubectl completion bash)&#39;</span> &gt;&gt;~/.bash_profile
</code></pre></div></li>
</ul>
<!-- 
- Add the completion script to the `/usr/local/etc/bash_completion.d` directory:
-->
<ul>
<li>
<p>将补全脚本添加到目录 <code>/usr/local/etc/bash_completion.d</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl completion bash &gt;/usr/local/etc/bash_completion.d/kubectl
</code></pre></div></li>
</ul>
<!-- 
- If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<ul>
<li>
<p>如果你为 kubectl 定义了别名，则可以扩展 shell 补全来兼容该别名：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.bash_profile
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.bash_profile
</code></pre></div></li>
</ul>
<!-- 
- If you installed kubectl with Homebrew (as explained [here](/docs/tasks/tools/install-kubectl-macos/#install-with-homebrew-on-macos)), then the kubectl completion script should already be in `/usr/local/etc/bash_completion.d/kubectl`. In that case, you don't need to do anything.
-->
<ul>
<li>
<p>如果你是用 Homebrew 安装的 kubectl（如
<a href="/zh/docs/tasks/install-with-homebrew-on-macos/#install-with-homebrew-on-macos">此页面</a>
所描述），则kubectl 补全脚本应该已经安装到目录 <code>/usr/local/etc/bash_completion.d/kubectl</code>
中了。这种情况下，你什么都不需要做。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
  The Homebrew installation of bash-completion v2 sources all the files in the `BASH_COMPLETION_COMPAT_DIR` directory, that's why the latter two methods work.
  -->
<p>用 Hommbrew 安装的 bash-completion v2 会初始化 目录 <code>BASH_COMPLETION_COMPAT_DIR</code>
中的所有文件，这就是后两种方法能正常工作的原因。</div>
</blockquote>
</li>
</ul>
<!-- 
In any case, after reloading your shell, kubectl completion should be working.
-->
<p>总之，重新加载 shell 之后，kubectl 补全功能将立即生效。</p>
</div>
  <div id="kubectl-autocompletion-1" class="tab-pane" role="tabpanel" aria-labelledby="kubectl-autocompletion-1">

<p><!-- 
---
title: "zsh auto-completion"
description: "Some optional configuration for zsh auto-completion."
headless: true
---
-->
<!-- 
The kubectl completion script for Zsh can be generated with the command `kubectl completion zsh`. Sourcing the completion script in your shell enables kubectl autocompletion.

To do so in all your shell sessions, add the following to your `~/.zshrc` file:
-->
<p>kubectl 通过命令 <code>kubectl completion zsh</code> 生成 Zsh 自动补全脚本。
在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。</p>
<p>为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 <code>~/.zshrc</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">source</span> &lt;<span style="color:#666">(</span>kubectl completion zsh<span style="color:#666">)</span>
</code></pre></div><!-- 
If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<p>如果你为 kubectl 定义了别名，可以扩展脚本补全，以兼容该别名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.zshrc
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.zshrc
</code></pre></div><!-- 
After reloading your shell, kubectl autocompletion should be working.

If you get an error like `complete:13: command not found: compdef`, then add the following to the beginning of your `~/.zshrc` file:
-->
<p>重新加载 shell 后，kubectl 自动补全功能将立即生效。</p>
<p>如果你收到 <code>complete:13: command not found: compdef</code> 这样的错误提示，那请将下面内容添加到 <code>~/.zshrc</code> 文件的开头：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh">autoload -Uz compinit
compinit
</code></pre></div></div></div>

<!--
### Install `kubectl convert` plugin
-->
<h3 id="安装-kubectl-convert-插件">安装 <code>kubectl convert</code> 插件</h3>

	<!--
---
title: "kubectl-convert overview"
description: >-
  A kubectl plugin that allows you to convert manifests from one version
  of a Kubernetes API to a different version.
headless: true
---
-->
<!--
A plugin for Kubernetes command-line tool `kubectl`, which allows you to convert manifests between different API 
versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.
For more info, visit [migrate to non deprecated apis](/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis)
-->
<p>一个 Kubernetes 命令行工具 <code>kubectl</code> 的插件，允许你将清单在不同 API 版本间转换。
在将清单迁移到具有较新 Kubernetes 版本的未弃用 API 版本时，这个插件特别有用。
更多信息请访问 <a href="/zh/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis">迁移到非弃用 API</a></p>

<!--
1. Download the latest release with the command:
-->
<ol>
<li>
<p>用以下命令下载最新发行版：</p>
<ul class="nav nav-tabs" id="download-convert-binary-macos" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#download-convert-binary-macos-0" role="tab" aria-controls="download-convert-binary-macos-0" aria-selected="true">Intel</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#download-convert-binary-macos-1" role="tab" aria-controls="download-convert-binary-macos-1">Apple Silicon</a></li></ul>
<div class="tab-content" id="download-convert-binary-macos"><div id="download-convert-binary-macos-0" class="tab-pane show active" role="tabpanel" aria-labelledby="download-convert-binary-macos-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/amd64/kubectl-convert&#34;</span>
   </code></pre></div></div>
  <div id="download-convert-binary-macos-1" class="tab-pane" role="tabpanel" aria-labelledby="download-convert-binary-macos-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/arm64/kubectl-convert&#34;</span>
   </code></pre></div></div></div>

</li>
</ol>
<!--
1. Validate the binary (optional)

   Download the kubectl-convert checksum file:
-->
<ol>
<li>
<p>验证该可执行文件（可选步骤）</p>
<p>下载 kubectl-convert 校验和文件：</p>
<ul class="nav nav-tabs" id="download-convert-checksum-macos" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#download-convert-checksum-macos-0" role="tab" aria-controls="download-convert-checksum-macos-0" aria-selected="true">Intel</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#download-convert-checksum-macos-1" role="tab" aria-controls="download-convert-checksum-macos-1">Apple Silicon</a></li></ul>
<div class="tab-content" id="download-convert-checksum-macos"><div id="download-convert-checksum-macos-0" class="tab-pane show active" role="tabpanel" aria-labelledby="download-convert-checksum-macos-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/amd64/kubectl-convert.sha256&#34;</span>
   </code></pre></div></div>
  <div id="download-convert-checksum-macos-1" class="tab-pane" role="tabpanel" aria-labelledby="download-convert-checksum-macos-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
   curl -LO <span style="color:#b44">&#34;https://dl.k8s.io/release/</span><span style="color:#a2f;font-weight:bold">$(</span>curl -L -s https://dl.k8s.io/release/stable.txt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">/bin/darwin/arm64/kubectl-convert.sha256&#34;</span>
   </code></pre></div></div></div>

<!--
Validate the kubectl-convert binary against the checksum file:
-->
<p>基于校验和，验证 kubectl-convert 的可执行文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>&lt;kubectl-convert.sha256<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">  kubectl-convert&#34;</span> | shasum -a <span style="color:#666">256</span> --check
</code></pre></div><!--
If valid, the output is:
-->
<p>验证通过时，输出为：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">kubectl-convert: OK
</code></pre><!--
If the check fails, `shasum` exits with nonzero status and prints output similar to:
-->
<p>验证失败时，<code>sha256</code> 将以非零值退出，并打印输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl-convert: FAILED
shasum: WARNING: <span style="color:#666">1</span> computed checksum did NOT match
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
   Download the same version of the binary and checksum.
   -->
<p>下载相同版本的可执行文件和校验和。</div>
</blockquote>
</li>
</ol>
<!--
1. Make kubectl-convert binary executable
-->
<ol>
<li>
<p>使 kubectl-convert 二进制文件可执行</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">chmod +x ./kubectl-convert
</code></pre></div></li>
</ol>
<!--
1. Move the kubectl-convert binary to a file location on your system `PATH`.
-->
<ol>
<li>
<p>将 kubectl-convert 可执行文件移动到系统 <code>PATH</code> 环境变量中的一个位置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo mv ./kubectl-convert /usr/local/bin/kubectl-convert
sudo chown root: /usr/local/bin/kubectl-convert
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
   Make sure `/usr/local/bin` is in your PATH environment variable.
   -->
<p>确保你的 PATH 环境变量中存在 <code>/usr/local/bin</code></div>
</blockquote>
</li>
</ol>
<!--
1. Verify plugin is successfully installed
-->
<ol>
<li>
<p>验证插件是否安装成功</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl convert --help
</code></pre></div><!--
If you do not see an error, it means the plugin is successfully installed.
-->
<p>如果你没有看到任何错误就代表插件安装成功了。</p>
</li>
</ol>
<h2 id="接下来">接下来</h2>

	<!-- 
---
title: "What's next?"
description: "What's next after installing kubectl."
headless: true
---
-->
<!-- 
* [Install Minikube](https://minikube.sigs.k8s.io/docs/start/)
* See the [getting started guides](/docs/setup/) for more about creating clusters.
* [Learn how to launch and expose your application.](/docs/tasks/access-application-cluster/service-access-application-cluster/)
* If you need access to a cluster you didn't create, see the
  [Sharing Cluster Access document](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/).
* Read the [kubectl reference docs](/docs/reference/kubectl/kubectl/)
-->
<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/start/">安装 Minikube</a></li>
<li>有关创建集群的更多信息，请参阅<a href="/zh/docs/setup/">入门指南</a>.</li>
<li><a href="/zh/docs/tasks/access-application-cluster/service-access-application-cluster/">学习如何启动并对外公开你的应用程序。</a></li>
<li>如果你需要访问其他人创建的集群，请参阅
<a href="/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">共享集群接入文档</a>.</li>
<li>阅读 <a href="/zh/docs/reference/kubectl/kubectl/">kubectl 参考文档</a></li>
</ul>


</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2cc93d3011d707aeb6564bab02048f7a">1.3 - 在 Windows 上安装 kubectl</h1>
    
	<!-- 
reviewers:
- mikedanese
title: Install and Set Up kubectl on Windows
content_type: task
weight: 10
card:
  name: tasks
  weight: 20
  title: Install kubectl on Windows
-->
<h2 id="准备开始">准备开始</h2>
<!-- 
You must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.22 client can communicate with v1.21, v1.22, and v1.23 control planes.
Using the latest version of kubectl helps avoid unforeseen issues.
-->
<p>kubectl 版本和集群版本之间的差异必须在一个小版本号内。
例如：v1.22 版本的客户端能与 v1.21、
v1.22 和 v1.23 版本的控制面通信。
用最新版的 kubectl 有助于避免不可预见的问题。</p>
<!-- 
## Install kubectl on Windows
-->
<h2 id="install-kubectl-on-windows">在 Windows 上安装 kubectl</h2>
<!-- 
The following methods exist for installing kubectl on Windows:
-->
<p>在 Windows 系统中安装 kubectl 有如下几种方法：</p>
<ul>
<li><a href="#install-kubectl-binary-with-curl-on-windows">用 curl 在 Windows 上安装 kubectl</a></li>
<li><a href="#install-on-windows-using-chocolatey-or-scoop">在 Windows 上用 Chocolatey 或 Scoop 安装</a></li>
</ul>
<!-- 
### Install kubectl binary with curl on Windows
-->
<h3 id="install-kubectl-binary-with-curl-on-windows">用 curl 在 Windows 上安装 kubectl</h3>
<!-- 
1. Download the [latest release v1.22.0](https://dl.k8s.io/release/v1.22.0/bin/windows/amd64/kubectl.exe).

   Or if you have `curl` installed, use this command:
-->
<ol>
<li>
<p>下载 <a href="https://dl.k8s.io/release/v1.22.0/bin/windows/amd64/kubectl.exe">最新发行版 v1.22.0</a>。</p>
<p>如果你已安装了 <code>curl</code>,也可以使用此命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">curl </span>-LO https<span style="">:</span>//dl.k8s.io/release/<span style="color:#a2f">/bin/windows/amd64/kubectl.exe
</code></pre></div><!-- 
   To find out the latest stable version (for example, for scripting), take a look at [https://dl.k8s.io/release/stable.txt](https://dl.k8s.io/release/stable.txt).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 要想找到最新稳定的版本（例如：为了编写脚本），可以看看这里 <a href="https://dl.k8s.io/release/stable.txt">https://dl.k8s.io/release/stable.txt</a>。</div>
</blockquote>
<!-- 
1. Validate the binary (optional)

   Download the kubectl checksum file:
-->
</li>
<li>
<p>验证该可执行文件（可选步骤）</p>
<p>下载 kubectl 校验和文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">curl </span>-LO https<span style="">:</span>//dl.k8s.io/<span style="color:#a2f">/bin/windows/amd64/kubectl.exe.sha256
</code></pre></div><!-- 
Validate the kubectl binary against the checksum file:
-->
<p>基于校验和文件，验证 kubectl 的可执行文件：</p>
<!-- 
- Using Command Prompt to manually compare `CertUtil`'s output to the checksum file downloaded:
-->
<ul>
<li>
<p>在命令行环境中，手工对比 <code>CertUtil</code> 命令的输出与校验和文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cmd" data-lang="cmd">CertUtil -hashfile kubectl.exe SHA256
<span style="color:#a2f;font-weight:bold">type</span> kubectl.exe.sha256
</code></pre></div></li>
</ul>
<!-- 
- Using PowerShell to automate the verification using the `-eq` operator to get a `True` or `False` result:
-->
<ul>
<li>
<p>用 PowerShell 自动验证，用运算符 <code>-eq</code> 来直接取得 <code>True</code> 或 <code>False</code> 的结果：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">$($(CertUtil -hashfile .\kubectl.exe SHA256)[1] <span style="color:#666">-replace</span> <span style="color:#b44">&#34; &#34;</span>, <span style="color:#b44">&#34;&#34;</span>) <span style="color:#666">-eq</span> $(<span style="color:#a2f">type </span>.\kubectl.exe.sha256)
</code></pre></div></li>
</ul>
<!-- 
1. Add the binary in to your `PATH`.

1. Test to ensure the version of `kubectl` is the same as downloaded:
-->
</li>
<li>
<p>将可执行文件的路径添加到 <code>PATH</code>。</p>
</li>
<li>
<p>测试一下，确保此 <code>kubectl</code> 的版本和期望版本一致：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cmd" data-lang="cmd">kubectl version --client
</code></pre></div></li>
</ol>
<!-- 
[Docker Desktop for Windows](https://docs.docker.com/docker-for-windows/#kubernetes) adds its own version of `kubectl` to `PATH`.
If you have installed Docker Desktop before, you may need to place your `PATH` entry before the one added by the Docker Desktop installer or remove the Docker Desktop's `kubectl`.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <a href="https://docs.docker.com/docker-for-windows/#kubernetes">Windows 版的 Docker Desktop</a>
将其自带版本的 <code>kubectl</code> 添加到 <code>PATH</code>。
如果你之前安装过 Docker Desktop，可能需要把此 <code>PATH</code> 条目置于 Docker Desktop 安装的条目之前，
或者直接删掉 Docker Desktop 的 <code>kubectl</code>。</div>
</blockquote>
<!-- 
### Install on Windows using Chocolatey or Scoop
-->
<h3 id="install-on-windows-using-chocolatey-or-scoop">在 Windows 上用 Chocolatey 或 Scoop 安装</h3>
<!-- 
1. To install kubectl on Windows you can use either [Chocolatey](https://chocolatey.org) package manager or [Scoop](https://scoop.sh) command-line installer.
-->
<ol>
<li>
<p>要在 Windows 上安装 kubectl，你可以使用包管理器 <a href="https://chocolatey.org">Chocolatey</a>
或是命令行安装器 <a href="https://scoop.sh">Scoop</a>。</p>
<ul class="nav nav-tabs" id="kubectl-win-install" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#kubectl-win-install-0" role="tab" aria-controls="kubectl-win-install-0" aria-selected="true">choco</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#kubectl-win-install-1" role="tab" aria-controls="kubectl-win-install-1">scoop</a></li></ul>
<div class="tab-content" id="kubectl-win-install"><div id="kubectl-win-install-0" class="tab-pane show active" role="tabpanel" aria-labelledby="kubectl-win-install-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">choco install <span style="color:#a2f">kubernetes-cli</span>
</code></pre></div></div>
  <div id="kubectl-win-install-1" class="tab-pane" role="tabpanel" aria-labelledby="kubectl-win-install-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">scoop install kubectl
</code></pre></div></div></div>

<!-- 
1. Test to ensure the version you installed is up-to-date:
-->
</li>
<li>
<p>测试一下，确保安装的是最新版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">kubectl version --client
</code></pre></div><!-- 
1. Navigate to your home directory:
-->
</li>
<li>
<p>导航到你的 home 目录：</p>
<!-- 
# If you're using cmd.exe, run: cd %USERPROFILE%
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#080;font-style:italic"># 当你用 cmd.exe 时，则运行： cd %USERPROFILE%</span>
<span style="color:#a2f">cd </span>~
</code></pre></div><!-- 
1. Create the `.kube` directory:
-->
</li>
<li>
<p>创建目录 <code>.kube</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">mkdir .kube
</code></pre></div><!-- 
1. Change to the `.kube` directory you just created:
-->
</li>
<li>
<p>切换到新创建的目录 <code>.kube</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">cd </span>.kube
</code></pre></div><!-- 
1. Configure kubectl to use a remote Kubernetes cluster:
-->
</li>
<li>
<p>配置 kubectl，以接入远程的 Kubernetes 集群：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">New-Item</span> config -type file
</code></pre></div></li>
</ol>
<!-- 
Edit the config file with a text editor of your choice, such as Notepad.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 编辑配置文件，你需要先选择一个文本编辑器，比如 Notepad。</div>
</blockquote>
<!-- 
## Verify kubectl configuration
-->
<h2 id="verify-kubectl-configration">验证 kubectl 配置</h2>

	<!-- 
---
title: "verify kubectl install"
description: "How to verify kubectl."
headless: true
---
 -->
<!-- 
In order for kubectl to find and access a Kubernetes cluster, it needs a
[kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/),
which is created automatically when you create a cluster using
[kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh)
or successfully deploy a Minikube cluster.
By default, kubectl configuration is located at `~/.kube/config`.

Check that kubectl is properly configured by getting the cluster state:
 -->
<p>为了让 kubectl 能发现并访问 Kubernetes 集群，你需要一个
<a href="/docs/zh/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>，
该文件在
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh">kube-up.sh</a>
创建集群时，或成功部署一个 Miniube 集群时，均会自动生成。
通常，kubectl 的配置信息存放于文件 <code>~/.kube/config</code> 中。</p>
<p>通过获取集群状态的方法，检查是否已恰当的配置了 kubectl：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info
</code></pre></div><!-- 
If you see a URL response, kubectl is correctly configured to access your cluster.

If you see a message similar to the following, kubectl is not configured correctly or is not able to connect to a Kubernetes cluster.
 -->
<p>如果返回一个 URL，则意味着 kubectl 成功的访问到了你的集群。</p>
<p>如果你看到如下所示的消息，则代表 kubectl 配置出了问题，或无法连接到 Kubernetes 集群。</p>
<pre tabindex="0"><code>The connection to the server &lt;server-name:port&gt; was refused - did you specify the right host or port?
（访问 &lt;server-name:port&gt; 被拒绝 - 你指定的主机和端口是否有误？）
</code></pre><!-- 
For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like Minikube to be installed first and then re-run the commands stated above.

If kubectl cluster-info returns the url response but you can't access your cluster, to check whether it is configured properly, use:
 -->
<p>例如，如果你想在自己的笔记本上（本地）运行 Kubernetes 集群，你需要先安装一个 Minikube 这样的工具，然后再重新运行上面的命令。</p>
<p>如果命令 <code>kubectl cluster-info</code> 返回了 url，但你还不能访问集群，那可以用以下命令来检查配置是否妥当：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info dump
</code></pre></div>
<!-- 
## Optional kubectl configurations and plugins

### Enable shell autocompletion
-->
<h2 id="optional-kubectl-configurations">kubectl 可选配置和插件</h2>
<h3 id="enable-shell-autocompletion">启用 shell 自动补全功能</h3>
<!-- 
kubectl provides autocompletion support for Bash and Zsh, which can save you a lot of typing.

Below are the procedures to set up autocompletion for Zsh, if you are running that on Windows.
-->
<p>kubectl 为 Bash 和 Zsh 提供自动补全功能，可以减轻许多输入的负担。</p>
<p>下面是设置 Zsh 自动补全功能的操作步骤，前提是你在 Windows 上面运行的是 Zsh。</p>

	<!-- 
---
title: "zsh auto-completion"
description: "Some optional configuration for zsh auto-completion."
headless: true
---
-->
<!-- 
The kubectl completion script for Zsh can be generated with the command `kubectl completion zsh`. Sourcing the completion script in your shell enables kubectl autocompletion.

To do so in all your shell sessions, add the following to your `~/.zshrc` file:
-->
<p>kubectl 通过命令 <code>kubectl completion zsh</code> 生成 Zsh 自动补全脚本。
在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。</p>
<p>为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 <code>~/.zshrc</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">source</span> &lt;<span style="color:#666">(</span>kubectl completion zsh<span style="color:#666">)</span>
</code></pre></div><!-- 
If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<p>如果你为 kubectl 定义了别名，可以扩展脚本补全，以兼容该别名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.zshrc
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.zshrc
</code></pre></div><!-- 
After reloading your shell, kubectl autocompletion should be working.

If you get an error like `complete:13: command not found: compdef`, then add the following to the beginning of your `~/.zshrc` file:
-->
<p>重新加载 shell 后，kubectl 自动补全功能将立即生效。</p>
<p>如果你收到 <code>complete:13: command not found: compdef</code> 这样的错误提示，那请将下面内容添加到 <code>~/.zshrc</code> 文件的开头：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh">autoload -Uz compinit
compinit
</code></pre></div>
<!--
### Install `kubectl convert` plugin
-->
<h3 id="安装-kubectl-convert-插件">安装 <code>kubectl convert</code> 插件</h3>

	<!--
---
title: "kubectl-convert overview"
description: >-
  A kubectl plugin that allows you to convert manifests from one version
  of a Kubernetes API to a different version.
headless: true
---
-->
<!--
A plugin for Kubernetes command-line tool `kubectl`, which allows you to convert manifests between different API 
versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.
For more info, visit [migrate to non deprecated apis](/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis)
-->
<p>一个 Kubernetes 命令行工具 <code>kubectl</code> 的插件，允许你将清单在不同 API 版本间转换。
在将清单迁移到具有较新 Kubernetes 版本的未弃用 API 版本时，这个插件特别有用。
更多信息请访问 <a href="/zh/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis">迁移到非弃用 API</a></p>

<!--
1. Download the latest release with the command:
-->
<ol>
<li>
<p>用以下命令下载最新发行版：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">curl </span>-LO https<span style="">:</span>//dl.k8s.io/release/<span style="color:#a2f">/bin/windows/amd64/<span style="color:#a2f">kubectl-convert</span>.exe
</code></pre></div></li>
</ol>
<!--
1. Validate the binary (optional)

   Download the kubectl-convert checksum file:
-->
<ol>
<li>
<p>验证该可执行文件（可选步骤）</p>
<p>下载 kubectl-convert 校验和文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">curl </span>-LO https<span style="">:</span>//dl.k8s.io/<span style="color:#a2f">/bin/windows/amd64/<span style="color:#a2f">kubectl-convert</span>.exe.sha256
</code></pre></div><!--
Validate the kubectl-convert binary against the checksum file:

- Using Command Prompt to manually compare `CertUtil`'s output to the checksum file downloaded:
-->
<p>基于校验和，验证 kubectl-convert 的可执行文件：</p>
<ul>
<li>
<p>用提示的命令对 <code>CertUtil</code> 的输出和下载的校验和文件进行手动比较。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cmd" data-lang="cmd">CertUtil -hashfile kubectl-convert.exe SHA256
<span style="color:#a2f;font-weight:bold">type</span> kubectl-convert.exe.sha256
</code></pre></div></li>
</ul>
<!--
- Using PowerShell to automate the verification using the `-eq` operator to get a `True` or `False` result:
-->
<ul>
<li>
<p>使用 PowerShell <code>-eq</code> 操作使验证自动化，获得 <code>True</code> 或者 <code>False</code> 的结果：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">$($(CertUtil -hashfile .\<span style="color:#a2f">kubectl-convert</span>.exe SHA256)[1] <span style="color:#666">-replace</span> <span style="color:#b44">&#34; &#34;</span>, <span style="color:#b44">&#34;&#34;</span>) <span style="color:#666">-eq</span> $(<span style="color:#a2f">type </span>.\<span style="color:#a2f">kubectl-convert</span>.exe.sha256)
</code></pre></div></li>
</ul>
</li>
</ol>
<!--
1. Add the binary in to your `PATH`.

1. Verify plugin is successfully installed
-->
<ol>
<li>
<p>将可执行文件添加到你的 <code>PATH</code> 环境变量。</p>
</li>
<li>
<p>验证插件是否安装成功</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl convert --help
</code></pre></div><!--
If you do not see an error, it means the plugin is successfully installed.
-->
<p>如果你没有看到任何错误就代表插件安装成功了。</p>
</li>
</ol>
<h2 id="接下来">接下来</h2>

	<!-- 
---
title: "What's next?"
description: "What's next after installing kubectl."
headless: true
---
-->
<!-- 
* [Install Minikube](https://minikube.sigs.k8s.io/docs/start/)
* See the [getting started guides](/docs/setup/) for more about creating clusters.
* [Learn how to launch and expose your application.](/docs/tasks/access-application-cluster/service-access-application-cluster/)
* If you need access to a cluster you didn't create, see the
  [Sharing Cluster Access document](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/).
* Read the [kubectl reference docs](/docs/reference/kubectl/kubectl/)
-->
<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/start/">安装 Minikube</a></li>
<li>有关创建集群的更多信息，请参阅<a href="/zh/docs/setup/">入门指南</a>.</li>
<li><a href="/zh/docs/tasks/access-application-cluster/service-access-application-cluster/">学习如何启动并对外公开你的应用程序。</a></li>
<li>如果你需要访问其他人创建的集群，请参阅
<a href="/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">共享集群接入文档</a>.</li>
<li>阅读 <a href="/zh/docs/reference/kubectl/kubectl/">kubectl 参考文档</a></li>
</ul>


</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-91639f08dfa86a6c88cf0099b2e097bc">1.4 - 内含的工具</h1>
    <div class="lead">在页面 kubectl-installs-*.md 中包含的代码片段</div>
	<!-- 
---
title: "Tools Included"
description: "Snippets to be included in the main kubectl-installs-*.md pages."
headless: true
toc_hide: true
---
-->

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-d4ebbbe0a2ddce15850a36dfede9ba52">1.4.1 - kubectl-convert 概述</h1>
    <div class="lead">一个 kubectl 插件，允许你将清单从一个 Kubernetes API 版本转换到不同的版本。</div>
	<!--
---
title: "kubectl-convert overview"
description: >-
  A kubectl plugin that allows you to convert manifests from one version
  of a Kubernetes API to a different version.
headless: true
---
-->
<!--
A plugin for Kubernetes command-line tool `kubectl`, which allows you to convert manifests between different API 
versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.
For more info, visit [migrate to non deprecated apis](/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis)
-->
<p>一个 Kubernetes 命令行工具 <code>kubectl</code> 的插件，允许你将清单在不同 API 版本间转换。
在将清单迁移到具有较新 Kubernetes 版本的未弃用 API 版本时，这个插件特别有用。
更多信息请访问 <a href="/zh/docs/reference/using-api/deprecation-guide/#migrate-to-non-deprecated-apis">迁移到非弃用 API</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2d600cc8ec4dec69673b5f9577b6da22">1.4.2 - Linux 系统中的 bash 自动补全功能</h1>
    <div class="lead">Linux 系统中 bash 自动补全功能的一些可选配置。</div>
	<!-- 
---
title: "bash auto-completion on Linux"
description: "Some optional configuration for bash auto-completion on Linux."
headless: true
---
-->
<!-- 
### Introduction
-->
<h3 id="introduction">简介</h3>
<!-- 
The kubectl completion script for Bash can be generated with the command `kubectl completion bash`. Sourcing the completion script in your shell enables kubectl autocompletion.

However, the completion script depends on [**bash-completion**](https://github.com/scop/bash-completion), which means that you have to install this software first (you can test if you have bash-completion already installed by running `type _init_completion`).
-->
<p>kubectl 的 Bash 补全脚本可以用命令 <code>kubectl completion bash</code> 生成。
在 shell 中导入（Sourcing）补全脚本，将启用 kubectl 自动补全功能。</p>
<p>然而，补全脚本依赖于工具 <a href="https://github.com/scop/bash-completion"><strong>bash-completion</strong></a>，
所以要先安装它（可以用命令 <code>type _init_completion</code> 检查 bash-completion 是否已安装）。</p>
<!-- 
### Install bash-completion
-->
<h3 id="install-bash-comletion">安装 bash-completion</h3>
<!-- 
bash-completion is provided by many package managers (see [here](https://github.com/scop/bash-completion#installation)). You can install it with `apt-get install bash-completion` or `yum install bash-completion`, etc.

The above commands create `/usr/share/bash-completion/bash_completion`, which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your `~/.bashrc` file.

To find out, reload your shell and run `type _init_completion`. If the command succeeds, you're already set, otherwise add the following to your `~/.bashrc` file:
-->
<p>很多包管理工具均支持 bash-completion（参见<a href="https://github.com/scop/bash-completion#installation">这里</a>）。
可以通过 <code>apt-get install bash-completion</code> 或 <code>yum install bash-completion</code> 等命令来安装它。</p>
<p>上述命令将创建文件 <code>/usr/share/bash-completion/bash_completion</code>，它是 bash-completion 的主脚本。
依据包管理工具的实际情况，你需要在 <code>~/.bashrc</code> 文件中手工导入此文件。</p>
<p>要查看结果，请重新加载你的 shell，并运行命令 <code>type _init_completion</code>。
如果命令执行成功，则设置完成，否则将下面内容添加到文件 <code>~/.bashrc</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">source</span> /usr/share/bash-completion/bash_completion
</code></pre></div><!-- 
Reload your shell and verify that bash-completion is correctly installed by typing `type _init_completion`.
-->
<p>重新加载 shell，再输入命令 <code>type _init_completion</code> 来验证 bash-completion 的安装状态。</p>
<!-- 
### Enable kubectl autocompletion
-->
<h3 id="enable-kubectl-autocompletion">启动 kubectl 自动补全功能</h3>
<!-- 
You now need to ensure that the kubectl completion script gets sourced in all your shell sessions. There are two ways in which you can do this:
-->
<p>你现在需要确保一点：kubectl 补全脚本已经导入（sourced）到 shell 会话中。
这里有两种验证方法：</p>
<!-- 
- Source the completion script in your `~/.bashrc` file:
-->
<ul>
<li>
<p>在文件 <code>~/.bashrc</code> 中导入（source）补全脚本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;source &lt;(kubectl completion bash)&#39;</span> &gt;&gt;~/.bashrc
</code></pre></div></li>
</ul>
<!-- 
- Add the completion script to the `/etc/bash_completion.d` directory:
-->
<ul>
<li>
<p>将补全脚本添加到目录 <code>/etc/bash_completion.d</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl completion bash &gt;/etc/bash_completion.d/kubectl
</code></pre></div></li>
</ul>
<!-- 
If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<p>如果 kubectl 有关联的别名，你可以扩展 shell 补全来适配此别名：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.bashrc
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.bashrc
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
bash-completion sources all completion scripts in `/etc/bash_completion.d`.
-->
<p>bash-completion 负责导入 <code>/etc/bash_completion.d</code> 目录中的所有补全脚本。</div>
</blockquote>
<!-- 
Both approaches are equivalent. After reloading your shell, kubectl autocompletion should be working.
-->
<p>两种方式的效果相同。重新加载 shell 后，kubectl 自动补全功能即可生效。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-68808b3ec5807517ed64fd8ae32a7d1b">1.4.3 - macOS 系统上的 bash 自动补全</h1>
    <div class="lead">在 macOS 上实现 Bash 自动补全的一些可选配置。</div>
	<!-- 
title: "bash auto-completion on macOS"
description: "Some optional configuration for bash auto-completion on macOS."
headless: true
 -->
<!-- 
### Introduction
-->
<h3 id="简介">简介</h3>
<!-- 
The kubectl completion script for Bash can be generated with `kubectl completion bash`. Sourcing this script in your shell enables kubectl completion.

However, the kubectl completion script depends on [**bash-completion**](https://github.com/scop/bash-completion) which you thus have to previously install.
-->
<p>kubectl 的 Bash 补全脚本可以通过 <code>kubectl completion bash</code> 命令生成。
在你的 shell 中导入（Sourcing）这个脚本即可启用补全功能。</p>
<p>此外，kubectl 补全脚本依赖于工具 <a href="https://github.com/scop/bash-completion"><strong>bash-completion</strong></a>，
所以你必须先安装它。</p>
<blockquote class="warning callout">
  <div><strong>警告：</strong> <!-- 
There are two versions of bash-completion, v1 and v2. V1 is for Bash 3.2 (which is the default on macOS), and v2 is for Bash 4.1+. The kubectl completion script **doesn't work** correctly with bash-completion v1 and Bash 3.2. It requires **bash-completion v2** and **Bash 4.1+**. Thus, to be able to correctly use kubectl completion on macOS, you have to install and use Bash 4.1+ ([*instructions*](https://itnext.io/upgrading-bash-on-macos-7138bd1066ba)). The following instructions assume that you use Bash 4.1+ (that is, any Bash version of 4.1 or newer).
-->
<p>bash-completion 有两个版本：v1 和 v2。v1 对应 Bash3.2（也是 macOS 的默认安装版本），v2 对应 Bash 4.1+。
kubectl 的补全脚本<strong>无法适配</strong> bash-completion v1 和 Bash 3.2。
必须为它配备  <strong>bash-completion v2</strong> 和 <strong>Bash 4.1+</strong>。
有鉴于此，为了在 macOS 上使用 kubectl 补全功能，你必须要安装和使用 Bash 4.1+
(<a href="https://itnext.io/upgrading-bash-on-macos-7138bd1066ba"><em>说明</em></a>)。
后续说明假定你用的是 Bash 4.1+（也就是 Bash 4.1 或更新的版本）</div>
</blockquote>

<!-- 
### Upgrade Bash
-->
<h3 id="升级-bash">升级 Bash</h3>
<!-- 
The instructions here assume you use Bash 4.1+. You can check your Bash's version by running:
-->
<p>后续说明假定你已使用 Bash 4.1+。你可以运行以下命令检查 Bash 版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b8860b">$BASH_VERSION</span>
</code></pre></div><!-- 
If it is too old, you can install/upgrade it using Homebrew:
-->
<p>如果版本太旧，可以用 Homebrew 安装/升级：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install bash
</code></pre></div><!-- 
Reload your shell and verify that the desired version is being used:
-->
<p>重新加载 shell，并验证所需的版本已经生效：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b8860b">$BASH_VERSION</span> <span style="color:#b8860b">$SHELL</span>
</code></pre></div><!-- 
Homebrew usually installs it at `/usr/local/bin/bash`.
-->
<p>Homebrew 通常把它安装为 <code>/usr/local/bin/bash</code>。</p>
<!-- 
### Install bash-completion
-->
<h3 id="安装-bash-completion">安装 bash-completion</h3>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
As mentioned, these instructions assume you use Bash 4.1+, which means you will install bash-completion v2 (in contrast to Bash 3.2 and bash-completion v1, in which case kubectl completion won't work).
-->
<p>如前所述，本说明假定你使用的 Bash 版本为 4.1+，这意味着你要安装 bash-completion v2
（不同于 Bash 3.2 和 bash-completion v1，kubectl 的补全功能在该场景下无法工作）。</div>
</blockquote>
<!-- 
You can test if you have bash-completion v2 already installed with `type _init_completion`. If not, you can install it with Homebrew:
-->
<p>你可以用命令 <code>type _init_completion</code> 测试 bash-completion v2 是否已经安装。
如未安装，用 Homebrew 来安装它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install bash-completion@2
</code></pre></div><!-- 
As stated in the output of this command, add the following to your `~/.bash_profile` file:
-->
<p>如命令的输出信息所显示的，将如下内容添加到文件 <code>~/.bash_profile</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">export</span> <span style="color:#b8860b">BASH_COMPLETION_COMPAT_DIR</span><span style="color:#666">=</span><span style="color:#b44">&#34;/usr/local/etc/bash_completion.d&#34;</span>
<span style="color:#666">[[</span> -r <span style="color:#b44">&#34;/usr/local/etc/profile.d/bash_completion.sh&#34;</span> <span style="color:#666">]]</span> <span style="color:#666">&amp;&amp;</span> . <span style="color:#b44">&#34;/usr/local/etc/profile.d/bash_completion.sh&#34;</span>
</code></pre></div><!-- 
Reload your shell and verify that bash-completion v2 is correctly installed with `type _init_completion`.
-->
<p>重新加载 shell，并用命令 <code>type _init_completion</code> 验证 bash-completion v2 已经恰当的安装。</p>
<!-- 
### Enable kubectl autocompletion
-->
<h3 id="启用-kubectl-自动补全功能">启用 kubectl 自动补全功能</h3>
<!-- 
You now have to ensure that the kubectl completion script gets sourced in all your shell sessions. There are multiple ways to achieve this:

- Source the completion script in your `~/.bash_profile` file:
-->
<p>你现在需要确保在所有的 shell 环境中均已导入（sourced） kubectl 的补全脚本，
有若干种方法可以实现这一点：</p>
<ul>
<li>
<p>在文件 <code>~/.bash_profile</code> 中导入（Source）补全脚本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;source &lt;(kubectl completion bash)&#39;</span> &gt;&gt;~/.bash_profile
</code></pre></div></li>
</ul>
<!-- 
- Add the completion script to the `/usr/local/etc/bash_completion.d` directory:
-->
<ul>
<li>
<p>将补全脚本添加到目录 <code>/usr/local/etc/bash_completion.d</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl completion bash &gt;/usr/local/etc/bash_completion.d/kubectl
</code></pre></div></li>
</ul>
<!-- 
- If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<ul>
<li>
<p>如果你为 kubectl 定义了别名，则可以扩展 shell 补全来兼容该别名：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.bash_profile
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.bash_profile
</code></pre></div></li>
</ul>
<!-- 
- If you installed kubectl with Homebrew (as explained [here](/docs/tasks/tools/install-kubectl-macos/#install-with-homebrew-on-macos)), then the kubectl completion script should already be in `/usr/local/etc/bash_completion.d/kubectl`. In that case, you don't need to do anything.
-->
<ul>
<li>
<p>如果你是用 Homebrew 安装的 kubectl（如
<a href="/zh/docs/tasks/install-with-homebrew-on-macos/#install-with-homebrew-on-macos">此页面</a>
所描述），则kubectl 补全脚本应该已经安装到目录 <code>/usr/local/etc/bash_completion.d/kubectl</code>
中了。这种情况下，你什么都不需要做。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!-- 
  The Homebrew installation of bash-completion v2 sources all the files in the `BASH_COMPLETION_COMPAT_DIR` directory, that's why the latter two methods work.
  -->
<p>用 Hommbrew 安装的 bash-completion v2 会初始化 目录 <code>BASH_COMPLETION_COMPAT_DIR</code>
中的所有文件，这就是后两种方法能正常工作的原因。</div>
</blockquote>
</li>
</ul>
<!-- 
In any case, after reloading your shell, kubectl completion should be working.
-->
<p>总之，重新加载 shell 之后，kubectl 补全功能将立即生效。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9ab27577326839cb4793fd670a916364">1.4.4 - zsh 自动补全</h1>
    <div class="lead">zsh 自动补全的一些可选配置</div>
	<!-- 
---
title: "zsh auto-completion"
description: "Some optional configuration for zsh auto-completion."
headless: true
---
-->
<!-- 
The kubectl completion script for Zsh can be generated with the command `kubectl completion zsh`. Sourcing the completion script in your shell enables kubectl autocompletion.

To do so in all your shell sessions, add the following to your `~/.zshrc` file:
-->
<p>kubectl 通过命令 <code>kubectl completion zsh</code> 生成 Zsh 自动补全脚本。
在 shell 中导入（Sourcing）该自动补全脚本，将启动 kubectl 自动补全功能。</p>
<p>为了在所有的 shell 会话中实现此功能，请将下面内容加入到文件 <code>~/.zshrc</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">source</span> &lt;<span style="color:#666">(</span>kubectl completion zsh<span style="color:#666">)</span>
</code></pre></div><!-- 
If you have an alias for kubectl, you can extend shell completion to work with that alias:
-->
<p>如果你为 kubectl 定义了别名，可以扩展脚本补全，以兼容该别名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;alias k=kubectl&#39;</span> &gt;&gt;~/.zshrc
<span style="color:#a2f">echo</span> <span style="color:#b44">&#39;complete -F __start_kubectl k&#39;</span> &gt;&gt;~/.zshrc
</code></pre></div><!-- 
After reloading your shell, kubectl autocompletion should be working.

If you get an error like `complete:13: command not found: compdef`, then add the following to the beginning of your `~/.zshrc` file:
-->
<p>重新加载 shell 后，kubectl 自动补全功能将立即生效。</p>
<p>如果你收到 <code>complete:13: command not found: compdef</code> 这样的错误提示，那请将下面内容添加到 <code>~/.zshrc</code> 文件的开头：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-zsh" data-lang="zsh">autoload -Uz compinit
compinit
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-99d563e9521796074ba3ca7f15a613ce">1.4.5 - 后续内容</h1>
    <div class="lead">安装 kubectl 之后，还可以做些什么？</div>
	<!-- 
---
title: "What's next?"
description: "What's next after installing kubectl."
headless: true
---
-->
<!-- 
* [Install Minikube](https://minikube.sigs.k8s.io/docs/start/)
* See the [getting started guides](/docs/setup/) for more about creating clusters.
* [Learn how to launch and expose your application.](/docs/tasks/access-application-cluster/service-access-application-cluster/)
* If you need access to a cluster you didn't create, see the
  [Sharing Cluster Access document](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/).
* Read the [kubectl reference docs](/docs/reference/kubectl/kubectl/)
-->
<ul>
<li><a href="https://minikube.sigs.k8s.io/docs/start/">安装 Minikube</a></li>
<li>有关创建集群的更多信息，请参阅<a href="/zh/docs/setup/">入门指南</a>.</li>
<li><a href="/zh/docs/tasks/access-application-cluster/service-access-application-cluster/">学习如何启动并对外公开你的应用程序。</a></li>
<li>如果你需要访问其他人创建的集群，请参阅
<a href="/zh/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">共享集群接入文档</a>.</li>
<li>阅读 <a href="/zh/docs/reference/kubectl/kubectl/">kubectl 参考文档</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f92adff24b39ef90a481e9aec2b8dd08">1.4.6 - 通过 gcloud 安装 kubectl</h1>
    <div class="lead">用各个特定操作系统标签页中包含的 gcloud 指令片段安装 kubectl。</div>
	<!-- 
---
title: "gcloud kubectl install"
description: "How to install kubectl with gcloud snippet for inclusion in each OS-specific tab."
headless: true
---
-->
<!-- 
You can install kubectl as part of the Google Cloud SDK.
-->
<p>kubectl 可以作为 Google Cloud SDK 的一部分被安装。</p>
<!-- 
1. Install the [Google Cloud SDK](https://cloud.google.com/sdk/).
1. Run the `kubectl` installation command:
-->
<ol>
<li>
<p>安装 <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a>。</p>
</li>
<li>
<p>运行安装 <code>kubectl</code> 的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud components install kubectl
</code></pre></div><!-- 
1. Test to ensure the version you installed is up-to-date:
-->
</li>
<li>
<p>验证一下，确保安装的是最新的版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl version --client
</code></pre></div></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b2d43b2ae3e8f26eefa83de2db4ba782">1.4.7 - 验证 kubectl 的安装效果</h1>
    <div class="lead">如何验证 kubectl。</div>
	<!-- 
---
title: "verify kubectl install"
description: "How to verify kubectl."
headless: true
---
 -->
<!-- 
In order for kubectl to find and access a Kubernetes cluster, it needs a
[kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/),
which is created automatically when you create a cluster using
[kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh)
or successfully deploy a Minikube cluster.
By default, kubectl configuration is located at `~/.kube/config`.

Check that kubectl is properly configured by getting the cluster state:
 -->
<p>为了让 kubectl 能发现并访问 Kubernetes 集群，你需要一个
<a href="/docs/zh/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>，
该文件在
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh">kube-up.sh</a>
创建集群时，或成功部署一个 Miniube 集群时，均会自动生成。
通常，kubectl 的配置信息存放于文件 <code>~/.kube/config</code> 中。</p>
<p>通过获取集群状态的方法，检查是否已恰当的配置了 kubectl：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info
</code></pre></div><!-- 
If you see a URL response, kubectl is correctly configured to access your cluster.

If you see a message similar to the following, kubectl is not configured correctly or is not able to connect to a Kubernetes cluster.
 -->
<p>如果返回一个 URL，则意味着 kubectl 成功的访问到了你的集群。</p>
<p>如果你看到如下所示的消息，则代表 kubectl 配置出了问题，或无法连接到 Kubernetes 集群。</p>
<pre tabindex="0"><code>The connection to the server &lt;server-name:port&gt; was refused - did you specify the right host or port?
（访问 &lt;server-name:port&gt; 被拒绝 - 你指定的主机和端口是否有误？）
</code></pre><!-- 
For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like Minikube to be installed first and then re-run the commands stated above.

If kubectl cluster-info returns the url response but you can't access your cluster, to check whether it is configured properly, use:
 -->
<p>例如，如果你想在自己的笔记本上（本地）运行 Kubernetes 集群，你需要先安装一个 Minikube 这样的工具，然后再重新运行上面的命令。</p>
<p>如果命令 <code>kubectl cluster-info</code> 返回了 url，但你还不能访问集群，那可以用以下命令来检查配置是否妥当：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info dump
</code></pre></div>
</div>



    
	
  

    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-34a810f1516ad9d99b2697e36e9b0d0f">2 - 管理集群</h1>
    <div class="lead">了解管理集群的常见任务。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-adb6c52e773f4d890595e14a9251f59b">2.1 - 从 dockershim 迁移</h1>
    
	<!-- 
title: "Migrating from dockershim"
weight: 10
content_type: task 
-->
<!-- overview -->
<!-- 
This section presents information you need to know when migrating from
dockershim to other container runtimes.
-->
<p>本节提供从 dockershim 迁移到其他容器运行时的必备知识。</p>
<!-- 
Since the announcement of [dockershim deprecation](/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation)
in Kubernetes 1.20, there were questions on how this will affect various workloads and Kubernetes
installations. You can find this blog post useful to understand the problem better: [Dockershim Deprecation FAQ](/blog/2020/12/02/dockershim-faq/)
-->
<p>自从 Kubernetes 1.20 宣布
<a href="/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation">弃用 dockershim</a>，
各类疑问随之而来：这对各类工作负载和 Kubernetes 部署会产生什么影响。
你会发现这篇博文对于更好地理解此问题非常有用：
<a href="/zh/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 常见问题</a></p>
<!-- It is recommended to migrate from dockershim to alternative container runtimes.
Check out [container runtimes](/docs/setup/production-environment/container-runtimes/)
section to know your options. Make sure to
[report issues](https://github.com/kubernetes/kubernetes/issues) you encountered
with the migration. So the issue can be fixed in a timely manner and your cluster would be
ready for dockershim removal.
-->
<p>建议从 dockershim 迁移到其他替代的容器运行时。
请参阅<a href="/zh/docs/setup/production-environment/container-runtimes/">容器运行时</a>
一节以了解可用的备选项。
当在迁移过程中遇到麻烦，请<a href="https://github.com/kubernetes/kubernetes/issues">上报问题</a>。
那么问题就可以及时修复，你的集群也可以进入移除 dockershim 前的就绪状态。</p>

</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-58702e4818c09c9b3d574349c1a71cb3">2.1.1 - 检查弃用 Dockershim 对你的影响</h1>
    
	<!-- 
title: Check whether Dockershim deprecation affects you
content_type: task 
reviewers:
- SergeyKanzhelev
weight: 20
-->
<!-- overview -->
<!-- 
The `dockershim` component of Kubernetes allows to use Docker as a Kubernetes's
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='container runtime'>container runtime</a>.
Kubernetes' built-in `dockershim` component was deprecated in release v1.20.
-->
<p>Kubernetes 的 <code>dockershim</code> 组件使得你可以把 Docker 用作 Kubernetes 的
<a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>。
在 Kubernetes v1.20 版本中，内建组件 <code>dockershim</code> 被弃用。</p>
<!-- 
This page explains how your cluster could be using Docker as a container runtime,
provides details on the role that `dockershim` plays when in use, and shows steps
you can take to check whether any workloads could be affected by `dockershim` deprecation.
-->
<p>本页讲解你的集群把 Docker 用作容器运行时的运作机制，
并提供使用 <code>dockershim</code> 时，它所扮演角色的详细信息，
继而展示了一组验证步骤，可用来检查弃用 <code>dockershim</code> 对你的工作负载的影响。</p>
<!-- 
## Finding if your app has a dependencies on Docker {#find-docker-dependencies} 
-->
<h2 id="find-docker-dependencies">检查你的应用是否依赖于 Docker</h2>
<!-- 
If you are using Docker for building your application containers, you can still
run these containers on any container runtime. This use of Docker does not count
as a dependency on Docker as a container runtime.
-->
<p>虽然你通过 Docker 创建了应用容器，但这些容器却可以运行于所有容器运行时。
所以这种使用 Docker 容器运行时的方式并不构成对 Docker 的依赖。</p>
<!-- 
When alternative container runtime is used, executing Docker commands may either
not work or yield unexpected output. This is how you can find whether you have a
dependency on Docker:
-->
<p>当用了替代的容器运行时之后，Docker 命令可能不工作，甚至产生意外的输出。
这才是判定你是否依赖于 Docker 的方法。</p>
<!-- 
1. Make sure no privileged Pods execute Docker commands.
2. Check that scripts and apps running on nodes outside of Kubernetes
   infrastructure do not execute Docker commands. It might be:
   - SSH to nodes to troubleshoot;
   - Node startup scripts;
   - Monitoring and security agents installed on nodes directly.
3. Third-party tools that perform above mentioned privileged operations. See
   [Migrating telemetry and security agents from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents)
   for more information.
4. Make sure there is no indirect dependencies on dockershim behavior.
   This is an edge case and unlikely to affect your application. Some tooling may be configured
   to react to Docker-specific behaviors, for example, raise alert on specific metrics or search for
   a specific log message as part of troubleshooting instructions.
   If you have such tooling configured, test the behavior on test
   cluster before migration.
 -->
<ol>
<li>确认没有特权 Pod 执行 docker 命令。</li>
<li>检查 Kubernetes 基础架构外部节点上的脚本和应用，确认它们没有执行 Docker 命令。可能的命令有：
<ul>
<li>SSH 到节点排查故障；</li>
<li>节点启动脚本；</li>
<li>直接安装在节点上的监视和安全代理。</li>
</ul>
</li>
<li>检查执行了上述特权操作的第三方工具。详细操作请参考:
<a href="/zh/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">从 dockershim 迁移遥测和安全代理</a></li>
<li>确认没有对 dockershim 行为的间接依赖。这是一种极端情况，不太可能影响你的应用。
一些工具很可能被配置为使用了 Docker 特性，比如，基于特定指标发警报，或者在故障排查指令的一个环节中搜索特定的日志信息。
如果你有此类配置的工具，需要在迁移之前，在测试集群上完成功能验证。</li>
</ol>
<!-- 
## Dependency on Docker explained {#role-of-dockershim}  
-->
<h2 id="role-of-dockershim">Docker 依赖详解</h2>
<!-- 
A [container runtime](/docs/concepts/containers/#container-runtimes) is software that can
execute the containers that make up a Kubernetes pod. Kubernetes is responsible for orchestration
and scheduling of Pods; on each node, the <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
uses the container runtime interface as an abstraction so that you can use any compatible
container runtime.
 -->
<p><a href="/zh/docs/concepts/containers/#container-runtimes">容器运行时</a>是一个软件，用来运行组成 Kubernetes Pod 的容器。
Kubernetes 负责编排和调度 Pod；在每一个节点上，
<a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a>
使用抽象的容器运行时接口，所以你可以任意选用兼容的容器运行时。</p>
<!-- 
In its earliest releases, Kubernetes offered compatibility with just one container runtime: Docker.
Later in the Kubernetes project's history, cluster operators wanted to adopt additional container runtimes.
The CRI was designed to allow this kind of flexibility - and the kubelet began supporting CRI. However,
because Docker existed before the CRI specification was invented, the Kubernetes project created an
adapter component, `dockershim`. The dockershim adapter allows the kubelet to interact with Docker as
if Docker were a CRI compatible runtime.
 -->
<p>在早期版本中，Kubernetes 提供的兼容性只支持一个容器运行时：Docker。
在 Kubernetes 发展历史中，集群运营人员希望采用更多的容器运行时。
于是 CRI 被设计出来满足这类灵活性需要 - 而 kubelet 亦开始支持 CRI。
然而，因为 Docker 在 CRI 规范创建之前就已经存在，Kubernetes 就创建了一个适配器组件：<code>dockershim</code>。
dockershim 适配器允许 kubelet 与 Docker交互，就好像 Docker 是一个 CRI 兼容的运行时一样。</p>
<!-- 
You can read about it in [Kubernetes Containerd integration goes GA](/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/) blog post.
 -->
<p>你可以阅读博文
<a href="/zh/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/">Kubernetes 容器集成功能的正式发布</a></p>
<!-- Dockershim vs. CRI with Containerd -->
<p><img src="/images/blog/2018-05-24-kubernetes-containerd-integration-goes-ga/cri-containerd.png" alt="Dockershim 和 Containerd CRI 的实现对比图"></p>
<!-- 
Switching to Containerd as a container runtime eliminates the middleman. All the
same containers can be run by container runtimes like Containerd as before. But
now, since containers schedule directly with the container runtime, they are not visible to Docker.
So any Docker tooling or fancy UI you might have used
before to check on these containers is no longer available.
 -->
<p>切换到容器运行时 Containerd 可以消除掉中间环节。
所有以前遗留的容器可由 Containerd 这类容器运行时来运行和管理，操作体验也和以前一样。
但是现在，由于直接用容器运行时调度容器，所以它们对 Docker 来说是不可见的。
因此，你以前用来检查这些容器的 Docker 工具或漂亮的 UI 都不再可用。</p>
<!-- 
You cannot get container information using `docker ps` or `docker inspect`
commands. As you cannot list containers, you cannot get logs, stop containers,
or execute something inside container using `docker exec`.
 -->
<p>你不能再使用 <code>docker ps</code> 或 <code>docker inspect</code> 命令来获取容器信息。
由于你不能列出容器，因此你不能获取日志、停止容器，甚至不能通过 <code>docker exec</code> 在容器中执行命令。</p>
<!-- 
If you're running workloads via Kubernetes, the best way to stop a container is through
the Kubernetes API rather than directly through the container runtime (this advice applies
for all container runtimes, not just Docker).
 -->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果你用 Kubernetes 运行工作负载，最好通过 Kubernetes API停止容器，而不是通过容器运行时
（此建议适用于所有容器运行时，不仅仅是针对 Docker）。</div>
</blockquote>
<!-- 
You can still pull images or build them using `docker build` command. But images
built or pulled by Docker would not be visible to container runtime and
Kubernetes. They needed to be pushed to some registry to allow them to be used
by Kubernetes.
 -->
<p>你仍然可以下载镜像，或者用 <code>docker build</code> 命令创建它们。
但用 Docker 创建、下载的镜像，对于容器运行时和 Kubernetes，均不可见。
为了在 Kubernetes 中使用，需要把镜像推送（push）到某注册中心。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eb3e279a6c5e1224e744080a52ee3f28">2.1.2 - 从 dockershim 迁移遥测和安全代理</h1>
    
	<!-- 
title: Migrating telemetry and security agents from dockershim
content_type: task 
reviewers:
- SergeyKanzhelev
weight: 70
-->
<!-- overview -->
<!-- 
With Kubernetes 1.20 dockershim was deprecated. From the
[Dockershim Deprecation FAQ](/blog/2020/12/02/dockershim-faq/)
you might already know that most apps do not have a direct dependency on runtime hosting
containers. However, there are still a lot of telemetry and security agents
that has a dependency on docker to collect containers metadata, logs and
metrics. This document aggregates information on how to detect tese
dependencies and links on how to migrate these agents to use generic tools or
alternative runtimes.
-->
<p>在 Kubernetes 1.20 版本中，dockershim 被弃用。
在博文<a href="/zh/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 常见问题</a>中，
你大概已经了解到，大多数应用并没有直接通过运行时来托管容器。
但是，仍然有大量的遥测和安全代理依赖 docker 来收集容器元数据、日志和指标。
本文汇总了一些信息和链接：信息用于阐述如何探查这些依赖，链接用于解释如何迁移这些代理去使用通用的工具或其他容器运行。</p>
<!-- 
## Telemetry and security agents 
-->
<h2 id="telemetry-and-security-agents">遥测和安全代理</h2>
<!-- 
There are a few ways agents may run on Kubernetes cluster. Agents may run on
nodes directly or as DaemonSets.
-->
<p>为了让代理运行在 Kubernetes 集群中，我们有几种办法。
代理既可以直接在节点上运行，也可以作为守护进程运行。</p>
<!-- 
### Why do telemetry agents rely on Docker?
-->
<h3 id="why-do-telemetry-agents-relyon-docker">为什么遥测代理依赖于 Docker？</h3>
<!-- 
Historically, Kubernetes was built on top of Docker. Kubernetes is managing
networking and scheduling, Docker was placing and operating containers on a
node. So you can get scheduling-related metadata like a pod name from Kubernetes
and containers state information from Docker. Over time more runtimes were
created to manage containers. Also there are projects and Kubernetes features
that generalize container status information extraction across many runtimes.
-->
<p>因为历史原因，Kubernetes 建立在 Docker 之上。
Kubernetes 管理网络和调度，Docker 则在具体的节点上定位并操作容器。
所以，你可以从 Kubernetes 取得调度相关的元数据，比如 Pod 名称；从 Docker 取得容器状态信息。
后来，人们开发了更多的运行时来管理容器。
同时一些项目和 Kubernetes 特性也不断涌现，支持跨多个运行时收集容器状态信息。</p>
<!-- 
Some agents are tied specifically to the Docker tool. The agents may run
commands like [`docker ps`](https://docs.docker.com/engine/reference/commandline/ps/)
or [`docker top`](https://docs.docker.com/engine/reference/commandline/top/) to list
containers and processes or [docker logs](https://docs.docker.com/engine/reference/commandline/logs/)
to subscribe on docker logs. With the deprecating of Docker as a container runtime,
these commands will not work any longer.
-->
<p>一些代理和 Docker 工具紧密绑定。此类代理可以这样运行命令，比如用
<a href="https://docs.docker.com/engine/reference/commandline/ps/"><code>docker ps</code></a>
或 <a href="https://docs.docker.com/engine/reference/commandline/top/"><code>docker top</code></a>
这类命令来列出容器和进程，用
<a href="https://docs.docker.com/engine/reference/commandline/logs/">docker logs</a>
订阅 Docker 的日志。
但随着 Docker 作为容器运行时被弃用，这些命令将不再工作。</p>
<!-- 
### Identify DaemonSets that depend on Docker {#identify-docker-dependency }
-->
<h3 id="identify-docker-dependency">识别依赖于 Docker 的 DaemonSet</h3>
<!-- 
If a pod wants to make calls to the `dockerd` running on the node, the pod must either:

- mount the filesystem containing the Docker daemon's privileged socket, as a
  <a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='volume'>volume</a>; or
- mount the specific path of the Docker daemon's privileged socket directly, also as a volume.
-->
<p>如果某 Pod 想调用运行在节点上的 <code>dockerd</code>，该 Pod 必须满足以下两个条件之一：</p>
<ul>
<li>将包含 Docker 守护进程特权套接字的文件系统挂载为一个<a class='glossary-tooltip' title='包含可被 Pod 中容器访问的数据的目录。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/volumes/' target='_blank' aria-label='卷'>卷</a>；或</li>
<li>直接以卷的形式挂载 Docker 守护进程特权套接字的特定路径。</li>
</ul>
<!-- 
For example: on COS images, Docker exposes its Unix domain socket at
`/var/run/docker.sock` This means that the pod spec will include a
`hostPath` volume mount of `/var/run/docker.sock`.
-->
<p>举例来说：在 COS 镜像中，Docker 通过 <code>/var/run/docker.sock</code> 开放其 Unix 域套接字。
这意味着 Pod 的规约中需要包含 <code>hostPath</code> 卷以挂载 <code>/var/run/docker.sock</code>。</p>
<!-- 
Here's a sample shell script to find Pods that have a mount directly mapping the
Docker socket. This script outputs the namespace and name of the pod. You can
remove the grep `/var/run/docker.sock` to review other mounts.
-->
<p>下面是一个 shell 示例脚本，用于查找包含直接映射 Docker 套接字的挂载点的 Pod。
你也可以删掉 grep <code>/var/run/docker.sock</code> 这一代码片段以查看其它挂载信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pods --all-namespaces <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>-o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{range .items[*]}{&#34;\n&#34;}{.metadata.namespace}{&#34;:\t&#34;}{.metadata.name}{&#34;:\t&#34;}{range .spec.volumes[*]}{.hostPath.path}{&#34;, &#34;}{end}{end}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>| sort <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>| grep <span style="color:#b44">&#39;/var/run/docker.sock&#39;</span>
</code></pre></div><!-- 
There are alternative ways for a pod to access Docker on the host. For instance, the parent
directory `/var/run` may be mounted instead of the full path (like in [this
example](https://gist.github.com/itaysk/7bc3e56d69c4d72a549286d98fd557dd)).
The script above only detects the most common uses.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 对于 Pod 来说，访问宿主机上的 Docker 还有其他方式。
例如，可以挂载 <code>/var/run</code> 的父目录而非其完整路径
（就像<a href="https://gist.github.com/itaysk/7bc3e56d69c4d72a549286d98fd557dd">这个例子</a>）。
上述脚本只检测最常见的使用方式。</div>
</blockquote>
<!-- 
### Detecting Docker dependency from node agents
-->
<h3 id="detecting-docker-dependency-from-node-agents">检测节点代理对 Docker 的依赖性</h3>
<!-- 
In case your cluster nodes are customized and install additional security and
telemetry agents on the node, make sure to check with the vendor of the agent whether it has dependency on Docker.
-->
<p>在你的集群节点被定制、且在各个节点上均安装了额外的安全和遥测代理的场景下，
一定要和代理的供应商确认：该代理是否依赖于 Docker。</p>
<!-- 
### Telemetry and security agent vendors
-->
<h3 id="telemetry-and-security-agent-vendors">遥测和安全代理的供应商</h3>
<!-- 
We keep the work in progress version of migration instructions for various telemetry and security agent vendors
in [Google doc](https://docs.google.com/document/d/1ZFi4uKit63ga5sxEiZblfb-c23lFhvy6RXVPikS8wf0/edit#).
Please contact the vendor to get up to date instructions for migrating from dockershim.
-->
<p>我们通过
<a href="https://docs.google.com/document/d/1ZFi4uKit63ga5sxEiZblfb-c23lFhvy6RXVPikS8wf0/edit#">谷歌文档</a>
提供了为各类遥测和安全代理供应商准备的持续更新的迁移指导。
请与供应商联系，获取从 dockershim 迁移的最新说明。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8e16d69617b175d61e2e7a6e1642c9d6">2.2 - 用 kubeadm 进行管理</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-f62fba1de4084f3be070785757c8079c">2.2.1 - 使用 kubeadm 进行证书管理</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Certificate Management with kubeadm
content_type: task
weight: 10
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.15 [stable]</code>
</div>

<!-- 
Client certificates generated by [kubeadm](/docs/reference/setup-tools/kubeadm/) expire after 1 year. This page explains how to manage certificate renewals with kubeadm. 
-->
<p>由 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm</a> 生成的客户端证书在 1 年后到期。
本页说明如何使用 kubeadm 管理证书续订。</p>
<h2 id="准备开始">准备开始</h2>
<!--
You should be familiar with [PKI certificates and requirements in Kubernetes](/docs/setup/best-practices/certificates/).
-->
<p>你应该熟悉 <a href="/zh/docs/setup/best-practices/certificates/">Kubernetes 中的 PKI 证书和要求</a>。</p>
<!-- steps -->
<!--
## Using custom certificates {#custom-certificates}

By default, kubeadm generates all the certificates needed for a cluster to run.
You can override this behavior by providing your own certificates.
-->
<h2 id="custom-certificates">使用自定义的证书</h2>
<p>默认情况下, kubeadm 会生成运行一个集群所需的全部证书。
你可以通过提供你自己的证书来改变这个行为策略。</p>
<!--
To do so, you must place them in whatever directory is specified by the
`--cert-dir` flag or the `CertificatesDir`field of kubeadm's `ClusterConfiguration` . By default this
is `/etc/kubernetes/pki`.
-->
<p>如果要这样做, 你必须将证书文件放置在通过 <code>--cert-dir</code> 命令行参数或者 kubeadm 配置中的
<code>CertificatesDir</code> 配置项指明的目录中。默认的值是 <code>/etc/kubernetes/pki</code>。</p>
<!--
If a given certificate and private key pair exists before running `kubeadm init`,
kubeadm does not overwrite them. This means you can, for example, copy an existing
CA into `/etc/kubernetes/pki/ca.crt` and `/etc/kubernetes/pki/ca.key`,
and kubeadm will use this CA for signing the rest of the certificates.
-->
<p>如果在运行 <code>kubeadm init</code> 之前存在给定的证书和私钥对，kubeadm 将不会重写它们。
例如，这意味着您可以将现有的 CA 复制到 <code>/etc/kubernetes/pki/ca.crt</code> 和
<code>/etc/kubernetes/pki/ca.key</code> 中，而 kubeadm 将使用此 CA 对其余证书进行签名。</p>
<!--
## External CA mode {#external-ca-mode}

It is also possible to provide only the `ca.crt` file and not the
`ca.key` file (this is only available for the root CA file, not other cert pairs).
If all other certificates and kubeconfig files are in place, kubeadm recognizes
this condition and activates the "External CA" mode. kubeadm will proceed without the CA key on disk.
-->
<h2 id="external-ca-mode">外部 CA 模式</h2>
<p>只提供了 <code>ca.crt</code> 文件但是不提供 <code>ca.key</code> 文件也是可以的
（这只对 CA 根证书可用，其它证书不可用）。
如果所有的其它证书和 kubeconfig 文件已就绪，kubeadm 检测到满足以上条件就会激活
&quot;外部 CA&quot; 模式。kubeadm 将会在没有 CA 密钥文件的情况下继续执行。</p>
<!--
Instead, run the controller-manager standalone with `--controllers=csrsigner` and
point to the CA certificate and key.
-->
<p>否则, kubeadm 将独立运行 controller-manager，附加一个
<code>--controllers=csrsigner</code> 的参数，并且指明 CA 证书和密钥。</p>
<!--
[PKI certificates and requirements](/docs/setup/best-practices/certificates/) includes guidance on
setting up a cluster to use an external CA.
-->
<p><a href="/zh/docs/setup/best-practices/certificates/">PKI 证书和要求</a>包括集群使用外部 CA 的设置指南。</p>
<!-- 
## Check certificate expiration 

You can use the `check-expiration` subcommand to check when certificates expire:
-->
<h2 id="检查证书是否过期">检查证书是否过期</h2>
<p>你可以使用 <code>check-expiration</code> 子命令来检查证书何时过期</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm certs check-expiration
</code></pre></div><!-- 
The output is similar to this: 
-->
<p>输出类似于以下内容：</p>
<pre tabindex="0"><code>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Dec 30, 2020 23:36 UTC   364d                                    no
apiserver                  Dec 30, 2020 23:36 UTC   364d            ca                      no
apiserver-etcd-client      Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
apiserver-kubelet-client   Dec 30, 2020 23:36 UTC   364d            ca                      no
controller-manager.conf    Dec 30, 2020 23:36 UTC   364d                                    no
etcd-healthcheck-client    Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
etcd-peer                  Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
etcd-server                Dec 30, 2020 23:36 UTC   364d            etcd-ca                 no
front-proxy-client         Dec 30, 2020 23:36 UTC   364d            front-proxy-ca          no
scheduler.conf             Dec 30, 2020 23:36 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Dec 28, 2029 23:36 UTC   9y              no
etcd-ca                 Dec 28, 2029 23:36 UTC   9y              no
front-proxy-ca          Dec 28, 2029 23:36 UTC   9y              no
</code></pre><!-- 
The command shows expiration/residual time for the client certificates in the `/etc/kubernetes/pki` folder and for the client certificate embedded in the KUBECONFIG files used by kubeadm (`admin.conf`, `controller-manager.conf` and `scheduler.conf`). 
-->
<p>该命令显示 <code>/etc/kubernetes/pki</code> 文件夹中的客户端证书以及
kubeadm（<code>admin.conf</code>, <code>controller-manager.conf</code> 和 <code>scheduler.conf</code>）
使用的 KUBECONFIG 文件中嵌入的客户端证书的到期时间/剩余时间。</p>
<!-- 
Additionally, kubeadm informs the user if the certificate is externally managed; in this case, the user should take care of managing certificate renewal manually/using other tools. 
-->
<p>另外， kubeadm 会通知用户证书是否由外部管理；
在这种情况下，用户应该小心的手动/使用其他工具来管理证书更新。</p>
<!--
`kubeadm` cannot manage certificates signed by an external CA.
 -->
<blockquote class="warning callout">
  <div><strong>警告：</strong> <code>kubeadm</code> 不能管理由外部 CA 签名的证书</div>
</blockquote>

<!-- 
`kubelet.conf` is not included in the list above because kubeadm configures kubelet
for [automatic certificate renewal](/docs/tasks/tls/certificate-rotation/)
with rotatable certificates under `/var/lib/kubelet/pki`.
To repair an expired kubelet client certificate see
[Kubelet client certificate rotation fails](/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 上面的列表中没有包含 <code>kubelet.conf</code>，因为 kubeadm 将 kubelet 配置为
<a href="/docs/tasks/tls/certificate-rotation/">自动更新证书</a>。
轮换的证书位于目录 <code>/var/lib/kubelet/pki</code>。
要修复过期的 kubelet 客户端证书，请参阅
<a href="/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert">kubelet 客户端证书轮换失败</a>。</div>
</blockquote>
<!--
On nodes created with `kubeadm init`, prior to kubeadm version 1.17, there is a
[bug](https://github.com/kubernetes/kubeadm/issues/1753) where you manually have to modify the contents of `kubelet.conf`. After `kubeadm init` finishes, you should update `kubelet.conf` to point to the
rotated kubelet client certificates, by replacing `client-certificate-data` and `client-key-data` with:
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> <p>在通过 <code>kubeadm init</code> 创建的节点上，在 kubeadm 1.17 版本之前有一个
<a href="https://github.com/kubernetes/kubeadm/issues/1753">缺陷</a>，该缺陷
使得你必须手动修改 <code>kubelet.conf</code> 文件的内容。
<code>kubeadm init</code> 操作结束之后，你必须更新 <code>kubelet.conf</code> 文件
将 <code>client-certificate-data</code> 和 <code>client-key-data</code> 改为如下所示的内容
以便使用轮换后的 kubelet 客户端证书：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">client-certificate</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">client-key</span>:<span style="color:#bbb"> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style="color:#bbb">
</span></code></pre></div></div>
</blockquote>

<!-- 
## Automatic certificate renewal

`kubeadm` renews all the certificates during control plane [upgrade](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/). 
-->
<h2 id="自动更新证书">自动更新证书</h2>
<p><code>kubeadm</code> 会在控制面
<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">升级</a>
的时候更新所有证书。</p>
<!-- 
This feature is designed for addressing the simplest use cases; 
if you don't have specific requirements on certificate renewal and perform Kubernetes version upgrades regularly (less than 1 year in between each upgrade), kubeadm will take care of keeping your cluster up to date and reasonably secure. 
-->
<p>这个功能旨在解决最简单的用例；如果你对此类证书的更新没有特殊要求，
并且定期执行 Kubernetes 版本升级（每次升级之间的间隔时间少于 1 年），
则 kubeadm 将确保你的集群保持最新状态并保持合理的安全性。</p>
<!-- 
It is a best practice to upgrade your cluster frequently in order to stay secure.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 最佳的做法是经常升级集群以确保安全。</div>
</blockquote>
<!-- 
If you have more complex requirements for certificate renewal, you can opt out from the default behavior by passing `--certificate-renewal=false` to `kubeadm upgrade apply` or to `kubeadm upgrade node`. 
-->
<p>如果你对证书更新有更复杂的需求，则可通过将 <code>--certificate-renewal=false</code> 传递给
<code>kubeadm upgrade apply</code> 或者 <code>kubeadm upgrade node</code>，从而选择不采用默认行为。</p>
<!--
Prior to kubeadm version 1.17 there is a [bug](https://github.com/kubernetes/kubeadm/issues/1818)
where the default value for `--certificate-renewal` is `false` for the `kubeadm upgrade node`
command. In that case, you should explicitly set `--certificate-renewal=true`.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> kubeadm 在 1.17 版本之前有一个<a href="https://github.com/kubernetes/kubeadm/issues/1818">缺陷</a>，
该缺陷导致 <code>kubeadm update node</code> 执行时 <code>--certificate-renewal</code> 的默认值被设置为 <code>false</code>。
在这种情况下，你需要显式地设置 <code>--certificate-renewal=true</code>。</div>
</blockquote>

<!-- 
## Manual certificate renewal 

You can renew your certificates manually at any time with the `kubeadm certs renew` command. 
-->
<h2 id="手动更新证书">手动更新证书</h2>
<p>你能随时通过 <code>kubeadm certs renew</code> 命令手动更新你的证书。</p>
<!-- 
This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in `/etc/kubernetes/pki`. 
-->
<p>此命令用 CA （或者 front-proxy-CA ）证书和存储在 <code>/etc/kubernetes/pki</code> 中的密钥执行更新。</p>
<!-- 
If you are running an HA cluster, this command needs to be executed on all the control-plane nodes. 
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 如果你运行了一个 HA 集群，这个命令需要在所有控制面板节点上执行。</div>
</blockquote>

<!-- 
` certs renew` uses the existing certificates as the authoritative source for attributes (Common Name, Organization, SAN, etc.) instead of the kubeadm-config ConfigMap. It is strongly recommended to keep them both in sync.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>certs renew</code> 使用现有的证书作为属性 (Common Name、Organization、SAN 等) 的权威来源，
而不是 kubeadm-config ConfigMap 。强烈建议使它们保持同步。</div>
</blockquote>
<!--
`kubeadm certs renew` provides the following options:
-->
<p><code>kubeadm certs renew</code>提供以下选项：</p>
<!--
The Kubernetes certificates normally reach their expiration date after one year.
-->
<p>Kubernetes 证书通常在一年后到期。</p>
<!-- 

- `--csr-only` can be used to renew certificats with an external CA by generating certificate signing requests (without actually renewing certificates in place); see next paragraph for more information. 
- It's also possible to renew a single certificate instead of all.
 -->
<ul>
<li><code>--csr-only</code> 可用于经过一个外部 CA 生成的证书签名请求来更新证书（无需实际替换更新证书）；
更多信息请参见下节。</li>
<li>可以更新单个证书而不是全部证书。</li>
</ul>
<!--
## Renew certificates with the Kubernetes certificates API

This section provide more details about how to execute manual certificate renewal using the Kubernetes certificates API.
-->
<h2 id="用-kubernetes-证书-api-更新证书">用 Kubernetes 证书 API 更新证书</h2>
<p>本节提供有关如何使用 Kubernetes 证书 API 执行手动证书更新的更多详细信息。</p>
<!-- 
These are advanced topics for users who need to integrate their organization's certificate infrastructure into a kubeadm-built cluster. If the default kubeadm configuration satisfies your needs, you should let kubeadm manage certificates instead. 
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 这些是针对需要将其组织的证书基础结构集成到 kubeadm 构建的集群中的用户的高级主题。
如果默认的 kubeadm 配置满足了你的需求，则应让 kubeadm 管理证书。</div>
</blockquote>

<!--
### Set up a signer

The Kubernetes Certificate Authority does not work out of the box.
You can configure an external signer such as [cert-manager](https://docs.cert-manager.io/en/latest/tasks/issuers/setup-ca.html), or you can use the build-in signer.
The built-in signer is part of [`kube-controller-manager`](/docs/reference/command-line-tools-reference/kube-controller-manager/).
To activate the build-in signer, you must pass the `--cluster-signing-cert-file` and `--cluster-signing-key-file` flags.
-->
<h3 id="设置一个签名者-signer">设置一个签名者（Signer）</h3>
<p>Kubernetes 证书颁发机构不是开箱即用。
你可以配置外部签名者，例如
<a href="https://docs.cert-manager.io/en/latest/tasks/issuers/setup-ca.html">cert-manager</a>，
也可以使用内置签名者。
内置签名者是
<a href="/zh/docs/reference/command-line-tools-reference/kube-controller-manager/"><code>kube-controller-manager</code></a>
的一部分。
要激活内置签名者，请传递 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数。</p>
<!--
If you're creating a new cluster, you can use a kubeadm [configuration file](/docs/reference/config-api/kubeadm-config.v1beta2/): 
-->
<p>如果你正在创建一个新的集群，你可以使用 kubeadm 的
<a href="/docs/reference/config-api/kubeadm-config.v1beta2/">配置文件</a>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerManager</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">extraArgs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster-signing-cert-file</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/ca.crt<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster-signing-key-file</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki/ca.key<span style="color:#bbb">
</span></code></pre></div><!-- 
### Create certificate signing requests (CSR)
-->
<h3 id="创建证书签名请求-csr">创建证书签名请求 (CSR)</h3>
<!--
See [Create CertificateSigningRequest](/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatesigningrequest) for creating CSRs with the Kubernetes API.
-->
<p>有关使用 Kubernetes API 创建 CSR 的信息，
请参见<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatesigningrequest">创建 CertificateSigningRequest</a>。</p>
<!--
## Renew certificates with external CA

This section provides more details about how to execute manual certificate renewal using an external CA.
-->
<h2 id="通过外部-ca-更新证书">通过外部 CA 更新证书</h2>
<p>本节提供有关如何使用外部 CA 执行手动更新证书的更多详细信息。</p>
<!--
To better integrate with external CAs, kubeadm can also produce certificate signing requests (CSRs).
A CSR represents a request to a CA for a signed certificate for a client.
In kubeadm terms, any certificate that would normally be signed by an on-disk CA can be produced as a CSR instead. A CA, however, cannot be produced as a CSR.
-->
<p>为了更好的与外部 CA 集成，kubeadm 还可以生成证书签名请求（CSR）。
CSR 表示向 CA 请求客户的签名证书。
在 kubeadm 术语中，通常由磁盘 CA 签名的任何证书都可以作为 CSR 生成。但是，CA 不能作为 CSR 生成。</p>
<!-- 
### Create certificate signing requests (CSR) 

You can create certificate signing requests with `kubeadm certs renew --csr-only`.

Both the CSR and the accompanying private key are given in the output.
You can pass in a directory with `--csr-dir` to output the CSRs to the specified location.
If `--csr-dir` is not specified, the default certificate directory (`/etc/kubernetes/pki`) is used.
-->
<h3 id="创建证书签名请求-csr-1">创建证书签名请求 (CSR)</h3>
<p>你可以通过 <code>kubeadm certs renew --csr-only</code> 命令创建证书签名请求。</p>
<p>CSR 和随附的私钥都在输出中给出。
你可以传入一个带有 <code>--csr-dir</code> 的目录，将 CRS 输出到指定位置。
如果未指定 <code>--csr-dir</code> ，则使用默认证书目录（<code>/etc/kubernetes/pki</code>）。</p>
<!--
Certificates can be renewed with `kubeadm certs renew --csr-only`.
As with `kubeadm init`, an output directory can be specified with the `--csr-dir` flag.
-->
<p>证书可以通过 <code>kubeadm certs renew --csr-only</code> 来续订。
和 <code>kubeadm init</code> 一样，可以使用 <code>--csr-dir</code> 标志指定一个输出目录。</p>
<p>CSR 签署证书后，必须将证书和私钥复制到 PKI 目录（默认情况下为 <code>/etc/kubernetes/pki</code>）。</p>
<!--
A CSR contains a certificate's name, domains, and IPs, but it does not specify usages.
It is the responsibility of the CA to specify [the correct cert usages](/docs/setup/best-practices/certificates/#all-certificates)
when issuing a certificate.
-->
<p>CSR 中包含一个证书的名字，域和 IP，但是未指定用法。
颁发证书时，CA 有责任指定<a href="/zh/docs/setup/best-practices/certificates/#all-certificates">正确的证书用法</a></p>
<!-- 
* In `openssl` this is done with the
  [`openssl ca` command](https://superuser.com/questions/738612/openssl-ca-keyusage-extension).
* In `cfssl` you specify
  [usages in the config file](https://github.com/cloudflare/cfssl/blob/master/doc/cmd/cfssl.txt#L170).
-->
<ul>
<li>在 <code>openssl</code> 中，这是通过
<a href="https://superuser.com/questions/738612/openssl-ca-keyusage-extension"><code>openssl ca</code> 命令</a>
来完成的。</li>
<li>在 <code>cfssl</code> 中，这是通过
<a href="https://github.com/cloudflare/cfssl/blob/master/doc/cmd/cfssl.txt#L170">在配置文件中指定用法</a>
来完成的。</li>
</ul>
<!-- 
After a certificate is signed using your preferred method, the certificate and the private key must be copied to the PKI directory (by default `/etc/kubernetes/pki`). 
-->
<p>使用首选方法对证书签名后，必须将证书和私钥复制到 PKI 目录（默认为 <code>/etc/kubernetes/pki</code> ）。</p>
<!--
## Certificate authority (CA) rotation {#certificate-authority-rotation}

Kubeadm does not support rotation or replacement of CA certificates out of the box.

For more information about manual rotation or replacement of CA, see [manual rotation of CA certificates](/docs/tasks/tls/manual-rotation-of-ca-certificates/).
-->
<h2 id="certificate-authority-rotation">证书机构（CA）轮换    </h2>
<p>kubeadm 并不直接支持对 CA 证书的轮换或者替换。</p>
<p>关于手动轮换或者置换 CA 的更多信息，可参阅
<a href="/zh/docs/tasks/tls/manual-rotation-of-ca-certificates/">手动轮换 CA 证书</a>。</p>
<!--
## Enabling signed kubelet serving certificates {#kubelet-serving-certs}

By default the kubelet serving certificate deployed by kubeadm is self-signed.
This means a connection from external services like the
[metrics-server](https://github.com/kubernetes-sigs/metrics-server) to a
kubelet cannot be secured with TLS.

To configure the kubelets in a new kubeadm cluster to obtain properly signed serving
certificates you must pass the following minimal configuration to `kubeadm init`:
-->
<h2 id="kubelet-serving-certs">启用已签名的 kubelet 服务证书  </h2>
<p>默认情况下，kubeadm 所部署的 kubelet 服务证书是自签名（Self-Signed））。
这意味着从 <a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>
这类外部服务发起向 kubelet 的链接时无法使用 TLS 来完成保护。</p>
<p>要在新的 kubeadm 集群中配置 kubelet 以使用被正确签名的服务证书，
你必须向 <code>kubeadm init</code> 传递如下最小配置数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">serverTLSBootstrap</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
If you have already created the cluster you must adapt it by doing the following:
 - Find and edit the `kubelet-config-1.22` ConfigMap in the `kube-system` namespace.
In that ConfigMap, the `kubelet` key has a
[KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)
document as its value. Edit the KubeletConfiguration document to set `serverTLSBootstrap: true`.
- On each node, add the `serverTLSBootstrap: true` field in `/var/lib/kubelet/config.yaml`
and restart the kubelet with `systemctl restart kubelet`
-->
<p>如果你已经创建了集群，你必须通过执行下面的操作来完成适配：</p>
<ul>
<li>找到 <code>kube-system</code> 名字空间中名为 <code>kubelet-config-1.22</code>
的 ConfigMap 并编辑之。
在该 ConfigMap 中，<code>kubelet</code> 键下面有一个
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration">KubeletConfiguration</a>
文档作为其取值。编辑该 KubeletConfiguration 文档以设置
<code>serverTLSBootstrap: true</code>。</li>
<li>在每个节点上，在 <code>/var/lib/kubelet/config.yaml</code> 文件中添加
<code>serverTLSBootstrap: true</code> 字段，并使用 <code>systemctl restart kubelet</code>
来重启 kubelet。</li>
</ul>
<!--
The field `serverTLSBootstrap: true` will enable the bootstrap of kubelet serving
certificates by requesting them from the `certificates.k8s.io` API. One known limitation
is that the CSRs (Certificate Signing Requests) for these certificates cannot be automatically
approved by the default signer in the kube-controller-manager -
[`kubernetes.io/kubelet-serving`](/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers).
This will require action from the user or a third party controller.

These CSRs can be viewed using:
-->
<p>字段 <code>serverTLSBootstrap</code> 将允许启动引导 kubelet 的服务证书，方式
是从 <code>certificates.k8s.io</code> API 处读取。这种方式的一种局限在于这些
证书的 CSR（证书签名请求）不能被 kube-controller-manager 中默认的
签名组件
<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers"><code>kubernetes.io/kubelet-serving</code></a>
批准。需要用户或者第三方控制器来执行此操作。</p>
<p>可以使用下面的命令来查看 CSR：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get csr
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME        AGE     SIGNERNAME                        REQUESTOR                      CONDITION
csr-9wvgt   112s    kubernetes.io/kubelet-serving     system:node:worker-1           Pending
csr-lz97v   1m58s   kubernetes.io/kubelet-serving     system:node:control-plane-1    Pending
</code></pre><!--
To approve them you can do the following:
-->
<p>你可以执行下面的操作来批准这些请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl certificate approve &lt;CSR-名称&gt;
</code></pre></div><!--
By default, these serving certificate will expire after one year. Kubeadm sets the
`KubeletConfiguration` field `rotateCertificates` to `true`, which means that close
to expiration a new set of CSRs for the serving certificates will be created and must
be approved to complete the rotation. To understand more see
[Certificate Rotation](/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#certificate-rotation).
-->
<p>默认情况下，这些服务证书上会在一年后过期。
kubeadm 将 <code>KubeletConfiguration</code> 的 <code>rotateCertificates</code> 字段设置为
<code>true</code>；这意味着证书快要过期时，会生成一组针对服务证书的新的 CSR，而
这些 CSR 也要被批准才能完成证书轮换。
要进一步了解这里的细节，可参阅
<a href="/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#certificate-rotation">证书轮换</a>
文档。</p>
<!--
If you are looking for a solution for automatic approval of these CSRs it is recommended
that you contact your cloud provider and ask if they have a CSR signer that verifies
the node identity with an out of band mechanism.
-->
<p>如果你在寻找一种能够自动批准这些 CSR 的解决方案，建议你与你的云提供商
联系，询问他们是否有 CSR 签名组件，用来以带外（out-of-band）的方式检查
节点的标识符。</p>
<blockquote class="callout caution" role="alert">
  <strong>注意：</strong>
  本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循<a href="https://github.com/cncf/foundation/blob/master/website-guidelines.md" target="_blank">CNCF 网站指南</a>，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读<a href="/docs/contribute/style/content-guide/#third-party-content">内容指南</a>。
</blockquote>
<!--
Third party custom controllers can be used:
- [kubelet-rubber-stamp](https://github.com/kontena/kubelet-rubber-stamp)

Such a controller is not a secure mechanism unless it not only verifies the CommonName
in the CSR but also verifies the requested IPs and domain names. This would prevent
a malicious actor that has access to a kubelet client certificate to create
CSRs requesting serving certificates for any IP or domain name.
-->
<p>也可以使用第三方定制的控制器：</p>
<ul>
<li><a href="https://github.com/kontena/kubelet-rubber-stamp">kubelet-rubber-stamp</a></li>
</ul>
<p>除非既能够验证 CSR 中的 CommonName，也能检查请求的 IP 和域名，
这类控制器还算不得安全的机制。
只有完成彻底的检查，才有可能避免有恶意的、能够访问 kubelet 客户端证书的第三方
为任何 IP 或域名请求服务证书。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6134c5061298affa145ddb801b5c29da">2.2.2 - 配置 cgroup 驱动</h1>
    
	<!-- 
---
title: Configuring a cgroup driver
content_type: task
weight: 10
---
-->
<!-- overview -->
<!-- 
This page explains how to configure the kubelet cgroup driver to match the container
runtime cgroup driver for kubeadm clusters.
-->
<p>本页阐述如何配置 kubelet 的 cgroup 驱动以匹配 kubeadm 集群中的容器运行时的 cgroup 驱动。</p>
<h2 id="准备开始">准备开始</h2>
<!-- 
You should be familiar with the Kubernetes
[container runtime requirements](/docs/setup/production-environment/container-runtimes).
-->
<p>你应该熟悉 Kubernetes 的<a href="/zh/docs/setup/production-environment/container-runtimes">容器运行时需求</a>。</p>
<!-- steps -->
<!-- 
## Configuring the container runtime cgroup driver
-->
<h2 id="configuring-the-container-runtime-cgroup-driver">配置容器运行时 cgroup 驱动</h2>
<!-- 
The [Container runtimes](/docs/setup/production-environment/container-runtimes) page
explains that the `systemd` driver is recommended for kubeadm based setups instead
of the `cgroupfs` driver, because kubeadm manages the kubelet as a systemd service.
-->
<p><a href="/zh/docs/setup/production-environment/container-runtimes">容器运行时</a>页面提到：
由于 kubeadm 把 kubelet 视为一个系统服务来管理，所以对基于 kubeadm 的安装，
我们推荐使用 <code>systemd</code> 驱动，不推荐 <code>cgroupfs</code> 驱动。</p>
<!-- 
The page also provides details on how to setup a number of different container runtimes with the
`systemd` driver by default.
-->
<p>此页还详述了如何安装若干不同的容器运行时，并将 <code>systemd</code> 设为其默认驱动。</p>
<!-- 
## Configuring the kubelet cgroup driver
-->
<h2 id="配置-kubelet-的-cgroup-驱动">配置 kubelet 的 cgroup 驱动</h2>
<!-- 
kubeadm allows you to pass a `KubeletConfiguration` structure during `kubeadm init`.
This `KubeletConfiguration` can include the `cgroupDriver` field which controls the cgroup
driver of the kubelet.
-->
<p>kubeadm 支持在执行 <code>kubeadm init</code> 时，传递一个 <code>KubeletConfiguration</code> 结构体。
<code>KubeletConfiguration</code> 包含 <code>cgroupDriver</code> 字段，可用于控制 kubelet 的 cgroup 驱动。</p>
<!-- 
If the user is not setting the `cgroupDriver` field under `KubeletConfiguration`,
`kubeadm init` will default it to `systemd`.
-->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>

<blockquote class="note callout">
  <div><strong>说明：</strong> 如果用户没有在 <code>KubeletConfiguration</code> 中设置 <code>cgroupDriver</code> 字段，
<code>kubeadm init</code> 会将它设置为默认值 <code>systemd</code>。</div>
</blockquote>
<!-- 
A minimal example of configuring the field explicitly:
-->
<p>这是一个最小化的示例，其中显式的配置了此字段：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># kubeadm-config.yaml</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubeadm.k8s.io/v1beta2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kubernetesVersion</span>:<span style="color:#bbb"> </span>v1.21.0<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">cgroupDriver</span>:<span style="color:#bbb"> </span>systemd<span style="color:#bbb">
</span></code></pre></div><!-- 
Such a configuration file can then be passed to the kubeadm command:
-->
<p>这样一个配置文件就可以传递给 kubeadm 命令了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm init --config kubeadm-config.yaml
</code></pre></div><!-- 
Kubeadm uses the same `KubeletConfiguration` for all nodes in the cluster.
The `KubeletConfiguration` is stored in a [ConfigMap](/docs/concepts/configuration/configmap)
object under the `kube-system` namespace.

Executing the sub commands `init`, `join` and `upgrade` would result in kubeadm
writing the `KubeletConfiguration` as a file under `/var/lib/kubelet/config.yaml`
and passing it to the local node kubelet.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>Kubeadm 对集群所有的节点，使用相同的 <code>KubeletConfiguration</code>。
<code>KubeletConfiguration</code> 存放于 <code>kube-system</code> 命名空间下的某个
<a href="/zh/docs/concepts/configuration/configmap">ConfigMap</a> 对象中。</p>
<p>执行 <code>init</code>、<code>join</code> 和 <code>upgrade</code> 等子命令会促使 kubeadm
将 <code>KubeletConfiguration</code> 写入到文件 <code>/var/lib/kubelet/config.yaml</code> 中，
继而把它传递给本地节点的 kubelet。</p>
</div>
</blockquote>
<!-- 
## Using the `cgroupfs` driver
-->
<h1 id="使用-cgroupfs-驱动">使用 <code>cgroupfs</code> 驱动</h1>
<!-- 
As this guide explains using the `cgroupfs` driver with kubeadm is not recommended.

To continue using `cgroupfs` and to prevent `kubeadm upgrade` from modifying the
`KubeletConfiguration` cgroup driver on existing setups, you must be explicit
about its value. This applies to a case where you do not wish future versions
of kubeadm to apply the `systemd` driver by default.
-->
<p>正如本指南阐述的：不推荐与 kubeadm 一起使用 <code>cgroupfs</code> 驱动。</p>
<p>如仍需使用 <code>cgroupfs</code>，
且要防止 <code>kubeadm upgrade</code> 修改现有系统中 <code>KubeletConfiguration</code> 的 cgroup 驱动，
你必须显式声明它的值。
此方法应对的场景为：在将来某个版本的 kubeadm 中，你不想使用默认的 <code>systemd</code> 驱动。</p>
<!-- 
See the below section on "Modify the kubelet ConfigMap" for details on
how to be explicit about the value.

If you wish to configure a container runtime to use the `cgroupfs` driver,
you must refer to the documentation of the container runtime of your choice.
-->
<p>参阅以下章节“修改 kubelet 的 ConfigMap”，了解显式设置该值的方法。</p>
<p>如果你希望配置容器运行时来使用 <code>cgroupfs</code> 驱动，
则必须参考所选容器运行时的文档。</p>
<!-- 
## Migrating to the `systemd` driver
-->
<h2 id="迁移到-systemd-驱动">迁移到 <code>systemd</code> 驱动</h2>
<!-- 
To change the cgroup driver of an existing kubeadm cluster to `systemd` in-place,
a similar procedure to a kubelet upgrade is required. This must include both
steps outlined below.
-->
<p>要将现有 kubeadm 集群的 cgroup 驱动就地升级为 <code>systemd</code>，
需要执行一个与 kubelet 升级类似的过程。
该过程必须包含下面两个步骤：</p>
<!-- 
Alternatively, it is possible to replace the old nodes in the cluster with new ones
that use the `systemd` driver. This requires executing only the first step below
before joining the new nodes and ensuring the workloads can safely move to the new
nodes before deleting the old nodes.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 还有一种方法，可以用已配置了 <code>systemd</code> 的新节点替换掉集群中的老节点。
按这种方法，在加入新节点、确保工作负载可以安全迁移到新节点、及至删除旧节点这一系列操作之前，
只需执行以下第一个步骤。</div>
</blockquote>
<!-- 
### Modify the kubelet ConfigMap
-->
<h3 id="修改-kubelet-的-configmap">修改 kubelet 的 ConfigMap</h3>
<!-- 
- Find the kubelet ConfigMap name using `kubectl get cm -n kube-system | grep kubelet-config`.
- Call `kubectl edit cm kubelet-config-x.yy -n kube-system` (replace `x.yy` with
the Kubernetes version).
- Either modify the existing `cgroupDriver` value or add a new field that looks like this:
-->
<ul>
<li>
<p>用命令 <code>kubectl get cm -n kube-system | grep kubelet-config</code> 找到 kubelet 的 ConfigMap 名称。</p>
</li>
<li>
<p>运行 <code>kubectl edit cm kubelet-config-x.yy -n kube-system</code> （把 <code>x.yy</code> 替换为 Kubernetes 版本）。</p>
</li>
<li>
<p>修改现有 <code>cgroupDriver</code> 的值，或者新增如下式样的字段：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">cgroupDriver</span>:<span style="color:#bbb"> </span>systemd<span style="color:#bbb">
</span></code></pre></div><!-- 
This field must be present under the `kubelet:` section of the ConfigMap.
-->
<p>该字段必须出现在 ConfigMap 的 <code>kubelet:</code> 小节下。</p>
</li>
</ul>
<!-- 
### Update the cgroup driver on all nodes
-->
<h3 id="更新所有节点的-cgroup-驱动">更新所有节点的 cgroup 驱动</h3>
<!-- 
For each node in the cluster:

- [Drain the node](/docs/tasks/administer-cluster/safely-drain-node) using `kubectl drain <node-name> --ignore-daemonsets`
- Stop the kubelet using `systemctl stop kubelet`
- Stop the container runtime
- Modify the container runtime cgroup driver to `systemd`
- Set `cgroupDriver: systemd` in `/var/lib/kubelet/config.yaml`
- Start the container runtime
- Start the kubelet using `systemctl start kubelet`
- [Uncordon the node](/docs/tasks/administer-cluster/safely-drain-node) using `kubectl uncordon <node-name>`
-->
<p>对于集群中的每一个节点：</p>
<ul>
<li>执行命令 <code>kubectl drain &lt;node-name&gt; --ignore-daemonsets</code>，以
<a href="/zh/docs/tasks/administer-cluster/safely-drain-node">腾空节点</a></li>
<li>执行命令 <code>systemctl stop kubelet</code>，以停止 kubelet</li>
<li>停止容器运行时</li>
<li>修改容器运行时 cgroup 驱动为 <code>systemd</code></li>
<li>在文件 <code>/var/lib/kubelet/config.yaml</code> 中添加设置 <code>cgroupDriver: systemd</code></li>
<li>启动容器运行时</li>
<li>执行命令 <code>systemctl start kubelet</code>，以启动 kubelet</li>
<li>执行命令 <code>kubectl uncordon &lt;node-name&gt;</code>，以
<a href="/zh/docs/tasks/administer-cluster/safely-drain-node">取消节点隔离</a></li>
</ul>
<!-- 
Execute these steps on nodes one at a time to ensure workloads
have sufficient time to schedule on different nodes.

Once the process is complete ensure that all nodes and workloads are healthy.
-->
<p>在节点上依次执行上述步骤，确保工作负载有充足的时间被调度到其他节点。</p>
<p>流程完成后，确认所有节点和工作负载均健康如常。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2e173356df5179cab9eec90a606f0aa4">2.2.3 - 升级 kubeadm 集群</h1>
    
	<!--
reviewers:
- sig-cluster-lifecycle
title: Upgrading kubeadm clusters
content_type: task
weight: 20
min-kubernetes-server-version: 1.18
-->
<!-- overview -->
<!--
This page explains how to upgrade a Kubernetes cluster created with kubeadm from version
1.21.x to version 1.22.x, and from version
1.22.x to 1.22.y (where `y > x`). Skipping MINOR versions
when upgrading is unsupported.
-->
<p>本页介绍如何将 <code>kubeadm</code> 创建的 Kubernetes 集群从 1.21.x 版本
升级到 1.22.x 版本以及从 1.22.x
升级到 1.22.y（其中 <code>y &gt; x</code>）。略过次版本号的升级是
不被支持的。</p>
<!--
To see information about upgrading clusters created using older versions of kubeadm,
please refer to following pages instead:
-->
<p>要查看 kubeadm 创建的有关旧版本集群升级的信息，请参考以下页面：</p>
<!--
- [Upgrading kubeadm cluster from 1.17 to 1.18](https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading kubeadm cluster from 1.16 to 1.17](https://v1-17.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading kubeadm cluster from 1.15 to 1.16](https://v1-16.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading kubeadm cluster from 1.14 to 1.15](https://v1-15.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/)
- [Upgrading kubeadm cluster from 1.13 to 1.14](https://v1-15.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-14/)
-->
<ul>
<li><a href="https://v1-18.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">将 kubeadm 集群从 1.17 升级到 1.18</a></li>
<li><a href="https://v1-17.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">将 kubeadm 集群从 1.16 升级到 1.17</a></li>
<li><a href="https://v1-16.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">将 kubeadm 集群从 1.15 升级到 1.16</a></li>
<li><a href="https://v1-15.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/">将 kubeadm 集群从 1.14 升级到 1.15</a></li>
<li><a href="https://v1-15.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-14/">将 kubeadm 集群从 1.13 升级到 1.14</a></li>
</ul>
<!--
The upgrade workflow at high level is the following:

1. Upgrade a primary control plane node.
1. Upgrade additional control plane nodes.
1. Upgrade worker nodes.
-->
<p>升级工作的基本流程如下：</p>
<ol>
<li>升级主控制平面节点</li>
<li>升级其他控制平面节点</li>
<li>升级工作节点</li>
</ol>
<h2 id="准备开始">准备开始</h2>
<!--
- Make sure you read the [release notes](https://git.k8s.io/kubernetes/CHANGELOG/CHANGELOG-1.22.md
) carefully.
- The cluster should use a static control plane and etcd pods or external etcd.
- Make sure to back up any important components, such as app-level state stored in a database.
  `kubeadm upgrade` does not touch your workloads, only components internal to Kubernetes, but backups are always a best practice.
-->
<ul>
<li>务必仔细认真阅读<a href="https://git.k8s.io/kubernetes/CHANGELOG/CHANGELOG-1.22.md
">发行说明</a>。</li>
<li>集群应使用静态的控制平面和 etcd Pod 或者外部 etcd。</li>
<li>务必备份所有重要组件，例如存储在数据库中应用层面的状态。
<code>kubeadm upgrade</code> 不会影响你的工作负载，只会涉及 Kubernetes 内部的组件，但备份终究是好的。</li>
<li><a href="https://serverfault.com/questions/684771/best-way-to-disable-swap-in-linux">必须禁用交换分区</a>。</li>
</ul>
<!--
### Additional information

- [Draining nodes](/docs/tasks/administer-cluster/safely-drain-node/) before kubelet MINOR version
  upgrades is required. In the case of control plane nodes, they could be running CoreDNS Pods or other critical workloads.
- All containers are restarted after upgrade, because the container spec hash value is changed.
-->
<h3 id="附加信息">附加信息</h3>
<ul>
<li>在对 kubelet 作次版本升版时需要<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">腾空节点</a>。
对于控制面节点，其上可能运行着 CoreDNS Pods 或者其它非常重要的负载。</li>
<li>升级后，因为容器规约的哈希值已更改，所有容器都会被重新启动。</li>
</ul>
<!-- steps -->
<!--
## Determine which version to upgrade to

Find the latest stable 1.22 version using the OS package manager:
-->
<h2 id="确定要升级到哪个版本">确定要升级到哪个版本</h2>
<p>使用操作系统的包管理器找到最新的稳定 1.22：</p>
<ul class="nav nav-tabs" id="k8s-install-versions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-versions-0" role="tab" aria-controls="k8s-install-versions-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-versions-1" role="tab" aria-controls="k8s-install-versions-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-versions"><div id="k8s-install-versions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-versions-0">

<p><pre tabindex="0"><code>apt update
apt-cache policy kubeadm
# 在列表中查找最新的 1.22 版本
# 它看起来应该是 1.22.x-00，其中 x 是最新的补丁版本
</code></pre></div>
  <div id="k8s-install-versions-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-versions-1">

<p><pre tabindex="0"><code>yum list --showduplicates kubeadm --disableexcludes=kubernetes
# 在列表中查找最新的 1.22 版本
# 它看起来应该是 1.22.x-0，其中 x 是最新的补丁版本
</code></pre></div></div>

<!--
## Upgrade the control plane node

The upgrade procedure on control plane nodes should be executed one node at a time.
Pick a control plane node that you wish to upgrade first. It must have the `/etc/kubernetes/admin.conf` file.

### Call "kubeadm upgrade"
-->
<h2 id="升级控制平面节点">升级控制平面节点</h2>
<p>控制面节点上的升级过程应该每次处理一个节点。
首先选择一个要先行升级的控制面节点。该节点上必须拥有
<code>/etc/kubernetes/admin.conf</code> 文件。</p>
<h3 id="执行-kubeadm-upgrade">执行 &quot;kubeadm upgrade&quot;</h3>
<!--
**Upgrade the first control plane node**
-->
<p><strong>升级第一个控制面节点</strong></p>
<!--
- Upgrade kubeadm:
-->
<ul>
<li>升级 kubeadm：</li>
</ul>
<ul class="nav nav-tabs" id="k8s-install-kubeadm-first-cp" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubeadm-first-cp-0" role="tab" aria-controls="k8s-install-kubeadm-first-cp-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubeadm-first-cp-1" role="tab" aria-controls="k8s-install-kubeadm-first-cp-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-kubeadm-first-cp"><div id="k8s-install-kubeadm-first-cp-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubeadm-first-cp-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 用最新的补丁版本号替换 1.22.x-00 中的 x</span>
apt-mark unhold kubeadm <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get update <span style="color:#666">&amp;&amp;</span> apt-get install -y <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span>1.22.x-00 <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-mark hold kubeadm
-
<span style="color:#080;font-style:italic"># 从 apt-get 1.1 版本起，你也可以使用下面的方法</span>
apt-get update <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get install -y --allow-change-held-packages <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span>1.22.x-00
</code></pre></div></div>
  <div id="k8s-install-kubeadm-first-cp-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubeadm-first-cp-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 用最新的补丁版本号替换 1.22.x-0 中的 x</span>
yum install -y kubeadm-1.22.x-0 --disableexcludes<span style="color:#666">=</span>kubernetes
</code></pre></div></div></div>

<!--
- Verify that the download works and has the expected version:
-->
<ul>
<li>
<p>验证下载操作正常，并且 kubeadm 版本正确：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm version
</code></pre></div></li>
</ul>
<!--
- Verify the upgrade plan:
-->
<ul>
<li>
<p>验证升级计划：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm upgrade plan
</code></pre></div><!--
This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to.
It also shows a table with the component config version states.
-->
<p>此命令检查你的集群是否可被升级，并取回你要升级的目标版本。
命令也会显示一个包含组件配置版本状态的表格。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
  `kubeadm upgrade` also automatically renews the certificates that it manages on this node.
  To opt-out of certificate renewal the flag `--certificate-renewal=false` can be used.
  For more information see the [certificate management guide](/docs/tasks/administer-cluster/kubeadm/kubeadm-certs).
  -->
<p><code>kubeadm upgrade</code> 也会自动对 kubeadm 在节点上所管理的证书执行续约操作。
如果需要略过证书续约操作，可以使用标志 <code>--certificate-renewal=false</code>。
更多的信息，可参阅<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs">证书管理指南</a>。</div>
</blockquote>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
  If `kubeadm upgrade plan` shows any component configs that require manual upgrade, users must provide
  a config file with replacement configs to `kubeadm upgrade apply` via the `--config` command line flag.
  Failing to do so will cause `kubeadm upgrade apply` to exit with an error and not perform an upgrade.
  -->
<p>如果 <code>kubeadm upgrade plan</code> 给出任何需要手动升级的组件配置，用户必须
通过 <code>--config</code> 命令行标志向 <code>kubeadm upgrade apply</code> 命令提供替代的配置文件。
如果不这样做，<code>kubeadm upgrade apply</code> 会出错并退出，不再执行升级操作。</div>
</blockquote>
</li>
</ul>
<!--
- Choose a version to upgrade to, and run the appropriate command. For example:

  ```shell
  # replace x with the patch version you picked for this upgrade
  sudo kubeadm upgrade apply v1.22.x
  ```
-->
<p>选择要升级到的目标版本，运行合适的命令。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 x 替换为你为此次升级所选择的补丁版本号</span>
sudo kubeadm upgrade apply v1.22.x
</code></pre></div>  <!--
  Once the command finishes you should see:
  -->
<p>一旦该命令结束，你应该会看到：</p>
<pre tabindex="0"><code>[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.22.x&quot;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
</code></pre><!--
- Manually upgrade your CNI provider plugin.

  Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow.
  Check the [addons](/docs/concepts/cluster-administration/addons/) page to
  find your CNI provider and see whether additional upgrade steps are required.

  This step is not required on additional control plane nodes if the CNI provider runs as a DaemonSet.
-->
<ul>
<li>
<p>手动升级你的 CNI 驱动插件。</p>
<p>你的容器网络接口（CNI）驱动应该提供了程序自身的升级说明。
参阅<a href="/zh/docs/concepts/cluster-administration/addons/">插件</a>页面查找你的 CNI 驱动，
并查看是否需要其他升级步骤。</p>
<p>如果 CNI 驱动作为 DaemonSet 运行，则在其他控制平面节点上不需要此步骤。</p>
</li>
</ul>
<!--
**For the other control plane nodes**
-->
<p><strong>对于其它控制面节点</strong></p>
<!--
Same as the first control plane node but use:
-->
<p>与第一个控制面节点相同，但是使用：</p>
<pre tabindex="0"><code>sudo kubeadm upgrade node
</code></pre><!--
instead of:
-->
<p>而不是：</p>
<pre tabindex="0"><code>sudo kubeadm upgrade apply
</code></pre><!--
Also calling `kubeadm upgrade plan` and upgrading the CNI provider plugin is no longer needed.
-->
<p>此外，不需要执行 <code>kubeadm upgrade plan</code> 和更新 CNI 驱动插件的操作。</p>
<!--
### Drain the node

-  Prepare the node for maintenance by marking it unschedulable and evicting the workloads:

    ```shell
    # replace <node-to-drain> with the name of your node you are draining
    kubectl drain <node-to-drain> --ignore-daemonsets
    ```
-->
<h3 id="腾空节点">腾空节点</h3>
<ul>
<li>
<p>通过将节点标记为不可调度并腾空节点为节点作升级准备：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为你要腾空的控制面节点名称</span>
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div></li>
</ul>
<!--
### Upgrade kubelet and kubectl

-  Upgrade the kubelet and kubectl
-->
<h3 id="升级-kubelet-和-kubectl">升级 kubelet 和 kubectl</h3>
<ul>
<li>
<p>升级 kubelet 和 kubectl</p>
<ul class="nav nav-tabs" id="k8s-install-kubelet" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubelet-0" role="tab" aria-controls="k8s-install-kubelet-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubelet-1" role="tab" aria-controls="k8s-install-kubelet-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-kubelet"><div id="k8s-install-kubelet-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubelet-0">

<p>  <pre>
  # 用最新的补丁版本替换 1.22.x-00 中的 x
  apt-mark unhold kubelet kubectl && \
  apt-get update && apt-get install -y kubelet=1.22.x-00 kubectl=1.22.x-00 && \
  apt-mark hold kubelet kubectl
  - 
  # 从 apt-get 的 1.1 版本开始，你也可以使用下面的方法：
  apt-get update && \
  apt-get install -y --allow-change-held-packages kubelet=1.22.x-00 kubectl=1.22.x-00
  </pre>
</div>
  <div id="k8s-install-kubelet-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubelet-1">

<p>  <pre> 
  # 用最新的补丁版本号替换 1.22.x-00 中的 x
  yum install -y kubelet-1.22.x-0 kubectl-1.22.x-0 --disableexcludes=kubernetes
  </pre>
</div></div>

</li>
</ul>
<!--
- Restart the kubelet
-->
<ul>
<li>
<p>重启 kubelet</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre></div></li>
</ul>
<!--
### Uncordon the node

- Bring the node back online by marking it schedulable:

  ```shell
  # replace <node-to-drain> with the name of your node
  kubectl uncordon <node-to-drain>

-->
<h3 id="解除节点的保护">解除节点的保护</h3>
<ul>
<li>
<p>通过将节点标记为可调度，让其重新上线：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为你的节点名称</span>
kubectl uncordon &lt;node-to-drain&gt;
</code></pre></div></li>
</ul>
<!--
## Upgrade worker nodes

The upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time,
without compromising the minimum required capacity for running your workloads.
-->
<h2 id="升级工作节点">升级工作节点</h2>
<p>工作节点上的升级过程应该一次执行一个节点，或者一次执行几个节点，
以不影响运行工作负载所需的最小容量。</p>
<!--
### Upgrade kubeadm
-->
<h3 id="升级-kubeadm">升级 kubeadm</h3>
<!--
- Upgrade kubeadm:
-->
<ul>
<li>
<p>升级 kubeadm：</p>
<ul class="nav nav-tabs" id="k8s-install-kubeadm-worker-nodes" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-install-kubeadm-worker-nodes-0" role="tab" aria-controls="k8s-install-kubeadm-worker-nodes-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-install-kubeadm-worker-nodes-1" role="tab" aria-controls="k8s-install-kubeadm-worker-nodes-1">CentOS、RHEL 或 Fedora</a></li></ul>
<div class="tab-content" id="k8s-install-kubeadm-worker-nodes"><div id="k8s-install-kubeadm-worker-nodes-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-install-kubeadm-worker-nodes-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 1.22.x-00 中的 x 替换为最新的补丁版本号</span>
apt-mark unhold kubeadm <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get update <span style="color:#666">&amp;&amp;</span> apt-get install -y <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span>1.22.x-00 <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-mark hold kubeadm
- 
<span style="color:#080;font-style:italic"># 从 apt-get 的 1.1 版本开始，你也可以使用下面的方法：</span>
apt-get update <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get install -y --allow-change-held-packages <span style="color:#b8860b">kubeadm</span><span style="color:#666">=</span>1.22.x-00
</code></pre></div></div>
  <div id="k8s-install-kubeadm-worker-nodes-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-install-kubeadm-worker-nodes-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 用最新的补丁版本替换 1.22.x-00 中的 x</span>
yum install -y kubeadm-1.22.x-0 --disableexcludes<span style="color:#666">=</span>kubernetes
</code></pre></div></div></div>

</li>
</ul>
<!--
### Call "kubeadm upgrade"

-  For worker nodes this upgrades the local kubelet configuration:
-->
<h3 id="执行-kubeadm-upgrade-1">执行 &quot;kubeadm upgrade&quot;</h3>
<ul>
<li>
<p>对于工作节点，下面的命令会升级本地的 kubelet 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo kubeadm upgrade node
</code></pre></div></li>
</ul>
<!--
### Drain the node

- Prepare the node for maintenance by marking it unschedulable and evicting the workloads:

  ```shell
  # replace <node-to-drain> with the name of your node you are draining
  kubectl drain <node-to-drain> --ignore-daemonsets
  ```
-->
<h3 id="腾空节点-1">腾空节点</h3>
<ul>
<li>
<p>将节点标记为不可调度并驱逐所有负载，准备节点的维护：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为你正在腾空的节点的名称</span>
kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets
</code></pre></div></li>
</ul>
<!--
### Upgrade kubelet and kubectl
-->
<h3 id="升级-kubelet-和-kubectl-1">升级 kubelet 和 kubectl</h3>
<!--
-  Upgrade the kubelet and kubectl:
-->
<ul>
<li>
<p>升级 kubelet 和 kubectl：</p>
<ul class="nav nav-tabs" id="k8s-kubelet-and-kubectl" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#k8s-kubelet-and-kubectl-0" role="tab" aria-controls="k8s-kubelet-and-kubectl-0" aria-selected="true">Ubuntu、Debian 或 HypriotOS</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#k8s-kubelet-and-kubectl-1" role="tab" aria-controls="k8s-kubelet-and-kubectl-1">CentOS, RHEL or Fedora</a></li></ul>
<div class="tab-content" id="k8s-kubelet-and-kubectl"><div id="k8s-kubelet-and-kubectl-0" class="tab-pane show active" role="tabpanel" aria-labelledby="k8s-kubelet-and-kubectl-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 1.22.x-00 中的 x 替换为最新的补丁版本</span>
apt-mark unhold kubelet kubectl <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get update <span style="color:#666">&amp;&amp;</span> apt-get install -y <span style="color:#b8860b">kubelet</span><span style="color:#666">=</span>1.22.x-00 <span style="color:#b8860b">kubectl</span><span style="color:#666">=</span>1.22.x-00 <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-mark hold kubelet kubectl

<span style="color:#080;font-style:italic"># 从 apt-get 的 1.1 版本开始，你也可以使用下面的方法：</span>

apt-get update <span style="color:#666">&amp;&amp;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>apt-get install -y --allow-change-held-packages <span style="color:#b8860b">kubelet</span><span style="color:#666">=</span>1.22.x-00 <span style="color:#b8860b">kubectl</span><span style="color:#666">=</span>1.22.x-00
</code></pre></div></div>
  <div id="k8s-kubelet-and-kubectl-1" class="tab-pane" role="tabpanel" aria-labelledby="k8s-kubelet-and-kubectl-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 1.22.x-0 x 替换为最新的补丁版本</span>
yum install -y kubelet-1.22.x-0 kubectl-1.22.x-0 --disableexcludes<span style="color:#666">=</span>kubernetes
</code></pre></div></div></div>

</li>
</ul>
<!--
- Restart the kubelet

    ```shell
    sudo systemctl daemon-reload
    sudo systemctl restart kubelet
    ```
-->
<ul>
<li>
<p>重启 kubelet</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre></div></li>
</ul>
<!--
### Uncordon the node
-->
<h3 id="取消对节点的保护">取消对节点的保护</h3>
<!--
-  Bring the node back online by marking it schedulable:

    ```shell
    # replace <node-to-drain> with the name of your node
    kubectl uncordon <node-to-drain>
    ```
-->
<ul>
<li>
<p>通过将节点标记为可调度，让节点重新上线:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;node-to-drain&gt; 替换为当前节点的名称</span>
kubectl uncordon &lt;node-to-drain&gt;
</code></pre></div></li>
</ul>
<!--
## Verify the status of the cluster

After the kubelet is upgraded on all nodes verify that all nodes are available again by running the following command
from anywhere kubectl can access the cluster:

```shell
kubectl get nodes
```
-->
<h2 id="验证集群的状态">验证集群的状态</h2>
<p>在所有节点上升级 kubelet 后，通过从 kubectl 可以访问集群的任何位置运行以下命令，
验证所有节点是否再次可用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!--
The `STATUS` column should show `Ready` for all your nodes, and the version number should be updated.
-->
<p><code>STATUS</code> 应显示所有节点为 <code>Ready</code> 状态，并且版本号已经被更新。</p>
<!--
## Recovering from a failure state

If `kubeadm upgrade` fails and does not roll back, for example because of an unexpected shutdown during execution, you can run `kubeadm upgrade` again.
This command is idempotent and eventually makes sure that the actual state is the desired state you declare.

To recover from a bad state, you can also run `kubeadm upgrade--force` without changing the version that your cluster is running.
-->
<h2 id="从故障状态恢复">从故障状态恢复</h2>
<p>如果 <code>kubeadm upgrade</code> 失败并且没有回滚，例如由于执行期间节点意外关闭，
你可以再次运行 <code>kubeadm upgrade</code>。
此命令是幂等的，并最终确保实际状态是你声明的期望状态。
要从故障状态恢复，你还可以运行 <code>kubeadm upgrade --force</code> 而无需更改集群正在运行的版本。</p>
<!--
During upgrade kubeadm writes the following backup folders under `/etc/kubernetes/tmp`:
- `kubeadm-backup-etcd-<date>-<time>`
- `kubeadm-backup-manifests-<date>-<time>`

`kubeadm-backup-etcd` contains a backup of the local etcd member data for this control-plane Node.
In case of an etcd upgrade failure and if the automatic rollback does not work, the contents of this folder
can be manually restored in `/var/lib/etcd`. In case external etcd is used this backup folder will be empty.

`kubeadm-backup-manifests` contains a backup of the static Pod manifest files for this control-plane Node.
In case of a upgrade failure and if the automatic rollback does not work, the contents of this folder can be
manually restored in `/etc/kubernetes/manifests`. If for some reason there is no difference between a pre-upgrade
and post-upgrade manifest file for a certain component, a backup file for it will not be written.
-->
<p>在升级期间，kubeadm 向 <code>/etc/kubernetes/tmp</code> 目录下的如下备份文件夹写入数据：</p>
<ul>
<li><code>kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;</code></li>
<li><code>kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code></li>
</ul>
<p><code>kubeadm-backup-etcd</code> 包含当前控制面节点本地 etcd 成员数据的备份。
如果 etcd 升级失败并且自动回滚也无法修复，则可以将此文件夹中的内容复制到
<code>/var/lib/etcd</code> 进行手工修复。如果使用的是外部的 etcd，则此备份文件夹为空。</p>
<p><code>kubeadm-backup-manifests</code> 包含当前控制面节点的静态 Pod 清单文件的备份版本。
如果升级失败并且无法自动回滚，则此文件夹中的内容可以复制到
<code>/etc/kubernetes/manifests</code> 目录实现手工恢复。
如果由于某些原因，在升级前后某个组件的清单未发生变化，则 kubeadm 也不会为之
生成备份版本。</p>
<!--
## How it works

`kubeadm upgrade apply` does the following:

- Checks that your cluster is in an upgradeable state:
  - The API server is reachable
  - All nodes are in the `Ready` state
  - The control plane is healthy
- Enforces the version skew policies.
- Makes sure the control plane images are available or available to pull to the machine.
- Generates replacements and/or uses user supplied overwrites if component configs require version upgrades.
- Upgrades the control plane components or rollbacks if any of them fails to come up.
- Applies the new `CoreDNS` and `kube-proxy` manifests and makes sure that all necessary RBAC rules are created.
- Creates new certificate and key files of the API server and backs up old files if they're about to expire in 180 days.
-->
<h2 id="how-it-works">工作原理  </h2>
<p><code>kubeadm upgrade apply</code> 做了以下工作：</p>
<ul>
<li>检查你的集群是否处于可升级状态:
<ul>
<li>API 服务器是可访问的</li>
<li>所有节点处于 <code>Ready</code> 状态</li>
<li>控制面是健康的</li>
</ul>
</li>
<li>强制执行版本偏差策略。</li>
<li>确保控制面的镜像是可用的或可拉取到服务器上。</li>
<li>如果组件配置要求版本升级，则生成替代配置与/或使用用户提供的覆盖版本配置。</li>
<li>升级控制面组件或回滚（如果其中任何一个组件无法启动）。</li>
<li>应用新的 <code>CoreDNS</code> 和 <code>kube-proxy</code> 清单，并强制创建所有必需的 RBAC 规则。</li>
<li>如果旧文件在 180 天后过期，将创建 API 服务器的新证书和密钥文件并备份旧文件。</li>
</ul>
<!--
`kubeadm upgrade node` does the following on additional control plane nodes:

- Fetches the kubeadm `ClusterConfiguration` from the cluster.
- Optionally backups the kube-apiserver certificate.
- Upgrades the static Pod manifests for the control plane components.
- Upgrades the kubelet configuration for this node.
-->
<p><code>kubeadm upgrade node</code> 在其他控制平节点上执行以下操作：</p>
<ul>
<li>从集群中获取 kubeadm <code>ClusterConfiguration</code>。</li>
<li>（可选操作）备份 kube-apiserver 证书。</li>
<li>升级控制平面组件的静态 Pod 清单。</li>
<li>为本节点升级 kubelet 配置</li>
</ul>
<!--
`kubeadm upgrade node` does the following on worker nodes:

- Fetches the kubeadm `ClusterConfiguration` from the cluster.
- Upgrades the kubelet configuration for this node.
-->
<p><code>kubeadm upgrade node</code> 在工作节点上完成以下工作：</p>
<ul>
<li>从集群取回 kubeadm <code>ClusterConfiguration</code>。</li>
<li>为本节点升级 kubelet 配置。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9133578f1e75663bb031e5a377ca896d">2.2.4 - 添加 Windows 节点</h1>
    
	<!--
reviewers:
- michmike
- patricklang
title: Adding Windows nodes
min-kubernetes-server-version: 1.17
content_type: tutorial
weight: 30
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>

<!--
You can use Kubernetes to run a mixture of Linux and Windows nodes, so you can mix Pods that run on Linux on with Pods that run on Windows. This page shows how to register Windows nodes to your cluster.
-->
<p>你可以使用 Kubernetes 来混合运行 Linux 和 Windows 节点，这样你就可以
混合使用运行于 Linux 上的 Pod 和运行于 Windows 上的 Pod。
本页面展示如何将 Windows 节点注册到你的集群。</p>
<h2 id="准备开始">准备开始</h2>


您的 Kubernetes 服务器版本必须不低于版本 1.17.
 要获知版本信息，请输入 <code>kubectl version</code>.

<!--
* Obtain a [Windows Server 2019 license](https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing)
(or higher) in order to configure the Windows node that hosts Windows containers.
If you are using VXLAN/Overlay networking you must have also have [KB4489899](https://support.microsoft.com/help/4489899) installed.

* A Linux-based Kubernetes kubeadm cluster in which you have access to the control plane (see [Creating a single control-plane cluster with kubeadm](/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)).
-->
<ul>
<li>
<p>获取 <a href="https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing">Windows Server 2019 或更高版本的授权</a>
以便配置托管 Windows 容器的 Windows 节点。
如果你在使用 VXLAN/覆盖（Overlay）联网设施，则你还必须安装 <a href="https://support.microsoft.com/help/4489899">KB4489899</a>。</p>
</li>
<li>
<p>一个利用 kubeadm 创建的基于 Linux 的 Kubernetes 集群；你能访问该集群的控制面
（参见<a href="/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">使用 kubeadm 创建一个单控制面的集群</a>)。</p>
</li>
</ul>
<h2 id="教程目标">教程目标</h2>
<!--
* Register a Windows node to the cluster
* Configure networking so Pods and Services on Linux and Windows can communicate with each other
-->
<ul>
<li>将一个 Windows 节点注册到集群上</li>
<li>配置网络，以便 Linux 和 Windows 上的 Pod 和 Service 之间能够相互通信。</li>
</ul>
<!-- lessoncontent -->
<!--
## Getting Started: Adding a Windows Node to Your Cluster

### Networking Configuration

Once you have a Linux-based Kubernetes control-plane node you are ready to choose a networking solution. This guide illustrates using Flannel in VXLAN mode for simplicity.
-->
<h2 id="开始行动-向你的集群添加一个-windows-节点">开始行动：向你的集群添加一个 Windows 节点</h2>
<h3 id="networking-configuration">联网配置  </h3>
<p>一旦你有了一个基于 Linux 的 Kubernetes 控制面节点，你就可以为其选择联网方案。
出于简单考虑，本指南展示如何使用 VXLAN 模式的 Flannel。</p>
<!--
#### Configuring Flannel

1. Prepare Kubernetes control plane for Flannel

    Some minor preparation is recommended on the Kubernetes control plane in our cluster. It is recommended to enable bridged IPv4 traffic to iptables chains when using Flannel. The following command must be run on all Linux nodes:

    ```bash
    sudo sysctl net.bridge.bridge-nf-call-iptables=1
    ```
-->
<h4 id="configuring-flannel">配置 Flannel </h4>
<ol>
<li>
<p>为 Flannel 准备 Kubernetes 的控制面</p>
<p>在我们的集群中，建议对 Kubernetes 的控制面进行少许准备处理。
建议在使用 Flannel 时为 iptables 链启用桥接方式的 IPv4 流处理，
必须在所有 Linux 节点上执行如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo sysctl net.bridge.bridge-nf-call-iptables<span style="color:#666">=</span><span style="color:#666">1</span>
</code></pre></div></li>
</ol>
<!--
1. Download & configure Flannel for Linux

    Download the most recent Flannel manifest:

    ```bash
    wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
    ```

    Modify the `net-conf.json` section of the flannel manifest in order to set the VNI to 4096 and the Port to 4789. It should look as follows:

    ```json
    net-conf.json: |
        {
          "Network": "10.244.0.0/16",
          "Backend": {
            "Type": "vxlan",
            "VNI": 4096,
            "Port": 4789
          }
        }
    ```

    The VNI must be set to 4096 and port 4789 for Flannel on Linux to interoperate with Flannel on Windows. See the [VXLAN documentation](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan).
    for an explanation of these fields.

    To use L2Bridge/Host-gateway mode instead change the value of `Type` to `"host-gw"` and omit `VNI` and `Port`.
-->
<ol start="2">
<li>
<p>下载并配置 Linux 版本的 Flannel</p>
<p>下载最新的 Flannel 清单文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre></div><p>修改 Flannel 清单中的 <code>net-conf.json</code> 部分，将 VNI 设置为 4096，并将 Port 设置为 4789。
结果看起来像下面这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="">net-conf.json:</span> <span style="">|</span>
    {
      <span style="color:#008000;font-weight:bold">&#34;Network&#34;</span>: <span style="color:#b44">&#34;10.244.0.0/16&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;Backend&#34;</span>: {
         <span style="color:#008000;font-weight:bold">&#34;Type&#34;</span>: <span style="color:#b44">&#34;vxlan&#34;</span>,
         <span style="color:#008000;font-weight:bold">&#34;VNI&#34;</span>: <span style="color:#666">4096</span>,
         <span style="color:#008000;font-weight:bold">&#34;Port&#34;</span>: <span style="color:#666">4789</span>
    }
}
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> 在 Linux 节点上 VNI 必须设置为 4096，端口必须设置为 4789，这样才能令其与 Windows 上的
Flannel 互操作。关于这些字段的详细说明，请参见
<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan">VXLAN 文档</a>。</div>
</blockquote>
<blockquote class="note callout">
  <div><strong>说明：</strong> 如要使用 L2Bridge/Host-gateway 模式，则可将 <code>Type</code> 值设置为
<code>&quot;host-gw&quot;</code>，并忽略 <code>VNI</code> 和 <code>Port</code> 的设置。</div>
</blockquote>
</li>
</ol>
<!--
1. Apply the Flannel manifest and validate

    Let's apply the Flannel configuration:

    ```bash
    kubectl apply -f kube-flannel.yml
    ```

    After a few minutes, you should see all the pods as running if the Flannel pod network was deployed.

    ```bash
    kubectl get pods -n kube-system
    ```

    The output should include the Linux flannel DaemonSet as running:

    ```
    NAMESPACE     NAME                                      READY        STATUS    RESTARTS   AGE
    ...
    kube-system   kube-flannel-ds-54954                     1/1          Running   0          1m
    ```
-->
<ol start="3">
<li>
<p>应用 Flannel 清单并验证</p>
<p>首先应用 Flannel 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f kube-flannel.yml
</code></pre></div><p>几分钟之后，如果 Flannel Pod 网络被正确部署，你应该会看到所有 Pods 都处于运行中状态。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pods -n kube-system
</code></pre></div><p>输出中应该包含处于运行中状态的 Linux Flannel DaemonSet：</p>
<pre tabindex="0"><code>NAMESPACE     NAME                                      READY        STATUS    RESTARTS   AGE
...
kube-system   kube-flannel-ds-54954                     1/1          Running   0          1m
</code></pre></li>
</ol>
<!--
1. Add Windows Flannel and kube-proxy DaemonSets

    Now you can add Windows-compatible versions of Flannel and kube-proxy. In order
    to ensure that you get a compatible version of kube-proxy, you'll need to substitute
    the tag of the image. The following example shows usage for Kubernetes v1.22.0,
    but you should adjust the version for your own deployment.

    ```bash
    curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed 's/VERSION/v1.22.0/g' | kubectl apply -f -
    kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml
    ```

    If you're using host-gateway use https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml instead

If you're using a different interface rather than Ethernet (i.e. "Ethernet0 2") on the Windows nodes, you have to modify the line:

```powershell
wins cli process run --path /k/flannel/setup.exe --args "--mode=overlay --interface=Ethernet"
```

in the `flannel-host-gw.yml` or `flannel-overlay.yml` file and specify your interface accordingly.

```bash
# Example
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml | sed 's/Ethernet/Ethernet0 2/g' | kubectl apply -f -
```
-->    
<ol start="4">
<li>
<p>添加 Windows Flannel 和 kube-proxy DaemonSet</p>
<p>现在你可以添加 Windows 兼容版本的 Flannel 和 kube-proxy。为了确保你能获得兼容
版本的 kube-proxy，你需要替换镜像中的标签。
下面的例子中展示的是针对 Kubernetes v1.22.0 版本的用法，
不过你应该根据你自己的集群部署调整其中的版本号。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed <span style="color:#b44">&#39;s/VERSION/v1.22.0/g&#39;</span> | kubectl apply -f -
kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> 如果你在使用 host-gateway 模式，则应该使用
<a href="https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml">https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-host-gw.yml</a>
这一清单。</div>
</blockquote>
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>如果你在 Windows 节点上使用的不是以太网（即，&quot;Ethernet0 2&quot;）接口，你需要
修改 <code>flannel-host-gw.yml</code> 或 <code>flannel-overlay.yml</code> 文件中的下面这行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">wins <span style="color:#a2f">cli </span><span style="color:#a2f;font-weight:bold">process</span> run --path /k/flannel/setup.exe --args <span style="color:#b44">&#34;--mode=overlay --interface=Ethernet&#34;</span>
</code></pre></div><p>在其中根据情况设置要使用的网络接口。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># Example</span>
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml | sed <span style="color:#b44">&#39;s/Ethernet/Ethernet0 2/g&#39;</span> | kubectl apply -f -
</code></pre></div></div>
</blockquote>
</li>
</ol>
<!--
### Joining a Windows worker node

You must install the `Containers` feature and install Docker. Instructions
to do so are available at [Install Docker Engine - Enterprise on Windows Servers](https://docs.mirantis.com/docker-enterprise/v3.1/dockeree-products/docker-engine-enterprise/dee-windows.html).

All code snippets in Windows sections are to be run in a PowerShell environment
with elevated permissions (Administrator) on the Windows worker node.
-->
<h3 id="joining-a-windows-worker-node">加入 Windows 工作节点  </h3>
<p>你必须安装 <code>Containers</code> 功能特性并安装 Docker 工具。相关的指令可以在
<a href="https://hub.docker.com/editions/enterprise/docker-ee-server-windows">Install Docker Engine - Enterprise on Windows Servers</a>
处找到。</p>
<p>Windows 节的所有代码片段都需要在 PowerShell 环境中执行，并且要求在
Windows 工作节点上具有提升的权限（Administrator）。</p>
<!--
1. Install wins, kubelet, and kubeadm.
-->
<ol>
<li>
<p>安装 wins、kubelet 和 kubeadm</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-PowerShell" data-lang="PowerShell">curl.exe -LO https<span style="">:</span>//github.com/<span style="color:#a2f">kubernetes-sigs</span>/<span style="color:#a2f">sig-windows</span>-tools/releases/latest/download/PrepareNode.ps1
.\PrepareNode.ps1 -KubernetesVersion <span style="color:#a2f">
</code></pre></div></li>
</ol>
<!--
1. Run `kubeadm` to join the node

    Use the command that was given to you when you ran `kubeadm init` on a control plane host.
    If you no longer have this command, or the token has expired, you can run `kubeadm token create -print-join-command`
    (on a control plane host) to generate a new token and join command.
-->
<ol start="2">
<li>
<p>运行 <code>kubeadm</code> 添加节点</p>
<p>当你在控制面主机上运行 <code>kubeadm init</code> 时，输出了一个命令。现在运行这个命令。
如果你找不到这个命令，或者命令中对应的令牌已经过期，你可以（在一个控制面主机上）运行
<code>kubeadm token create --print-join-command</code> 来生成新的令牌和 join 命令。</p>
</li>
</ol>
<!--
#### Verifying your installation

You should now be able to view the Windows node in your cluster by running:
-->
<h4 id="verifying-your-installation">检查你的安装  </h4>
<p>你现在应该能够通过运行下面的命令来查看集群中的 Windows 节点了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes -o wide
</code></pre></div><!--
If your new node is in the `NotReady` state it is likely because the flannel image is still downloading.
You can check the progress as before by checking on the flannel pods in the `kube-system` namespace:
-->
<p>如果你的新节点处于 <code>NotReady</code> 状态，很可能的原因是系统仍在下载 Flannel 镜像。
你可以像之前一样，通过检查 <code>kube-system</code> 名字空间中的 Flannel Pods 来了解
安装进度。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl -n kube-system get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>flannel
</code></pre></div><!--
Once the flannel Pod is running, your node should enter the `Ready` state and then be available to handle workloads.
-->
<p>一旦 Flannel Pod 运行起来，你的节点就应该能进入 <code>Ready</code> 状态并可
用来处理负载。</p>
<h2 id="接下来">接下来</h2>
<!--
- [Upgrading Windows kubeadm nodes](/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes)
-->
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/kubeadm/upgrading-windows-nodes">升级 kubeadm 安装的 Windows 节点</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e805c7d8d4ad6195cb82dbbc843bfc29">2.2.5 - 升级 Windows 节点</h1>
    
	<!--
title: Upgrading Windows nodes
min-kubernetes-server-version: 1.17
content_type: task
weight: 40
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>

<!--
This page explains how to upgrade a Windows node [created with kubeadm](/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes).
-->
<p>本页解释如何升级<a href="/zh/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes">用 kubeadm 创建的</a>
Windows 节点。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 1.17.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
* Familiarize yourself with [the process for upgrading the rest of your kubeadm
cluster](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade). You will want to
upgrade the control plane nodes before upgrading your Windows nodes.
-->
<ul>
<li>熟悉<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade">更新 kubeadm 集群中的其余组件</a>。
在升级你的 Windows 节点之前你会想要升级控制面节点。</li>
</ul>
<!-- steps -->
<!--
## Upgrading worker nodes

### Upgrade kubeadm
-->
<h2 id="upgrading-worker-nodes">升级工作节点  </h2>
<h3 id="upgrade-kubeadm">升级 kubeadm   </h3>
<!--
1.  From the Windows node, upgrade kubeadm:

    ```powershell
    # replace v1.22.0 with your desired version
    curl.exe -Lo C:\k\kubeadm.exe https://dl.k8s.io/v1.22.0/bin/windows/amd64/kubeadm.exe
    ```
-->
<ol>
<li>
<p>在 Windows 节点上升级 kubeadm：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#080;font-style:italic"># 将 v1.22.0 替换为你希望的版本</span>
curl.exe -Lo C:\k\kubeadm.exe https<span style="">:</span>//dl.k8s.io/<span style="color:#a2f">/bin/windows/amd64/kubeadm.exe
</code></pre></div></li>
</ol>
<!--
### Drain the node

1.  From a machine with access to the Kubernetes API,
    prepare the node for maintenance by marking it unschedulable and evicting the workloads:

    ```shell
    # replace <node-to-drain> with the name of your node you are draining
    kubectl drain <node-to-drain> -ignore-daemonsets
    ```

    You should see output similar to this:

    ```
    node/ip-172-31-85-18 cordoned
    node/ip-172-31-85-18 drained
    ```
-->
<h3 id="drain-the-node">腾空节点  </h3>
<ol>
<li>
<p>在一个能访问到 Kubernetes API 的机器上，将 Windows 节点标记为不可调度并
驱逐其上的所有负载，以便准备节点维护操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;要腾空的节点&gt; 替换为你要腾空的节点的名称</span>
kubectl drain &lt;要腾空的节点&gt; -ignore-daemonsets
</code></pre></div><p>你应该会看到类似下面的输出：</p>
<pre tabindex="0"><code>node/ip-172-31-85-18 cordoned
node/ip-172-31-85-18 drained
</code></pre></li>
</ol>
<!--
### Upgrade the kubelet configuration

1.  From the Windows node, call the following command to sync new kubelet configuration:

    ```powershell
    kubeadm upgrade node
    ```
-->
<h3 id="upgrade-the-kubelet-configuration">升级 kubelet 配置  </h3>
<ol>
<li>
<p>在 Windows 节点上，执行下面的命令来同步新的 kubelet 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">kubeadm upgrade node
</code></pre></div></li>
</ol>
<!--
### Upgrade kubelet

1.  From the Windows node, upgrade and restart the kubelet:

    ```powershell
    stop-service kubelet
    curl.exe -Lo C:\k\kubelet.exe https://dl.k8s.io/v1.22.0/bin/windows/amd64/kubelet.exe
    restart-service kubelet
    ```
-->
<h3 id="upgrade-kubelet">升级 kubelet  </h3>
<ol>
<li>
<p>在 Windows 节点上升级并重启 kubelet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">stop-service</span> kubelet
curl.exe -Lo C:\k\kubelet.exe https<span style="">:</span>//dl.k8s.io/<span style="color:#a2f">/bin/windows/amd64/kubelet.exe
<span style="color:#a2f">restart-service</span> kubelet
</code></pre></div></li>
</ol>
<!--
### Uncordon the node

1.  From a machine with access to the Kubernetes API,
bring the node back online by marking it schedulable:

    ```shell
    # replace <node-to-drain> with the name of your node
    kubectl uncordon <node-to-drain>
    ```
-->
<h3 id="uncordon-the-node">对节点执行 uncordon 操作  </h3>
<ol>
<li>
<p>从一台能够访问到 Kubernetes API 的机器上，通过将节点标记为可调度，使之
重新上线：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将 &lt;要腾空的节点&gt; 替换为你的节点名称</span>
kubectl uncordon &lt;要腾空的节点&gt;
</code></pre></div></li>
</ol>
<!--
### Upgrade kube-proxy

1. From a machine with access to the Kubernetes API, run the following,
again replacing v1.22.0 with your desired version:

    ```shell
    curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed 's/VERSION/v1.22.0/g' | kubectl apply -f -
    ```
-->
<h3 id="upgrade-kube-proxy">升级 kube-proxy  </h3>
<ol>
<li>
<p>在一台可访问 Kubernetes API 的机器上和，将 v1.22.0 替换成你
期望的版本后再次执行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed <span style="color:#b44">&#39;s/VERSION/v1.22.0/g&#39;</span> | kubectl apply -f -
</code></pre></div></li>
</ol>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-47be5dd51f686017f1766e6ec7aa6f41">2.3 - 管理内存，CPU 和 API 资源</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-337620c76587e4aeb32009cb23be46de">2.3.1 - 为命名空间配置默认的内存请求和限制</h1>
    
	<!--
title: Configure Default Memory Requests and Limits for a Namespace
content_type: task
weight: 10
-->
<!-- overview -->
<!--
This page shows how to configure default memory requests and limits for a namespace.
If a Container is created in a namespace that has a default memory limit, and the Container
does not specify its own memory limit, then the Container is assigned the default memory limit.
Kubernetes assigns a default memory request under certain conditions that are explained later in this topic.
-->
<p>本文介绍怎样给命名空间配置默认的内存请求和限制。
如果在一个有默认内存限制的命名空间创建容器，该容器没有声明自己的内存限制时，
将会被指定默认内存限制。
Kubernetes 还为某些情况指定了默认的内存请求，本章后面会进行介绍。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 2 GiB of memory.
-->
<p>你的集群中的每个节点必须至少有 2 GiB 的内存。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中所建的资源与集群的其余资源相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace default-mem-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange object. The configuration specifies
a default memory request and a default memory limit.
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>这里给出了一个限制范围对象的配置文件。该配置声明了一个默认的内存请求和一个默认的内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults.yaml" download="admin/resource/memory-defaults.yaml"><code>admin/resource/memory-defaults.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-yaml')" title="Copy admin/resource/memory-defaults.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mem-limit-range<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>512Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">defaultRequest</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>256Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange in the default-mem-example namespace:
-->
<p>在 default-mem-example 命名空间创建限制范围：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
Now if a Container is created in the default-mem-example namespace, and the
Container does not specify its own values for memory request and memory limit,
the Container is given a default memory request of 256 MiB and a default
memory limit of 512 MiB.

Here's the configuration file for a Pod that has one Container. The Container
does not specify a memory request and limit.
-->
<p>现在，如果在 default-mem-example 命名空间创建容器，并且该容器没有声明自己的内存请求和限制值，
它将被指定默认的内存请求 256 MiB 和默认的内存限制 512 MiB。</p>
<p>下面是具有一个容器的 Pod 的配置文件。
容器未指定内存请求和限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults-pod.yaml" download="admin/resource/memory-defaults-pod.yaml"><code>admin/resource/memory-defaults-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-pod-yaml')" title="Copy admin/resource/memory-defaults-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod.
-->
<p>创建 Pod</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-mem-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
The output shows that the Pod's Container has a memory request of 256 MiB and
a memory limit of 512 MiB. These are the default values specified by the LimitRange.
-->
<p>输出内容显示该 Pod 的容器有 256 MiB 的内存请求和 512 MiB 的内存限制。
这些都是 LimitRange 设置的默认值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">containers:
- image: nginx
  imagePullPolicy: Always
  name: default-mem-demo-ctr
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 256Mi
</code></pre></div><!--
Delete your Pod:
-->
<p>删除你的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod default-mem-demo --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
## What if you specify a Container's limit, but not its request?

Here's the configuration file for a Pod that has one Container. The Container
specifies a memory limit, but not a request:
-->
<h2 id="声明容器的限制而不声明它的请求会怎么样">声明容器的限制而不声明它的请求会怎么样？</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器声明了内存限制，而没有声明内存请求：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults-pod-2.yaml" download="admin/resource/memory-defaults-pod-2.yaml"><code>admin/resource/memory-defaults-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-pod-2-yaml')" title="Copy admin/resource/memory-defaults-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1Gi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-mem-demo-2 --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
The output shows that the Container's memory request is set to match its memory limit.
Notice that the Container was not assigned the default memory request value of 256Mi.
-->
<p>输出结果显示容器的内存请求被设置为它的内存限制相同的值。注意该容器没有被指定默认的内存请求值 256MiB。</p>
<pre tabindex="0"><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><!--
## What if you specify a Container's request, but not its limit?
-->
<h2 id="声明容器的内存请求而不声明内存限制会怎么样">声明容器的内存请求而不声明内存限制会怎么样？</h2>
<!--
Here's the configuration file for a Pod that has one Container. The Container
specifies a memory request, but not a limit:
-->
<p>这里给出了一个包含一个容器的 Pod 的配置文件。该容器声明了内存请求，但没有内存限制：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-defaults-pod-3.yaml" download="admin/resource/memory-defaults-pod-3.yaml"><code>admin/resource/memory-defaults-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-defaults-pod-3-yaml')" title="Copy admin/resource/memory-defaults-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-defaults-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-mem-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;128Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
View the Pod's specification:
-->
<p>查看 Pod 声明：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-mem-demo-3 --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-mem-example
</code></pre></div><!--
The output shows that the Container's memory request is set to the value specified in the
Container's configuration file. The Container's memory limit is set to 512Mi, which is the
default memory limit for the namespace.
-->
<p>输出结果显示该容器的内存请求被设置为了容器配置文件中声明的数值。
容器的内存限制被设置为 512MiB，即命名空间的默认内存限制。</p>
<pre tabindex="0"><code>resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi
</code></pre><!--
## Motivation for default memory limits and requests

If your namespace has a resource quota,
it is helpful to have a default value in place for memory limit.
Here are two of the restrictions that a resource quota imposes on a namespace:
-->
<h2 id="设置默认内存限制和请求的动机">设置默认内存限制和请求的动机</h2>
<p>如果你的命名空间有资源配额，那么默认内存限制是很有帮助的。
下面是一个例子，通过资源配额为命名空间设置两项约束：</p>
<!--
* Every Container that runs in the namespace must have its own memory limit.
* The total amount of memory used by all Containers in the namespace must not exceed a specified limit.
-->
<ul>
<li>运行在命名空间中的每个容器必须有自己的内存限制。</li>
<li>命名空间中所有容器的内存使用量之和不能超过声明的限制值。</li>
</ul>
<!--
If a Container does not specify its own memory limit, it is given the default limit, and then
it can be allowed to run in a namespace that is restricted by a quota.
-->
<p>如果一个容器没有声明自己的内存限制，会被指定默认限制，然后它才会被允许在限定了配额的命名空间中运行。</p>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace default-mem-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For cluster administrators

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)

* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)

* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为命名空间配置默认的 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置最小和最大内存限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置最小和最大 CPU 限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)

* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)

* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-320af95e480962c538ebef7ae205845c">2.3.2 - 为命名空间配置默认的 CPU 请求和限制</h1>
    
	<!--
title: Configure Default CPU Requests and Limits for a Namespace
content_type: task
weight: 20
-->
<!-- overview -->
<!--
This page shows how to configure default CPU requests and limits for a namespace.
A Kubernetes cluster can be divided into namespaces. If a Container is created in a namespace
that has a default CPU limit, and the Container does not specify its own CPU limit, then
the Container is assigned the default CPU limit. Kubernetes assigns a default CPU request
under certain conditions that are explained later in this topic.
-->
<p>本章介绍怎样为命名空间配置默认的 CPU 请求和限制。
一个 Kubernetes 集群可被划分为多个命名空间。如果在配置了 CPU 限制的命名空间创建容器，
并且该容器没有声明自己的 CPU 限制，那么这个容器会被指定默认的 CPU 限制。
Kubernetes 在一些特定情况还会指定 CPU 请求，本文后续章节将会对其进行解释。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace default-cpu-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange object. The configuration specifies
a default CPU request and a default CPU limit.
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>这里给出了 LimitRange 对象的配置文件。该配置声明了一个默认的 CPU 请求和一个默认的 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults.yaml" download="admin/resource/cpu-defaults.yaml"><code>admin/resource/cpu-defaults.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-yaml')" title="Copy admin/resource/cpu-defaults.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-limit-range<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">defaultRequest</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">0.5</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange in the default-cpu-example namespace:
-->
<p>在命名空间 default-cpu-example 中创建 LimitRange 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
Now if a Container is created in the default-cpu-example namespace, and the
Container does not specify its own values for CPU request and CPU limit,
the Container is given a default CPU request of 0.5 and a default
CPU limit of 1.

Here's the configuration file for a Pod that has one Container. The Container
does not specify a CPU request and limit.
-->
<p>现在如果在 default-cpu-example 命名空间创建一个容器，该容器没有声明自己的 CPU 请求和限制时，
将会给它指定默认的 CPU 请求0.5和默认的 CPU 限制值1.</p>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器没有声明 CPU 请求和限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults-pod.yaml" download="admin/resource/cpu-defaults-pod.yaml"><code>admin/resource/cpu-defaults-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-pod-yaml')" title="Copy admin/resource/cpu-defaults-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod.
-->
<p>创建 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
View the Pod's specification:
-->
<p>查看该 Pod 的声明：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod default-cpu-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
The output shows that the Pod's Container has a CPU request of 500 millicpus and
a CPU limit of 1 cpu. These are the default values specified by the LimitRange.
-->
<p>输出显示该 Pod 的容器有一个500 millicpus的 CPU 请求和一个1 cpu的 CPU 限制。这些是 LimitRange 声明的默认值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">containers:
- image: nginx
  imagePullPolicy: Always
  name: default-cpu-demo-ctr
  resources:
    limits:
      cpu: <span style="color:#b44">&#34;1&#34;</span>
    requests:
      cpu: 500m
</code></pre></div><!--
## What if you specify a Container's limit, but not its request?

Here's the configuration file for a Pod that has one Container. The Container
specifies a CPU limit, but not a request:
-->
<h2 id="你只声明容器的限制-而不声明请求会怎么样">你只声明容器的限制，而不声明请求会怎么样？</h2>
<p>这是包含一个容器的 Pod 的配置文件。该容器声明了 CPU 限制，而没有声明 CPU 请求。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults-pod-2.yaml" download="admin/resource/cpu-defaults-pod-2.yaml"><code>admin/resource/cpu-defaults-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-pod-2-yaml')" title="Copy admin/resource/cpu-defaults-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
View the Pod specification:
-->
<p>查看 Pod 的声明：</p>
<pre tabindex="0"><code>kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example
</code></pre><!--
The output shows that the Container's CPU request is set to match its CPU limit.
Notice that the Container was not assigned the default CPU request value of 0.5 cpu.
-->
<p>输出显示该容器的 CPU 请求和 CPU 限制设置相同。注意该容器没有被指定默认的 CPU 请求值0.5 cpu。</p>
<pre tabindex="0"><code>resources:
  limits:
    cpu: &quot;1&quot;
  requests:
    cpu: &quot;1&quot;
</code></pre><!--
## What if you specify a Container's request, but not its limit?

Here's the configuration file for a Pod that has one Container. The Container
specifies a CPU request, but not a limit:
-->
<h2 id="你只声明容器的请求-而不声明它的限制会怎么样">你只声明容器的请求，而不声明它的限制会怎么样？</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器声明了 CPU 请求，而没有声明 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-defaults-pod-3.yaml" download="admin/resource/cpu-defaults-pod-3.yaml"><code>admin/resource/cpu-defaults-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-defaults-pod-3-yaml')" title="Copy admin/resource/cpu-defaults-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-defaults-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-cpu-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0.75&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace<span style="color:#666">=</span>default-cpu-example
</code></pre></div><!--
View the Pod specification:
-->
<p>查看 Pod 的规约：</p>
<pre tabindex="0"><code>kubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example
</code></pre><!--
The output shows that the Container's CPU request is set to the value specified in the
Container's configuration file. The Container's CPU limit is set to 1 cpu, which is the
default CPU limit for the namespace.
-->
<p>结果显示该容器的 CPU 请求被设置为容器配置文件中声明的数值。
容器的CPU限制被设置为 1 CPU，即该命名空间的默认 CPU 限制值。</p>
<pre tabindex="0"><code>resources:
  limits:
    cpu: &quot;1&quot;
  requests:
    cpu: 750m
</code></pre><!--
## Motivation for default CPU limits and requests

If your namespace has a
[resource quota](/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/),
it is helpful to have a default value in place for CPU limit.
Here are two of the restrictions that a resource quota imposes on a namespace:

* Every Container that runs in the namespace must have its own CPU limit.
* The total amount of CPU used by all Containers in the namespace must not exceed a specified limit.

If a Container does not specify its own CPU limit, it is given the default limit, and then
it can be allowed to run in a namespace that is restricted by a quota.
-->
<h2 id="默认-cpu-限制和请求的动机">默认 CPU 限制和请求的动机</h2>
<p>如果你的命名空间有一个
<a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">资源配额</a>，
那么有一个默认的 CPU 限制是有帮助的。这里有资源配额强加给命名空间的两条限制：</p>
<ul>
<li>命名空间中运行的每个容器必须有自己的 CPU 限制。</li>
<li>命名空间中所有容器使用的 CPU 总和不能超过一个声明值。</li>
</ul>
<p>如果容器没有声明自己的 CPU 限制，将会给它一个默认限制，这样它就能被允许运行在一个有配额限制的命名空间中。</p>
<!--
## Clean up

Delete your namespace:

```shell
kubectl delete namespace default-cpu-example
```
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace constraints-cpu-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)
* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-adb489b1ab985c9215657b0d4c6ae92b">2.3.3 - 配置命名空间的最小和最大内存约束</h1>
    
	<!--
title: Configure Minimum and Maximum Memory Constraints for a Namespace
content_type: task
weight: 30
-->
<!-- overview -->
<!--
This page shows how to set minimum and maximum values for memory used by Containers
running in a namespace. You specify minimum and maximum memory values in a
[LimitRange](/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core)
object. If a Pod does not meet the constraints imposed by the LimitRange,
it cannot be created in the namespace.
-->
<p>本页介绍如何设置在命名空间中运行的容器使用的内存的最小值和最大值。 你可以在
<a href="/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core">LimitRange</a>
对象中指定最小和最大内存值。如果 Pod 不满足 LimitRange 施加的约束，则无法在命名空间中创建它。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 1 GiB of memory.
-->
<p>集群中每个节点必须至少要有 1 GiB 的内存。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便在此练习中创建的资源与群集的其余资源隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace constraints-mem-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange:
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>下面是 LimitRange 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints.yaml" download="admin/resource/memory-constraints.yaml"><code>admin/resource/memory-constraints.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-yaml')" title="Copy admin/resource/memory-constraints.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mem-min-max-demo-lr<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>500Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange:
-->
<p>创建 LimitRange:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
View detailed information about the LimitRange:
-->
<p>查看 LimitRange 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get limitrange mem-min-max-demo-lr --namespace<span style="color:#666">=</span>constraints-mem-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows the minimum and maximum memory constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.
-->
<p>输出显示预期的最小和最大内存约束。 但请注意，即使你没有在 LimitRange 的配置文件中指定默认值，也会自动创建它们。</p>
<pre tabindex="0"><code>  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
</code></pre><!--
Now whenever a Container is created in the constraints-mem-example namespace, Kubernetes
performs these steps:

* If the Container does not specify its own memory request and limit, assign the default
memory request and limit to the Container.

* Verify that the Container has a memory request that is greater than or equal to 500 MiB.

* Verify that the Container has a memory limit that is less than or equal to 1 GiB.

Here's the configuration file for a Pod that has one Container. The Container manifest
specifies a memory request of 600 MiB and a memory limit of 800 MiB. These satisfy the
minimum and maximum memory constraints imposed by the LimitRange.
-->
<p>现在，只要在 constraints-mem-example 命名空间中创建容器，Kubernetes 就会执行下面的步骤：</p>
<ul>
<li>
<p>如果 Container 未指定自己的内存请求和限制，将为它指定默认的内存请求和限制。</p>
</li>
<li>
<p>验证 Container 的内存请求是否大于或等于500 MiB。</p>
</li>
<li>
<p>验证 Container 的内存限制是否小于或等于1 GiB。</p>
</li>
</ul>
<p>这里给出了包含一个 Container 的 Pod 配置文件。Container 声明了 600 MiB 的内存请求和
800 MiB 的内存限制， 这些满足了 LimitRange 施加的最小和最大内存约束。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod.yaml" download="admin/resource/memory-constraints-pod.yaml"><code>admin/resource/memory-constraints-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-yaml')" title="Copy admin/resource/memory-constraints-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;600Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>确认下 Pod 中的容器在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-mem-demo --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-mem-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
The output shows that the Container has a memory request of 600 MiB and a memory limit
of 800 MiB. These satisfy the constraints imposed by the LimitRange.
-->
<p>输出结果显示容器的内存请求为600 MiB，内存限制为800 MiB。这些满足了 LimitRange 设定的限制范围。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>800Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>600Mi<span style="color:#bbb">
</span></code></pre></div><!--
Delete your Pod:
-->
<p>删除你创建的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod constraints-mem-demo --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
## Attempt to create a Pod that exceeds the maximum memory constraint

Here's the configuration file for a Pod that has one Container. The Container specifies a
memory request of 800 MiB and a memory limit of 1.5 GiB.
-->
<h2 id="尝试创建一个超过最大内存限制的-pod">尝试创建一个超过最大内存限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器声明了800 MiB 的内存请求和1.5 GiB 的内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod-2.yaml" download="admin/resource/memory-constraints-pod-2.yaml"><code>admin/resource/memory-constraints-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-2-yaml')" title="Copy admin/resource/memory-constraints-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a memory limit that is
too large:
-->
<p>输出结果显示 Pod 没有创建成功，因为容器声明的内存限制太大了：</p>
<pre tabindex="0"><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/memory-constraints-pod-2.yaml&quot;:
pods &quot;constraints-mem-demo-2&quot; is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi.
</code></pre><!--
## Attempt to create a Pod that does not meet the minimum memory request

Here's the configuration file for a Pod that has one Container. The Container specifies a
memory request of 100 MiB and a memory limit of 800 MiB.
-->
<h2 id="尝试创建一个不满足最小内存请求的-pod">尝试创建一个不满足最小内存请求的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器声明了100 MiB 的内存请求和800 MiB 的内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod-3.yaml" download="admin/resource/memory-constraints-pod-3.yaml"><code>admin/resource/memory-constraints-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-3-yaml')" title="Copy admin/resource/memory-constraints-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a memory
request that is too small:
-->
<p>输出结果显示 Pod 没有创建成功，因为容器声明的内存请求太小了：</p>
<pre tabindex="0"><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/memory-constraints-pod-3.yaml&quot;:
pods &quot;constraints-mem-demo-3&quot; is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi.
</code></pre><!--
## Create a Pod that does not specify any memory request or limit

Here's the configuration file for a Pod that has one Container. The Container does not
specify a memory request, and it does not specify a memory limit.
-->
<h2 id="创建一个没有声明内存请求和限制的-pod">创建一个没有声明内存请求和限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器没有声明内存请求，也没有声明内存限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/memory-constraints-pod-4.yaml" download="admin/resource/memory-constraints-pod-4.yaml"><code>admin/resource/memory-constraints-pod-4.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-memory-constraints-pod-4-yaml')" title="Copy admin/resource/memory-constraints-pod-4.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-memory-constraints-pod-4-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-mem-demo-4-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace<span style="color:#666">=</span>constraints-mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<pre tabindex="0"><code>kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml
</code></pre><!--
The output shows that the Pod's Container has a memory request of 1 GiB and a memory limit of 1 GiB.
How did the Container get those values?
-->
<p>输出结果显示 Pod 的内存请求为1 GiB，内存限制为1 GiB。容器怎样获得哪些数值呢？</p>
<pre tabindex="0"><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><!--
Because your Container did not specify its own memory request and limit, it was given the
[default memory request and limit](/docs/tasks/administer-cluster/memory-default-namespace/)
from the LimitRange.
-->
<p>因为你的容器没有声明自己的内存请求和限制，它从 LimitRange 那里获得了
<a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">默认的内存请求和限制</a>。</p>
<!--
At this point, your Container might be running or it might not be running. Recall that a prerequisite
for this task is that your Nodes have at least 1 GiB of memory. If each of your Nodes has only
1 GiB of memory, then there is not enough allocatable memory on any Node to accommodate a memory
request of 1 GiB. If you happen to be using Nodes with 2 GiB of memory, then you probably have
enough space to accommodate the 1 GiB request.

Delete your Pod:
-->
<p>此时，你的容器可能运行起来也可能没有运行起来。
回想一下我们本次任务的先决条件是你的每个节点都至少有1 GiB 的内存。
如果你的每个节点都只有1 GiB 的内存，那将没有一个节点拥有足够的可分配内存来满足1 GiB 的内存请求。</p>
<p>删除你的 Pod：</p>
<pre tabindex="0"><code>kubectl delete pod constraints-mem-demo-4 --namespace=constraints-mem-example
</code></pre><!--
## Enforcement of minimum and maximum memory constraints

The maximum and minimum memory constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.
-->
<h2 id="强制执行内存最小和最大限制">强制执行内存最小和最大限制</h2>
<p>LimitRange 为命名空间设定的最小和最大内存限制只有在 Pod 创建和更新时才会强制执行。
如果你更新 LimitRange，它不会影响此前创建的 Pod。</p>
<!--
## Motivation for minimum and maximum memory constraints
-->
<h2 id="设置内存最小和最大限制的动因">设置内存最小和最大限制的动因</h2>
<!--
As a cluster administrator, you might want to impose restrictions on the amount of memory that Pods can use.
For example:

* Each Node in a cluster has 2 GB of memory. You do not want to accept any Pod that requests
  more than 2 GB of memory, because no Node in the cluster can support the request.

* A cluster is shared by your production and development departments.
  You want to allow production workloads to consume up to 8 GB of memory, but
  you want development workloads to be limited to 512 MB. You create separate namespaces
  for production and development, and you apply memory constraints to each namespace.
-->
<p>作为集群管理员，你可能想规定 Pod 可以使用的内存总量限制。例如：</p>
<ul>
<li>集群的每个节点有 2 GB 内存。你不想接受任何请求超过 2 GB 的 Pod，因为集群中没有节点可以满足。</li>
<li>集群由生产部门和开发部门共享。你希望允许产品部门的负载最多耗用 8 GB 内存，
但是开发部门的负载最多可使用 512 MiB。
这时，你可以为产品部门和开发部门分别创建名字空间，并为各个名字空间设置内存约束。</li>
</ul>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace constraints-mem-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)
* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a87cbd1f9379dac7a48ae320da68a9ad">2.3.4 - 为命名空间配置 CPU 最小和最大约束</h1>
    
	<!--
title: Configure Minimum and Maximum CPU Constraints for a Namespace
content_type: task
weight: 40
-->
<!-- overview -->
<!--
This page shows how to set minimum and maximum values for the CPU resources used by Containers
and Pods in a namespace. You specify minimum and maximum CPU values in a
[LimitRange](/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core)
object. If a Pod does not meet the constraints imposed by the LimitRange, it cannot be created
in the namespace.
-->
<p>本页介绍如何为命名空间中容器和 Pod 使用的 CPU 资源设置最小和最大值。
你可以通过
<a href="/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core">LimitRange</a>
对象声明 CPU 的最小和最大值. 如果 Pod 不能满足 LimitRange 的限制，它就不能在命名空间中创建。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Your cluster must have at least 1 CPU available for use to run the task examples.
-->
<p>你的集群中每个节点至少要有 1 个 CPU 可用才能运行本任务示例。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中创建的资源和集群的其余资源相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace constraints-cpu-example
</code></pre></div><!--
## Create a LimitRange and a Pod

Here's the configuration file for a LimitRange:
-->
<h2 id="创建-limitrange-和-pod">创建 LimitRange 和 Pod</h2>
<p>这里给出了 LimitRange 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints.yaml" download="admin/resource/cpu-constraints.yaml"><code>admin/resource/cpu-constraints.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-yaml')" title="Copy admin/resource/cpu-constraints.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-min-max-demo-lr<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the LimitRange:
-->
<p>创建 LimitRange:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
View detailed information about the LimitRange:
-->
<p>查看 LimitRange 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get limitrange cpu-min-max-demo-lr --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows the minimum and maximum CPU constraints as expected. But
notice that even though you didn't specify default values in the configuration
file for the LimitRange, they were created automatically.
-->
<p>输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange
的配置文件中你没有声明默认值，默认值也会被自动创建。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">defaultRequest</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>200m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb">
</span></code></pre></div><!--
Now whenever a Container is created in the constraints-cpu-example namespace, Kubernetes
performs these steps:

* If the Container does not specify its own CPU request and limit, assign the default
CPU request and limit to the Container.

* Verify that the Container specifies a CPU request that is greater than or equal to 200 millicpu.

* Verify that the Container specifies a CPU limit that is less than or equal to 800 millicpu.
-->
<p>现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：</p>
<ul>
<li>
<p>如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。</p>
</li>
<li>
<p>核查容器声明的 CPU 请求确保其大于或者等于 200 millicpu。</p>
</li>
<li>
<p>核查容器声明的 CPU 限制确保其小于或者等于 800 millicpu。</p>
</li>
</ul>
<!--
When creating a `LimitRange` object, you can specify limits on huge-pages
or GPUs as well. However, when both `default` and `defaultRequest` are specified
on these resources, the two values must be the same.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 当创建 LimitRange 对象时，你也可以声明大页面和 GPU 的限制。
当这些资源同时声明了 'default' 和 'defaultRequest' 参数时，两个参数值必须相同。</div>
</blockquote>
<!--
Here's the configuration file for a Pod that has one Container. The Container manifest
specifies a CPU request of 500 millicpu and a CPU limit of 800 millicpu. These satisfy the
minimum and maximum CPU constraints imposed by the LimitRange.
-->
<p>这里给出了包含一个容器的 Pod 的配置文件。
该容器声明了 500 millicpu 的 CPU 请求和 800 millicpu 的 CPU 限制。
这些参数满足了 LimitRange 对象规定的 CPU 最小和最大限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod.yaml" download="admin/resource/cpu-constraints-pod.yaml"><code>admin/resource/cpu-constraints-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-yaml')" title="Copy admin/resource/cpu-constraints-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>确认一下 Pod 中的容器在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-cpu-demo --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod constraints-cpu-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows that the Container has a CPU request of 500 millicpu and CPU limit
of 800 millicpu. These satisfy the constraints imposed by the LimitRange.
-->
<p>输出结果表明容器的 CPU 请求为 500 millicpu，CPU 限制为 800 millicpu。
这些参数满足 LimitRange 规定的限制范围。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span></code></pre></div><!--
## Delete the Pod
-->
<h2 id="删除-pod">删除 Pod</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod constraints-cpu-demo --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
## Attempt to create a Pod that exceeds the maximum CPU constraint

Here's the configuration file for a Pod that has one Container. The Container specifies a
CPU request of 500 millicpu and a cpu limit of 1.5 cpu.
-->
<h2 id="尝试创建一个超过最大-cpu-限制的-pod">尝试创建一个超过最大 CPU 限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。容器声明了 500 millicpu 的 CPU
请求和 1.5 CPU 的 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod-2.yaml" download="admin/resource/cpu-constraints-pod-2.yaml"><code>admin/resource/cpu-constraints-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-2-yaml')" title="Copy admin/resource/cpu-constraints-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a CPU limit that is
too large:
-->
<p>输出结果表明 Pod 没有创建成功，因为容器声明的 CPU 限制太大了：</p>
<pre tabindex="0"><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/cpu-constraints-pod-2.yaml&quot;:
pods &quot;constraints-cpu-demo-2&quot; is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m.
</code></pre><!--
## Attempt to create a Pod that does not meet the minimum CPU request

Here's the configuration file for a Pod that has one Container. The Container specifies a
CPU request of 100 millicpu and a CPU limit of 800 millicpu.
-->
<h2 id="尝试创建一个不满足最小-cpu-请求的-pod">尝试创建一个不满足最小 CPU 请求的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器声明了100 millicpu的 CPU 请求和800 millicpu的 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod-3.yaml" download="admin/resource/cpu-constraints-pod-3.yaml"><code>admin/resource/cpu-constraints-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-3-yaml')" title="Copy admin/resource/cpu-constraints-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the Pod:
-->
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
The output shows that the Pod does not get created, because the Container specifies a CPU
request that is too small:
-->
<p>输出结果显示 Pod 没有创建成功，因为容器声明的 CPU 请求太小了：</p>
<pre tabindex="0"><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/cpu-constraints-pod-3.yaml&quot;:
pods &quot;constraints-cpu-demo-4&quot; is forbidden: minimum cpu usage per Container is 200m, but request is 100m.
</code></pre><!--
## Create a Pod that does not specify any CPU request or limit

Here's the configuration file for a Pod that has one Container. The Container does not
specify a CPU request, and it does not specify a CPU limit.
-->
<h2 id="创建一个没有声明-cpu-请求和-cpu-限制的-pod">创建一个没有声明 CPU 请求和 CPU 限制的 Pod</h2>
<p>这里给出了包含一个容器的 Pod 的配置文件。该容器没有设定 CPU 请求和 CPU 限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/cpu-constraints-pod-4.yaml" download="admin/resource/cpu-constraints-pod-4.yaml"><code>admin/resource/cpu-constraints-pod-4.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-cpu-constraints-pod-4-yaml')" title="Copy admin/resource/cpu-constraints-pod-4.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-cpu-constraints-pod-4-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>constraints-cpu-demo-4-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>vish/stress<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace<span style="color:#666">=</span>constraints-cpu-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 的详情：</p>
<pre tabindex="0"><code>kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml
</code></pre><!--
The output shows that the Pod's Container has a CPU request of 800 millicpu and a CPU limit of 800 millicpu.
How did the Container get those values?
-->
<p>输出结果显示 Pod 的容器有个 800 millicpu 的 CPU 请求和 800 millicpu 的 CPU 限制。
容器是怎样得到那些值的呢？</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>800m<span style="color:#bbb">
</span></code></pre></div><!--
Because your Container did not specify its own CPU request and limit, it was given the
[default CPU request and limit](/docs/tasks/administer-cluster/cpu-default-namespace/)
from the LimitRange.
-->
<p>因为你的 Container 没有声明自己的 CPU 请求和限制，LimitRange 给它指定了
<a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">默认的 CPU 请求和限制</a></p>
<!--
At this point, your Container might be running or it might not be running. Recall that a prerequisite
for this task is that your Nodes have at least 1 CPU. If each of your Nodes has only
1 CPU, then there might not be enough allocatable CPU on any Node to accommodate a request
of 800 millicpu. If you happen to be using Nodes with 2 CPU, then you probably have
enough CPU to accommodate the 800 millicpu request.

Delete your Pod:
-->
<p>此时，你的容器可能运行也可能没有运行。
回想一下，本任务的先决条件是你的节点要有 1 个 CPU。
如果你的每个节点仅有 1 个 CPU，那么可能没有任何一个节点可以满足 800 millicpu 的 CPU 请求。
如果你在用的节点恰好有两个 CPU，那么你才可能有足够的 CPU 来满足 800 millicpu 的请求。</p>
<pre tabindex="0"><code>kubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example
</code></pre><!--
## Enforcement of minimum and maximum CPU constraints

The maximum and minimum CPU constraints imposed on a namespace by a LimitRange are enforced only
when a Pod is created or updated. If you change the LimitRange, it does not affect
Pods that were created previously.
-->
<h2 id="cpu-最小和最大限制的强制执行">CPU 最小和最大限制的强制执行</h2>
<p>只有当 Pod 创建或者更新时，LimitRange 为命名空间规定的 CPU 最小和最大限制才会被强制执行。
如果你对 LimitRange 进行修改，那不会影响此前创建的 Pod。</p>
<!--
## Motivation for minimum and maximum CPU constraints

As a cluster administrator, you might want to impose restrictions on the CPU resources that Pods can use.
For example:
-->
<h2 id="最小和最大-cpu-限制范围的动机">最小和最大 CPU 限制范围的动机</h2>
<p>作为集群管理员，你可能想设定 Pod 可以使用的 CPU 资源限制。例如：</p>
<!--
* Each Node in a cluster has 2 CPU. You do not want to accept any Pod that requests
more than 2 CPU, because no Node in the cluster can support the request.

* A cluster is shared by your production and development departments.
You want to allow production workloads to consume up to 3 CPU, but you want development workloads to be limited
to 1 CPU. You create separate namespaces for production and development, and you apply CPU constraints to
each namespace.
-->
<ul>
<li>集群中的每个节点有两个 CPU。你不想接受任何请求超过 2 个 CPU 的 Pod，因为集群中没有节点可以支持这种请求。</li>
<li>你的生产和开发部门共享一个集群。你想允许生产工作负载消耗 3 个 CPU，
而开发部门工作负载的消耗限制为 1 个 CPU。
你可以为生产和开发创建不同的命名空间，并且为每个命名空间都应用 CPU 限制。</li>
</ul>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace constraints-cpu-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)

* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考：</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考：</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fe3283559a3df299aae3ee00ecea2fad">2.3.5 - 为命名空间配置内存和 CPU 配额</h1>
    
	<!--
title: Configure Memory and CPU Quotas for a Namespace
content_type: task
weight: 50
-->
<!-- overview -->
<!--
This page shows how to set quotas for the total amount memory and CPU that
can be used by all Containers running in a namespace. You specify quotas in a
[ResourceQuota](/docs/reference/generated/kubernetes-api/v1.22/#resourcequota-v1-core)
object.
-->
<p>本文介绍怎样为命名空间设置容器可用的内存和 CPU 总量。你可以通过
<a href="/docs/reference/generated/kubernetes-api/v1.22/#resourcequota-v1-core">ResourceQuota</a>
对象设置配额.</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 1 GiB of memory.
-->
<p>集群中每个节点至少有 1 GiB 的内存。</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace quota-mem-cpu-example
</code></pre></div><!--
## Create a ResourceQuota

Here is the configuration file for a ResourceQuota object:
-->
<h2 id="创建-resourcequota">创建 ResourceQuota</h2>
<p>这里给出一个 ResourceQuota 对象的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-mem-cpu.yaml" download="admin/resource/quota-mem-cpu.yaml"><code>admin/resource/quota-mem-cpu.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-mem-cpu-yaml')" title="Copy admin/resource/quota-mem-cpu.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-mem-cpu-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mem-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests.cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests.memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limits.cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">limits.memory</span>:<span style="color:#bbb"> </span>2Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the ResourceQuota:
-->
<p>创建 ResourceQuota</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</code></pre></div><!--
View detailed information about the ResourceQuota:
-->
<p>查看 ResourceQuota 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get resourcequota mem-cpu-demo --namespace<span style="color:#666">=</span>quota-mem-cpu-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:

* Every Container must have a memory request, memory limit, cpu request, and cpu limit.
* The memory request total for all Containers must not exceed 1 GiB.
* The memory limit total for all Containers must not exceed 2 GiB.
* The CPU request total for all Containers must not exceed 1 cpu.
* The CPU limit total for all Containers must not exceed 2 cpu.
-->
<p>ResourceQuota 在 quota-mem-cpu-example 命名空间中设置了如下要求：</p>
<ul>
<li>每个容器必须有内存请求和限制，以及 CPU 请求和限制。</li>
<li>所有容器的内存请求总和不能超过1 GiB。</li>
<li>所有容器的内存限制总和不能超过2 GiB。</li>
<li>所有容器的 CPU 请求总和不能超过1 cpu。</li>
<li>所有容器的 CPU 限制总和不能超过2 cpu。</li>
</ul>
<!--
## Create a Pod

Here is the configuration file for a Pod:
-->
<h2 id="创建-pod">创建 Pod</h2>
<p>这里给出 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-mem-cpu-pod.yaml" download="admin/resource/quota-mem-cpu-pod.yaml"><code>admin/resource/quota-mem-cpu-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-mem-cpu-pod-yaml')" title="Copy admin/resource/quota-mem-cpu-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-mem-cpu-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;600Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;400m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>检查下 Pod 中的容器在运行：</p>
<pre tabindex="0"><code>kubectl get pod quota-mem-cpu-demo --namespace=quota-mem-cpu-example
</code></pre><!--
Once again, view detailed information about the ResourceQuota:
-->
<p>再查看 ResourceQuota 的详情：</p>
<pre tabindex="0"><code>kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml
</code></pre><!--
The output shows the quota along with how much of the quota has been used.
You can see that the memory and CPU requests and limits for your Pod do not
exceed the quota.
-->
<p>输出结果显示了配额以及有多少配额已经被使用。你可以看到 Pod 的内存和 CPU 请求值及限制值没有超过配额。</p>
<pre tabindex="0"><code>status:
  hard:
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
  used:
    limits.cpu: 800m
    limits.memory: 800Mi
    requests.cpu: 400m
    requests.memory: 600Mi
</code></pre><!--
## Attempt to create a second Pod

Here is the configuration file for a second Pod:
-->
<h2 id="尝试创建第二个-pod">尝试创建第二个 Pod</h2>
<p>这里给出了第二个 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-mem-cpu-pod-2.yaml" download="admin/resource/quota-mem-cpu-pod-2.yaml"><code>admin/resource/quota-mem-cpu-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-mem-cpu-pod-2-yaml')" title="Copy admin/resource/quota-mem-cpu-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-mem-cpu-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>quota-mem-cpu-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;800m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;700Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;400m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a memory request of 700 MiB.
Notice that the sum of the used memory request and this new memory
request exceeds the memory request quota. 600 MiB + 700 MiB > 1 GiB.

Attempt to create the Pod:
-->
<p>配置文件中，你可以看到 Pod 的内存请求为 700 MiB。
请注意新的内存请求与已经使用的内存请求只和超过了内存请求的配额。
600 MiB + 700 MiB &gt; 1 GiB。</p>
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace<span style="color:#666">=</span>quota-mem-cpu-example
</code></pre></div><!--
The second Pod does not get created. The output shows that creating the second Pod
would cause the memory request total to exceed the memory request quota.
-->
<p>第二个 Pod 不能被创建成功。输出结果显示创建第二个 Pod 会导致内存请求总量超过内存请求配额。</p>
<pre tabindex="0"><code>Error from server (Forbidden): error when creating &quot;examples/admin/resource/quota-mem-cpu-pod-2.yaml&quot;:
pods &quot;quota-mem-cpu-demo-2&quot; is forbidden: exceeded quota: mem-cpu-demo,
requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi
</code></pre><!--
## Discussion

As you have seen in this exercise, you can use a ResourceQuota to restrict
the memory request total for all Containers running in a namespace.
You can also restrict the totals for memory limit, cpu request, and cpu limit.

If you want to restrict individual Containers, instead of totals for all Containers, use a
[LimitRange](/docs/tasks/administer-cluster/memory-constraint-namespace/).
-->
<h2 id="讨论">讨论</h2>
<p>如你在本练习中所见，你可以用 ResourceQuota 限制命名空间中所有容器的内存请求总量。
同样你也可以限制内存限制总量、CPU 请求总量、CPU 限制总量。</p>
<p>如果你想对单个容器而不是所有容器进行限制，就请使用
<a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">LimitRange</a>。</p>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace quota-mem-cpu-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)
* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)
* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 限制的最小值和最大值</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配CPU资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-40e30a9209e0c9f4153707e43243e9d7">2.3.6 - 配置命名空间下 Pod 配额</h1>
    
	<!-- overview -->
<!--
This page shows how to set a quota for the total number of Pods that can run
in a namespace. You specify quotas in a
[ResourceQuota](/docs/reference/generated/kubernetes-api/v1.22/#resourcequota-v1-core)
object.
-->
<p>本文主要描述如何配置一个命名空间下可运行的 Pod 个数配额。
你可以使用
<a href="/docs/reference/generated/kubernetes-api/v1.22/#resourcequota-v1-core">ResourceQuota</a>
对象来配置配额。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建一个命名空间">创建一个命名空间</h2>
<p>首先创建一个命名空间，这样可以将本次操作中创建的资源与集群其他资源隔离开来。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace quota-pod-example
</code></pre></div><!--
## Create a ResourceQuota

Here is the configuration file for a ResourceQuota object:
-->
<h2 id="创建-resourcequota">创建 ResourceQuota</h2>
<p>下面是一个 ResourceQuota 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-pod.yaml" download="admin/resource/quota-pod.yaml"><code>admin/resource/quota-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-pod-yaml')" title="Copy admin/resource/quota-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 创建 ResourceQuota: -->
<p>创建这个 ResourceQuota：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace<span style="color:#666">=</span>quota-pod-example
</code></pre></div><!--
View detailed information about the ResourceQuota:
-->
<p>查看资源配额的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get resourcequota pod-demo --namespace<span style="color:#666">=</span>quota-pod-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that the namespace has a quota of two Pods, and that currently there are
no Pods; that is, none of the quota is used.
-->
<p>从输出的信息我们可以看到，该命名空间下 Pod 的配额是 2 个，目前创建的 Pod 数为 0，
配额使用率为 0。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">used</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Here is the configuration file for a Deployment:
-->
<p>下面是一个 Deployment 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-pod-deployment.yaml" download="admin/resource/quota-pod-deployment.yaml"><code>admin/resource/quota-pod-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-pod-deployment-yaml')" title="Copy admin/resource/quota-pod-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-pod-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">purpose</span>:<span style="color:#bbb"> </span>quota-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">purpose</span>:<span style="color:#bbb"> </span>quota-demo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, `replicas: 3` tells Kubernetes to attempt to create three Pods, all running the same application.

Create the Deployment:
-->
<p>在配置文件中，<code>replicas: 3</code> 告诉 Kubernetes 尝试创建三个 Pods，且运行相同的应用。</p>
<p>创建这个 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace<span style="color:#666">=</span>quota-pod-example
</code></pre></div><!--
View detailed information about the Deployment:
-->
<p>查看 Deployment 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment pod-quota-demo --namespace<span style="color:#666">=</span>quota-pod-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that even though the Deployment specifies three replicas, only two
Pods were created because of the quota.
-->
<p>从输出的信息我们可以看到，尽管尝试创建三个 Pod，但是由于配额的限制，只有两个 Pod 能被成功创建。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">availableReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">lastUpdateTime</span>:<span style="color:#bbb"> </span>2017-07-07T20:57:05Z<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">message: &#39;unable to create pods</span>:<span style="color:#bbb"> </span>pods &#34;pod-quota-demo-1650323038-&#34; is forbidden:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited</span>:<span style="color:#bbb"> </span>pods=2&#39;<span style="color:#bbb">
</span></code></pre></div><!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace quota-pod-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/)
* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/)
* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/)
* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理人员参考">集群管理人员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认的内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为命名空间配置默认的的 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存的最小值和最大值约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 的最小值和最大值约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象的设置配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发人员参考">应用开发人员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">给容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">配置 Pod 的服务质量</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7743f043c43f7b12e8654e2227dbc658">2.4 - 证书</h1>
    
	<!-- 
---
title: Certificates
content_type: task
weight: 20
---
-->
<!-- overview -->
<!-- 
When using client certificate authentication, you can generate certificates
manually through `easyrsa`, `openssl` or `cfssl`.
-->
<p>在使用客户端证书认证的场景下，你可以通过 <code>easyrsa</code>、<code>openssl</code> 或 <code>cfssl</code> 等工具以手工方式生成证书。</p>
<!-- body -->
<h3 id="easyrsa">easyrsa</h3>
<!-- 
**easyrsa** can manually generate certificates for your cluster.
-->
<p><strong>easyrsa</strong> 支持以手工方式为你的集群生成证书。</p>
<!-- 
1.  Download, unpack, and initialize the patched version of easyrsa3.
-->
<ol>
<li>
<p>下载、解压、初始化打过补丁的 easyrsa3。</p>
<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
tar xzf easy-rsa.tar.gz
cd easy-rsa-master/easyrsa3
./easyrsa init-pki
</code></pre>
<!-- 
1.  Generate a new certificate authority (CA). `--batch` sets automatic mode;
`--req-cn` specifies the Common Name (CN) for the CA's new root certificate.
-->
</li>
<li>
<p>生成新的证书颁发机构（CA）。参数 <code>--batch</code> 用于设置自动模式；
参数 <code>--req-cn</code> 用于设置新的根证书的通用名称（CN）。</p>
<pre><code>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre>
<!-- 
1.  Generate server certificate and key.
The argument `--subject-alt-name` sets the possible IPs and DNS names the API server will
be accessed with. The `MASTER_CLUSTER_IP` is usually the first IP from the service CIDR
that is specified as the `--service-cluster-ip-range` argument for both the API server and
the controller manager component. The argument `--days` is used to set the number of days
after which the certificate expires.
The sample below also assumes that you are using `cluster.local` as the default
DNS domain name.
-->
</li>
<li>
<p>生成服务器证书和秘钥。
参数 <code>--subject-alt-name</code> 设置 API 服务器的 IP 和 DNS 名称。
<code>MASTER_CLUSTER_IP</code> 用于 API 服务器和控制管理器，通常取 CIDR 的第一个 IP，由 <code>--service-cluster-ip-range</code> 的参数提供。
参数 <code>--days</code> 用于设置证书的过期时间。
下面的示例假定你的默认 DNS 域名为 <code>cluster.local</code>。</p>
<pre><code>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
&quot;IP:${MASTER_CLUSTER_IP},&quot;\
&quot;DNS:kubernetes,&quot;\
&quot;DNS:kubernetes.default,&quot;\
&quot;DNS:kubernetes.default.svc,&quot;\
&quot;DNS:kubernetes.default.svc.cluster,&quot;\
&quot;DNS:kubernetes.default.svc.cluster.local&quot; \
--days=10000 \
build-server-full server nopass
</code></pre>
<!-- 
1.  Copy `pki/ca.crt`, `pki/issued/server.crt`, and `pki/private/server.key` to your directory.
1.  Fill in and add the following parameters into the API server start parameters:
-->
</li>
<li>
<p>拷贝文件 <code>pki/ca.crt</code>、<code>pki/issued/server.crt</code> 和 <code>pki/private/server.key</code> 到你的目录中。</p>
</li>
<li>
<p>在 API 服务器的启动参数中添加以下参数：</p>
<pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre>
</li>
</ol>
<h3 id="openssl">openssl</h3>
<!-- 
**openssl** can manually generate certificates for your cluster.
-->
<p><strong>openssl</strong> 支持以手工方式为你的集群生成证书。</p>
<!-- 
1.  Generate a ca.key with 2048bit:
-->
<ol>
<li>
<p>生成一个 2048 位的 ca.key 文件</p>
<pre><code>openssl genrsa -out ca.key 2048
</code></pre>
<!-- 
1.  According to the ca.key generate a ca.crt (use -days to set the certificate effective time):
-->
</li>
<li>
<p>在 ca.key 文件的基础上，生成 ca.crt 文件（用参数 -days 设置证书有效期）</p>
<pre><code>openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre>
<!-- 
1.  Generate a server.key with 2048bit:
-->
</li>
<li>
<p>生成一个 2048 位的 server.key 文件：</p>
<pre><code>openssl genrsa -out server.key 2048
</code></pre>
<!-- 
1.  Create a config file for generating a Certificate Signing Request (CSR).
Be sure to substitute the values marked with angle brackets (e.g. `<MASTER_IP>`)
with real values before saving this to a file (e.g. `csr.conf`).
Note that the value for `MASTER_CLUSTER_IP` is the service cluster IP for the
API server as described in previous subsection.
The sample below also assumes that you are using `cluster.local` as the default
DNS domain name.
-->
</li>
<li>
<p>创建一个用于生成证书签名请求（CSR）的配置文件。
保存文件（例如：<code>csr.conf</code>）前，记得用真实值替换掉尖括号中的值（例如：<code>&lt;MASTER_IP&gt;</code>）。
注意：<code>MASTER_CLUSTER_IP</code> 就像前一小节所述，它的值是 API 服务器的服务集群 IP。
下面的例子假定你的默认 DNS 域名为 <code>cluster.local</code>。</p>
<pre><code>[ req ]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C = &lt;country&gt;
ST = &lt;state&gt;
L = &lt;city&gt;
O = &lt;organization&gt;
OU = &lt;organization unit&gt;
CN = &lt;MASTER_IP&gt;

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
IP.1 = &lt;MASTER_IP&gt;
IP.2 = &lt;MASTER_CLUSTER_IP&gt;

[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth
subjectAltName=@alt_names
</code></pre>
<!-- 
1.  Generate the certificate signing request based on the config file:
-->
</li>
<li>
<p>基于上面的配置文件生成证书签名请求：</p>
<pre><code>openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre>
<!-- 
1.  Generate the server certificate using the ca.key, ca.crt and server.csr:
-->
</li>
<li>
<p>基于 ca.key、ca.key 和 server.csr 等三个文件生成服务端证书：</p>
<pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out server.crt -days 10000 \
-extensions v3_ext -extfile csr.conf
</code></pre>
<!-- 
1.  View the certificate:
-->
</li>
<li>
<p>查看证书：</p>
<pre><code>openssl x509  -noout -text -in ./server.crt
</code></pre>
</li>
</ol>
<!-- 
Finally, add the same parameters into the API server start parameters.
-->
<p>最后，为 API 服务器添加相同的启动参数。</p>
<h3 id="cfssl">cfssl</h3>
<!-- 
**cfssl** is another tool for certificate generation.
-->
<p><strong>cfssl</strong> 是另一个用于生成证书的工具。</p>
<!-- 
1.  Download, unpack and prepare the command line tools as shown below.
    Note that you may need to adapt the sample commands based on the hardware
    architecture and cfssl version you are using.
-->
<ol>
<li>
<p>下载、解压并准备如下所示的命令行工具。
注意：你可能需要根据所用的硬件体系架构和 cfssl 版本调整示例命令。</p>
<pre><code>curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl
chmod +x cfssl
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson
chmod +x cfssljson
curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre>
<!-- 
1.  Create a directory to hold the artifacts and initialize cfssl:
-->
</li>
<li>
<p>创建一个目录，用它保存所生成的构件和初始化 cfssl：</p>
<pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre>
<!-- 
1.  Create a JSON config file for generating the CA file, for example, `ca-config.json`:
-->
</li>
<li>
<p>创建一个 JSON 配置文件来生成 CA 文件，例如：<code>ca-config.json</code>：</p>
<pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre>
<!-- 
1.  Create a JSON config file for CA certificate signing request (CSR), for example,
`ca-csr.json`. Be sure to replace the values marked with angle brackets with
real values you want to use.
-->
</li>
<li>
<p>创建一个 JSON 配置文件，用于 CA 证书签名请求（CSR），例如：<code>ca-csr.json</code>。
确认用你需要的值替换掉尖括号中的值。</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
<!-- 
1.  Generate CA key (`ca-key.pem`) and certificate (`ca.pem`):
-->
</li>
<li>
<p>生成 CA 秘钥文件（<code>ca-key.pem</code>）和证书文件（<code>ca.pem</code>）：</p>
<pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre>
<!-- 
1.  Create a JSON config file for generating keys and certificates for the API
server, for example, `server-csr.json`. Be sure to replace the values in angle brackets with
real values you want to use. The `MASTER_CLUSTER_IP` is the service cluster
IP for the API server as described in previous subsection.
The sample below also assumes that you are using `cluster.local` as the default
DNS domain name.
-->
</li>
<li>
<p>创建一个 JSON 配置文件，用来为 API 服务器生成秘钥和证书，例如：<code>server-csr.json</code>。
确认用你需要的值替换掉尖括号中的值。<code>MASTER_CLUSTER_IP</code> 是为 API 服务器 指定的服务集群 IP，就像前面小节描述的那样。
以下示例假定你的默认 DSN 域名为<code>cluster.local</code>。</p>
<pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre>
<!-- 
1.  Generate the key and certificate for the API server, which are by default
saved into file `server-key.pem` and `server.pem` respectively:
-->
</li>
<li>
<p>为 API 服务器生成秘钥和证书，默认会分别存储为<code>server-key.pem</code> 和 <code>server.pem</code> 两个文件。</p>
<pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre>
</li>
</ol>
<!-- 
## Distributing Self-Signed CA Certificate
-->
<h2 id="分发自签名的-ca-证书">分发自签名的 CA 证书</h2>
<!-- 
A client node may refuse to recognize a self-signed CA certificate as valid.
For a non-production deployment, or for a deployment that runs behind a company
firewall, you can distribute a self-signed CA certificate to all clients and
refresh the local list for valid certificates.

On each client, perform the following operations:
-->
<p>客户端节点可能不认可自签名 CA 证书的有效性。
对于非生产环境，或者运行在公司防火墙后的环境，你可以分发自签名的 CA 证书到所有客户节点，并刷新本地列表以使证书生效。</p>
<p>在每一个客户节点，执行以下操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
sudo update-ca-certificates
</code></pre></div><pre tabindex="0"><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><!-- 
## Certificates API
-->
<h2 id="certificates-api">证书 API</h2>
<!-- 
You can use the `certificates.k8s.io` API to provision
x509 certificates to use for authentication as documented
[here](/docs/tasks/tls/managing-tls-in-a-cluster).
-->
<p>你可以通过 <code>certificates.k8s.io</code> API 提供 x509 证书，用来做身份验证，
如<a href="/zh/docs/tasks/tls/managing-tls-in-a-cluster">本</a>文档所述。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8c31aafd38fad5b0de0bd191758d6f93">2.5 - 安装网络规则驱动</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-1239a77618c6278373832a142cd85519">2.5.1 - 使用 Calico 提供 NetworkPolicy</h1>
    
	<!-- overview -->
<!--
This page shows a couple of quick ways to create a Calico cluster on Kubernetes.
-->
<p>本页展示了几种在 Kubernetes 上快速创建 Calico 集群的方法。</p>
<h2 id="准备开始">准备开始</h2>
<!--
Decide whether you want to deploy a [cloud](#creating-a-calico-cluster-with-google-kubernetes-engine-gke) or [local](#creating-a-local-calico-cluster-with-kubeadm) cluster.
-->
<p>确定你想部署一个<a href="#gke-cluster">云版本</a>还是<a href="#local-cluster">本地版本</a>的集群。</p>
<!-- steps -->
<!--
## Creating a Calico cluster with Google Kubernetes Engine (GKE)

**Prerequisite**: [gcloud](https://cloud.google.com/sdk/docs/quickstarts).
-->
<h2 id="gke-cluster">在 Google Kubernetes Engine (GKE) 上创建一个 Calico 集群</h2>
<p><strong>先决条件</strong>: <a href="https://cloud.google.com/sdk/docs/quickstarts">gcloud</a></p>
<!--
1.  To launch a GKE cluster with Calico, just include the `--enable-network-policy` flag.
-->
<ol>
<li>
<p>启动一个带有 Calico 的 GKE 集群，只需加上参数 <code>--enable-network-policy</code>。</p>
<p><strong>语法</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud container clusters create <span style="color:#666">[</span>CLUSTER_NAME<span style="color:#666">]</span> --enable-network-policy
</code></pre></div><p><strong>示例</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">gcloud container clusters create my-calico-cluster --enable-network-policy
</code></pre></div></li>
</ol>
<!--
1.  To verify the deployment, use the following command.
-->
<ol start="2">
<li>
<p>使用如下命令验证部署是否正确。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
The Calico pods begin with `calico`. Check to make sure each one has a status of `Running`.
-->
<p>Calico 的 pods 名以 <code>calico</code> 打头，检查确认每个 pods 状态为 <code>Running</code>。</p>
</li>
</ol>
<!-- 
## Creating a local Calico cluster with kubeadm

To get a local single-host Calico cluster in fifteen minutes using kubeadm, refer to the
[Calico Quickstart](https://docs.projectcalico.org/latest/getting-started/kubernetes/).
-->
<h2 id="local-cluster">使用 kubeadm 创建一个本地 Calico 集群  </h2>
<p>使用 kubeadm 在 15 分钟内得到一个本地单主机 Calico 集群，请参考
<a href="https://docs.projectcalico.org/latest/getting-started/kubernetes/">Calico 快速入门</a>。</p>
<h2 id="接下来">接下来</h2>
<!--
Once your cluster is running, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy.
-->
<p>集群运行后，您可以按照<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
去尝试使用 Kubernetes NetworkPolicy。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-95039241255a31df196beaa405b68eba">2.5.2 - 使用 Cilium 提供 NetworkPolicy</h1>
    
	<!--
reviewers:
- danwent
- aanm
title: Use Cilium for NetworkPolicy
content_type: task
weight: 20
-->
<!-- overview -->
<!--
This page shows how to use Cilium for NetworkPolicy.

For background on Cilium, read the [Introduction to Cilium](https://docs.cilium.io/en/stable/intro).
-->
<p>本页展示如何使用 Cilium 提供 NetworkPolicy。</p>
<p>关于 Cilium 的背景知识，请阅读 <a href="https://docs.cilium.io/en/stable/intro">Cilium 介绍</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Deploying Cilium on Minikube for Basic Testing

To get familiar with Cilium easily you can follow the
[Cilium Kubernetes Getting Started Guide](https://docs.cilium.io/en/stable/gettingstarted/minikube/)
to perform a basic DaemonSet installation of Cilium in minikube.

To start minikube, minimal version required is >= v1.3.1, run the with the
following arguments:
-->
<h2 id="在-minikube-上部署-cilium-用于基本测试">在 Minikube 上部署 Cilium 用于基本测试</h2>
<p>为了轻松熟悉 Cilium 你可以根据
<a href="https://docs.cilium.io/en/stable/gettingstarted/minikube/">Cilium Kubernetes 入门指南</a>
在 minikube 中执行一个 cilium 的基本 DaemonSet 安装。</p>
<p>要启动 minikube，需要的最低版本为 1.3.1，使用下面的参数运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube version
</code></pre></div><pre tabindex="0"><code>minikube version: v1.3.1
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start --network-plugin<span style="color:#666">=</span>cni --memory<span style="color:#666">=</span><span style="color:#666">4096</span>
</code></pre></div><!--
Mount the BPF filesystem:
-->
<p>挂载 BPF 文件系统：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube ssh -- sudo mount bpffs -t bpf /sys/fs/bpf
</code></pre></div><!--
For minikube you can deploy this simple ''all-in-one'' YAML file that includes
DaemonSet configurations for Cilium as well as appropriate RBAC settings:
-->
<p>在 minikube 环境中，你可以部署下面的&quot;一体化&quot; YAML 文件，其中包含 Cilium
的 DaemonSet 配置以及适当的 RBAC 配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.8/install/kubernetes/quick-install.yaml
</code></pre></div><pre tabindex="0"><code>configmap/cilium-config created
serviceaccount/cilium created
serviceaccount/cilium-operator created
clusterrole.rbac.authorization.k8s.io/cilium created
clusterrole.rbac.authorization.k8s.io/cilium-operator created
clusterrolebinding.rbac.authorization.k8s.io/cilium created
clusterrolebinding.rbac.authorization.k8s.io/cilium-operator created
daemonset.apps/cilium create
deployment.apps/cilium-operator created
</code></pre><!--
The remainder of the Getting Started Guide explains how to enforce both L3/L4
(i.e., IP address + port) security policies, as well as L7 (e.g., HTTP) security
policies using an example application.
-->
<p>入门指南其余的部分用一个示例应用说明了如何强制执行 L3/L4（即 IP 地址+端口）的安全策略
以及L7 （如 HTTP）的安全策略。</p>
<!--
## Deploying Cilium for Production Use

For detailed instructions around deploying Cilium for production, see:
[Cilium Kubernetes Installation Guide](https://docs.cilium.io/en/stable/concepts/kubernetes/intro/)
This documentation includes detailed requirements, instructions and example
production DaemonSet files.
 -->
<h2 id="部署-cilium-用于生产用途">部署 Cilium 用于生产用途</h2>
<p>关于部署 Cilium 用于生产的详细说明，请见
<a href="https://docs.cilium.io/en/stable/concepts/kubernetes/intro/">Cilium Kubernetes 安装指南</a>
此文档包括详细的需求、说明和生产用途 DaemonSet 文件示例。</p>
<!-- discussion -->
<!--
##  Understanding Cilium components

Deploying a cluster with Cilium adds Pods to the `kube-system` namespace. To see
this list of Pods run:
 -->
<h2 id="了解-cilium-组件">了解 Cilium 组件</h2>
<p>部署使用 Cilium 的集群会添加 Pods 到 <code>kube-system</code> 命名空间。要查看 Pod 列表，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!-- You'll see a list of Pods similar to this: -->
<p>你将看到像这样的 Pods 列表：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">NAME            READY   STATUS    RESTARTS   AGE
cilium-6rxbd    1/1     Running   0          1m
...
</code></pre><!--
A `cilium` Pod runs on each node in your cluster and enforces network policy
on the traffic to/from Pods on that node using Linux BPF.
-->
<p>你的集群中的每个节点上都会运行一个 <code>cilium</code> Pod，通过使用 Linux BPF
针对该节点上的 Pod 的入站、出站流量实施网络策略控制。</p>
<h2 id="接下来">接下来</h2>
<!--
Once your cluster is running, you can follow the
[Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/)
to try out Kubernetes NetworkPolicy with Cilium.
Have fun, and if you have questions, contact us using the
[Cilium Slack Channel](https://cilium.herokuapp.com/).
-->
<p>集群运行后，你可以按照
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
试用基于 Cilium 的 Kubernetes NetworkPolicy。
玩得开心，如果你有任何疑问，请到 <a href="https://cilium.herokuapp.com/">Cilium Slack 频道</a>
联系我们。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-505a0a6a7e6eff361bbb3be81c84b2e0">2.5.3 - 使用 kube-router 提供 NetworkPolicy</h1>
    
	<!-- overview -->
<!--
This page shows how to use [Kube-router](https://github.com/cloudnativelabs/kube-router) for NetworkPolicy.
-->
<p>本页展示如何使用 <a href="https://github.com/cloudnativelabs/kube-router">Kube-router</a> 提供 NetworkPolicy。</p>
<h2 id="准备开始">准备开始</h2>
<!--
You need to have a Kubernetes cluster running. If you do not already have a cluster, you can create one by using any of the cluster installers like Kops, Bootkube, Kubeadm etc.
-->
<p>你需要拥有一个运行中的 Kubernetes 集群。如果你还没有集群，可以使用任意的集群
安装程序如 Kops、Bootkube、Kubeadm 等创建一个。</p>
<!-- steps -->
<!--
## Installing Kube-router addon

The Kube-router Addon comes with a Network Policy Controller that watches Kubernetes API server for any NetworkPolicy and pods updated and configures iptables rules and ipsets to allow or block traffic as directed by the policies. Please follow the [trying Kube-router with cluster installers](https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers) guide to install Kube-router addon.
-->
<h2 id="安装-kube-router-插件">安装 kube-router 插件</h2>
<p>kube-router 插件自带一个网络策略控制器，监视来自于 Kubernetes API 服务器的
NetworkPolicy 和 Pod 的变化，根据策略指示配置 iptables 规则和 ipsets 来允许或阻止流量。
请根据 <a href="https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers">通过集群安装程序尝试 kube-router</a> 指南安装 kube-router 插件。</p>
<h2 id="接下来">接下来</h2>
<!--
Once you have installed the Kube-router addon, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy.
-->
<p>在你安装了 kube-router 插件后，可以参考
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
去尝试使用 Kubernetes NetworkPolicy。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2842eac98aa0e229a5c6755c4c83d2a7">2.5.4 - 使用 Romana 提供 NetworkPolicy</h1>
    
	<!--
reviewers:
- chrismarino
title: Romana for NetworkPolicy
content_type: task
weight: 40
-->
<!-- overview -->
<!--
This page shows how to use Romana for NetworkPolicy.
-->
<p>本页展示如何使用 Romana 作为 NetworkPolicy。</p>
<h2 id="准备开始">准备开始</h2>
<!--
Complete steps 1, 2, and 3 of  the [kubeadm getting started guide](/docs/getting-started-guides/kubeadm/).
-->
<p>完成 <a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm 入门指南</a>中的 1、2、3 步。</p>
<!-- steps -->
<!--
## Installing Romana with kubeadm

Follow the [containerized installation guide](https://github.com/romana/romana/tree/master/containerize) for kubeadm.

## Applying network policies

To apply network policies use one of the following:

* [Romana network policies](https://github.com/romana/romana/wiki/Romana-policies).
    * [Example of Romana network policy](https://github.com/romana/core/blob/master/doc/policy.md).
* The NetworkPolicy API.
 -->
<h2 id="使用-kubeadm-安装-romana">使用 kubeadm 安装 Romana</h2>
<p>按照<a href="https://github.com/romana/romana/tree/master/containerize">容器化安装指南</a>，
使用 kubeadm 安装。</p>
<h2 id="应用网络策略">应用网络策略</h2>
<p>使用以下的一种方式应用网络策略：</p>
<ul>
<li><a href="https://github.com/romana/romana/wiki/Romana-policies">Romana 网络策略</a>
<ul>
<li><a href="https://github.com/romana/core/blob/master/doc/policy.md">Romana 网络策略例子</a></li>
</ul>
</li>
<li>NetworkPolicy API</li>
</ul>
<h2 id="接下来">接下来</h2>
<!--
Once you have installed Romana, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy.
 -->
<p>Romana 安装完成后，你可以按照
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
去尝试使用 Kubernetes NetworkPolicy。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ac075c3fdfd0d41aa753cc70e42be064">2.5.5 - 使用 Weave Net 提供 NetworkPolicy</h1>
    
	<!--
reviewers:
- bboreham
title: Weave Net for NetworkPolicy
content_type: task
weight: 50
-->
<!-- overview -->
<!--
This page shows how to use Weave Net for NetworkPolicy.
-->
<p>本页展示如何使用使用 Weave Net 提供 NetworkPolicy。</p>
<h2 id="准备开始">准备开始</h2>
<!--
You need to have a Kubernetes cluster. Follow the
[kubeadm getting started guide](/docs/reference/setup-tools/kubeadm/) to bootstrap one.
 -->
<p>你需要拥有一个 Kubernetes 集群。按照
<a href="/zh/docs/reference/setup-tools/kubeadm/">kubeadm 入门指南</a>
来启动一个。</p>
<!-- steps -->
<!--
## Install the Weave Net addon

Follow the [Integrating Kubernetes via the Addon](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/) guide.

The Weave Net addon for Kubernetes comes with a
[Network Policy Controller](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#npc)
that automatically monitors Kubernetes for any NetworkPolicy annotations on all
namespaces and configures `iptables` rules to allow or block traffic as directed by the policies.
-->
<h2 id="安装-weave-net-插件">安装 Weave Net 插件</h2>
<p>按照<a href="https://www.weave.works/docs/net/latest/kubernetes/kube-addon/">通过插件集成 Kubernetes</a>
指南执行安装。</p>
<p>Kubernetes 的 Weave Net 插件带有
<a href="https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#npc">网络策略控制器</a>，
可自动监控 Kubernetes 所有名字空间的 NetworkPolicy 注释，
配置 <code>iptables</code> 规则以允许或阻止策略指示的流量。</p>
<!--
## Test the installation

Verify that the weave works.

Enter the following command:
-->
<h2 id="测试安装">测试安装</h2>
<p>验证 weave 是否有效。</p>
<p>输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get po -n kube-system -o wide
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似这样：</p>
<pre tabindex="0"><code>NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
weave-net-1t1qg                         2/2       Running   0          9d        192.168.2.10    worknode3
weave-net-231d7                         2/2       Running   1          7d        10.2.0.17       worknodegpu
weave-net-7nmwt                         2/2       Running   3          9d        192.168.2.131   masternode
weave-net-pmw8w                         2/2       Running   0          9d        192.168.2.216   worknode2
</code></pre><!--
Each Node has a weave Pod, and all Pods are `Running` and `2/2 READY`. (`2/2` means that each Pod has `weave` and `weave-npc`.)
-->
<p>每个 Node 都有一个 weave Pod，所有 Pod 都是<code>Running</code> 和 <code>2/2 READY</code>。
（<code>2/2</code> 表示每个 Pod 都有 <code>weave</code> 和 <code>weave-npc</code>）</p>
<h2 id="接下来">接下来</h2>
<!--
Once you have installed the Weave Net addon, you can follow the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) to try out Kubernetes NetworkPolicy. If you have any question, contact us at [#weave-community on Slack or Weave User Group](https://github.com/weaveworks/weave#getting-help).
 -->
<p>安装 Weave Net 插件后，你可以参考
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">声明网络策略</a>
来试用 Kubernetes NetworkPolicy。
如果你有任何疑问，请通过
<a href="https://github.com/weaveworks/weave#getting-help">Slack 上的 #weave-community 频道或者 Weave 用户组</a>
联系我们。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b45f024608e1b367cdacb1fd9d77278a">2.6 - IP Masquerade Agent 用户指南</h1>
    
	<!--
title: IP Masquerade Agent User Guide
content_type: task
-->
<!-- overview -->
<!--
This page shows how to configure and enable the ip-masq-agent.
-->
<p>此页面展示如何配置和启用 ip-masq-agent。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- discussion -->
<!--
## IP Masquerade Agent User Guide
-->
<h2 id="ip-masquerade-agent-用户指南">IP Masquerade Agent 用户指南</h2>
<!--
The ip-masq-agent configures iptables rules to hide a pod's IP address behind the cluster node's IP address. This is typically done when sending traffic to destinations outside the cluster's pod [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) range.
-->
<p>ip-masq-agent 配置 iptables 规则以隐藏位于集群节点 IP 地址后面的 Pod 的 IP 地址。
这通常在将流量发送到集群的 Pod
<a href="https://zh.wikipedia.org/wiki/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1">CIDR</a>
范围之外的目的地时使用。</p>
<!--
### **Key Terms**
-->
<h3 id="关键术语"><strong>关键术语</strong></h3>
<!--
* **NAT (Network Address Translation)**
  Is a method of remapping one IP address to another by modifying either the source and/or destination address information in the IP header.  Typically performed by a device doing IP routing.
-->
<ul>
<li><strong>NAT (网络地址转译)</strong>
是一种通过修改 IP 地址头中的源和/或目标地址信息将一个 IP 地址重新映射
到另一个 IP 地址的方法。通常由执行 IP 路由的设备执行。</li>
</ul>
<!--
* **Masquerading**
  A form of NAT that is typically used to perform a many to one address translation, where multiple source IP addresses are masked behind a single address, which is typically the device doing the IP routing. In Kubernetes this is the Node's IP address.
-->    
<ul>
<li><strong>伪装</strong>
NAT 的一种形式，通常用于执行多对一地址转换，其中多个源 IP 地址被隐藏在
单个地址后面，该地址通常是执行 IP 路由的设备。在 Kubernetes 中，
这是节点的 IP 地址。</li>
</ul>
<!--
* **CIDR (Classless Inter-Domain Routing)**
  Based on the variable-length subnet masking, allows specifying arbitrary-length prefixes. CIDR introduced a new method of representation for IP addresses, now commonly known as **CIDR notation**, in which an address or routing prefix is written with a suffix indicating the number of bits of the prefix, such as 192.168.2.0/24.
-->
<ul>
<li><strong>CIDR (无类别域间路由)</strong>
基于可变长度子网掩码，允许指定任意长度的前缀。
CIDR 引入了一种新的 IP 地址表示方法，现在通常称为<strong>CIDR表示法</strong>，
其中地址或路由前缀后添加一个后缀，用来表示前缀的位数，例如 192.168.2.0/24。</li>
</ul>
<!--
* **Link Local**
  A link-local address is a network address that is valid only for communications within the network segment or the broadcast domain that the host is connected to. Link-local addresses for IPv4 are defined in the address block 169.254.0.0/16 in CIDR notation.
-->
<ul>
<li><strong>本地链路</strong>
本地链路是仅对网段或主机所连接的广播域内的通信有效的网络地址。
IPv4 的本地链路地址在 CIDR 表示法的地址块 169.254.0.0/16 中定义。</li>
</ul>
<!--
The ip-masq-agent configures iptables rules to handle masquerading node/pod IP addresses when sending traffic to destinations outside the cluster node's IP and the Cluster IP range.  This essentially hides pod IP addresses behind the cluster node's IP address.  In some environments, traffic to "external" addresses must come from a known machine address. For example, in Google Cloud, any traffic to the internet must come from a VM's IP.  When containers are used, as in Google Kubernetes Engine, the Pod IP will be rejected for egress. To avoid this, we must hide the Pod IP behind the VM's own IP address - generally known as "masquerade". By default, the agent is configured to treat the three private IP ranges specified by [RFC 1918](https://tools.ietf.org/html/rfc1918) as non-masquerade [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing).  These ranges are 10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16. The agent will also treat link-local (169.254.0.0/16) as a non-masquerade CIDR by default.  The agent is configured to reload its configuration from the location */etc/config/ip-masq-agent* every 60 seconds, which is also configurable.
-->
<p>ip-masq-agent 配置 iptables 规则，以便在将流量发送到集群节点的 IP 和集群 IP 范围之外的目标时
处理伪装节点或 Pod 的 IP 地址。这本质上隐藏了集群节点 IP 地址后面的 Pod IP 地址。
在某些环境中，去往“外部”地址的流量必须从已知的机器地址发出。
例如，在 Google Cloud 中，任何到互联网的流量都必须来自 VM 的 IP。
使用容器时，如 Google Kubernetes Engine，从 Pod IP 发出的流量将被拒绝出站。
为了避免这种情况，我们必须将 Pod IP 隐藏在 VM 自己的 IP 地址后面 - 通常称为“伪装”。
默认情况下，代理配置为将
<a href="https://tools.ietf.org/html/rfc1918">RFC 1918</a>
指定的三个私有 IP 范围视为非伪装
<a href="https://zh.wikipedia.org/wiki/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1">CIDR</a>。
这些范围是 10.0.0.0/8,172.16.0.0/12 和 192.168.0.0/16。
默认情况下，代理还将链路本地地址（169.254.0.0/16）视为非伪装 CIDR。
代理程序配置为每隔 60 秒从 <em>/etc/config/ip-masq-agent</em> 重新加载其配置，
这也是可修改的。</p>
<p><img src="/images/docs/ip-masq.png" alt="masq/non-masq example"></p>
<!--
The agent configuration file must be written in YAML or JSON syntax, and may contain three optional keys:
-->
<p>代理配置文件必须使用 YAML 或 JSON 语法编写，并且可能包含三个可选值：</p>
<!--
*   **nonMasqueradeCIDRs:** A list of strings in [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) notation that specify the non-masquerade ranges.
-->
<ul>
<li><strong>nonMasqueradeCIDRs:</strong>
<a href="https://zh.wikipedia.org/wiki/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1">CIDR</a>
表示法中的字符串列表，用于指定不需伪装的地址范围。</li>
</ul>
<!--
*   **masqLinkLocal:** A Boolean (true / false) which indicates whether to masquerade traffic to the link local prefix 169.254.0.0/16. False by default.
-->
<ul>
<li><strong>masqLinkLocal:</strong> 布尔值 (true / false)，表示是否将流量伪装到
本地链路前缀 169.254.0.0/16。默认为 false。</li>
</ul>
<!--
*   **resyncInterval:** An interval at which the agent attempts to reload config from disk. e.g. '30s' where 's' is seconds, 'ms' is milliseconds etc...
-->
<ul>
<li><strong>resyncInterval:</strong> 代理尝试从磁盘重新加载配置的时间间隔。
例如 '30s'，其中 's' 是秒，'ms' 是毫秒等...</li>
</ul>
<!--
Traffic to 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16) ranges will NOT be masqueraded. Any other traffic (assumed to be internet) will be masqueraded.  An example of a local destination from a pod could be its Node's IP address as well as another node's address or one of the IP addresses in Cluster's IP range.   Any other traffic will be masqueraded by default.  The below entries show the default set of rules that are applied by the ip-masq-agent:
-->
<p>10.0.0.0/8、172.16.0.0/12 和 192.168.0.0/16 范围内的流量不会被伪装。
任何其他流量（假设是互联网）将被伪装。
Pod 访问本地目的地的例子，可以是其节点的 IP 地址、另一节点的地址或集群的 IP 地址范围内的一个 IP 地址。
默认情况下，任何其他流量都将伪装。以下条目展示了 ip-masq-agent 的默认使用的规则：</p>
<pre tabindex="0"><code>iptables -t nat -L IP-MASQ-AGENT
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL

</code></pre><!--
By default, in GCE/Google Kubernetes Engine starting with Kubernetes version 1.7.0, if network policy is enabled or you are using a cluster CIDR not in the 10.0.0.0/8 range, the ip-masq-agent will run in your cluster.  If you are running in another environment, you can add the ip-masq-agent [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) to your cluster:
-->
<p>默认情况下，从 Kubernetes 1.7.0 版本开始的 GCE/Google Kubernetes Engine 中，
如果启用了网络策略，或者你使用的集群 CIDR 不在 10.0.0.0/8 范围内，
则 ip-masq-agent 将在你的集群中运行。
如果你在其他环境中运行，则可以将 ip-masq-agent
<a href="/zh/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a> 添加到你的集群：</p>
<!-- steps -->
<!--
## Create an ip-masq-agent
To create an ip-masq-agent, run the following kubectl command:
-->
<h2 id="创建-ip-masq-agent">创建 ip-masq-agent</h2>
<p>通过运行以下 kubectl 指令创建 ip-masq-agent:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre></div><!--
You must also apply the appropriate node label to any nodes in your cluster that you want the agent to run on.
-->
<p>你必须同时将适当的节点标签应用于集群中希望代理运行的任何节点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label nodes my-node beta.kubernetes.io/masq-agent-ds-ready<span style="color:#666">=</span><span style="color:#a2f">true</span>
</code></pre></div><!--
More information can be found in the ip-masq-agent documentation [here](https://github.com/kubernetes-sigs/ip-masq-agent)
-->
<p>更多信息可以通过 ip-masq-agent 文档 <a href="https://github.com/kubernetes-sigs/ip-masq-agent">这里</a> 找到。</p>
<!--
In most cases, the default set of rules should be sufficient; however, if this is not the case for your cluster, you can create and apply a [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/) to customize the IP ranges that are affected.  For example, to allow only 10.0.0.0/8 to be considered by the ip-masq-agent, you can create the following [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/) in a file called "config".
-->
<p>在大多数情况下，默认的规则集应该足够；但是，如果你的群集不是这种情况，则可以创建并应用
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>
来自定义受影响的 IP 范围。
例如，要允许 ip-masq-agent 仅作用于 10.0.0.0/8，你可以在一个名为 “config” 的文件中创建以下
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a> 。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
It is important that the file is called config since, by default, that will be used as the key for lookup by the ip-masq-agent:
-->
<p>重要的是，该文件之所以被称为 config，因为默认情况下，该文件将被用作
ip-masq-agent 查找的主键：</p>
<pre tabindex="0"><code>nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
</code></pre></div>
</blockquote>
<!--
Run the following command to add the config map to your cluster:
-->
<p>运行以下命令将配置映射添加到你的集群：</p>
<pre tabindex="0"><code>kubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system
</code></pre><!--
This will update a file located at */etc/config/ip-masq-agent* which is periodically checked every *resyncInterval* and applied to the cluster node.
After the resync interval has expired, you should see the iptables rules reflect your changes:
-->
<p>这将更新位于 <em>/etc/config/ip-masq-agent</em> 的一个文件，该文件以 <em>resyncInterval</em>
为周期定期检查并应用于集群节点。
重新同步间隔到期后，你应该看到你的更改在 iptables 规则中体现：</p>
<pre tabindex="0"><code>iptables -t nat -L IP-MASQ-AGENT
Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre><!--
By default, the link local range (169.254.0.0/16) is also handled by the ip-masq agent, which sets up the appropriate iptables rules.  To have the ip-masq-agent ignore link local, you can set *masqLinkLocal*  to true in the config map.
-->
<p>默认情况下，本地链路范围 (169.254.0.0/16) 也由 ip-masq agent 处理，
该代理设置适当的 iptables 规则。 要使 ip-masq-agent 忽略本地链路，
可以在配置映射中将 <em>masqLinkLocal</em> 设置为 true。</p>
<pre tabindex="0"><code>nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
masqLinkLocal: true
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ce4cd28c8feb9faa783e79b48af37961">2.7 - Kubernetes 云管理控制器</h1>
    
	<!--
reviewers:
- luxas
- thockin
- wlan0
title: Kubernetes Cloud Controller Manager
content_type: concept
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>

<!--
Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the `<a class='glossary-tooltip' title='将 Kubernetes 与第三方云提供商进行集成的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/cloud-controller/' target='_blank' aria-label='cloud-controller-manager'>cloud-controller-manager</a>` binary allows cloud vendors to evolve independently from the core Kubernetes code.
-->
<p>由于云驱动的开发和发布的步调与 Kubernetes 项目不同，将服务提供商专用代码抽象到
<code><a class='glossary-tooltip' title='将 Kubernetes 与第三方云提供商进行集成的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/cloud-controller/' target='_blank' aria-label='cloud-controller-manager'>cloud-controller-manager</a></code>
二进制中有助于云服务厂商在 Kubernetes 核心代码之外独立进行开发。</p>
<!--
The `cloud-controller-manager` can be linked to any cloud provider that satisfies [cloudprovider.Interface](https://github.com/kubernetes/cloud-provider/blob/master/cloud.go). For backwards compatibility, the [cloud-controller-manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager) provided in the core Kubernetes project uses the same cloud libraries as `kube-controller-manager`. Cloud providers already supported in Kubernetes core are expected to use the in-tree cloud-controller-manager to transition out of Kubernetes core.
-->
<p><code>cloud-controller-manager</code> 可以被链接到任何满足
<a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go">cloudprovider.Interface</a>
约束的云服务提供商。为了兼容旧版本，Kubernetes 核心项目中提供的
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">cloud-controller-manager</a>
使用和 <code>kube-controller-manager</code> 相同的云服务类库。
已经在 Kubernetes 核心项目中支持的云服务提供商预计将通过使用 in-tree 的 cloud-controller-manager
过渡为非 Kubernetes 核心代码。</p>
<!-- body -->
<!--
## Administration

### Requirements

Every cloud has their own set of requirements for running their own cloud provider integration, it should not be too different from the requirements when running `kube-controller-manager`. As a general rule of thumb you'll need:

* cloud authentication/authorization: your cloud may require a token or IAM rules to allow access to their APIs
* kubernetes authentication/authorization: cloud-controller-manager may need RBAC rules set to speak to the kubernetes apiserver
* high availability: like kube-controller-manager, you may want a high available setup for cloud controller manager using leader election (on by default).
-->
<h2 id="管理">管理</h2>
<h3 id="需求">需求</h3>
<p>每个云服务都有一套各自的需求用于系统平台的集成，这不应与运行
<code>kube-controller-manager</code> 的需求有太大差异。作为经验法则，你需要：</p>
<ul>
<li>云服务认证/授权：你的云服务可能需要使用令牌或者 IAM 规则以允许对其 API 的访问</li>
<li>kubernetes 认证/授权：cloud-controller-manager 可能需要 RBAC 规则以访问 kubernetes apiserver</li>
<li>高可用：类似于 kube-controller-manager，你可能希望通过主节点选举（默认开启）配置一个高可用的云管理控制器。</li>
</ul>
<!--
### Running cloud-controller-manager

Successfully running cloud-controller-manager requires some changes to your cluster configuration.
-->
<h3 id="运行云管理控制器">运行云管理控制器</h3>
<p>你需要对集群配置做适当的修改以成功地运行云管理控制器：</p>
<!--
* `kube-apiserver` and `kube-controller-manager` MUST NOT specify the `--cloud-provider` flag. This ensures that it does not run any cloud specific loops that would be run by cloud controller manager. In the future, this flag will be deprecated and removed.
* `kubelet` must run with `--cloud-provider=external`. This is to ensure that the kubelet is aware that it must be initialized by the cloud controller manager before it is scheduled any work.
-->
<ul>
<li>一定不要为 <code>kube-apiserver</code> 和 <code>kube-controller-manager</code> 指定 <code>--cloud-provider</code> 标志。
这将保证它们不会运行任何云服务专用循环逻辑，这将会由云管理控制器运行。未来这个标记将被废弃并去除。</li>
<li><code>kubelet</code> 必须使用 <code>--cloud-provider=external</code> 运行。
这是为了保证让 kubelet 知道在执行任何任务前，它必须被云管理控制器初始化。</li>
</ul>
<!--
Keep in mind that setting up your cluster to use cloud controller manager will change your cluster behaviour in a few ways:
-->
<p>请记住，设置群集使用云管理控制器将用多种方式更改群集行为：</p>
<!--
* kubelets specifying `--cloud-provider=external` will add a taint `node.cloudprovider.kubernetes.io/uninitialized` with an effect `NoSchedule` during initialization. This marks the node as needing a second initialization from an external controller before it can be scheduled work. Note that in the event that cloud controller manager is not available, new nodes in the cluster will be left unschedulable. The taint is important since the scheduler may require cloud specific information about nodes such as their region or type (high cpu, gpu, high memory, spot instance, etc).
-->
<ul>
<li>指定了 <code>--cloud-provider=external</code> 的 kubelet 将被添加一个 <code>node.cloudprovider.kubernetes.io/uninitialized</code>
的污点，导致其在初始化过程中不可调度（<code>NoSchedule</code>）。
这将标记该节点在能够正常调度前，需要外部的控制器进行二次初始化。
请注意，如果云管理控制器不可用，集群中的新节点会一直处于不可调度的状态。
这个污点很重要，因为调度器可能需要关于节点的云服务特定的信息，比如他们的区域或类型
（高端 CPU、GPU 支持、内存较大、临时实例等）。</li>
</ul>
<!--
* cloud information about nodes in the cluster will no longer be retrieved using local metadata, but instead all API calls to retrieve node information will go through cloud controller manager. This may mean you can restrict access to your cloud API on the kubelets for better security. For larger clusters you may want to consider if cloud controller manager will hit rate limits since it is now responsible for almost all API calls to your cloud from within the cluster.
-->
<ul>
<li>集群中节点的云服务信息将不再能够从本地元数据中获取，取而代之的是所有获取节点信息的
API 调用都将通过云管理控制器。这意味着你可以通过限制到 kubelet 云服务 API 的访问来提升安全性。
在更大的集群中你可能需要考虑云管理控制器是否会遇到速率限制，
因为它现在负责集群中几乎所有到云服务的 API 调用。</li>
</ul>
<!--
Cloud controller manager can implement:

* node controller - responsible for updating kubernetes nodes using cloud APIs and deleting kubernetes nodes that were deleted on your cloud.
* service controller - responsible for loadbalancers on your cloud against services of type LoadBalancer.
* route controller - responsible for setting up network routes on your cloud
* any other features you would like to implement if you are running an out-of-tree provider.
-->
<p>云管理控制器可以实现：</p>
<ul>
<li>节点控制器 - 负责使用云服务 API 更新 kubernetes 节点并删除在云服务上已经删除的 kubernetes 节点。</li>
<li>服务控制器 - 负责在云服务上为类型为 LoadBalancer 的 service 提供负载均衡器。</li>
<li>路由控制器 - 负责在云服务上配置网络路由。</li>
<li>如果你使用的是 out-of-tree 提供商，请按需实现其余任意特性。</li>
</ul>
<!--
## Examples

If you are using a cloud that is currently supported in Kubernetes core and would like to adopt cloud controller manager, see the [cloud controller manager in kubernetes core](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager).

For cloud controller managers not in Kubernetes core, you can find the respective projects in repos maintained by cloud vendors or sig leads.
-->
<h2 id="示例">示例</h2>
<p>如果当前 Kubernetes 内核支持你使用的云服务，并且想要采用云管理控制器，请参见
<a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager">kubernetes 内核中的云管理控制器</a>。</p>
<p>对于不在 Kubernetes 核心代码库中的云管理控制器，你可以在云服务厂商或 SIG 领导者的源中找到对应的项目。</p>
<ul>
<li><a href="https://github.com/digitalocean/digitalocean-cloud-controller-manager">DigitalOcean</a></li>
<li><a href="https://github.com/munnerz/keepalived-cloud-provider">keepalived</a></li>
<li><a href="https://github.com/oracle/oci-cloud-controller-manager">Oracle Cloud Infrastructure</a></li>
<li><a href="https://github.com/rancher/rancher-cloud-controller-manager">Rancher</a></li>
</ul>
<!--
For providers already in Kubernetes core, you can run the in-tree cloud controller manager as a Daemonset in your cluster, use the following as a guideline:
-->
<p>对于已经存在于 Kubernetes 内核中的提供商，你可以在集群中将 in-tree 云管理控制器作为守护进程运行。请使用如下指南：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/cloud/ccm-example.yaml" download="admin/cloud/ccm-example.yaml"><code>admin/cloud/ccm-example.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-cloud-ccm-example-yaml')" title="Copy admin/cloud/ccm-example.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-cloud-ccm-example-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># This is an example of how to setup cloud-controller-manager as a Daemonset in your cluster.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># It assumes that your masters can run pods and has the role node-role.kubernetes.io/master</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># Note that this Daemonset will not work straight out of the box for your cloud, this is</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># meant to be a guideline.</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">roleRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cluster-admin<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># for in-tree providers we use k8s.gcr.io/cloud-controller-manager</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># this can be replaced with any other image for out-of-tree providers</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/cloud-controller-manager:v1.8.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- /usr/local/bin/cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --cloud-provider=[YOUR_CLOUD_PROVIDER] <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Add your own cloud provider here!</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --leader-elect=true<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --use-service-account-credentials<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># these flags will vary for every cloud provider</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --allocate-node-cidrs=true<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --configure-cloud-routes=true<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --cluster-cidr=172.17.0.0/16<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is required so CCM can bootstrap itself</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node.cloudprovider.kubernetes.io/uninitialized<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is to have the daemonset runnable on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># the taint may vary depending on your cluster setup</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this is to restrict CCM to only run on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># the node selector may vary depending on your cluster setup</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">node-role.kubernetes.io/master</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
## Limitations

Running cloud controller manager comes with a few possible limitations. Although these limitations are being addressed in upcoming releases, it's important that you are aware of these limitations for production workloads.
-->
<h2 id="限制">限制</h2>
<p>运行云管理控制器会有一些可能的限制。虽然以后的版本将处理这些限制，但是知道这些生产负载的限制很重要。</p>
<!--
### Support for Volumes

Cloud controller manager does not implement any of the volume controllers found in `kube-controller-manager` as the volume integrations also require coordination with kubelets. As we evolve CSI (container storage interface) and add stronger support for flex volume plugins, necessary support will be added to cloud controller manager so that clouds can fully integrate with volumes. Learn more about out-of-tree CSI volume plugins [here](https://github.com/kubernetes/features/issues/178).
-->
<h3 id="对-volume-的支持">对 Volume 的支持</h3>
<p>云管理控制器未实现 <code>kube-controller-manager</code> 中的任何 volume 控制器，因为和 volume 的集成还需要与 kubelet 协作。由于我们引入了 CSI (容器存储接口，container storage interface) 并对弹性 volume 插件添加了更强大的支持，云管理控制器将添加必要的支持，以使云服务同 volume 更好的集成。请在 <a href="https://github.com/kubernetes/features/issues/178">这里</a> 了解更多关于 out-of-tree CSI volume 插件的信息。</p>
<!--
### Scalability

In the previous architecture for cloud providers, we relied on kubelets using a local metadata service to retrieve node information about itself. With this new architecture, we now fully rely on the cloud controller managers to retrieve information for all nodes. For very larger clusters, you should consider possible bottle necks such as resource requirements and API rate limiting.
-->
<h3 id="可扩展性">可扩展性</h3>
<p>在以前为云服务提供商提供的架构中，我们依赖 kubelet 的本地元数据服务来获取关于它本身的节点信息。通过这个新的架构，现在我们完全依赖云管理控制器来获取所有节点的信息。对于非常大的集群，你需要考虑可能的瓶颈，例如资源需求和 API 速率限制。</p>
<!--
### Chicken and Egg

The goal of the cloud controller manager project is to decouple development of cloud features from the core Kubernetes project. Unfortunately, many aspects of the Kubernetes project has assumptions that cloud provider features are tightly integrated into the project. As a result, adopting this new architecture can create several situations where a request is being made for information from a cloud provider, but the cloud controller manager may not be able to return that information without the original request being complete.
-->
<h3 id="鸡和蛋的问题">鸡和蛋的问题</h3>
<p>云管理控制器的目标是将云服务特性的开发从 Kubernetes 核心项目中解耦。
不幸的是，Kubernetes 项目的许多方面都假设云服务提供商的特性同项目紧密结合。
因此，这种新架构的采用可能导致某些场景下，当一个请求需要从云服务提供商获取信息时，
在该请求没有完成的情况下云管理控制器不能返回那些信息。</p>
<!--
A good example of this is the TLS bootstrapping feature in the Kubelet. Currently, TLS bootstrapping assumes that the Kubelet has the ability to ask the cloud provider (or a local metadata service) for all its address types (private, public, etc) but cloud controller manager cannot set a node's address types without being initialized in the first place which requires that the kubelet has TLS certificates to communicate with the apiserver.

As this initiative evolves, changes will be made to address these issues in upcoming releases.
-->
<p>Kubelet 中的 TLS 引导特性是一个很好的例子。
目前，TLS 引导认为 kubelet 有能力从云提供商（或本地元数据服务）获取所有的地址类型（私有、公用等），
但在被初始化之前，云管理控制器不能设置节点地址类型，而这需要 kubelet 拥有
TLS 证书以和 API 服务器通信。</p>
<p>随着整个动议的演进，将来的发行版中将作出改变来解决这些问题。</p>
<h2 id="接下来">接下来</h2>
<!--
To build and develop your own cloud controller manager, read the [Developing Cloud Controller Manager](/docs/tasks/administer-cluster/developing-cloud-controller-manager.md) doc.
-->
<p>要构建和开发你自己的云管理控制器，请阅读
<a href="/zh/docs/tasks/administer-cluster/developing-cloud-controller-manager/">开发云管理控制器</a>
文档。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c4d0832845adc92b7ccd54aed63fc932">2.8 - 为 Kubernetes 运行 etcd 集群</h1>
    
	<!--
---
reviewers:
- mml
- wojtek-t
title: Operating etcd clusters for Kubernetes
content_type: task
---
-->
<!-- overview -->
<!--
---
title: etcd
id: etcd
date: 2018-04-12
full_link: /docs/tasks/administer-cluster/configure-upgrade-etcd/
short_description: >
  Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.

aka: 
tags:
- architecture
- storage
---
-->
<!--
 Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.
-->
<p>etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。</p>
<!--
If your Kubernetes cluster uses etcd as its backing store, make sure you have a
[back up](/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) plan
for those data.
-->	
<p>您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。</p>
<!--
You can find in-depth information about etcd in the official [documentation](https://etcd.io/docs/).
-->
<p>要了解 etcd 更深层次的信息，请参考 <a href="https://etcd.io/docs/">etcd 文档</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Prerequisites

* Run etcd as a cluster of odd members.

* etcd is a leader-based distributed system. Ensure that the leader
  periodically send heartbeats on time to all followers to keep the cluster
  stable.

* Ensure that no resource starvation occurs.

  Performance and stability of the cluster is sensitive to network and disk
  I/O. Any resource starvation can lead to heartbeat timeout, causing instability
  of the cluster. An unstable etcd indicates that no leader is elected. Under
  such circumstances, a cluster cannot make any changes to its current state,
  which implies no new pods can be scheduled.

* Keeping etcd clusters stable is critical to the stability of Kubernetes
  clusters. Therefore, run etcd clusters on dedicated machines or isolated
  environments for [guaranteed resource requirements](https://etcd.io/docs/current/op-guide/hardware/).

* The minimum recommended version of etcd to run in production is `3.2.10+`.
-->
<h2 id="先决条件">先决条件</h2>
<ul>
<li>
<p>运行的 etcd 集群个数成员为奇数。</p>
</li>
<li>
<p>etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。</p>
</li>
<li>
<p>确保不发生资源不足。</p>
<p>集群的性能和稳定性对网络和磁盘 I/O 非常敏感。任何资源匮乏都会导致心跳超时，
从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。
在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 pod。</p>
</li>
<li>
<p>保持 etcd 集群的稳定对 Kubernetes 集群的稳定性至关重要。
因此，请在专用机器或隔离环境上运行 etcd 集群，以满足
<a href="https://etcd.io/docs/current/op-guide/hardware/">所需资源需求</a>。</p>
</li>
<li>
<p>在生产中运行的 etcd 的最低推荐版本是 <code>3.2.10+</code>。</p>
</li>
</ul>
<!--
## Resource requirements

Operating etcd with limited resources is suitable only for testing purposes.
For deploying in production, advanced hardware configuration is required.
Before deploying etcd in production, see
[resource requirement reference](https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations).

## Starting etcd clusters

This section covers starting a single-node and multi-node etcd cluster. 
-->
<h2 id="资源要求">资源要求</h2>
<p>使用有限的资源运行 etcd 只适合测试目的。为了在生产中部署，需要先进的硬件配置。
在生产中部署 etcd 之前，请查看
<a href="https://etcd.io/docs/current/op-guide/hardware/#example-hardware-configurations">所需资源参考文档</a>。</p>
<h2 id="启动-etcd-集群">启动 etcd 集群</h2>
<p>本节介绍如何启动单节点和多节点 etcd 集群。</p>
<!--
### Single-node etcd cluster

Use a single-node etcd cluster only for testing purpose.

1. Run the following:

   ```sh
   etcd --listen-client-urls=http://$PRIVATE_IP:2379 \
      --advertise-client-urls=http://$PRIVATE_IP:2379
   ```

2. Start the Kubernetes API server with the flag
   `--etcd-servers=$PRIVATE_IP:2379`.

    Make sure `PRIVATE_IP` is set to your etcd client IP.
-->
<h3 id="单节点-etcd-集群">单节点 etcd 集群</h3>
<p>只为测试目的使用单节点 etcd 集群。</p>
<ol>
<li>
<p>运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379 <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>   --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$PRIVATE_IP</span>:2379
</code></pre></div></li>
<li>
<p>使用参数 <code>--etcd-servers=$PRIVATE_IP:2379</code> 启动 Kubernetes API 服务器。</p>
<p>确保将 <code>PRIVATE_IP</code> 设置为etcd客户端 IP。</p>
</li>
</ol>
<!--
### Multi-node etcd cluster

For durability and high availability, run etcd as a multi-node cluster in
production and back it up periodically. A five-member cluster is recommended
in production. For more information, see
[FAQ documentation](https://etcd.io/docs/current/faq/#what-is-failure-tolerance).

Configure an etcd cluster either by static member information or by dynamic
discovery. For more information on clustering, see
[etcd clustering documentation](https://etcd.io/docs/current/op-guide/clustering/).

For an example, consider a five-member etcd cluster running with the following
client URLs: `http://$IP1:2379`, `http://$IP2:2379`, `http://$IP3:2379`,
`http://$IP4:2379`, and `http://$IP5:2379`. To start a Kubernetes API server:

1. Run the following:

   ```shell
   etcd --listen-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379 --advertise-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379
   ```

2. Start the Kubernetes API servers with the flag
   `--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379`.

   Make sure the `IP<n>` variables are set to your client IP addresses.
-->
<h3 id="多节点-etcd-集群">多节点 etcd 集群</h3>
<p>为了耐用性和高可用性，在生产中将以多节点集群的方式运行 etcd，并且定期备份。
建议在生产中使用五个成员的集群。
有关该内容的更多信息，请参阅
<a href="https://etcd.io/docs/current/faq/#what-is-failure-tolerance">常见问题文档</a>。</p>
<p>可以通过静态成员信息或动态发现的方式配置 etcd 集群。
有关集群的详细信息，请参阅
<a href="https://etcd.io/docs/current/op-guide/clustering/">etcd 集群文档</a>。</p>
<p>例如，考虑运行以下客户端 URL 的五个成员的 etcd 集群：<code>http://$IP1:2379</code>，
<code>http://$IP2:2379</code>，<code>http://$IP3:2379</code>，<code>http://$IP4:2379</code> 和 <code>http://$IP5:2379</code>。
要启动 Kubernetes API 服务器：</p>
<ol>
<li>
<p>运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcd --listen-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379 --advertise-client-urls<span style="color:#666">=</span>http://<span style="color:#b8860b">$IP1</span>:2379,http://<span style="color:#b8860b">$IP2</span>:2379,http://<span style="color:#b8860b">$IP3</span>:2379,http://<span style="color:#b8860b">$IP4</span>:2379,http://<span style="color:#b8860b">$IP5</span>:2379
</code></pre></div></li>
<li>
<p>使用参数 <code>--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379</code>
启动 Kubernetes API 服务器。</p>
<p>确保将 <code>IP&lt;n&gt;</code> 变量设置为客户端 IP 地址。</p>
</li>
</ol>
<!--
### Multi-node etcd cluster with load balancer

To run a load balancing etcd cluster:

1. Set up an etcd cluster.
2. Configure a load balancer in front of the etcd cluster.
   For example, let the address of the load balancer be `$LB`.
3. Start Kubernetes API Servers with the flag `--etcd-servers=$LB:2379`.
-->
<h3 id="使用负载均衡的多节点-etcd-集群">使用负载均衡的多节点 etcd 集群</h3>
<p>要运行负载均衡的 etcd 集群：</p>
<ol>
<li>建立一个 etcd 集群。</li>
<li>在 etcd 集群前面配置负载均衡器。例如，让负载均衡器的地址为 <code>$LB</code>。</li>
<li>使用参数 <code>--etcd-servers=$LB:2379</code> 启动 Kubernetes API 服务器。</li>
</ol>
<!--
## Securing etcd clusters

Access to etcd is equivalent to root permission in the cluster so ideally only
the API server should have access to it. Considering the sensitivity of the
data, it is recommended to grant permission to only those nodes that require
access to etcd clusters.

To secure etcd, either set up firewall rules or use the security features
provided by etcd. etcd security features depend on x509 Public Key
Infrastructure (PKI). To begin, establish secure communication channels by
generating a key and certificate pair. For example, use key pairs `peer.key`
and `peer.cert` for securing communication between etcd members, and
`client.key` and `client.cert` for securing communication between etcd and its
clients. See the [example scripts](https://github.com/coreos/etcd/tree/master/hack/tls-setup)
provided by the etcd project to generate key pairs and CA files for client
authentication.
-->
<h2 id="安全的-etcd-集群">安全的 etcd 集群</h2>
<p>对 etcd 的访问相当于集群中的 root 权限，因此理想情况下只有 API 服务器才能访问它。
考虑到数据的敏感性，建议只向需要访问 etcd 集群的节点授予权限。</p>
<p>想要确保 etcd 的安全，可以设置防火墙规则或使用 etcd 提供的安全特性，这些安全特性依赖于 x509 公钥基础设施（PKI）。
首先，通过生成密钥和证书对来建立安全的通信通道。
例如，使用密钥对 <code>peer.key</code> 和 <code>peer.cert</code> 来保护 etcd 成员之间的通信，
而 <code>client.key</code> 和 <code>client.cert</code> 用于保护 etcd 与其客户端之间的通信。
请参阅 etcd 项目提供的<a href="https://github.com/coreos/etcd/tree/master/hack/tls-setup">示例脚本</a>，
以生成用于客户端身份验证的密钥对和 CA 文件。</p>
<!--
### Securing communication

To configure etcd with secure peer communication, specify flags
`--peer-key-file=peer.key` and `--peer-cert-file=peer.cert`, and use HTTPS as
the URL schema.

Similarly, to configure etcd with secure client communication, specify flags
`--key-file=k8sclient.key` and `--cert-file=k8sclient.cert`, and use HTTPS as
the URL schema. Here is an example on a client command that uses secure
communication:

```
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
```
-->
<h3 id="安全通信">安全通信</h3>
<p>若要使用安全对等通信对 etcd 进行配置，请指定参数 <code>--peer-key-file=peer.key</code>
和 <code>--peer-cert-file=peer.cert</code>，并使用 HTTPS 作为 URL 模式。</p>
<p>类似地，要使用安全客户端通信对 etcd 进行配置，请指定参数 <code>--key-file=k8sclient.key</code>
和 <code>--cert-file=k8sclient.cert</code>，并使用 HTTPS 作为 URL 模式。
使用安全通信的客户端命令的示例：</p>
<pre tabindex="0"><code>ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  member list
</code></pre><!--
### Limiting access of etcd clusters

After configuring secure communication, restrict the access of etcd cluster to
only the Kubernetes API servers. Use TLS authentication to do so.

For example, consider key pairs `k8sclient.key` and `k8sclient.cert` that are
trusted by the CA `etcd.ca`. When etcd is configured with `--client-cert-auth`
along with TLS, it verifies the certificates from clients by using system CAs
or the CA passed in by `--trusted-ca-file` flag. Specifying flags
`--client-cert-auth=true` and `--trusted-ca-file=etcd.ca` will restrict the
access to clients with the certificate `k8sclient.cert`.

Once etcd is configured correctly, only clients with valid certificates can
access it. To give Kubernetes API servers the access, configure them with the
flags `--etcd-certfile=k8sclient.cert`,`--etcd-keyfile=k8sclient.key` and
`--etcd-cafile=ca.cert`.

<blockquote class="note callout">
  <div><strong>说明：</strong> etcd authentication is not currently supported by Kubernetes. For more
information, see the related issue
<a href="https://github.com/kubernetes/kubernetes/issues/23398">Support Basic Auth for Etcd v2</a>.</div>
</blockquote>
-->
<h3 id="限制-etcd-集群的访问">限制 etcd 集群的访问</h3>
<p>配置安全通信后，将 etcd 集群的访问限制在 Kubernetes API 服务器上。使用 TLS 身份验证来完成此任务。</p>
<p>例如，考虑由 CA <code>etcd.ca</code> 信任的密钥对 <code>k8sclient.key</code> 和 <code>k8sclient.cert</code>。
当 etcd 配置为 <code>--client-cert-auth</code> 和 TLS 时，它使用系统 CA 或由 <code>--trusted-ca-file</code> 参数传入的 CA 验证来自客户端的证书。
指定参数 <code>--client-cert-auth=true</code> 和 <code>--trusted-ca-file=etcd.ca</code> 将限制对具有证书 <code>k8sclient.cert</code> 的客户端的访问。</p>
<p>一旦正确配置了 etcd，只有具有有效证书的客户端才能访问它。要让 Kubernetes API 服务器访问，
可以使用参数 <code>--etcd-certfile=k8sclient.cert</code>,<code>--etcd-keyfile=k8sclient.key</code> 和 <code>--etcd-cafile=ca.cert</code> 配置。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> Kubernetes 目前不支持 etcd 身份验证。
想要了解更多信息，请参阅相关的问题
<a href="https://github.com/kubernetes/kubernetes/issues/23398">支持 etcd v2 的基本认证</a>。</div>
</blockquote>
<!--
## Replacing a failed etcd member

etcd cluster achieves high availability by tolerating minor member failures.
However, to improve the overall health of the cluster, replace failed members
immediately. When multiple members fail, replace them one by one. Replacing a
failed member involves two steps: removing the failed member and adding a new
member.

Though etcd keeps unique member IDs internally, it is recommended to use a
unique name for each member to avoid human errors. For example, consider a
three-member etcd cluster. Let the URLs be, `member1=http://10.0.0.1`,
`member2=http://10.0.0.2`, and `member3=http://10.0.0.3`. When `member1` fails,
replace it with `member4=http://10.0.0.4`.

1. Get the member ID of the failed `member1`:

   ```shell
   etcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list
   ```

   The following message is displayed:

   ```console
   8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
   91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
   fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
   ```

2. Remove the failed member:

   ```shell
   etcdctl member remove 8211f1d0f64f3269
   ```

   The following message is displayed:

   ```console
   Removed member 8211f1d0f64f3269 from cluster
   ```

3. Add the new member:

   ```shell
   etcdctl member add member4 --peer-urls=http://10.0.0.4:2380
   ```

   The following message is displayed:

   ```console
   Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
   ```

4. Start the newly added member on a machine with the IP `10.0.0.4`:

   ```shell
   export ETCD_NAME="member4"
   export ETCD_INITIAL_CLUSTER="member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380"
   export ETCD_INITIAL_CLUSTER_STATE=existing
   etcd [flags]
   ```

5. Do either of the following:

   1. Update the `--etcd-servers` flag for the Kubernetes API servers to make
      Kubernetes aware of the configuration changes, then restart the
      Kubernetes API servers.
   2. Update the load balancer configuration if a load balancer is used in the
      deployment.

For more information on cluster reconfiguration, see
[etcd reconfiguration documentation](https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member).
-->
<h2 id="替换失败的-etcd-成员">替换失败的 etcd 成员</h2>
<p>etcd 集群通过容忍少数成员故障实现高可用性。
但是，要改善集群的整体健康状况，请立即替换失败的成员。当多个成员失败时，逐个替换它们。
替换失败成员需要两个步骤：删除失败成员和添加新成员。</p>
<p>虽然 etcd 在内部保留唯一的成员 ID，但建议为每个成员使用唯一的名称，以避免人为错误。
例如，考虑一个三成员的 etcd 集群。让 URL 为：<code>member1=http://10.0.0.1</code>， <code>member2=http://10.0.0.2</code>
和 <code>member3=http://10.0.0.3</code>。当 <code>member1</code> 失败时，将其替换为 <code>member4=http://10.0.0.4</code>。</p>
<ol>
<li>
<p>获取失败的 <code>member1</code> 的成员 ID：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcdctl --endpoints<span style="color:#666">=</span>http://10.0.0.2,http://10.0.0.3 member list
</code></pre></div><p>显示以下信息：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
</code></pre></li>
<li>
<p>移除失败的成员</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcdctl member remove 8211f1d0f64f3269
</code></pre></div><p>显示以下信息：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">Removed member 8211f1d0f64f3269 from cluster
</code></pre></li>
<li>
<p>增加新成员：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">etcdctl member add member4 --peer-urls<span style="color:#666">=</span>http://10.0.0.4:2380
</code></pre></div><p>显示以下信息：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
</code></pre></li>
<li>
<p>在 IP 为 <code>10.0.0.4</code> 的机器上启动新增加的成员：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_NAME</span><span style="color:#666">=</span><span style="color:#b44">&#34;member4&#34;</span>
<span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER</span><span style="color:#666">=</span><span style="color:#b44">&#34;member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380&#34;</span>
<span style="color:#a2f">export</span> <span style="color:#b8860b">ETCD_INITIAL_CLUSTER_STATE</span><span style="color:#666">=</span>existing
etcd <span style="color:#666">[</span>flags<span style="color:#666">]</span>
</code></pre></div></li>
<li>
<p>做以下事情之一：</p>
<ol>
<li>更新 Kubernetes API 服务器的 <code>--etcd-servers</code> 参数，使 Kubernetes 知道配置进行了更改，然后重新启动 Kubernetes API 服务器。</li>
<li>如果在 deployment 中使用了负载均衡，更新负载均衡配置。</li>
</ol>
</li>
</ol>
<p>有关集群重新配置的详细信息，请参阅 <a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd 重构文档</a>。</p>
<!--
## Backing up an etcd cluster

All Kubernetes objects are stored on etcd. Periodically backing up the etcd
cluster data is important to recover Kubernetes clusters under disaster
scenarios, such as losing all control plane nodes. The snapshot file contains
all the Kubernetes states and critical information. In order to keep the
sensitive Kubernetes data safe, encrypt the snapshot files.

Backing up an etcd cluster can be accomplished in two ways: etcd built-in
snapshot and volume snapshot.
-->
<h2 id="备份-etcd-集群">备份 etcd 集群</h2>
<p>所有 Kubernetes 对象都存储在 etcd 上。定期备份 etcd 集群数据对于在灾难场景（例如丢失所有控制平面节点）下恢复 Kubernetes 集群非常重要。
快照文件包含所有 Kubernetes 状态和关键信息。为了保证敏感的 Kubernetes 数据的安全，可以对快照文件进行加密。</p>
<p>备份 etcd 集群可以通过两种方式完成：etcd 内置快照和卷快照。</p>
<!--
### Built-in snapshot

etcd supports built-in snapshot. A snapshot may either be taken from a live
member with the `etcdctl snapshot save` command or by copying the
`member/snap/db` file from an etcd
[data directory](https://etcd.io/docs/current/op-guide/configuration/#--data-dir)
that is not currently used by an etcd process. Taking the snapshot will
not affect the performance of the member.

Below is an example for taking a snapshot of the keyspace served by
`$ENDPOINT` to the file `snapshotdb`:

```shell
ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb
```

Verify the snapshot:

```shell
ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb
```

```console
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+
```
-->
<h3 id="内置快照">内置快照</h3>
<p>etcd 支持内置快照。快照可以从使用 <code>etcdctl snapshot save</code> 命令的活动成员中获取，
也可以通过从 etcd <a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">数据目录</a>
复制 <code>member/snap/db</code> 文件，该 etcd 数据目录目前没有被 etcd 进程使用。获取快照不会影响成员的性能。</p>
<p>下面是一个示例，用于获取 <code>$ENDPOINT</code> 所提供的键空间的快照到文件 <code>snapshotdb</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints <span style="color:#b8860b">$ENDPOINT</span> snapshot save snapshotdb
</code></pre></div><p>验证快照:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --write-out<span style="color:#666">=</span>table snapshot status snapshotdb
</code></pre></div><pre tabindex="0"><code class="language-console" data-lang="console">+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+
</code></pre><!--
### Volume snapshot

If etcd is running on a storage volume that supports backup, such as Amazon
Elastic Block Store, back up etcd data by taking a snapshot of the storage
volume.

### Snapshot using etcdctl options

We can also take the snapshot using various options given by etcdctl. For example 

```shell
ETCDCTL_API=3 etcdctl -h 
``` 

will list various options available from etcdctl. For example, you can take a snapshot by specifying
the endpoint, certificates etc as shown below:

```shell
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>
```
where `trusted-ca-file`, `cert-file` and `key-file` can be obtained from the description of the etcd Pod.

## Scaling up etcd clusters

Scaling up etcd clusters increases availability by trading off performance.
Scaling does not increase cluster performance nor capability. A general rule
is not to scale up or down etcd clusters. Do not configure any auto scaling
groups for etcd clusters. It is highly recommended to always run a static
five-member etcd cluster for production Kubernetes clusters at any officially
supported scale.

A reasonable scaling is to upgrade a three-member cluster to a five-member
one, when more reliability is desired. See
[etcd reconfiguration documentation](https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member)
for information on how to add members into an existing cluster.
-->
<h3 id="卷快照">卷快照</h3>
<p>如果 etcd 运行在支持备份的存储卷（如 Amazon Elastic Block 存储）上，则可以通过获取存储卷的快照来备份 etcd 数据。</p>
<h3 id="使用-etcdctl-选项的快照">使用 etcdctl 选项的快照</h3>
<p>我们还可以使用 etcdctl 提供的各种选项来拍摄快照。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl -h 
</code></pre></div><p>列出 etcdctl 可用的各种选项。例如，你可以通过指定端点，证书等来拍摄快照，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints<span style="color:#666">=</span>https://127.0.0.1:2379 <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --cacert<span style="color:#666">=</span>&lt;trusted-ca-file&gt; --cert<span style="color:#666">=</span>&lt;cert-file&gt; --key<span style="color:#666">=</span>&lt;key-file&gt; <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  snapshot save &lt;backup-file-location&gt;
</code></pre></div><p>可以从 etcd Pod 的描述中获得 <code>trusted-ca-file</code>, <code>cert-file</code> 和 <code>key-file</code> 。</p>
<h2 id="扩展-etcd-集群">扩展 etcd 集群</h2>
<p>通过交换性能，扩展 etcd 集群可以提高可用性。缩放不会提高集群性能和能力。
一般情况下不要扩大或缩小 etcd 集群的集合。不要为 etcd 集群配置任何自动缩放组。
强烈建议始终在任何官方支持的规模上运行生产 Kubernetes 集群时使用静态的五成员 etcd 集群。</p>
<p>合理的扩展是在需要更高可靠性的情况下，将三成员集群升级为五成员集群。
请参阅 <a href="https://etcd.io/docs/current/op-guide/runtime-configuration/#remove-a-member">etcd 重新配置文档</a>
以了解如何将成员添加到现有集群中的信息。</p>
<!--
## Restoring an etcd cluster

etcd supports restoring from snapshots that are taken from an etcd process of
the [major.minor](http://semver.org/) version. Restoring a version from a
different patch version of etcd also is supported. A restore operation is
employed to recover the data of a failed cluster.

Before starting the restore operation, a snapshot file must be present. It can
either be a snapshot file from a previous backup operation, or from a remaining
[data directory]( https://etcd.io/docs/current/op-guide/configuration/#--data-dir).
Here is an example:

```shell
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 snapshot restore snapshotdb
```

For more information and examples on restoring a cluster from a snapshot file, see
[etcd disaster recovery documentation](https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster).

If the access URLs of the restored cluster is changed from the previous
cluster, the Kubernetes API server must be reconfigured accordingly. In this
case, restart Kubernetes API servers with the flag
`--etcd-servers=$NEW_ETCD_CLUSTER` instead of the flag
`--etcd-servers=$OLD_ETCD_CLUSTER`. Replace `$NEW_ETCD_CLUSTER` and
`$OLD_ETCD_CLUSTER` with the respective IP addresses. If a load balancer is
used in front of an etcd cluster, you might need to update the load balancer
instead.

If the majority of etcd members have permanently failed, the etcd cluster is
considered failed. In this scenario, Kubernetes cannot make any changes to its
current state. Although the scheduled pods might continue to run, no new pods
can be scheduled. In such cases, recover the etcd cluster and potentially
reconfigure Kubernetes API servers to fix the issue.

<blockquote class="note callout">
  <div><strong>说明：</strong> <p>If any API servers are running in your cluster, you should not attempt to
restore instances of etcd. Instead, follow these steps to restore etcd:</p>
<ul>
<li>stop <em>all</em> API server instances</li>
<li>restore state in all etcd instances</li>
<li>restart all API server instances</li>
</ul>
<p>We also recommend restarting any components (e.g. <code>kube-scheduler</code>,
<code>kube-controller-manager</code>, <code>kubelet</code>) to ensure that they don't rely on some
stale data. Note that in practice, the restore takes a bit of time.  During the
restoration, critical components will lose leader lock and restart themselves.</p>
</div>
</blockquote>
-->
<h2 id="恢复-etcd-集群">恢复 etcd 集群</h2>
<p>etcd 支持从 <a href="http://semver.org/">major.minor</a> 或其他不同 patch 版本的 etcd 进程中获取的快照进行恢复。
还原操作用于恢复失败的集群的数据。</p>
<p>在启动还原操作之前，必须有一个快照文件。它可以是来自以前备份操作的快照文件，
也可以是来自剩余<a href="https://etcd.io/docs/current/op-guide/configuration/#--data-dir">数据目录</a>的快照文件。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl --endpoints 10.2.0.9:2379 snapshot restore snapshotdb
</code></pre></div><p>有关从快照文件还原集群的详细信息和示例，请参阅
<a href="https://etcd.io/docs/current/op-guide/recovery/#restoring-a-cluster">etcd 灾难恢复文档</a>。</p>
<p>如果还原的集群的访问 URL 与前一个集群不同，则必须相应地重新配置 Kubernetes API 服务器。
在本例中，使用参数 <code>--etcd-servers=$NEW_ETCD_CLUSTER</code> 而不是参数 <code>--etcd-servers=$OLD_ETCD_CLUSTER</code> 重新启动 Kubernetes API 服务器。
用相应的 IP 地址替换 <code>$NEW_ETCD_CLUSTER</code> 和 <code>$OLD_ETCD_CLUSTER</code>。如果在 etcd 集群前面使用负载平衡，则可能需要更新负载均衡器。</p>
<p>如果大多数 etcd 成员永久失败，则认为 etcd 集群失败。在这种情况下，Kubernetes 不能对其当前状态进行任何更改。
虽然已调度的 pod 可能继续运行，但新的 pod 无法调度。在这种情况下，恢复 etcd 集群并可能需要重新配置 Kubernetes API 服务器以修复问题。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>如果集群中正在运行任何 API 服务器，则不应尝试还原 etcd 的实例。相反，请按照以下步骤还原 etcd：</p>
<ul>
<li>停止 <em>所有</em> API 服务实例</li>
<li>在所有 etcd 实例中恢复状态</li>
<li>重启所有 API 服务实例</li>
</ul>
<p>我们还建议重启所有组件（例如 <code>kube-scheduler</code>、<code>kube-controller-manager</code>、<code>kubelet</code>），以确保它们不会
依赖一些过时的数据。请注意，实际中还原会花费一些时间。
在还原过程中，关键组件将丢失领导锁并自行重启。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> </div>
</blockquote></div>
</blockquote>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b64a1d2bb3f4ed9f7021134e09a75c36">2.9 - 为系统守护进程预留计算资源</h1>
    
	<!--
reviewers:
- vishh
- derekwaynecarr
- dashpole
title: Reserve Compute Resources for System Daemons
content_type: task
min-kubernetes-server-version: 1.8
-->
<!-- overview -->
<!--
Kubernetes nodes can be scheduled to `Capacity`. Pods can consume all the
available capacity on a node by default. This is an issue because nodes
typically run quite a few system daemons that power the OS and Kubernetes
itself. Unless resources are set aside for these system daemons, pods and system
daemons compete for resources and lead to resource starvation issues on the
node.

The `kubelet` exposes a feature named `Node Allocatable` that helps to reserve
compute resources for system daemons. Kubernetes recommends cluster
administrators to configure `Node Allocatable` based on their workload density
on each node.
-->
<p>Kubernetes 的节点可以按照 <code>Capacity</code> 调度。默认情况下 pod 能够使用节点全部可用容量。
这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。
除非为这些系统守护进程留出资源，否则它们将与 pod 争夺资源并导致节点资源短缺问题。</p>
<p><code>kubelet</code> 公开了一个名为 <code>Node Allocatable</code> 的特性，有助于为系统守护进程预留计算资源。
Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 <code>Node Allocatable</code>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 1.8.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- 
Your Kubernetes server must be at or later than version 1.17 to use
the kubelet command line option `--reserved-cpus` to set an
[explicitly reserved CPU list](#explicitly-reserved-cpu-list).
-->
<p>您的 kubernetes 服务器版本必须至少是 1.17 版本，才能使用 kubelet
命令行选项 <code>--reserved-cpus</code> 设置
<a href="#explicitly-reserved-cpu-list">显式预留 CPU 列表</a>。</p>
<!-- steps -->
<!--
## Node Allocatable

![node capacity](/images/docs/node-capacity.svg)

`Allocatable` on a Kubernetes node is defined as the amount of compute resources
that are available for pods. The scheduler does not over-subscribe
`Allocatable`. `CPU`, `memory` and `ephemeral-storage` are supported as of now.

Node Allocatable is exposed as part of `v1.Node` object in the API and as part
of `kubectl describe node` in the CLI.

Resources can be reserved for two categories of system daemons in the `kubelet`.
-->
<h2 id="node-allocatable">节点可分配  </h2>
<p><img src="/images/docs/node-capacity.svg" alt="节点容量"></p>
<p>Kubernetes 节点上的 <code>Allocatable</code> 被定义为 pod 可用计算资源量。
调度器不会超额申请 <code>Allocatable</code>。
目前支持 <code>CPU</code>, <code>memory</code> 和 <code>ephemeral-storage</code> 这几个参数。</p>
<p>可分配的节点暴露为 API 中 <code>v1.Node</code> 对象的一部分，也是 CLI 中
<code>kubectl describe node</code> 的一部分。</p>
<p>在 <code>kubelet</code> 中，可以为两类系统守护进程预留资源。</p>
<!--
### Enabling QoS and Pod level cgroups

To properly enforce node allocatable constraints on the node, you must
enable the new cgroup hierarchy via the `--cgroups-per-qos` flag. This flag is
enabled by default. When enabled, the `kubelet` will parent all end-user pods
under a cgroup hierarchy managed by the `kubelet`.
-->
<h3 id="启用-qos-和-pod-级别的-cgroups">启用 QoS 和 Pod 级别的 cgroups</h3>
<p>为了恰当的在节点范围实施节点可分配约束，你必须通过 <code>--cgroups-per-qos</code>
标志启用新的 cgroup 层次结构。这个标志是默认启用的。
启用后，<code>kubelet</code> 将在其管理的 cgroup 层次结构中创建所有终端用户的 Pod。</p>
<!--
### Configuring a cgroup driver

The `kubelet` supports manipulation of the cgroup hierarchy on
the host using a cgroup driver. The driver is configured via the
`--cgroup-driver` flag.

The supported values are the following:

* `cgroupfs` is the default driver that performs direct manipulation of the
cgroup filesystem on the host in order to manage cgroup sandboxes.
* `systemd` is an alternative driver that manages cgroup sandboxes using
transient slices for resources that are supported by that init system.

Depending on the configuration of the associated container runtime,
operators may have to choose a particular cgroup driver to ensure
proper system behavior. For example, if operators use the `systemd`
cgroup driver provided by the `docker` runtime, the `kubelet` must
be configured to use the `systemd` cgroup driver.
-->
<h3 id="配置-cgroup-驱动">配置 cgroup 驱动</h3>
<p><code>kubelet</code> 支持在主机上使用 cgroup 驱动操作 cgroup 层次结构。
驱动通过 <code>--cgroup-driver</code> 标志配置。</p>
<p>支持的参数值如下：</p>
<ul>
<li><code>cgroupfs</code> 是默认的驱动，在主机上直接操作 cgroup 文件系统以对 cgroup
沙箱进行管理。</li>
<li><code>systemd</code> 是可选的驱动，使用 init 系统支持的资源的瞬时切片管理
cgroup 沙箱。</li>
</ul>
<p>取决于相关容器运行时的配置，操作员可能需要选择一个特定的 cgroup 驱动
来保证系统正常运行。
例如，如果操作员使用 <code>docker</code> 运行时提供的 <code>systemd</code> cgroup 驱动时，
必须配置 <code>kubelet</code> 使用 <code>systemd</code> cgroup 驱动。</p>
<!--
### Kube Reserved

- **Kubelet Flag**: `--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]`
- **Kubelet Flag**: `--kube-reserved-cgroup=`

`kube-reserved` is meant to capture resource reservation for kubernetes system
daemons like the `kubelet`, `container runtime`, `node problem detector`, etc.
It is not meant to reserve resources for system daemons that are run as pods.
`kube-reserved` is typically a function of `pod density` on the nodes.

In addition to `cpu`, `memory`, and `ephemeral-storage`, `pid` may be
specified to reserve the specified number of process IDs for
kubernetes system daemons.
-->
<h3 id="kube-reserved">Kube 预留值 </h3>
<ul>
<li><strong>Kubelet 标志</strong>: <code>--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li>
<li><strong>Kubelet 标志</strong>: <code>--kube-reserved-cgroup=</code></li>
</ul>
<p><code>kube-reserved</code> 用来给诸如 <code>kubelet</code>、容器运行时、节点问题监测器等
kubernetes 系统守护进程记述其资源预留值。
该配置并非用来给以 Pod 形式运行的系统守护进程保留资源。<code>kube-reserved</code>
通常是节点上 <code>pod 密度</code> 的函数。</p>
<p>除了 <code>cpu</code>，<code>内存</code> 和 <code>ephemeral-storage</code> 之外，<code>pid</code> 可用来指定为
kubernetes 系统守护进程预留指定数量的进程 ID。</p>
<!--
To optionally enforce `kube-reserved` on kubernetes system daemons, specify the parent
control group for kube daemons as the value for `--kube-reserved-cgroup` kubelet
flag.

It is recommended that the kubernetes system daemons are placed under a top
level control group (`runtime.slice` on systemd machines for example). Each
system daemon should ideally run within its own child control group. Refer to
[this
doc](https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md#recommended-cgroups-setup)
for more details on recommended control group hierarchy.

Note that Kubelet **does not** create `--kube-reserved-cgroup` if it doesn't
exist. Kubelet will fail if an invalid cgroup is specified.
-->
<p>要选择性地对 kubernetes 系统守护进程上执行 <code>kube-reserved</code> 保护，需要把 kubelet 的
<code>--kube-reserved-cgroup</code> 标志的值设置为 kube 守护进程的父控制组。</p>
<p>推荐将 kubernetes 系统守护进程放置于顶级控制组之下（例如 systemd 机器上的
<code>runtime.slice</code>）。
理想情况下每个系统守护进程都应该在其自己的子控制组中运行。
请参考
<a href="https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md#recommended-cgroups-setup">这篇文档</a>，
进一步了解关于推荐控制组层次结构的细节。</p>
<p>请注意，如果 <code>--kube-reserved-cgroup</code> 不存在，Kubelet 将 <strong>不会</strong> 创建它。
如果指定了一个无效的 cgroup，Kubelet 将会失败。</p>
<!--
### System Reserved

- **Kubelet Flag**: `--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]`
- **Kubelet Flag**: `--system-reserved-cgroup=`


`system-reserved` is meant to capture resource reservation for OS system daemons
like `sshd`, `udev`, etc. `system-reserved` should reserve `memory` for the
`kernel` too since `kernel` memory is not accounted to pods in Kubernetes at this time.
Reserving resources for user login sessions is also recommended (`user.slice` in
systemd world).

In addition to `cpu`, `memory`, and `ephemeral-storage`, `pid` may be
specified to reserve the specified number of process IDs for OS system
daemons.
-->
<h3 id="system-reserved">系统预留值 </h3>
<ul>
<li><strong>Kubelet 标志</strong>: <code>--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li>
<li><strong>Kubelet 标志</strong>: <code>--system-reserved-cgroup=</code></li>
</ul>
<p><code>system-reserved</code> 用于为诸如 <code>sshd</code>、<code>udev</code> 等系统守护进程记述其资源预留值。
<code>system-reserved</code> 也应该为 <code>kernel</code> 预留 <code>内存</code>，因为目前 <code>kernel</code>
使用的内存并不记在 Kubernetes 的 Pod 上。
同时还推荐为用户登录会话预留资源（systemd 体系中的 <code>user.slice</code>）。</p>
<p>除了 <code>cpu</code>，<code>内存</code> 和 <code>ephemeral-storage</code> 之外，<code>pid</code> 可用来指定为
kubernetes 系统守护进程预留指定数量的进程 ID。</p>
<!--
To optionally enforce `system-reserved` on system daemons, specify the parent
control group for OS system daemons as the value for `--system-reserved-cgroup`
kubelet flag.

It is recommended that the OS system daemons are placed under a top level
control group (`system.slice` on systemd machines for example).

Note that Kubelet **does not** create `--system-reserved-cgroup` if it doesn't
exist. Kubelet will fail if an invalid cgroup is specified.
-->
<p>要想为系统守护进程上可选地实施 <code>system-reserved</code> 约束，请指定 kubelet 的
<code>--system-reserved-cgroup</code> 标志值为 OS 系统守护进程的父级控制组。</p>
<p>推荐将 OS 系统守护进程放在一个顶级控制组之下（例如 systemd 机器上的
<code>system.slice</code>）。</p>
<p>请注意，如果 <code>--system-reserved-cgroup</code> 不存在，Kubelet <strong>不会</strong> 创建它。
如果指定了无效的 cgroup，Kubelet 将会失败。</p>
<!--
### Explicitly Reserved CPU List

- **Kubelet Flag**: `--reserved-cpus=0-3`
-->
<h3 id="explicitly-reserved-cpu-list">显式保留的 CPU 列表</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>

<ul>
<li><strong>Kubelet 标志</strong>: <code>--reserved-cpus=0-3</code></li>
</ul>
<!--
`reserved-cpus` is meant to define an explicit CPU set for OS system daemons and
kubernetes system daemons. `reserved-cpus` is for systems that do not intend to
define separate top level cgroups for OS system daemons and kubernetes system daemons
with regard to cpuset resource.
If the Kubelet **does not** have `--system-reserved-cgroup` and `--kube-reserved-cgroup`,
the explicit cpuset provided by `reserved-cpus` will take precedence over the CPUs
defined by `--kube-reserved` and `--system-reserved` options.
-->
<p><code>reserved-cpus</code> 旨在为操作系统守护程序和 kubernetes 系统守护程序保留一组明确指定编号的
CPU。<code>reserved-cpus</code> 适用于不打算针对 cpuset 资源为操作系统守护程序和 kubernetes
系统守护程序定义独立的顶级 cgroups 的系统。
如果 Kubelet <strong>没有</strong> 指定参数 <code>--system-reserved-cgroup</code> 和 <code>--kube-reserved-cgroup</code>，
则 <code>reserved-cpus</code> 的设置将优先于 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 选项。</p>
<!--
This option is specifically designed for Telco/NFV use cases where uncontrolled
interrupts/timers may impact the workload performance. you can use this option
to define the explicit cpuset for the system/kubernetes daemons as well as the
interrupts/timers, so the rest CPUs on the system can be used exclusively for
workloads, with less impact from uncontrolled interrupts/timers. To move the
system daemon, kubernetes daemons and interrupts/timers to the explicit cpuset
defined by this option, other mechanism outside Kubernetes should be used.
For example: in Centos, you can do this using the tuned toolset.
-->
<p>此选项是专门为电信/NFV 用例设计的，在这些用例中不受控制的中断或计时器可能会
影响其工作负载性能。
你可以使用此选项为系统或 kubernetes 守护程序以及中断或计时器显式定义 cpuset，
这样系统上的其余 CPU 可以专门用于工作负载，因不受控制的中断或计时器的影响得以
降低。
要将系统守护程序、kubernetes 守护程序和中断或计时器移动到此选项定义的显式
cpuset 上，应使用 Kubernetes 之外的其他机制。
例如：在 Centos 系统中，可以使用 tuned 工具集来执行此操作。</p>
<!--
### Eviction Thresholds

- **Kubelet Flag**: `--eviction-hard=[memory.available<500Mi]`

Memory pressure at the node level leads to System OOMs which affects the entire
node and all pods running on it. Nodes can go offline temporarily until memory
has been reclaimed. To avoid (or reduce the probability of) system OOMs kubelet
provides [`Out of Resource`](/docs/tasks/administer-cluster/out-of-resource/) management. Evictions are
supported for `memory` and `ephemeral-storage` only. By reserving some memory via
`--eviction-hard` flag, the `kubelet` attempts to `evict` pods whenever memory
availability on the node drops below the reserved value. Hypothetically, if
system daemons did not exist on a node, pods cannot use more than `capacity -
eviction-hard`. For this reason, resources reserved for evictions are not
available for pods.
-->
<h3 id="eviction-Thresholds">驱逐阈值  </h3>
<ul>
<li><strong>Kubelet 标志</strong>: <code>--eviction-hard=[memory.available&lt;500Mi]</code></li>
</ul>
<p>节点级别的内存压力将导致系统内存不足，这将影响到整个节点及其上运行的所有 Pod。
节点可以暂时离线直到内存已经回收为止。
为了防止（或减少可能性）系统内存不足，kubelet 提供了
<a href="/zh/docs/tasks/administer-cluster/out-of-resource/">资源不足</a>管理。
驱逐操作只支持 <code>memory</code> 和 <code>ephemeral-storage</code>。
通过 <code>--eviction-hard</code> 标志预留一些内存后，当节点上的可用内存降至保留值以下时，
<code>kubelet</code> 将尝试<code>驱逐</code> Pod。
如果节点上不存在系统守护进程，Pod 将不能使用超过 <code>capacity-eviction-hard</code> 所
指定的资源量。因此，为驱逐而预留的资源对 Pod 是不可用的。</p>
<!--
### Enforcing Node Allocatable

- **Kubelet Flag**: `--enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]`

The scheduler treats `Allocatable` as the available `capacity` for pods.

`kubelet` enforce `Allocatable` across pods by default. Enforcement is performed
by evicting pods whenever the overall usage across all pods exceeds
`Allocatable`. More details on eviction policy can be found
[here](/docs/tasks/administer-cluster/out-of-resource/#eviction-policy). This enforcement is controlled by
specifying `pods` value to the kubelet flag `--enforce-node-allocatable`.

Optionally, `kubelet` can be made to enforce `kube-reserved` and
`system-reserved` by specifying `kube-reserved` & `system-reserved` values in
the same flag. Note that to enforce `kube-reserved` or `system-reserved`,
`--kube-reserved-cgroup` or `--system-reserved-cgroup` needs to be specified
respectively.
-->
<h3 id="enforcing-node-allocatable">实施节点可分配约束  </h3>
<ul>
<li><strong>Kubelet 标志</strong>: <code>--enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]</code></li>
</ul>
<p>调度器将 <code>Allocatable</code> 视为 Pod 可用的 <code>capacity</code>（资源容量）。</p>
<p><code>kubelet</code> 默认对 Pod 执行 <code>Allocatable</code> 约束。
无论何时，如果所有 Pod 的总用量超过了 <code>Allocatable</code>，驱逐 Pod 的措施将被执行。
有关驱逐策略的更多细节可以在
<a href="/zh/docs/tasks/administer-cluster/out-of-resource/#eviction-policy">这里</a>找到。
可通过设置 kubelet <code>--enforce-node-allocatable</code> 标志值为 <code>pods</code> 控制这个措施。</p>
<p>可选地，通过在同一标志中同时指定 <code>kube-reserved</code> 和 <code>system-reserved</code> 值，
可以使 <code>kubelet</code> 强制实施 <code>kube-reserved</code> 和 <code>system-reserved</code>约束。
请注意，要想执行 <code>kube-reserved</code> 或者 <code>system-reserved</code> 约束，
需要对应设置 <code>--kube-reserved-cgroup</code> 或者 <code>--system-reserved-cgroup</code>。</p>
<!--
## General Guidelines

System daemons are expected to be treated similar to `Guaranteed` pods. System
daemons can burst within their bounding control groups and this behavior needs
to be managed as part of kubernetes deployments. For example, `kubelet` should
have its own control group and share `Kube-reserved` resources with the
container runtime. However, Kubelet cannot burst and use up all available Node
resources if `kube-reserved` is enforced.
-->
<h2 id="general-guidelines">一般原则  </h2>
<p>系统守护进程一般会被按照类似 <code>Guaranteed</code> Pod 一样对待。
系统守护进程可以在与其对应的控制组中出现突发资源用量，这一行为要作为
kubernetes 部署的一部分进行管理。
例如，<code>kubelet</code> 应该有它自己的控制组并和容器运行时共享 <code>Kube-reserved</code> 资源。
不过，如果执行了 <code>kube-reserved</code> 约束，则 kubelet 不可出现突发负载并用光
节点的所有可用资源。</p>
<!--
Be extra careful while enforcing `system-reserved` reservation since it can lead
to critical system services being CPU starved, OOM killed, or unable
to fork on the node. The
recommendation is to enforce `system-reserved` only if a user has profiled their
nodes exhaustively to come up with precise estimates and is confident in their
ability to recover if any process in that group is oom_killed.

* To begin with enforce `Allocatable` on `pods`.
* Once adequate monitoring and alerting is in place to track kube system
  daemons, attempt to enforce `kube-reserved` based on usage heuristics.
* If absolutely necessary, enforce `system-reserved` over time.
-->
<p>在执行 <code>system-reserved</code> 预留策略时请加倍小心，因为它可能导致节点上的
关键系统服务出现 CPU 资源短缺、因为内存不足而被终止或者无法在节点上创建进程。
建议只有当用户详尽地描述了他们的节点以得出精确的估计值，
并且对该组中进程因内存不足而被杀死时，有足够的信心将其恢复时，
才可以强制执行 <code>system-reserved</code> 策略。</p>
<ul>
<li>作为起步，可以先针对 <code>pods</code> 上执行 <code>Allocatable</code> 约束。</li>
<li>一旦用于追踪系统守护进程的监控和告警的机制到位，可尝试基于用量估计的
方式执行 <code>kube-reserved</code>策略。</li>
<li>随着时间推进，在绝对必要的时候可以执行 <code>system-reserved</code> 策略。</li>
</ul>
<!--
The resource requirements of kube system daemons may grow over time as more and
more features are added. Over time, kubernetes project will attempt to bring
down utilization of node system daemons, but that is not a priority as of now.
So expect a drop in `Allocatable` capacity in future releases.
-->
<p>随着时间推进和越来越多特性被加入，kube 系统守护进程对资源的需求可能也会增加。
以后 kubernetes 项目将尝试减少对节点系统守护进程的利用，但目前这件事的优先级
并不是最高。
所以，将来的发布版本中 <code>Allocatable</code> 容量是有可能降低的。</p>
<!-- discussion -->
<!--
## Example Scenario

Here is an example to illustrate Node Allocatable computation:

* Node has `32Gi` of `memory`, `16 CPUs` and `100Gi` of `Storage`
* `--kube-reserved` is set to `cpu=1,memory=2Gi,ephemeral-storage=1Gi`
* `--system-reserved` is set to `cpu=500m,memory=1Gi,ephemeral-storage=1Gi`
* `--eviction-hard` is set to `memory.available<500Mi,nodefs.available<10%`
-->
<h2 id="example-scenario">示例场景  </h2>
<p>这是一个用于说明节点可分配（Node Allocatable）计算方式的示例：</p>
<ul>
<li>节点拥有 <code>32Gi</code> <code>memeory</code>，<code>16 CPU</code> 和 <code>100Gi</code> <code>Storage</code> 资源</li>
<li><code>--kube-reserved</code> 被设置为 <code>cpu=1,memory=2Gi,ephemeral-storage=1Gi</code></li>
<li><code>--system-reserved</code> 被设置为 <code>cpu=500m,memory=1Gi,ephemeral-storage=1Gi</code></li>
<li><code>--eviction-hard</code> 被设置为 <code>memory.available&lt;500Mi,nodefs.available&lt;10%</code></li>
</ul>
<!--
Under this scenario, `Allocatable` will be `14.5 CPUs`, `28.5Gi` of memory and
`88Gi` of local storage.
Scheduler ensures that the total memory `requests` across all pods on this node does
not exceed `28.5Gi` and storage doesn't exceed `88Gi`.
Kubelet evicts pods whenever the overall memory usage across pods exceeds `28.5Gi`,
or if overall disk usage exceeds `88Gi` If all processes on the node consume as
much CPU as they can, pods together cannot consume more than `14.5 CPUs`.

If `kube-reserved` and/or `system-reserved` is not enforced and system daemons
exceed their reservation, `kubelet` evicts pods whenever the overall node memory
usage is higher than `31.5Gi` or `storage` is greater than `90Gi`
-->
<p>在这个场景下，<code>Allocatable</code> 将会是 <code>14.5 CPUs</code>、<code>28.5Gi</code> 内存以及 <code>88Gi</code> 本地存储。
调度器保证这个节点上的所有 Pod 的内存 <code>requests</code> 总量不超过 <code>28.5Gi</code>，
存储不超过 <code>88Gi</code>。
当 Pod 的内存使用总量超过 <code>28.5Gi</code> 或者磁盘使用总量超过 <code>88Gi</code> 时，
kubelet 将会驱逐它们。
如果节点上的所有进程都尽可能多地使用 CPU，则 Pod 加起来不能使用超过
<code>14.5 CPUs</code> 的资源。</p>
<p>当没有执行 <code>kube-reserved</code> 和/或 <code>system-reserved</code> 策略且系统守护进程
使用量超过其预留时，如果节点内存用量高于 <code>31.5Gi</code> 或<code>存储</code>大于 <code>90Gi</code>，
kubelet 将会驱逐 Pod。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a8f6511197efcd7d0db80ade49620f9d">2.10 - 为节点发布扩展资源</h1>
    
	<!--
title: Advertise Extended Resources for a Node
content_type: task
-->
<!-- overview -->
<!--
This page shows how to specify extended resources for a Node.
Extended resources allow cluster administrators to advertise node-level
resources that would otherwise be unknown to Kubernetes.
-->
<p>本文展示了如何为节点指定扩展资源（Extended Resource）。
扩展资源允许集群管理员发布节点级别的资源，这些资源在不进行发布的情况下无法被 Kubernetes 感知。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Get the names of your Nodes

Choose one of your Nodes to use for this exercise.
-->
<h2 id="获取你的节点名称">获取你的节点名称</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><p>选择一个节点用于此练习。</p>
<!--
## Advertise a new extended resource on one of your Nodes

To advertise a new extended resource on a Node, send an HTTP PATCH request to
the Kubernetes API server. For example, suppose one of your Nodes has four dongles
attached. Here's an example of a PATCH request that advertises four dongle resources
for your Node.
-->
<h2 id="在你的一个节点上发布一种新的扩展资源">在你的一个节点上发布一种新的扩展资源</h2>
<p>为在一个节点上发布一种新的扩展资源，需要发送一个 HTTP PATCH 请求到 Kubernetes API server。
例如：假设你的一个节点上带有四个 dongle 资源。
下面是一个 PATCH 请求的示例，该请求为你的节点发布四个 dongle 资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

<span style="color:#666">[</span>
  <span style="color:#666">{</span>
    <span style="color:#b44">&#34;op&#34;</span>: <span style="color:#b44">&#34;add&#34;</span>,
    <span style="color:#b44">&#34;path&#34;</span>: <span style="color:#b44">&#34;/status/capacity/example.com~1dongle&#34;</span>,
    <span style="color:#b44">&#34;value&#34;</span>: <span style="color:#b44">&#34;4&#34;</span>
  <span style="color:#666">}</span>
<span style="color:#666">]</span>
</code></pre></div><!--
Note that Kubernetes does not need to know what a dongle is or what a dongle is for.
The preceding PATCH request just tells Kubernetes that your Node has four things that
you call dongles.

Start a proxy, so that you can easily send requests to the Kubernetes API server:
-->
<p>注意：Kubernetes 不需要了解 dongle 资源的含义和用途。
前面的 PATCH 请求仅仅告诉 Kubernetes 你的节点拥有四个你称之为 dongle 的东西。</p>
<p>启动一个代理（proxy），以便你可以很容易地向 Kubernetes API server 发送请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy
</code></pre></div><!--
In another command window, send the HTTP PATCH request.
Replace `<your-node-name>` with the name of your Node:
-->
<p>在另一个命令窗口中，发送 HTTP PATCH 请求。 用你的节点名称替换 <code>&lt;your-node-name&gt;</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl --header <span style="color:#b44">&#34;Content-Type: application/json-patch+json&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --request PATCH <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --data <span style="color:#b44">&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1dongle&#34;, &#34;value&#34;: &#34;4&#34;}]&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</code></pre></div><!--
In the preceding request, `~1` is the encoding for the character / in
the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
[IETF RFC 6901](https://tools.ietf.org/html/rfc6901), section 3.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 在前面的请求中，<code>~1</code> 为 patch 路径中 “/” 符号的编码。
JSON-Patch 中的操作路径值被解析为 JSON 指针。
更多细节，请查看 <a href="https://tools.ietf.org/html/rfc6901">IETF RFC 6901</a> 的第 3 节。</div>
</blockquote>
<!--
The output shows that the Node has a capacity of 4 dongles:
-->
<p>输出显示该节点的 dongle 资源容量（capacity）为 4：</p>
<pre tabindex="0"><code>&quot;capacity&quot;: {
  &quot;cpu&quot;: &quot;2&quot;,
  &quot;memory&quot;: &quot;2049008Ki&quot;,
  &quot;example.com/dongle&quot;: &quot;4&quot;,
</code></pre><!-- Describe your Node: -->
<p>描述你的节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe node &lt;your-node-name&gt;
</code></pre></div><!-- Once again, the output shows the dongle resource: -->
<p>输出再次展示了 dongle 资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb">  </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb">  </span>2049008Ki<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb">  </span><span style="color:#666">4</span><span style="color:#bbb">
</span></code></pre></div><!--
Now, application developers can create Pods that request a certain
number of dongles. See
[Assign Extended Resources to a Container](/docs/tasks/configure-pod-container/extended-resource/).
-->
<p>现在，应用开发者可以创建请求一定数量 dongle 资源的 Pod 了。
参见<a href="/zh/docs/tasks/configure-pod-container/extended-resource/">将扩展资源分配给容器</a>。</p>
<!--
## Discussion

Extended resources are similar to memory and CPU resources. For example,
just as a Node has a certain amount of memory and CPU to be shared by all components
running on the Node, it can have a certain number of dongles to be shared
by all components running on the Node. And just as application developers
can create Pods that request a certain amount of memory and CPU, they can
create Pods that request a certain number of dongles.
-->
<h2 id="讨论">讨论</h2>
<p>扩展资源类似于内存和 CPU 资源。例如，正如一个节点拥有一定数量的内存和 CPU 资源，
它们被节点上运行的所有组件共享，该节点也可以拥有一定数量的 dongle 资源，
这些资源同样被节点上运行的所有组件共享。
此外，正如应用开发者可以创建请求一定数量的内存和 CPU 资源的 Pod，
他们也可以创建请求一定数量 dongle 资源的 Pod。</p>
<!--
Extended resources are opaque to Kubernetes; Kubernetes does not
know anything about what they are. Kubernetes knows only that a Node
has a certain number of them. Extended resources must be advertised in integer
amounts. For example, a Node can advertise four dongles, but not 4.5 dongles.
-->
<p>扩展资源对 Kubernetes 是不透明的。Kubernetes 不知道扩展资源含义相关的任何信息。
Kubernetes 只了解一个节点拥有一定数量的扩展资源。
扩展资源必须以整形数量进行发布。
例如，一个节点可以发布 4 个 dongle 资源，但是不能发布 4.5 个。</p>
<!--
### Storage example

Suppose a Node has 800 GiB of a special kind of disk storage. You could
create a name for the special storage, say example.com/special-storage.
Then you could advertise it in chunks of a certain size, say 100 GiB. In that case,
your Node would advertise that it has eight resources of type
example.com/special-storage.
-->
<h3 id="存储示例">存储示例</h3>
<p>假设一个节点拥有一种特殊类型的磁盘存储，其容量为 800 GiB。
你可以为该特殊存储创建一个名称，如 <code>example.com/special-storage</code>。
然后你就可以按照一定规格的块（如 100 GiB）对其进行发布。
在这种情况下，你的节点将会通知它拥有八个 <code>example.com/special-storage</code> 类型的资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/special-storage</span>:<span style="color:#bbb"> </span><span style="color:#666">8</span><span style="color:#bbb">
</span></code></pre></div><!--
If you want to allow arbitrary requests for special storage, you
could advertise special storage in chunks of size 1 byte. In that case, you would advertise
800Gi resources of type example.com/special-storage.
-->
<p>如果你想要允许针对特殊存储任意（数量）的请求，你可以按照 1 字节大小的块来发布特殊存储。
在这种情况下，你将会发布 800Gi 数量的 example.com/special-storage 类型的资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/special-storage</span>:<span style="color:#bbb">  </span>800Gi<span style="color:#bbb">
</span></code></pre></div><!--
Then a Container could request any number of bytes of special storage, up to 800Gi.
-->
<p>然后，容器就能够请求任意数量（多达 800Gi）字节的特殊存储。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">example.com/special-storage</span>:<span style="color:#bbb">  </span>800Gi<span style="color:#bbb">
</span></code></pre></div><!--
## Clean up

Here is a PATCH request that removes the dongle advertisement from a Node.
-->
<h2 id="清理">清理</h2>
<p>这里是一个从节点移除 dongle 资源发布的 PATCH 请求。</p>
<pre tabindex="0"><code>PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    &quot;op&quot;: &quot;remove&quot;,
    &quot;path&quot;: &quot;/status/capacity/example.com~1dongle&quot;,
  }
]
</code></pre><!--
Start a proxy, so that you can easily send requests to the Kubernetes API server:
-->
<p>启动一个代理，以便你可以很容易地向 Kubernetes API 服务器发送请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy
</code></pre></div><!--
In another command window, send the HTTP PATCH request.
Replace `<your-node-name>` with the name of your Node:
-->
<p>在另一个命令窗口中，发送 HTTP PATCH 请求。用你的节点名称替换 <code>&lt;your-node-name&gt;</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl --header <span style="color:#b44">&#34;Content-Type: application/json-patch+json&#34;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--request PATCH <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--data <span style="color:#b44">&#39;[{&#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1dongle&#34;}]&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status
</code></pre></div><!--
Verify that the dongle advertisement has been removed:
-->
<p>验证 dongle 资源的发布已经被移除：</p>
<pre tabindex="0"><code>kubectl describe node &lt;your-node-name&gt; | grep dongle
</code></pre><!--
(you should not see any output)  
-->
<p>(你应该看不到任何输出)</p>
<h2 id="接下来">接下来</h2>
<!--
### For application developers

* [Assign Extended Resources to a Container](/docs/tasks/configure-pod-container/extended-resource/)

### For cluster administrators

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-->
<h3 id="针对应用开发人员">针对应用开发人员</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/extended-resource/">将扩展资源分配给容器</a></li>
</ul>
<h3 id="针对集群管理员">针对集群管理员</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为名字空间配置最小和最大内存约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为名字空间配置最小和最大 CPU 约束</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e1afcdac8d5e8458274b3c481c5ebcda">2.11 - 使用 CoreDNS 进行服务发现</h1>
    
	<!--
reviewers:
- johnbelamaric
title: Using CoreDNS for Service Discovery
min-kubernetes-server-version: v1.9
content_type: task
-->
<!-- overview -->
<!--
This page describes the CoreDNS upgrade process and how to install CoreDNS instead of kube-dns.
-->
<p>此页面介绍了 CoreDNS 升级过程以及如何安装 CoreDNS 而不是 kube-dns。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.9.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## About CoreDNS

[CoreDNS](https://coredns.io) is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.
Like Kubernetes, the CoreDNS project is hosted by the <a class='glossary-tooltip' title='云原生计算基金会' data-toggle='tooltip' data-placement='top' href='https://cncf.io/' target='_blank' aria-label='CNCF'>CNCF</a>.
-->
<h2 id="关于-coredns">关于 CoreDNS</h2>
<p><a href="https://coredns.io">CoreDNS</a> 是一个灵活可扩展的 DNS 服务器，可以作为 Kubernetes 集群 DNS。
与 Kubernetes 一样，CoreDNS 项目由 <a class='glossary-tooltip' title='云原生计算基金会' data-toggle='tooltip' data-placement='top' href='https://cncf.io/' target='_blank' aria-label='CNCF'>CNCF</a> 托管。</p>
<!--
You can use CoreDNS instead of kube-dns in your cluster by replacing kube-dns in an existing
deployment, or by using tools like kubeadm that will deploy and upgrade the cluster for you.
-->
<p>通过在现有的集群中替换 kube-dns，可以在集群中使用 CoreDNS 代替 kube-dns 部署，
或者使用 kubeadm 等工具来为你部署和升级集群。</p>
<!--
## Installing CoreDNS

For manual deployment or replacement of kube-dns, see the documentation at the
[CoreDNS GitHub project.](https://github.com/coredns/deployment/tree/master/kubernetes)
-->
<h2 id="安装-coredns">安装 CoreDNS</h2>
<p>有关手动部署或替换 kube-dns，请参阅
<a href="https://github.com/coredns/deployment/tree/master/kubernetes">CoreDNS GitHub 工程</a>。</p>
<!--
## Migrating to CoreDNS

### Upgrading an existing cluster with kubeadm
-->
<h2 id="迁移到-coredns">迁移到 CoreDNS</h2>
<h3 id="使用-kubeadm-升级现有集群">使用 kubeadm 升级现有集群</h3>
<!--
In Kubernetes version 1.10 and later, you can also move to CoreDNS when you use `kubeadm` to upgrade
a cluster that is using `kube-dns`. In this case, `kubeadm` will generate the CoreDNS configuration
("Corefile") based upon the `kube-dns` ConfigMap, preserving configurations for federation,
stub domains, and upstream name server.
-->
<p>在 Kubernetes 1.10 及更高版本中，当你使用 <code>kubeadm</code> 升级使用 <code>kube-dns</code> 的集群时，你还可以迁移到 CoreDNS。
在本例中 <code>kubeadm</code> 将生成 CoreDNS 配置（&quot;Corefile&quot;）基于 <code>kube-dns</code> ConfigMap，
保存联邦、存根域和上游名称服务器的配置。</p>
<!--
If you are moving from kube-dns to CoreDNS, make sure to set the `CoreDNS` feature gate to `true`
during an upgrade. For example, here is what a `v1.11.0` upgrade would look like:
-->
<p>如果你正在从 kube-dns 迁移到 CoreDNS，请确保在升级期间将 <code>CoreDNS</code> 特性门设置为 <code>true</code>。
例如，<code>v1.11.0</code> 升级应该是这样的:</p>
<pre tabindex="0"><code>kubeadm upgrade apply v1.11.0 --feature-gates=CoreDNS=true
</code></pre><!--
In Kubernetes version 1.13 and later the `CoreDNS` feature gate is removed and CoreDNS
is used by default. Follow the guide outlined [here](/docs/reference/setup-tools/kubeadm/kubeadm-init-phase#cmd-phase-addon) if you want
your upgraded cluster to use kube-dns.
-->
<p>在 Kubernetes 版本 1.13 和更高版本中，<code>CoreDNS</code>特性门已经删除，CoreDNS 在默认情况下使用。
如果你想升级集群以使用 kube-dns，请遵循
<a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-init-phase#cmd-phase-addon">此处</a> 。</p>
<!--
In versions prior to 1.11 the Corefile will be **overwritten** by the one created during upgrade.
**You should save your existing ConfigMap if you have customized it.** You may re-apply your
customizations after the new ConfigMap is up and running.
-->
<p>在 1.11 之前的版本中，核心文件将被升级过程中创建的文件覆盖。
<strong>如果已对其进行自定义，则应保存现有的 ConfigMap。</strong>
在新的 ConfigMap 启动并运行后，你可以重新应用自定义。</p>
<!--
If you are running CoreDNS in Kubernetes version 1.11 and later, during upgrade,
your existing Corefile will be retained.
-->
<p>如果你在 Kubernetes 1.11 及更高版本中运行 CoreDNS，则在升级期间，将保留现有的 Corefile。</p>
<!--
## Installing kube-dns instead of CoreDNS with kubeadm
-->
<h2 id="使用-kubeadm-安装-kube-dns-而不是-coredns">使用 kubeadm 安装 kube-dns 而不是 CoreDNS</h2>
<!--
In Kubernetes 1.11, CoreDNS has graduated to General Availability (GA)
and is installed by default.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 在 Kubernetes 1.11 中，CoreDNS 已经升级到通用可用性（GA），并默认安装。</div>
</blockquote>
<!--
In Kubernetes 1.18, kube-dns usage with kubeadm has been deprecated and will be removed in a future version.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 在 Kubernetes 1.18 中，用 kubeadm 来安装 kube-dns 这一做法已经被废弃，
会在将来版本中移除。</div>
</blockquote>

<!--
To install kube-dns on versions prior to 1.13, set the `CoreDNS` feature gate
value to `false`:
-->
<p>若要在 1.13 之前版本上安装 kube-dns，请将 <code>CoreDNS</code> 特性门控设置为 <code>false</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubeadm init --feature-gates<span style="color:#666">=</span><span style="color:#b8860b">CoreDNS</span><span style="color:#666">=</span><span style="color:#a2f">false</span>
</code></pre></div><!--
For versions 1.13 and later, follow the guide outlined [here](/docs/reference/setup-tools/kubeadm/kubeadm-init-phase#cmd-phase-addon).
-->
<p>对于 1.13 版和更高版本，请遵循
<a href="/zh/docs/reference/setup-tools/kubeadm/kubeadm-init-phase#cmd-phase-addon">此处</a>概述到指南。</p>
<!--
## Upgrading CoreDNS 

CoreDNS is available in Kubernetes since v1.9. 
You can check the version of CoreDNS shipped with Kubernetes and the changes made to CoreDNS [here](https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md).
-->
<h2 id="升级-coredns">升级 CoreDNS</h2>
<p>从 v1.9 起，Kubernetes 提供了 CoreDNS。
你可以在<a href="https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md">此处</a>
查看 Kubernetes 随附的 CoreDNS 版本以及对 CoreDNS 所做的更改。</p>
<!--
CoreDNS can be upgraded manually in case you want to only upgrade CoreDNS or use your own custom image.
There is a helpful [guideline and walkthrough](https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md) available to ensure a smooth upgrade.
-->
<p>如果你只想升级 CoreDNS 或使用自己的自定义镜像，则可以手动升级 CoreDNS。
参看<a href="https://github.com/coredns/deployment/blob/master/kubernetes/Upgrading_CoreDNS.md">指南和演练</a>
文档了解如何平滑升级。</p>
<!--
## Tuning CoreDNS

When resource utilisation is a concern, it may be useful to tune the configuration of CoreDNS. For more details, check out the
[documentation on scaling CoreDNS]((https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md)).
-->
<h2 id="coredns-调优">CoreDNS 调优</h2>
<p>当资源利用方面有问题时，优化 CoreDNS 的配置可能是有用的。
有关详细信息，请参阅<a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md">有关扩缩 CoreDNS 的文档</a>。</p>
<h2 id="接下来">接下来</h2>
<!--
You can configure [CoreDNS](https://coredns.io) to support many more use cases than
kube-dns by modifying the `Corefile`. For more information, see the
[CoreDNS site](https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/).
-->
<p>你可以通过修改 <code>Corefile</code> 来配置 <a href="https://coredns.io">CoreDNS</a>，以支持比 kube-dns 更多的用例。
请参考 <a href="https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/">CoreDNS 网站</a>
以了解更多信息。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-669c88964b4a9eb2b040057266e4b60d">2.12 - 使用 KMS 驱动进行数据加密</h1>
    
	<!--
reviewers:
- smarterclayton
title: Using a KMS provider for data encryption
content_type: task
-->
<!-- overview -->
<!-- This page shows how to configure a Key Management Service (KMS) provider and plugin to enable secret data encryption. -->
<p>本页展示了如何配置秘钥管理服务—— Key Management Service (KMS) 驱动和插件以启用
Secret 数据加密。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!-- * Kubernetes version 1.10.0 or later is required -->
<!-- * etcd v3 or later is required -->
<ul>
<li>需要 Kubernetes 1.10.0 或更新版本</li>
<li>需要 etcd v3 或更新版本</li>
</ul>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code>
</div>

<!-- steps -->
<!--
The KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd. The data is encrypted using a data encryption key (DEK); a new DEK is generated for each encryption. The DEKs are encrypted with a key encryption key (KEK) that is stored and managed in a remote KMS. The KMS provider uses gRPC to communicate with a specific KMS 
plugin. The KMS plugin, which is implemented as a gRPC server and deployed on the same host(s) as the Kubernetes master(s), is responsible for all communication with the remote KMS.
-->
<p>KMS 加密驱动使用封套加密模型来加密 etcd 中的数据。
数据使用数据加密秘钥（DEK）加密；每次加密都生成一个新的 DEK。
这些 DEK 经一个秘钥加密秘钥（KEK）加密后在一个远端的 KMS 中存储和管理。
KMS 驱动使用 gRPC 与一个特定的 KMS 插件通信。这个 KMS 插件作为一个 gRPC
服务器被部署在 Kubernetes 主服务器的同一个主机上，负责与远端 KMS 的通信。</p>
<!--
## Configuring the KMS provider

To configure a KMS provider on the API server, include a provider of type ```kms``` in the providers array in the encryption configuration file and set the following properties:
-->
<h2 id="配置-kms-驱动">配置 KMS 驱动</h2>
<p>为了在 API 服务器上配置 KMS 驱动，在加密配置文件中的驱动数组中加入一个类型为 <code>kms</code>
的驱动，并设置下列属性：</p>
<!--
* `name`: Display name of the KMS plugin.
* `endpoint`: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.
* `cachesize`: Number of data encryption keys (DEKs) to be cached in the clear. When cached, DEKs can be used without another call to the KMS; whereas DEKs that are not cached require a call to the KMS to unwrap.
* `timeout`: How long should kube-apiserver wait for kms-plugin to respond before returning an error (default is 3 seconds).
-->
<ul>
<li><code>name</code>: KMS 插件的显示名称。</li>
<li><code>endpoint</code>: gRPC 服务器（KMS 插件）的监听地址。该端点是一个 UNIX 域套接字。</li>
<li><code>cachesize</code>: 以明文缓存的数据加密秘钥（DEKs）的数量。一旦被缓存，
就可以直接使用 DEKs 而无需另外调用 KMS；而未被缓存的 DEKs 需要调用一次 KMS 才能解包。</li>
<li><code>timeout</code>: 在返回一个错误之前，kube-apiserver 等待 kms-plugin 响应的时间（默认是 3 秒）。</li>
</ul>
<!-- See [Understanding the encryption at rest configuration.](/docs/tasks/administer-cluster/encrypt-data) -->
<p>参见<a href="/zh/docs/tasks/administer-cluster/encrypt-data">理解静态数据加密配置</a></p>
<!--
## Implementing a KMS plugin

To implement a KMS plugin, you can develop a new plugin gRPC server or enable a KMS plugin already provided by your cloud provider. You then integrate the plugin with the remote KMS and deploy it on the Kubernetes master.
-->
<h2 id="实现-kms-插件">实现 KMS 插件</h2>
<p>为实现一个 KMS 插件，你可以开发一个新的插件 gRPC 服务器或启用一个由你的云服务驱动提供的 KMS 插件。
你可以将这个插件与远程 KMS 集成，并把它部署到 Kubernetes 的主服务器上。</p>
<!--
### Enabling the KMS supported by your cloud provider 
Refer to your cloud provider for instructions on enabling the cloud provider-specific KMS plugin.
-->
<h3 id="启用由云服务驱动支持的-kms">启用由云服务驱动支持的 KMS</h3>
<p>有关启用云服务驱动特定的 KMS 插件的说明，请咨询你的云服务驱动商。</p>
<!--
### Developing a KMS plugin gRPC server

You can develop a KMS plugin gRPC server using a stub file available for Go. For other languages, you use a proto file to create a stub file that you can use to develop the gRPC server code.
-->
<h3 id="开发-kms-插件-grpc-服务器">开发 KMS 插件 gRPC 服务器</h3>
<p>你可以使用 Go 语言的存根文件开发 KMS 插件 gRPC 服务器。
对于其他语言，你可以用 proto 文件创建可以用于开发 gRPC 服务器代码的存根文件。</p>
<!--
* Using Go: Use the functions and data structures in the stub file:
[service.pb.go](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.pb.go) to develop the gRPC server code
-->
<ul>
<li>使用 Go：使用存根文件 <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.pb.go">service.pb.go</a>
中的函数和数据结构开发 gRPC 服务器代码。</li>
</ul>
<!--
* Using languages other than Go: Use the protoc compiler with the proto file: [service.proto](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.proto) to generate a stub file for the specific language
-->
<ul>
<li>使用 Go 以外的其他语言：用 protoc 编译器编译 proto 文件：
<a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.proto">service.proto</a>
为指定语言生成存根文件。</li>
</ul>
<!--
Then use the functions and data structures in the stub file to develop the server code.
-->
<p>然后使用存根文件中的函数和数据结构开发服务器代码。</p>
<!-- **Notes:** -->
<p><strong>注意：</strong></p>
<!--
* kms plugin version: `v1beta1`

  In response to procedure call Version, a compatible KMS plugin should return v1beta1 as VersionResponse.version.

* message version: `v1beta1`

  All messages from KMS provider have the version field set to current version v1beta1.

* protocol: UNIX domain socket (`unix`)

  The gRPC server should listen at UNIX domain socket.
-->
<ul>
<li>
<p>kms 插件版本：<code>v1beta1</code></p>
<p>作为对过程调用 Version 的响应，兼容的 KMS 插件应把 v1beta1 作为 VersionResponse.version 返回</p>
</li>
<li>
<p>消息版本：<code>v1beta1</code></p>
<p>所有来自 KMS 驱动的消息都把 version 字段设置为当前版本 v1beta1</p>
</li>
<li>
<p>协议：UNIX 域套接字 (<code>unix</code>)</p>
<p>gRPC 服务器应监听 UNIX 域套接字</p>
</li>
</ul>
<!--
### Integrating a KMS plugin with the remote KMS

The KMS plugin can communicate with the remote KMS using any protocol supported by the KMS.
All configuration data, including authentication credentials the KMS plugin uses to communicate with the remote KMS, 
are stored and managed by the KMS plugin independently. The KMS plugin can encode the ciphertext with additional metadata that may be required before sending it to the KMS for decryption.
-->
<h3 id="将-kms-插件与远程-kms-整合">将 KMS 插件与远程 KMS 整合</h3>
<p>KMS 插件可以用任何受 KMS 支持的协议与远程 KMS 通信。
所有的配置数据，包括 KMS 插件用于与远程 KMS 通信的认证凭据，都由 KMS 插件独立地存储和管理。
KMS 插件可以用额外的元数据对密文进行编码，这些元数据是在把它发往 KMS 进行解密之前可能要用到的。</p>
<!--
### Deploying the KMS plugin 

Ensure that the KMS plugin runs on the same host(s) as the Kubernetes master(s).
-->
<h3 id="部署-kms-插件">部署 KMS 插件</h3>
<p>确保 KMS 插件与 Kubernetes 主服务器运行在同一主机上。</p>
<!--
## Encrypting your data with the KMS provider

To encrypt the data:
-->
<h2 id="使用-kms-驱动加密数据">使用 KMS 驱动加密数据</h2>
<p>为了加密数据：</p>
<!--
1. Create a new encryption configuration file using the appropriate properties for the `kms` provider:
-->
<ol>
<li>
<p>使用 <code>kms</code> 驱动的相应的属性创建一个新的加密配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myKmsPlugin<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile.sock<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">timeout</span>:<span style="color:#bbb"> </span>3s<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2. Set the `--encryption-provider-config` flag on the kube-apiserver to point to the location of the configuration file.
3. Restart your API server.
-->
<ol start="2">
<li>设置 kube-apiserver 的 <code>--encryption-provider-config</code> 参数指向配置文件的位置。</li>
<li>重启 API 服务器。</li>
</ol>
<!--
## Verifying that the data is encrypted

Data is encrypted when written to etcd. After restarting your kube-apiserver, any newly created or updated secret should be encrypted when stored. To verify, you can use the etcdctl command line program to retrieve the contents of your secret.
-->
<h2 id="验证数据已经加密">验证数据已经加密</h2>
<p>写入 etcd 时数据被加密。重启 kube-apiserver 后，任何新建或更新的 Secret 在存储时应该已被加密。
要验证这点，你可以用 etcdctl 命令行程序获取 Secret 内容。</p>
<!--
1. Create a new secret called secret1 in the default namespace:
-->
<ol>
<li>
<p>在默认的命名空间里创建一个名为 secret1 的 Secret：</p>
<pre tabindex="0"><code>kubectl create secret generic secret1 -n default --from-literal=mykey=mydata
</code></pre></li>
</ol>
<!--
2. Using the etcdctl command line, read that secret out of etcd:
-->
<ol start="2">
<li>
<p>用 etcdctl 命令行，从 etcd 读取出 Secret：</p>
<pre tabindex="0"><code>ETCDCTL_API=3 etcdctl get /kubernetes.io/secrets/default/secret1 [...] | hexdump -C
</code></pre><!--
where `[...]` must be the additional arguments for connecting to the etcd server.
-->
<p>其中 <code>[...]</code> 是用于连接 etcd 服务器的额外参数。</p>
</li>
</ol>
<!--
3. Verify the stored secret is prefixed with `k8s:enc:kms:v1:`, which indicates that the `kms` provider has encrypted the resulting data.
-->
<ol start="3">
<li>验证保存的 Secret 是否是以 <code>k8s:enc:kms:v1:</code> 开头的，这表明 <code>kms</code> 驱动已经对结果数据加密。</li>
</ol>
<!--
4. Verify that the secret is correctly decrypted when retrieved via the API:
-->
<ol start="4">
<li>
<p>验证 Secret 在被 API 获取时已被正确解密：</p>
<pre tabindex="0"><code>kubectl describe secret secret1 -n default
</code></pre><p>结果应该是 <code>mykey: mydata</code>。</p>
</li>
</ol>
<!--
## Ensuring all secrets are encrypted

Because secrets are encrypted on write, performing an update on a secret encrypts that content.
-->
<h2 id="确保所有-secret-都已被加密">确保所有 Secret 都已被加密</h2>
<p>因为 Secret 是在写入时被加密的，所以在更新 Secret 时也会加密该内容。</p>
<!--
The following command reads all secrets and then updates them to apply server side encryption. If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.
-->
<p>下列命令读取所有 Secret 并更新它们以便应用服务器端加密。如果因为写入冲突导致错误发生，
请重试此命令。对较大的集群，你可能希望根据命名空间或脚本更新去细分 Secret 内容。</p>
<pre tabindex="0"><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre><!--
## Switching from a local encryption provider to the KMS provider

To switch from a local encryption provider to the `kms` provider and re-encrypt all of the secrets:
-->
<h2 id="从本地加密驱动切换到-kms-驱动">从本地加密驱动切换到 KMS 驱动</h2>
<p>为了从本地加密驱动切换到 <code>kms</code> 驱动并重新加密所有 Secret 内容：</p>
<!--
1. Add the `kms` provider as the first entry in the configuration file as shown in the following example.
-->
<ol>
<li>
<p>在配置文件中加入 <code>kms</code> 驱动作为第一个条目，如下列样例所示</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name </span>:<span style="color:#bbb"> </span>myKmsPlugin<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile.sock<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">         </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">         </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">           </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2. Restart all kube-apiserver processes.
3. Run the following command to force all secrets to be re-encrypted using the `kms` provider.
-->
<ol start="2">
<li>
<p>重启所有 kube-apiserver 进程。</p>
</li>
<li>
<p>运行下列命令使用 <code>kms</code> 驱动强制重新加密所有 Secret。</p>
<pre tabindex="0"><code>kubectl get secrets --all-namespaces -o json| kubectl replace -f -
</code></pre></li>
</ol>
<!--
## Disabling encryption at rest
To disable encryption at rest:
-->
<h2 id="禁用静态数据加密">禁用静态数据加密</h2>
<p>要禁用静态数据加密：</p>
<!--
1. Place the `identity` provider as the first entry in the configuration file:
-->
<ol>
<li>
<p>将 <code>identity</code> 驱动作为配置文件中的第一个条目：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name </span>:<span style="color:#bbb"> </span>myKmsPlugin<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">endpoint</span>:<span style="color:#bbb"> </span>unix:///tmp/socketfile.sock<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cachesize</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2.  Restart all kube-apiserver processes.
3. Run the following command to force all secrets to be decrypted.
-->
<ol start="2">
<li>
<p>重启所有 kube-apiserver 进程。</p>
</li>
<li>
<p>运行下列命令强制重新加密所有 Secret。</p>
<pre tabindex="0"><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e77685d5b88d2db5c7631a27b9472eea">2.13 - 使用 Kubernetes API 访问集群</h1>
    
	<!--
title: Access Clusters Using the Kubernetes API
content_type: task
-->
<!-- overview -->
<!--
This page shows how to access clusters using the Kubernetes API.
-->
<p>本页展示了如何使用 Kubernetes API 访问集群</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Accessing the cluster API

### Accessing for the first time with kubectl
-->
<h2 id="访问集群-api">访问集群 API</h2>
<h3 id="使用-kubectl-进行首次访问">使用 kubectl 进行首次访问</h3>
<!--
When accessing the Kubernetes API for the first time, use the
Kubernetes command-line tool, `kubectl`.
-->
<p>首次访问 Kubernetes API 时，请使用 Kubernetes 命令行工具 <code>kubectl</code> 。</p>
<!--
To access a cluster, you need to know the location of the cluster and have credentials
to access it. Typically, this is automatically set-up when you work through
a [Getting started guide](/docs/setup/),
or someone else setup the cluster and provided you with credentials and a location.
-->
<p>要访问集群，你需要知道集群位置并拥有访问它的凭证。
通常，当你完成<a href="/zh/docs/setup/">入门指南</a>时，这会自动设置完成，或者由其他人设置好集群并将凭证和位置提供给你。</p>
<!--
Check the location and credentials that kubectl knows about with this command:
-->
<p>使用此命令检查 kubectl 已知的位置和凭证：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><!--
Many of the [examples](https://github.com/kubernetes/examples/tree/master/) provide an introduction to using
kubectl. Complete documentation is found in the [kubectl manual](/docs/reference/kubectl/overview/).
-->
<p>许多<a href="https://github.com/kubernetes/examples/tree/master/">样例</a>
提供了使用 kubectl 的介绍。完整文档请见 <a href="/zh/docs/reference/kubectl/overview/">kubectl 手册</a>。</p>
<!--
### Directly accessing the REST API

kubectl handles locating and authenticating to the API server. If you want to directly access the REST API with an http client like
`curl` or `wget`, or a browser, there are multiple ways you can locate and authenticate against the API server:
-->
<h3 id="直接访问-rest-api">直接访问 REST API</h3>
<p>kubectl 处理对 API 服务器的定位和身份验证。如果你想通过 http 客户端（如 <code>curl</code> 或 <code>wget</code>，或浏览器）直接访问 REST API，你可以通过多种方式对 API 服务器进行定位和身份验证：</p>
 <!--
1. Run kubectl in proxy mode (recommended). This method is recommended, since it uses the stored apiserver location and verifies the identity of the API server using a self-signed cert. No man-in-the-middle (MITM) attack is possible using this method.
 1. Alternatively, you can provide the location and credentials directly to the http client. This works with client code that is confused by proxies. To protect against man in the middle attacks, you'll need to import a root cert into your browser.
-->
<ol>
<li>以代理模式运行 kubectl（推荐）。
推荐使用此方法，因为它用存储的 apiserver 位置并使用自签名证书验证 API 服务器的标识。
使用这种方法无法进行中间人（MITM）攻击。</li>
<li>另外，你可以直接为 HTTP 客户端提供位置和身份认证。
这适用于被代理混淆的客户端代码。
为防止中间人攻击，你需要将根证书导入浏览器。</li>
</ol>
<!--
Using the Go or Python client libraries provides accessing kubectl in proxy mode.
-->
<p>使用 Go 或 Python 客户端库可以在代理模式下访问 kubectl。</p>
<!--
#### Using kubectl proxy

The following command runs kubectl in a mode where it acts as a reverse proxy. It handles
locating the API server and authenticating.
-->
<h4 id="使用-kubectl-代理">使用 kubectl 代理</h4>
<p>下列命令使 kubectl 运行在反向代理模式下。它处理 API 服务器的定位和身份认证。</p>
<!-- Run it like this: -->
<p>像这样运行它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span> &amp;
</code></pre></div><!--
See [kubectl proxy](/docs/reference/generated/kubectl/kubectl-commands/#proxy) for more details.
-->
<p>参见 <a href="/docs/reference/generated/kubectl/kubectl-commands/#proxy">kubectl 代理</a> 获取更多细节。</p>
<!--
Then you can explore the API with curl, wget, or a browser, like so:
-->
<p>然后你可以通过 curl，wget，或浏览器浏览 API，像这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl http://localhost:8080/api/
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><!--
#### Without kubectl proxy

It is possible to avoid using kubectl proxy by passing an authentication token
directly to the API server, like this:

Using `grep/cut` approach:
-->
<h4 id="不使用-kubectl-代理">不使用 kubectl 代理</h4>
<p>通过将身份认证令牌直接传给 API 服务器，可以避免使用 kubectl 代理，像这样：</p>
<p>使用 <code>grep/cut</code> 方式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 查看所有的集群，因为你的 .kubeconfig 文件中可能包含多个上下文</span>
kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{&#34;Cluster name\tServer\n&#34;}{range .clusters[*]}{.name}{&#34;\t&#34;}{.cluster.server}{&#34;\n&#34;}{end}&#39;</span>

<span style="color:#080;font-style:italic"># 从上述命令输出中选择你要与之交互的集群的名称</span>
<span style="color:#a2f">export</span> <span style="color:#b8860b">CLUSTER_NAME</span><span style="color:#666">=</span><span style="color:#b44">&#34;some_server_name&#34;</span>

<span style="color:#080;font-style:italic"># 指向引用该集群名称的 API 服务器</span>
<span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#34;{.clusters[?(@.name==\&#34;</span><span style="color:#b8860b">$CLUSTER_NAME</span><span style="color:#b44">\&#34;)].cluster.server}&#34;</span><span style="color:#a2f;font-weight:bold">)</span>

<span style="color:#080;font-style:italic"># 获得令牌</span>
<span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get secrets -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#34;{.items[?(@.metadata.annotations[&#39;kubernetes\.io/service-account\.name&#39;]==&#39;default&#39;)].data.token}&#34;</span>|base64 -d<span style="color:#a2f;font-weight:bold">)</span>

<span style="color:#080;font-style:italic"># 使用令牌玩转 API</span>
curl -X GET <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">&#34;Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">&#34;</span> --insecure
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;APIVersions&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><!-- Using `jsonpath` approach: -->
<p>使用 <code>jsonpath</code> 方式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl config view --minify -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.clusters[0].cluster.server}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get secret <span style="color:#a2f;font-weight:bold">$(</span>kubectl get serviceaccount default -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.secrets[0].name}&#39;</span><span style="color:#a2f;font-weight:bold">)</span> -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.data.token}&#39;</span> | base64 --decode <span style="color:#a2f;font-weight:bold">)</span>
curl <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">&#34;Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">&#34;</span> --insecure
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;APIVersions&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><!--
The above example uses the `--insecure` flag. This leaves it subject to MITM
attacks. When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server. (These are installed in the
`~/.kube` directory). Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.
-->
<p>上面例子使用了 <code>--insecure</code> 标志位。这使它易受到 MITM 攻击。
当 kubectl 访问集群时，它使用存储的根证书和客户端证书访问服务器。
（已安装在 <code>~/.kube</code> 目录下）。
由于集群认证通常是自签名的，因此可能需要特殊设置才能让你的 http 客户端使用根证书。</p>
<!--
On some clusters, the API server does not require authentication; it may serve
on localhost, or be protected by a firewall. There is not a standard
for this. [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access)
describes how you can configure this as a cluster administrator.
-->
<p>在一些集群中，API 服务器不需要身份认证；它运行在本地，或由防火墙保护着。
对此并没有一个标准。
<a href="/zh/docs/concepts/security/controlling-access/">配置对 API 的访问</a>
讲解了作为集群管理员可如何对此进行配置。</p>
<!--
### Programmatic access to the API

Kubernetes officially supports client libraries for [Go](#go-client), [Python](#python-client), [Java](#java-client), [dotnet](#dotnet-client), [Javascript](#javascript-client), and [Haskell](#haskell-client). There are other client libraries that are provided and maintained by their authors, not the Kubernetes team. See [client libraries](/docs/reference/using-api/client-libraries/) for accessing the API from other languages and how they authenticate.
-->
<h3 id="编程方式访问-api">编程方式访问 API</h3>
<p>Kubernetes 官方支持 <a href="#go-client">Go</a>、<a href="#python-client">Python</a>、<a href="#java-client">Java</a>、
<a href="#dotnet-client">dotnet</a>、<a href="#javascript-client">Javascript</a> 和 <a href="#haskell-client">Haskell</a>
语言的客户端库。还有一些其他客户端库由对应作者而非 Kubernetes 团队提供并维护。
参考<a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>了解如何使用其他语言
来访问 API 以及如何执行身份认证。</p>
<!-- #### Go client -->
<h4 id="go-client">Go 客户端 </h4>
<!--
* To get the library, run the following command: `go get k8s.io/client-go@kubernetes-<kubernetes-version-number>` See [https://github.com/kubernetes/client-go/releases](https://github.com/kubernetes/client-go/releases) to see which versions are supported.
* Write an application atop of the client-go clients.
-->
<ul>
<li>要获取库，运行下列命令：<code>go get k8s.io/client-go/kubernetes-&lt;kubernetes 版本号&gt;</code>，
参见 <a href="https://github.com/kubernetes/client-go/releases">https://github.com/kubernetes/client-go/releases</a> 查看受支持的版本。</li>
<li>基于 client-go 客户端编写应用程序。</li>
</ul>
<!--
Note that client-go defines its own API objects, so if needed, please import API definitions from client-go rather than from the main repository, e.g., `import "k8s.io/client-go/kubernetes"` is correct.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 注意 client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是主仓库导入
API 定义，例如 <code>import &quot;k8s.io/client-go/kubernetes&quot;</code> 是正确做法。</div>
</blockquote>
<!--
The Go client can use the same [kubeconfig file](/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go):
-->
<p>Go 客户端可以使用与 kubectl 命令行工具相同的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
定位和验证 API 服务器。参见这个
<a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">例子</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-golang" data-lang="golang"><span style="color:#a2f;font-weight:bold">package</span> main

<span style="color:#a2f;font-weight:bold">import</span> (
   <span style="color:#b44">&#34;context&#34;</span>
   <span style="color:#b44">&#34;fmt&#34;</span>
   <span style="color:#b44">&#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;</span>
   <span style="color:#b44">&#34;k8s.io/client-go/kubernetes&#34;</span>
   <span style="color:#b44">&#34;k8s.io/client-go/tools/clientcmd&#34;</span>
)

<span style="color:#a2f;font-weight:bold">func</span> <span style="color:#00a000">main</span>() {
  <span style="color:#080;font-style:italic">// uses the current context in kubeconfig
</span><span style="color:#080;font-style:italic"></span>  <span style="color:#080;font-style:italic">// path-to-kubeconfig -- for example, /root/.kube/config
</span><span style="color:#080;font-style:italic"></span>  config, _ <span style="color:#666">:=</span> clientcmd.<span style="color:#00a000">BuildConfigFromFlags</span>(<span style="color:#b44">&#34;&#34;</span>, <span style="color:#b44">&#34;&lt;path-to-kubeconfig&gt;&#34;</span>)
  <span style="color:#080;font-style:italic">// creates the clientset
</span><span style="color:#080;font-style:italic"></span>  clientset, _ <span style="color:#666">:=</span> kubernetes.<span style="color:#00a000">NewForConfig</span>(config)
  <span style="color:#080;font-style:italic">// access the API to list pods
</span><span style="color:#080;font-style:italic"></span>  pods, _ <span style="color:#666">:=</span> clientset.<span style="color:#00a000">CoreV1</span>().<span style="color:#00a000">Pods</span>(<span style="color:#b44">&#34;&#34;</span>).<span style="color:#00a000">List</span>(context.<span style="color:#00a000">TODO</span>(), v1.ListOptions{})
  fmt.<span style="color:#00a000">Printf</span>(<span style="color:#b44">&#34;There are %d pods in the cluster\n&#34;</span>, <span style="color:#a2f">len</span>(pods.Items))
}
</code></pre></div><!--
If the application is deployed as a Pod in the cluster, see [Accessing the API from within a Pod](/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod).
-->
<p>如果该应用程序部署为集群中的一个
Pod，请参阅<a href="/zh/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod">从 Pod 内访问 API</a>。</p>
<!-- #### Python client -->
<h4 id="python-client">Python 客户端</h4>
<!--
To use [Python client](https://github.com/kubernetes-client/python), run the following command: `pip install kubernetes` See [Python Client Library page](https://github.com/kubernetes-client/python) for more installation options.
-->
<p>要使用 <a href="https://github.com/kubernetes-client/python">Python 客户端</a>，运行下列命令：
<code>pip install kubernetes</code>。
参见 <a href="https://github.com/kubernetes-client/python">Python 客户端库主页</a> 了解更多安装选项。</p>
<!--
The Python client can use the same [kubeconfig file](/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/python/blob/master/examples/out_of_cluster_config.py):
-->
<p>Python 客户端可以使用与 kubectl 命令行工具相同的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
定位和验证 API 服务器。参见这个
<a href="https://github.com/kubernetes-client/python/blob/master/examples/out_of_cluster_config.py">例子</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">kubernetes</span> <span style="color:#a2f;font-weight:bold">import</span> client, config

config<span style="color:#666">.</span>load_kube_config()

v1<span style="color:#666">=</span>client<span style="color:#666">.</span>CoreV1Api()
<span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Listing pods with their IPs:&#34;</span>)
ret <span style="color:#666">=</span> v1<span style="color:#666">.</span>list_pod_for_all_namespaces(watch<span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">False</span>)
<span style="color:#a2f;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> ret<span style="color:#666">.</span>items:
    <span style="color:#a2f">print</span>(<span style="color:#b44">&#34;</span><span style="color:#b68;font-weight:bold">%s</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b68;font-weight:bold">%s</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b68;font-weight:bold">%s</span><span style="color:#b44">&#34;</span> <span style="color:#666">%</span> (i<span style="color:#666">.</span>status<span style="color:#666">.</span>pod_ip, i<span style="color:#666">.</span>metadata<span style="color:#666">.</span>namespace, i<span style="color:#666">.</span>metadata<span style="color:#666">.</span>name))
</code></pre></div><!-- #### Java client -->
<h4 id="java-client">Java 客户端   </h4>
<!--
To install the [Java Client](https://github.com/kubernetes-client/java), run:
-->
<p>要安装 <a href="https://github.com/kubernetes-client/java">Java 客户端</a>，运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 克隆 Java 库</span>
git clone --recursive https://github.com/kubernetes-client/java

<span style="color:#080;font-style:italic"># 安装项目文件、POM 等</span>
<span style="color:#a2f">cd</span> java
mvn install
</code></pre></div><!--
See [https://github.com/kubernetes-client/java/releases](https://github.com/kubernetes-client/java/releases) to see which versions are supported.

The Java client can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java):
-->
<p>参阅<a href="https://github.com/kubernetes-client/java/releases">https://github.com/kubernetes-client/java/releases</a>
了解当前支持的版本。</p>
<p>Java 客户端可以使用 kubectl 命令行所使用的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
以定位 API 服务器并向其认证身份。
参看此<a href="https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java">示例</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java"><span style="color:#a2f;font-weight:bold">package</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.examples</span><span style="color:#666">;</span>

<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.ApiClient</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.ApiException</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.Configuration</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.apis.CoreV1Api</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.models.V1Pod</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.models.V1PodList</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.util.ClientBuilder</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io.kubernetes.client.util.KubeConfig</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">java.io.FileReader</span><span style="color:#666">;</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">java.io.IOException</span><span style="color:#666">;</span>

<span style="color:#080;font-style:italic">/**
</span><span style="color:#080;font-style:italic"> * A simple example of how to use the Java API from an application outside a kubernetes cluster
</span><span style="color:#080;font-style:italic"> *
</span><span style="color:#080;font-style:italic"> * &lt;p&gt;Easiest way to run this: mvn exec:java
</span><span style="color:#080;font-style:italic"> * -Dexec.mainClass=&#34;io.kubernetes.client.examples.KubeConfigFileClientExample&#34;
</span><span style="color:#080;font-style:italic"> *
</span><span style="color:#080;font-style:italic"> */</span>
<span style="color:#a2f;font-weight:bold">public</span> <span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">KubeConfigFileClientExample</span> <span style="color:#666">{</span>
  <span style="color:#a2f;font-weight:bold">public</span> <span style="color:#a2f;font-weight:bold">static</span> <span style="color:#0b0;font-weight:bold">void</span> <span style="color:#00a000">main</span><span style="color:#666">(</span>String<span style="color:#666">[]</span> args<span style="color:#666">)</span> <span style="color:#a2f;font-weight:bold">throws</span> IOException<span style="color:#666">,</span> ApiException <span style="color:#666">{</span>

    <span style="color:#080;font-style:italic">// file path to your KubeConfig
</span><span style="color:#080;font-style:italic"></span>    String kubeConfigPath <span style="color:#666">=</span> <span style="color:#b44">&#34;~/.kube/config&#34;</span><span style="color:#666">;</span>

    <span style="color:#080;font-style:italic">// loading the out-of-cluster config, a kubeconfig from file-system
</span><span style="color:#080;font-style:italic"></span>    ApiClient client <span style="color:#666">=</span>
        ClientBuilder<span style="color:#666">.</span><span style="color:#b44">kubeconfig</span><span style="color:#666">(</span>KubeConfig<span style="color:#666">.</span><span style="color:#b44">loadKubeConfig</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:bold">new</span> FileReader<span style="color:#666">(</span>kubeConfigPath<span style="color:#666">))).</span><span style="color:#b44">build</span><span style="color:#666">();</span>

    <span style="color:#080;font-style:italic">// set the global default api-client to the in-cluster one from above
</span><span style="color:#080;font-style:italic"></span>    Configuration<span style="color:#666">.</span><span style="color:#b44">setDefaultApiClient</span><span style="color:#666">(</span>client<span style="color:#666">);</span>

    <span style="color:#080;font-style:italic">// the CoreV1Api loads default api-client from global configuration.
</span><span style="color:#080;font-style:italic"></span>    CoreV1Api api <span style="color:#666">=</span> <span style="color:#a2f;font-weight:bold">new</span> CoreV1Api<span style="color:#666">();</span>

    <span style="color:#080;font-style:italic">// invokes the CoreV1Api client
</span><span style="color:#080;font-style:italic"></span>    V1PodList list <span style="color:#666">=</span> api<span style="color:#666">.</span><span style="color:#b44">listPodForAllNamespaces</span><span style="color:#666">(</span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">,</span> <span style="color:#a2f;font-weight:bold">null</span><span style="color:#666">);</span>
    System<span style="color:#666">.</span><span style="color:#b44">out</span><span style="color:#666">.</span><span style="color:#b44">println</span><span style="color:#666">(</span><span style="color:#b44">&#34;Listing all pods: &#34;</span><span style="color:#666">);</span>
    <span style="color:#a2f;font-weight:bold">for</span> <span style="color:#666">(</span>V1Pod item <span style="color:#666">:</span> list<span style="color:#666">.</span><span style="color:#b44">getItems</span><span style="color:#666">())</span> <span style="color:#666">{</span>
      System<span style="color:#666">.</span><span style="color:#b44">out</span><span style="color:#666">.</span><span style="color:#b44">println</span><span style="color:#666">(</span>item<span style="color:#666">.</span><span style="color:#b44">getMetadata</span><span style="color:#666">().</span><span style="color:#b44">getName</span><span style="color:#666">());</span>
    <span style="color:#666">}</span>
  <span style="color:#666">}</span>
<span style="color:#666">}</span>
</code></pre></div><!--
#### dotnet client

To use [dotnet client](https://github.com/kubernetes-client/csharp), run the following command: `dotnet add package KubernetesClient --version 1.6.1` See [dotnet Client Library page](https://github.com/kubernetes-client/csharp) for more installation options. See [https://github.com/kubernetes-client/csharp/releases](https://github.com/kubernetes-client/csharp/releases) to see which versions are supported.

The dotnet client can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/csharp/blob/master/examples/simple/PodList.cs):
-->
<h4 id="dotnet-client">.Net 客户端   </h4>
<p>要使用<a href="https://github.com/kubernetes-client/csharp">.Net 客户端</a>，运行下面的命令：
<code>dotnet add package KubernetesClient --version 1.6.1</code>。
参见<a href="https://github.com/kubernetes-client/csharp">.Net 客户端库页面</a>了解更多安装选项。
关于可支持的版本，参见<a href="https://github.com/kubernetes-client/csharp/releases">https://github.com/kubernetes-client/csharp/releases</a>。</p>
<p>.Net 客户端可以使用与 kubectl CLI 相同的 <a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
来定位并验证 API 服务器。
参见<a href="https://github.com/kubernetes-client/csharp/blob/master/examples/simple/PodList.cs">样例</a>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#a2f;font-weight:bold">using</span> <span style="color:#00f;font-weight:bold">System</span>;
<span style="color:#a2f;font-weight:bold">using</span> <span style="color:#00f;font-weight:bold">k8s</span>;

<span style="color:#a2f;font-weight:bold">namespace</span> <span style="color:#00f;font-weight:bold">simple</span>
{
    <span style="color:#a2f;font-weight:bold">internal</span> <span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">PodList</span>
    {
        <span style="color:#a2f;font-weight:bold">private</span> <span style="color:#a2f;font-weight:bold">static</span> <span style="color:#a2f;font-weight:bold">void</span> Main(<span style="color:#0b0;font-weight:bold">string</span>[] args)
        {
            <span style="color:#0b0;font-weight:bold">var</span> config = KubernetesClientConfiguration.BuildDefaultConfig();
            IKubernetes client = <span style="color:#a2f;font-weight:bold">new</span> Kubernetes(config);
            Console.WriteLine(<span style="color:#b44">&#34;Starting Request!&#34;</span>);

            <span style="color:#0b0;font-weight:bold">var</span> list = client.ListNamespacedPod(<span style="color:#b44">&#34;default&#34;</span>);
            <span style="color:#a2f;font-weight:bold">foreach</span> (<span style="color:#0b0;font-weight:bold">var</span> item <span style="color:#a2f;font-weight:bold">in</span> list.Items)
            {
                Console.WriteLine(item.Metadata.Name);
            }
            <span style="color:#a2f;font-weight:bold">if</span> (list.Items.Count == <span style="color:#666">0</span>)
            {
                Console.WriteLine(<span style="color:#b44">&#34;Empty!&#34;</span>);
            }
        }
    }
}
</code></pre></div><!--
#### JavaScript client

To install [JavaScript client](https://github.com/kubernetes-client/javascript), run the following command: `npm install @kubernetes/client-node`. See [https://github.com/kubernetes-client/javascript/releases](https://github.com/kubernetes-client/javascript/releases) to see which versions are supported.

The JavaScript client can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/javascript/blob/master/examples/example.js):
-->
<h4 id="javascript-client">JavaScript 客户端   </h4>
<p>要安装 <a href="https://github.com/kubernetes-client/javascript">JavaScript 客户端</a>，运行下面的命令：
<code>npm install @kubernetes/client-node</code>。
参考<a href="https://github.com/kubernetes-client/javascript/releases">https://github.com/kubernetes-client/javascript/releases</a>了解可支持的版本。</p>
<p>JavaScript 客户端可以使用 kubectl 命令行所使用的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
以定位 API 服务器并向其认证身份。
参见<a href="https://github.com/kubernetes-client/javascript/blob/master/examples/example.js">此例</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#a2f;font-weight:bold">const</span> k8s <span style="color:#666">=</span> require(<span style="color:#b44">&#39;@kubernetes/client-node&#39;</span>);

<span style="color:#a2f;font-weight:bold">const</span> kc <span style="color:#666">=</span> <span style="color:#a2f;font-weight:bold">new</span> k8s.KubeConfig();
kc.loadFromDefault();

<span style="color:#a2f;font-weight:bold">const</span> k8sApi <span style="color:#666">=</span> kc.makeApiClient(k8s.CoreV1Api);

k8sApi.listNamespacedPod(<span style="color:#b44">&#39;default&#39;</span>).then((res) =&gt; {
    console.log(res.body);
});
</code></pre></div><!--
#### Haskell client

See [https://github.com/kubernetes-client/haskell/releases](https://github.com/kubernetes-client/haskell/releases) to see which versions are supported.

The [Haskell client](https://github.com/kubernetes-client/haskell) can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the API server. See this [example](https://github.com/kubernetes-client/haskell/blob/master/kubernetes-client/example/App.hs):
-->
<h4 id="haskell-client">Haskell 客户端   </h4>
<p>参考 <a href="https://github.com/kubernetes-client/haskell/releases">https://github.com/kubernetes-client/haskell/releases</a> 了解支持的版本。</p>
<p><a href="https://github.com/kubernetes-client/haskell">Haskell 客户端</a>
可以使用 kubectl 命令行所使用的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
以定位 API 服务器并向其认证身份。
参见<a href="https://github.com/kubernetes-client/haskell/blob/master/kubernetes-client/example/App.hs">此例</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#00a000">exampleWithKubeConfig</span> <span style="color:#a2f;font-weight:bold">::</span> <span style="color:#0b0;font-weight:bold">IO</span> <span style="color:#a2f">()</span>
<span style="color:#00a000">exampleWithKubeConfig</span> <span style="color:#a2f;font-weight:bold">=</span> <span style="color:#a2f;font-weight:bold">do</span>
    oidcCache <span style="color:#a2f;font-weight:bold">&lt;-</span> atomically <span style="color:#666">$</span> newTVar <span style="color:#666">$</span> <span style="color:#0b0;font-weight:bold">Map</span><span style="color:#666">.</span>fromList <span style="color:#0b0;font-weight:bold">[]</span>
    (mgr, kcfg) <span style="color:#a2f;font-weight:bold">&lt;-</span> mkKubeClientConfig oidcCache <span style="color:#666">$</span> <span style="color:#0b0;font-weight:bold">KubeConfigFile</span> <span style="color:#b44">&#34;/path/to/kubeconfig&#34;</span>
    dispatchMime
            mgr
            kcfg
            (<span style="color:#0b0;font-weight:bold">CoreV1</span><span style="color:#666">.</span>listPodForAllNamespaces (<span style="color:#0b0;font-weight:bold">Accept</span> <span style="color:#0b0;font-weight:bold">MimeJSON</span>))
        <span style="color:#666">&gt;&gt;=</span> print
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* [Accessing the Kubernetes API from a Pod](/docs/tasks/run-application/access-api-from-pod/)
-->
<ul>
<li><a href="/zh/docs/tasks/run-application/access-api-from-pod/">从 Pod 中访问 API</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-12001be83d15fcd7f3242313a55777df">2.14 - 保护集群安全</h1>
    
	<!--
reviewers:
- smarterclayton
- liggitt
- ericchiang
- destijl
title: Securing a Cluster
content_type: task
-->
<!-- overview -->
<!--
This document covers topics related to protecting a cluster from accidental or malicious access
and provides recommendations on overall security.
-->
<p>本文档涉及与保护集群免受意外或恶意访问有关的主题，并对总体安全性提出建议。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!-- steps -->
<!--
## Controlling access to the Kubernetes API

As Kubernetes is entirely API driven, controlling and limiting who can access the cluster and what actions
they are allowed to perform is the first line of defense.
-->
<h2 id="控制对-kubernetes-api-的访问">控制对 Kubernetes API 的访问</h2>
<p>因为 Kubernetes 是完全通过 API 驱动的，所以，控制和限制谁可以通过 API 访问集群，以及允许这些访问者执行什么样的 API 动作，就成为了安全控制的第一道防线。</p>
<!--
### Use Transport Layer Security (TLS) for all API traffic

Kubernetes expects that all API communication in the cluster is encrypted by default with TLS, and the
majority of installation methods will allow the necessary certificates to be created and distributed to
the cluster components. Note that some components and installation methods may enable local ports over
HTTP and administrators should familiarize themselves with the settings of each component to identify
potentially unsecured traffic.
-->
<h3 id="为所有-api-交互使用传输层安全-tls">为所有 API 交互使用传输层安全 （TLS）</h3>
<p>Kubernetes 期望集群中所有的 API 通信在默认情况下都使用 TLS 加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。请注意，某些组件和安装方法可能使用 HTTP 来访问本地端口， 管理员应该熟悉每个组件的设置，以识别潜在的不安全的流量。</p>
<!--
### API Authentication

Choose an authentication mechanism for the API servers to use that matches the common access patterns
when you install a cluster. For instance, small single user clusters may wish to use a simple certificate
or static Bearer token approach. Larger clusters may wish to integrate an existing OIDC or LDAP server that
allow users to be subdivided into groups.

All API clients must be authenticated, even those that are part of the infrastructure like nodes,
proxies, the scheduler, and volume plugins. These clients are typically [service accounts](/docs/reference/access-authn-authz/service-accounts-admin/) or use x509 client certificates, and they are created automatically at cluster startup or are setup as part of the cluster installation.

Consult the [authentication reference document](/docs/reference/access-authn-authz/authentication/) for more information.
-->
<h3 id="api-认证">API 认证</h3>
<p>安装集群时，选择一个 API 服务器的身份验证机制，去使用与之匹配的公共访问模式。
例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。
更大的集群则可能希望整合现有的、OIDC、LDAP 等允许用户分组的服务器。</p>
<p>所有 API 客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。
这些客户端通常使用 <a href="/zh/docs/reference/access-authn-authz/service-accounts-admin/">服务帐户</a>
或 X509 客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置。</p>
<p>如果你希望获取更多信息，请参考<a href="/zh/docs/reference/access-authn-authz/authentication/">认证参考文档</a>。</p>
<!--
### API Authorization

Once authenticated, every API call is also expected to pass an authorization check. Kubernetes ships
an integrated [Role-Based Access Control (RBAC)](/docs/reference/access-authn-authz/rbac/) component that matches an incoming user or group to a
set of permissions bundled into roles. These permissions combine verbs (get, create, delete) with
resources (pods, services, nodes) and can be namespace or cluster scoped. A set of out of the box
roles are provided that offer reasonable default separation of responsibility depending on what
actions a client might want to perform. It is recommended that you use the [Node](/docs/reference/access-authn-authz/node/) and [RBAC](/docs/reference/access-authn-authz/rbac/) authorizers together, in combination with the
[NodeRestriction](/docs/reference/access-authn-authz/admission-controllers/#noderestriction) admission plugin.
-->
<h3 id="api-授权">API 授权</h3>
<p>一旦通过身份认证，每个 API 的调用都将通过鉴权检查。
Kubernetes 集成<a href="/zh/docs/reference/access-authn-authz/rbac/">基于角色的访问控制（RBAC）</a>组件，
将传入的用户或组与一组绑定到角色的权限匹配。
这些权限将动作（get，create，delete）和资源（pod，service, node）在命名空间或者集群范围内结合起来，
根据客户可能希望执行的操作，提供了一组提供合理的违约责任分离的外包角色。
建议你将<a href="/zh/docs/reference/access-authn-authz/node/">节点</a> 和
<a href="/zh/docs/reference/access-authn-authz/rbac/">RBAC</a> 一起作为授权者，再与
<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction</a>
准入插件结合使用。</p>
<!--
As with authentication, simple and broad roles may be appropriate for smaller clusters, but as
more users interact with the cluster, it may become necessary to separate teams into separate
namespaces with more limited roles.
-->
<p>与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，
可能需要将团队划分成有更多角色限制的单独的命名空间。</p>
<!--
With authorization, it is important to understand how updates on one object may cause actions in
other places. For instance, a user may not be able to create pods directly, but allowing them to
create a deployment, which creates pods on their behalf, will let them create those pods
indirectly. Likewise, deleting a node from the API will result in the pods scheduled to that node
being terminated and recreated on other nodes. The out of the box roles represent a balance
between flexibility and the common use cases, but more limited roles should be carefully reviewed
to prevent accidental escalation. You can make roles specific to your use case if the out-of-box ones don't meet your needs.

Consult the [authorization reference section](/docs/reference/access-authn-authz/authorization/) for more information.
-->
<p>就鉴权而言，理解怎么样更新一个对象可能导致在其它地方的发生什么样的行为是非常重要的。
例如，用户可能不能直接创建 Pod，但允许他们通过创建一个 Deployment 来创建这些 Pod，
这将让他们间接创建这些 Pod。
同样地，从 API 删除一个节点将导致调度到这些节点上的 Pod 被中止，并在其他节点上重新创建。
原生的角色设计代表了灵活性和常见用例之间的平衡，但有限制的角色应该仔细审查，
以防止意外升级。如果外包角色不满足你的需求，则可以为用例指定特定的角色。</p>
<p>如果你希望获取更多信息，请参阅<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权参考</a>。</p>
<!--
## Controlling access to the Kubelet

Kubelets expose HTTPS endpoints which grant powerful control over the node and containers. By default Kubelets allow unauthenticated access to this API.

Production clusters should enable Kubelet authentication and authorization.

Consult the [Kubelet authentication/authorization reference](/docs/admin/kubelet-authentication-authorization) for more information.
-->
<h2 id="控制对-kubelet-的访问">控制对 Kubelet 的访问</h2>
<p>Kubelet 公开 HTTPS 端点，这些端点授予节点和容器强大的控制权。
默认情况下，Kubelet 允许对此 API 进行未经身份验证的访问。</p>
<p>生产级别的集群应启用 Kubelet 身份验证和授权。</p>
<p>如果你希望获取更多信息，请参考
<a href="/zh/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/">Kubelet 身份验证/授权参考</a>。</p>
<!--
## Controlling the capabilities of a workload or user at runtime

Authorization in Kubernetes is intentionally high level, focused on coarse actions on resources.
More powerful controls exist as **policies** to limit by use case how those objects act on the
cluster, themselves, and other resources.
-->
<h2 id="控制运行时负载或用户的能力">控制运行时负载或用户的能力</h2>
<p>Kubernetes 中的授权故意设置为了高层级，它侧重于对资源的粗粒度行为。
更强大的控制是以通过用例限制这些对象如何作用于集群、自身和其他资源上的<strong>策略</strong>存在的。</p>
<!--
### Limiting resource usage on a cluster

[Resource quota](/docs/concepts/policy/resource-quotas/) limits the number or capacity of
resources granted to a namespace. This is most often used to limit the amount of CPU, memory,
or persistent disk a namespace can allocate, but can also control how many pods, services, or
volumes exist in each namespace.

[Limit ranges](/docs/tasks/administer-cluster/memory-default-namespace/) restrict the maximum or minimum size of some of the
resources above, to prevent users from requesting unreasonably high or low values for commonly
reserved resources like memory, or to provide default limits when none are specified.
-->
<h3 id="限制集群上的资源使用">限制集群上的资源使用</h3>
<p><a href="/zh/docs/concepts/policy/resource-quotas/">资源配额</a>
限制了授予命名空间的资源的数量或容量。
这通常用于限制命名空间可以分配的 CPU、内存或持久磁盘的数量，但也可以控制
每个命名空间中有多少个 Pod、服务或卷的存在。</p>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">限制范围</a>限制
上述某些资源的最大值或者最小值，以防止用户使用类似内存这样的通用保留资源时请求
不合理的过高或过低的值，或者在没有指定的情况下提供默认限制。</p>
<!--
### Controlling what privileges containers run with

A pod definition contains a [security context](/docs/tasks/configure-pod-container/security-context/)
that allows it to request access to running as a specific Linux user on a node (like root),
access to run privileged or access the host network, and other controls that would otherwise
allow it to run unfettered on a hosting node. [Pod security policies](/docs/concepts/policy/pod-security-policy/)
can limit which users or service accounts can provide dangerous security context settings. For example, pod security policies can limit volume mounts, especially `hostPath`, which are aspects of a pod that should be controlled.
-->
<h3 id="控制容器运行的特权">控制容器运行的特权</h3>
<p>Pod 定义包含了一个<a href="/zh/docs/tasks/configure-pod-container/security-context/">安全上下文</a>，
用于描述允许它请求访问某个节点上的特定 Linux 用户（如 root）、获得特权或访问主机网络、
以及允许它在主机节点上不受约束地运行的其它控件。
<a href="/zh/docs/concepts/policy/pod-security-policy/">Pod 安全策略</a>
可以限制哪些用户或服务帐户可以提供危险的安全上下文设置。
例如，Pod 的安全策略可以限制卷挂载，尤其是 <code>hostpath</code>，这些都是 Pod 应该控制的一些方面。</p>
<!--
Generally, most application workloads need limited access to host resources so they can
successfully run as a root process (uid 0) without access to host information. However,
considering the privileges associated with the root user, you should write application
containers to run as a non-root user. Similarly, administrators who wish to prevent
client applications from escaping their containers should use a restrictive pod security
policy.
-->
<p>一般来说，大多数应用程序需要限制对主机资源的访问，
他们可以在不能访问主机信息的情况下成功以根进程（UID 0）运行。
但是，考虑到与 root 用户相关的特权，在编写应用程序容器时，你应该使用非 root 用户运行。
类似地，希望阻止客户端应用程序逃避其容器的管理员，应该使用限制性的 pod 安全策略。</p>
<!--
### Restricting network access

The [network policies](/docs/tasks/administer-cluster/declare-network-policy/) for a namespace
allows application authors to restrict which pods in other namespaces may access pods and ports
within their namespaces. Many of the supported [Kubernetes networking providers](/docs/concepts/cluster-administration/networking/)
now respect network policy.
-->
<h3 id="限制网络访问">限制网络访问</h3>
<p>基于命名空间的<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">网络策略</a>
允许应用程序作者限制其它命名空间中的哪些 Pod 可以访问它们命名空间内的 Pod 和端口。
现在已经有许多支持网络策略的
<a href="/zh/docs/concepts/cluster-administration/networking/">Kubernetes 网络供应商</a>。</p>
<!--
Quota and limit ranges can also be used to control whether users may request node ports or
load balanced services, which on many clusters can control whether those users applications
are visible outside of the cluster.

Additional protections may be available that control network rules on a per plugin or per
environment basis, such as per-node firewalls, physically separating cluster nodes to
prevent cross talk, or advanced networking policy.
-->
<p>对于可以控制用户的应用程序是否在集群之外可见的许多集群，配额和限制范围也可用于
控制用户是否可以请求节点端口或负载均衡服务。</p>
<p>在插件或者环境基础上控制网络规则可以增加额外的保护措施，比如节点防火墙、物理分离
群集节点以防止串扰、或者高级的网络策略。</p>
<!--
### Restricting cloud metadata API access

Cloud platforms (AWS, Azure, GCE, etc.) often expose metadata services locally to instances.
By default these APIs are accessible by pods running on an instance and can contain cloud
credentials for that node, or provisioning data such as kubelet credentials. These credentials
can be used to escalate within the cluster or to other cloud services under the same account.

When running Kubernetes on a cloud platform limit permissions given to instance credentials, use
[network policies](/docs/tasks/administer-cluster/declare-network-policy/) to restrict pod access
to the metadata API, and avoid using provisioning data to deliver secrets.
-->
<h3 id="限制云-metadata-api-访问">限制云 metadata API 访问</h3>
<p>云平台（AWS,  Azure, GCE 等）经常讲 metadate 本地服务暴露给实例。
默认情况下，这些 API 可由运行在实例上的 Pod 访问，并且可以包含
该云节点的凭据或配置数据（如 kubelet 凭据）。
这些凭据可以用于在集群内升级或在同一账户下升级到其他云服务。</p>
<p>在云平台上运行 Kubernetes 时，限制对实例凭据的权限，使用
<a href="/zh/docs/tasks/administer-cluster/declare-network-policy/">网络策略</a>
限制对 metadata API 的 pod 访问，并避免使用配置数据来传递机密。</p>
<!--
### Controlling which nodes pods may access

By default, there are no restrictions on which nodes may run a pod.  Kubernetes offers a
[rich set of policies for controlling placement of pods onto nodes](/docs/concepts/configuration/assign-pod-node/)
and the [taint based pod placement and eviction](/docs/concepts/configuration/taint-and-toleration/)
that are available to end users. For many clusters use of these policies to separate workloads
can be a convention that authors adopt or enforce via tooling.

As an administrator, a beta admission plugin `PodNodeSelector` can be used to force pods
within a namespace to default or require a specific node selector, and if end users cannot
alter namespaces, this can strongly limit the placement of all of the pods in a specific workload.
-->
<h3 id="控制-pod-可以访问哪些节点">控制 Pod 可以访问哪些节点</h3>
<p>默认情况下，对哪些节点可以运行 pod 没有任何限制。
Kubernetes 给最终用户提供了
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/">一组丰富的策略用于控制 pod 放在节点上的位置</a>，
以及<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">基于污点的 Pod 放置和驱逐</a>。
对于许多集群，可以约定由作者采用或者强制通过工具使用这些策略来分离工作负载。</p>
<p>对于管理员，Beta 阶段的准入插件 <code>PodNodeSelector</code> 可用于强制命名空间中的 Pod
使用默认或需要使用特定的节点选择器。
如果最终用户无法改变命名空间，这可以强烈地限制所有的 pod 在特定工作负载的位置。</p>
<!--
## Protecting cluster components from compromise

This section describes some common patterns for protecting clusters from compromise.
-->
<h2 id="保护集群组件免受破坏">保护集群组件免受破坏</h2>
<p>本节描述保护集群免受破坏的一些常见模式。</p>
<!--
### Restrict access to etcd

Write access to the etcd backend for the API is equivalent to gaining root on the entire cluster,
and read access can be used to escalate fairly quickly. Administrators should always use strong
credentials from the API servers to their etcd server, such as mutual auth via TLS client certificates,
and it is often recommended to isolate the etcd servers behind a firewall that only the API servers
may access.

<blockquote class="caution callout">
  <div><strong>注意：</strong> Allowing other components within the cluster to access the master etcd instance with
read or write access to the full keyspace is equivalent to granting cluster-admin access. Using
separate etcd instances for non-master components or using etcd ACLs to restrict read and write
access to a subset of the keyspace is strongly recommended.</div>
</blockquote>

-->
<h3 id="限制访问-etcd">限制访问 etcd</h3>
<p>对于 API 来说，拥有 etcd 后端的写访问权限，相当于获得了整个集群的 root 权限，
并且可以使用写访问权限来相当快速地升级。
从 API 服务器访问它们的 etcd 服务器，管理员应该使用广受信任的凭证，
如通过 TLS 客户端证书的相互认证。
通常，我们建议将 etcd 服务器隔离到只有API服务器可以访问的防火墙后面。</p>
<blockquote class="caution callout">
  <div><strong>注意：</strong> 允许集群中其它组件拥有读或写全空间的权限去访问 etcd 实例，相当于授予群集管理员访问的权限。
对于非主控组件，强烈推荐使用单独的 etcd 实例，或者使用 etcd 的访问控制列表
去限制只能读或者写空间的一个子集。</div>
</blockquote>

<!--
### Enable audit logging

The [audit logger](/docs/tasks/debug-application-cluster/audit/) is a beta feature that records actions taken by the
API for later analysis in the event of a compromise. It is recommended to enable audit logging
and archive the audit file on a secure server.
-->
<h3 id="开启审计日志">开启审计日志</h3>
<p><a href="/zh/docs/tasks/debug-application-cluster/audit/">审计日志</a>是 Beta 特性，
负责记录 API 操作以便在发生破坏时进行事后分析。
建议启用审计日志，并将审计文件归档到安全服务器上。</p>
<!--
### Restrict access to alpha or beta features

Alpha and beta Kubernetes features are in active development and may have limitations or bugs
that result in security vulnerabilities. Always assess the value an alpha or beta feature may
provide against the possible risk to your security posture. When in doubt, disable features you
do not use.
-->
<h3 id="限制使用-alpha-和-beta-特性">限制使用 alpha 和 beta 特性</h3>
<p>Kubernetes 的 alpha 和 beta 特性还在努力开发中，可能存在导致安全漏洞的缺陷或错误。
要始终评估 alpha 和 beta 特性可能为你的安全态势带来的风险。
当你怀疑存在风险时，可以禁用那些不需要使用的特性。</p>
<!--
### Rotate infrastructure credentials frequently

The shorter the lifetime of a secret or credential the harder it is for an attacker to make
use of that credential. Set short lifetimes on certificates and automate their rotation. Use
an authentication provider that can control how long issued tokens are available and use short
lifetimes where possible. If you use service account tokens in external integrations, plan to
rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap token used for setting up nodes should be revoked or its authorization removed.
-->
<h3 id="频繁回收基础设施证书">频繁回收基础设施证书</h3>
<p>一个 Secret 或凭据的寿命越短，攻击者就越难使用该凭据。
在证书上设置短生命周期并实现自动回收，是控制安全的一个好方法。
因此，使用身份验证提供程序时，应该要求可以控制发布令牌的可用时间，并尽可能使用短寿命。
如果在外部集成中使用服务帐户令牌，则应该频繁地回收这些令牌。
例如，一旦引导阶段完成，就应该撤销用于设置节点的引导令牌，或者取消它的授权。</p>
<!--
### Review third party integrations before enabling them

Many third party integrations to Kubernetes may alter the security profile of your cluster. When
enabling an integration, always review the permissions that an extension requests before granting
it access. For example, many security integrations may request access to view all secrets on
your cluster which is effectively making that component a cluster admin. When in doubt,
restrict the integration to functioning in a single namespace if possible.

Components that create pods may also be unexpectedly powerful if they can do so inside namespaces
like the `kube-system` namespace, because those pods can gain access to service account secrets
or run with elevated permissions if those service accounts are granted access to permissive
[pod security policies](/docs/concepts/policy/pod-security-policy/).
-->
<h3 id="在启用第三方集成之前-请先审查它们">在启用第三方集成之前，请先审查它们</h3>
<p>许多集成到 Kubernetes 的第三方都可以改变你集群的安全配置。
启用集成时，在授予访问权限之前，你应该始终检查扩展所请求的权限。
例如，许多安全集成可以请求访问来查看集群上的所有 Secret，
从而有效地使该组件成为集群管理。
当有疑问时，如果可能的话，将集成限制在单个命名空间中运行。</p>
<p>如果组件创建的 Pod 能够在命名空间中做一些类似 <code>kube-system</code> 命名空间中的事情，
那么它也可能是出乎意料的强大。
因为这些 Pod 可以访问服务账户的 Secret，或者，如果这些服务帐户被授予访问许可的
<a href="/zh/docs/concepts/policy/pod-security-policy/">Pod 安全策略</a>的权限，它们能以高权限运行。</p>
<!--
### Encrypt secrets at rest

In general, the etcd database will contain any information accessible via the Kubernetes API
and may grant an attacker significant visibility into the state of your cluster. Always encrypt
your backups using a well reviewed backup and encryption solution, and consider using full disk
encryption where possible.

Kubernetes 1.7 contains [encryption at rest](/docs/tasks/administer-cluster/encrypt-data/), an alpha feature that will encrypt `Secret` resources in etcd, preventing
parties that gain access to your etcd backups from viewing the content of those secrets. While
this feature is currently experimental, it may offer an additional level of defense when backups
are not encrypted or an attacker gains read access to etcd.
-->
<h3 id="对-secret-进行静态加密">对 Secret 进行静态加密</h3>
<p>一般情况下，etcd 数据库包含了通过 Kubernetes API 可以访问到的所有信息，
并且可以授予攻击者对集群状态的可见性。
始终使用经过良好审查的备份和加密解决方案来加密备份，并考虑在可能的情况下使用全磁盘加密。</p>
<p>Kubernetes 1.7 包含了<a href="/zh/docs/tasks/administer-cluster/encrypt-data/">静态数据加密</a>，
它是一个 alpha 特性，会加密 etcd 里面的 <code>Secret</code> 资源，以防止某一方通过查看
etcd 的备份文件查看到这些 Secret 的内容。虽然目前这还只是实验性的功能，
但是在备份没有加密或者攻击者获取到 etcd 的读访问权限的时候，它能提供额外的防御层级。</p>
<!--
### Receiving alerts for security updates and reporting vulnerabilities

Join the [kubernetes-announce](https://groups.google.com/forum/#!forum/kubernetes-announce)
group for emails about security announcements. See the [security reporting](/security/)
page for more on how to report vulnerabilities.
-->
<h3 id="接收安全更新和报告漏洞的警报">接收安全更新和报告漏洞的警报</h3>
<p>加入 <a href="https://groups.google.com/forum/#!forum/kubernetes-announce">kubernetes-announce</a>
组，能够获取有关安全公告的邮件。有关如何报告漏洞的更多信息，请参见
<a href="/zh/docs/reference/issues-security/security/">安全报告</a>页面。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4a02bcca41439e16655f43fa37c81da4">2.15 - 关键插件 Pod 的调度保证</h1>
    
	<!-- overview -->
<!-- 
In addition to Kubernetes core components like api-server, scheduler, controller-manager running on a master machine
there are a number of add-ons which, for various reasons, must run on a regular cluster node (rather than the Kubernetes master).
Some of these add-ons are critical to a fully functional cluster, such as metrics-server, DNS, and UI.
A cluster may stop working properly if a critical add-on is evicted (either manually or as a side effect of another operation like upgrade)
and becomes pending (for example when the cluster is highly utilized and either there are other pending pods that schedule into the space
vacated by the evicted critical add-on pod or the amount of resources available on the node changed for some other reason).
-->
<p>除了在主机上运行的 Kubernetes 核心组件（如 api-server 、scheduler 、controller-manager）之外，还有许多插件，由于各种原因，
必须在常规集群节点（而不是 Kubernetes 主节点）上运行。
其中一些插件对于功能完备的群集至关重要，例如 Heapster、DNS 和 UI。
如果关键插件被逐出（手动或作为升级等其他操作的副作用）或者变成挂起状态，群集可能会停止正常工作。
关键插件进入挂起状态的例子有：集群利用率过高；被逐出的关键插件 Pod 释放了空间，但该空间被之前悬决的 Pod 占用；由于其它原因导致节点上可用资源的总量发生变化。</p>
<!-- body -->
<!--
### Marking pod as critical
-->
<h3 id="标记关键-pod">标记关键 Pod</h3>
<!--
To be considered critical, the pod has to run in the `kube-system` namespace (configurable via flag) and
* Have the priorityClassName set as "system-cluster-critical" or "system-node-critical", the latter being the highest for entire cluster. Alternatively, you could add an annotation `scheduler.alpha.kubernetes.io/critical-pod` as key and empty string as value to your pod, but this annotation is deprecated as of version 1.13 and will be removed in 1.14.
-->
<p>要将 pod 标记为关键性（critical），pod 必须在 kube-system 命名空间中运行（可通过参数配置）。
同时，需要将 <code>priorityClassName</code> 设置为 <code>system-cluster-critical</code> 或 <code>system-node-critical</code> ，后者是整个群集的最高级别。
或者，也可以为 Pod 添加名为 <code>scheduler.alpha.kubernetes.io/critical-pod</code>、值为空字符串的注解。
不过，这一注解从 1.13 版本开始不再推荐使用，并将在 1.14 中删除。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fe6b50655c29ab0b7c1ee549ff64c138">2.16 - 升级集群</h1>
    
	<!-- 
---
title: Upgrade A Cluster
content_type: task
---
-->
<!-- overview -->
<!-- 
This page provides an overview of the steps you should follow to upgrade a
Kubernetes cluster.

The way that you upgrade a cluster depends on how you initially deployed it
and on any subsequent changes.

At a high level, the steps you perform are:
-->
<p>本页概述升级 Kubernetes 集群的步骤。</p>
<p>升级集群的方式取决于你最初部署它的方式、以及后续更改它的方式。</p>
<p>从高层规划的角度看，要执行的步骤是：</p>
<!-- 
- Upgrade the <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>
- Upgrade the nodes in your cluster
- Upgrade clients such as <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes API 服务器进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a>
- Adjust manifests and other resources based on the API changes that accompany the
  new Kubernetes version
-->
<ul>
<li>升级<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a></li>
<li>升级集群中的节点</li>
<li>升级 <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes API 服务器进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a> 之类的客户端</li>
<li>根据新 Kubernetes 版本带来的 API 变化，调整清单文件和其他资源</li>
</ul>
<h2 id="准备开始">准备开始</h2>
<!-- 
You must have an existing cluster. This page is about upgrading from Kubernetes
1.21 to Kubernetes 1.22. If your cluster
is not currently running Kubernetes 1.21 then please check
the documentation for the version of Kubernetes that you plan to upgrade to.
-->
<p>你必须有一个集群。
本页内容涉及从 Kubernetes 1.21
升级到 Kubernetes 1.22。
如果你的集群未运行 Kubernetes 1.21，
那请参考目标 Kubernetes 版本的文档。</p>
<!-- ## Upgrade approaches -->
<h2 id="upgrade-approaches">升级方法</h2>
<h3 id="upgrade-kubeadm">kubeadm</h3>
<!-- 
If your cluster was deployed using the `kubeadm` tool, refer to 
[Upgrading kubeadm clusters](/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
for detailed information on how to upgrade the cluster.

Once you have upgraded the cluster, remember to
[install the latest version of `kubectl`](/docs/tasks/tools/).
-->
<p>如果你的集群是使用 <code>kubeadm</code> 安装工具部署而来，
那么升级群集的详细信息，请参阅
<a href="/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">升级 kubeadm 集群</a>。</p>
<p>升级集群之后，要记得
<a href="/zh/docs/tasks/tools/">安装最新版本的 <code>kubectl</code></a>.</p>
<!-- ### Manual deployments -->
<h3 id="manual-deployments">手动部署</h3>
<!-- 
These steps do not account for third-party extensions such as network and storage
plugins.

You should manually update the control plane following this sequence:
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 这些步骤不考虑第三方扩展，例如网络和存储插件。</div>
</blockquote>

<p>你应该跟随下面操作顺序，手动更新控制平面：</p>
<!-- 
- etcd (all instances)
- kube-apiserver (all control plane hosts)
- kube-controller-manager
- kube-scheduler
- cloud controller manager, if you use one
-->
<ul>
<li>etcd (所有实例)</li>
<li>kube-apiserver (所有控制平面的宿主机)</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>cloud controller manager, 在你用到时</li>
</ul>
<!-- 
At this point you should
[install the latest version of `kubectl`](/docs/tasks/tools/).

For each node in your cluster, [drain](/docs/tasks/administer-cluster/safely-drain-node/)
that node and then either replace it with a new node that uses the 1.22
kubelet, or upgrade the 1.22
kubelet on that node and bring the node back into service.
-->
<p>现在，你应该
<a href="/zh/docs/tasks/tools/">安装最新版本的 <code>kubectl</code></a>.</p>
<p>对于群集中的每个节点，
<a href="/zh/docs/tasks/administer-cluster/safely-drain-node/">排空</a>
节点，然后，或者用一个运行了 1.22 kubelet 的新节点替换它；
或者升级此节点的 kubelet，并使节点恢复服务。</p>
<!-- 
### Other deployments {#upgrade-other}

Refer to the documentation for your cluster deployment tool to learn the recommended set
up steps for maintenance.

## Post-upgrade tasks

### Switch your cluster's storage API version
-->
<h3 id="upgrade-other">其他部署方式</h3>
<p>参阅你的集群部署工具对应的文档，了解用于维护的推荐设置步骤。</p>
<h2 id="post-upgrade-tasks">升级后的任务</h2>
<h3 id="switch-your-clusters-storage-api-version">切换群集的存储 API 版本</h3>
<!-- 
The objects that are serialized into etcd for a cluster's internal
representation of the Kubernetes resources active in the cluster are
written using a particular version of the API.

When the supported API changes, these objects may need to be rewritten
in the newer API. Failure to do this will eventually result in resources
that are no longer decodable or usable by the Kubernetes API server.

For each affected object, fetch it using the latest supported API and then
write it back also using the latest supported API.
-->
<p>对象序列化到 etcd，是为了提供集群中活动 Kubernetes 资源的内部表示法，
这些对象都使用特定版本的 API 编写。</p>
<p>当底层的 API 更改时，这些对象可能需要用新 API 重写。
如果不能做到这一点，会导致再也不能用 Kubernetes API 服务器解码、使用该对象。</p>
<p>对于每个受影响的对象，用最新支持的 API 获取它，然后再用最新支持的 API 写回来。</p>
<!-- 
### Update manifests

Upgrading to a new Kubernetes version can provide new APIs.

You can use `kubectl convert` command to convert manifests between different API versions.
For example:
-->
<h3 id="update-manifests">更新清单</h3>
<p>升级到新版本 Kubernetes 就可以提供新的 API。</p>
<p>你可以使用 <code>kubectl convert</code> 命令在不同 API 版本之间转换清单。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl convert -f pod.yaml --output-version v1
</code></pre></div><!-- 
The `kubectl` tool replaces the contents of `pod.yaml` with a manifest that sets `kind` to
Pod (unchanged), but with a revised `apiVersion`.
-->
<p><code>kubectl</code> 替换了 <code>pod.yaml</code> 的内容，
在新的清单文件中，<code>kind</code> 被设置为 Pod（未变），
但 <code>apiVersion</code> 则被修订了。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-56de8c25b1486599777034111645b803">2.17 - 名字空间演练</h1>
    
	<!--
reviewers:
- derekwaynecarr
- janetkuo
title: Namespaces Walkthrough
content_type: task
-->
<!-- overview -->
<!--
Kubernetes <a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespaces'>namespaces</a>
help different projects, teams, or customers to share a Kubernetes cluster.
-->
<p>Kubernetes <a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>
有助于不同的项目、团队或客户去共享 Kubernetes 集群。</p>
<!--
It does this by providing the following:

1. A scope for [Names](/docs/concepts/overview/working-with-objects/names/).
2. A mechanism to attach authorization and policy to a subsection of the cluster.
-->
<p>名字空间通过以下方式实现这点：</p>
<ol>
<li>为<a href="/zh/docs/concepts/overview/working-with-objects/names/">名字</a>设置作用域.</li>
<li>为集群中的部分资源关联鉴权和策略的机制。</li>
</ol>
<!--
Use of multiple namespaces is optional.

This example demonstrates how to use Kubernetes namespaces to subdivide your cluster.
-->
<p>使用多个名字空间是可选的。</p>
<p>此示例演示了如何使用 Kubernetes 名字空间细分群集。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Prerequisites

This example assumes the following:

1. You have an [existing Kubernetes cluster](/docs/setup/).
2. You have a basic understanding of Kubernetes _[Pods](/docs/concepts/workloads/pods/pod/)_, _[Services](/docs/concepts/services-networking/service/)_, and _[Deployments](/docs/concepts/workloads/controllers/deployment/)_.
-->
<h2 id="环境准备">环境准备</h2>
<p>此示例作如下假设：</p>
<ol>
<li>你已拥有一个<a href="/zh/docs/setup/">配置好的 Kubernetes 集群</a>。</li>
<li>你已对 Kubernetes 的 <em><a href="/zh/docs/concepts/workloads/pods/">Pods</a></em>、
<em><a href="/zh/docs/concepts/services-networking/service/">Services</a></em> 和
<em><a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployments</a></em>
有基本理解。</li>
</ol>
<!--
## Understand the default namespace

By default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods,
Services, and Deployments used by the cluster.
-->
<ol>
<li>理解默认名字空间</li>
</ol>
<p>默认情况下，Kubernetes 集群会在配置集群时实例化一个默认名字空间，用以存放集群所使用的默认
Pod、Service 和 Deployment 集合。</p>
<!--
Assuming you have a fresh cluster, you can inspect the available namespaces by doing the following:
-->
<p>假设你有一个新的集群，你可以通过执行以下操作来检查可用的名字空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces
</code></pre></div><pre tabindex="0"><code>NAME      STATUS    AGE
default   Active    13m
</code></pre><!--
## Create new namespaces

For this exercise, we will create two additional Kubernetes namespaces to hold our content.
-->
<h2 id="创建新的名字空间">创建新的名字空间</h2>
<p>在本练习中，我们将创建两个额外的 Kubernetes 名字空间来保存我们的内容。</p>
<!--
Let's imagine a scenario where an organization is using a shared Kubernetes cluster for development and production use cases.
-->
<p>我们假设一个场景，某组织正在使用共享的 Kubernetes 集群来支持开发和生产：</p>
<!--
The development team would like to maintain a space in the cluster where they can get a view on the list of Pods, Services, and Deployments
they use to build and run their application.  In this space, Kubernetes resources come and go, and the restrictions on who can or cannot modify resources
are relaxed to enable agile development.
-->
<p>开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pod、Service
和 Deployment 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，
对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。</p>
<!--
The operations team would like to maintain a space in the cluster where they can enforce strict procedures on who can or cannot manipulate the set of
Pods, Services, and Deployments that run the production site.
-->
<p>运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，
对谁可以或谁不可以操作运行生产站点的 Pod、Service 和 Deployment 集合进行控制。</p>
<!--
One pattern this organization could follow is to partition the Kubernetes cluster into two namespaces: `development` and `production`.
-->
<p>该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个名字空间：<code>development</code> 和 <code>production</code>。</p>
<!--
Let's create two new namespaces to hold our work.
-->
<p>让我们创建两个新的名字空间来保存我们的工作。</p>
<!--
Use the file [`namespace-dev.json`](/examples/admin/namespace-dev.json) which describes a `development` namespace:
-->
<p>文件 <a href="/examples/admin/namespace-dev.json"><code>namespace-dev.json</code></a> 描述了 <code>development</code> 名字空间:</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/namespace-dev.json" download="admin/namespace-dev.json"><code>admin/namespace-dev.json</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-namespace-dev-json')" title="Copy admin/namespace-dev.json to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-namespace-dev-json">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;v1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Namespace&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;development&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;labels&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;development&#34;</span>
    }
  }
}
</code></pre></div>
    </div>
</div>


<!--
Create the `development` namespace using kubectl.
-->
<p>使用 kubectl 创建 <code>development</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</code></pre></div><!--
Save the following contents into file [`namespace-prod.json`](/examples/admin/namespace-prod.json) which describes a `production` namespace:
-->
<p>将下列的内容保存到文件 <a href="/examples/admin/namespace-prod.json"><code>namespace-prod.json</code></a> 中，
这些内容是对 <code>production</code> 名字空间的描述：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/namespace-prod.json" download="admin/namespace-prod.json"><code>admin/namespace-prod.json</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-namespace-prod-json')" title="Copy admin/namespace-prod.json to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-namespace-prod-json">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;v1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Namespace&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;production&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;labels&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;production&#34;</span>
    }
  }
}
</code></pre></div>
    </div>
</div>


<!--
And then let's create the `production` namespace using kubectl.
-->
<p>让我们使用 kubectl 创建 <code>production</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</code></pre></div><!--
To be sure things are right, let's list all of the namespaces in our cluster.
-->
<p>为了确保一切正常，我们列出集群中的所有名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces --show-labels
</code></pre></div><pre tabindex="0"><code>NAME          STATUS    AGE       LABELS
default       Active    32m       &lt;none&gt;
development   Active    29s       name=development
production    Active    23s       name=production
</code></pre><!--
## Create pods in each namespace

A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.

Users interacting with one namespace do not see the content in another namespace.

To demonstrate this, let's spin up a simple Deployment and Pods in the `development` namespace.
-->
<h2 id="在每个名字空间中创建-pod">在每个名字空间中创建 pod</h2>
<p>Kubernetes 名字空间为集群中的 Pod、Service 和 Deployment 提供了作用域。</p>
<p>与一个名字空间交互的用户不会看到另一个名字空间中的内容。</p>
<p>为了演示这一点，让我们在 development 名字空间中启动一个简单的 Deployment 和 Pod。</p>
<!--
We first check what is the current context:
-->
<p>我们首先检查一下当前的上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">certificate-authority-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>https://130.211.122.180<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">current-context</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-certificate-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-key-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">token</span>:<span style="color:#bbb"> </span>65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes-basic-auth<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>h5M0FtUUIflBSdI7<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>admin<span style="color:#bbb">
</span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config current-context
</code></pre></div><pre tabindex="0"><code>lithe-cocoa-92103_kubernetes
</code></pre><!--
The next step is to define a context for the kubectl client to work in each namespace. The value of "cluster" and "user" fields are copied from the current context.
-->
<p>下一步是为 kubectl 客户端定义一个上下文，以便在每个名字空间中工作。
&quot;cluster&quot; 和 &quot;user&quot; 字段的值将从当前上下文中复制。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config set-context dev --namespace<span style="color:#666">=</span>development <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --cluster<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --user<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes

kubectl config set-context prod --namespace<span style="color:#666">=</span>production <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --cluster<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --user<span style="color:#666">=</span>lithe-cocoa-92103_kubernetes
</code></pre></div><!--
By default, the above commands adds two contexts that are saved into file
`.kube/config`. You can now view the contexts and alternate against the two
new request contexts depending on which namespace you wish to work against.
-->
<p>默认情况下，上述命令会添加两个上下文到 <code>.kube/config</code> 文件中。
你现在可以查看上下文并根据你希望使用的名字空间并在这两个新的请求上下文之间切换。</p>
<!--
To view the new contexts:
-->
<p>查看新的上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">certificate-authority-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>https://130.211.122.180<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>production<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>prod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">current-context</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-certificate-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-key-data</span>:<span style="color:#bbb"> </span>REDACTED<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">token</span>:<span style="color:#bbb"> </span>65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lithe-cocoa-92103_kubernetes-basic-auth<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>h5M0FtUUIflBSdI7<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>admin<span style="color:#bbb">
</span></code></pre></div><!--
Let's switch to operate in the `development` namespace.
-->
<p>让我们切换到 <code>development</code> 名字空间进行操作。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config use-context dev
</code></pre></div><!--
You can verify your current context by doing the following:
-->
<p>你可以使用下列命令验证当前上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config current-context
</code></pre></div><pre tabindex="0"><code>dev
</code></pre><!--
At this point, all requests we make to the Kubernetes cluster from the command line are scoped to the `development` namespace.
-->
<p>此时，我们从命令行向 Kubernetes 集群发出的所有请求都限定在 <code>development</code> 名字空间中。</p>
<!--
Let's create some contents.
-->
<p>让我们创建一些内容。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/snowflake-deployment.yaml" download="admin/snowflake-deployment.yaml"><code>admin/snowflake-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-snowflake-deployment-yaml')" title="Copy admin/snowflake-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-snowflake-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/serve_hostname<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>snowflake<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Apply the manifest to create a Deployment 
-->
<p>应用清单文件来创建 Deployment。</p>
<!--
We have just created a deployment whose replica size is 2 that is running the pod called `snowflake` with a basic container that just serves the hostname.
-->
<p>我们刚刚创建了一个副本大小为 2 的 Deployment，该 Deployment 运行名为 <code>snowflake</code> 的 Pod，
其中包含一个仅提供主机名服务的基本容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment
</code></pre></div><pre tabindex="0"><code>NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
snowflake   2         2         2            2           2m
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>snowflake
</code></pre></div><pre tabindex="0"><code>NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
</code></pre><!--
And this is great, developers are able to do what they want, and they do not have to worry about affecting content in the `production` namespace.
-->
<p>这很棒，开发人员可以做他们想要的事情，而不必担心影响 <code>production</code> 名字空间中的内容。</p>
<!--
Let's switch to the `production` namespace and show how resources in one namespace are hidden from the other.
-->
<p>让我们切换到 <code>production</code> 名字空间，展示一个名字空间中的资源如何对另一个名字空间不可见。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config use-context prod
</code></pre></div><!--
The `production` namespace should be empty, and the following commands should return nothing.
-->
<p><code>production</code> 名字空间应该是空的，下列命令应该返回的内容为空。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment
kubectl get pods
</code></pre></div><!--
Production likes to run cattle, so let's create some cattle pods.
-->
<p>生产环境需要以放牛的方式运维，让我们创建一些名为 <code>cattle</code> 的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment cattle --image<span style="color:#666">=</span>k8s.gcr.io/serve_hostname --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
kubectl get deployment
</code></pre></div><pre tabindex="0"><code>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
cattle    5         5         5            5           10s
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">run</span><span style="color:#666">=</span>cattle
</code></pre></div><pre tabindex="0"><code>NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
</code></pre><!--
At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.
-->
<p>此时，应该很清楚的展示了用户在一个名字空间中创建的资源对另一个名字空间是不可见的。</p>
<!--
As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.
-->
<p>随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个名字空间提供不同的授权规则。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-09cc2cf3e0f23a3996e6cb31dc4d867c">2.18 - 启用/禁用 Kubernetes API</h1>
    
	<!-- 
---
title: Enable Or Disable A Kubernetes API
content_type: task
---
-->
<!-- overview -->
<!-- 
This page shows how to enable or disable an API version from your cluster's
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>.
-->
<p>本页展示怎么用集群的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>.
启用/禁用 API 版本。</p>
<!-- steps -->
<!-- 
Specific API versions can be turned on or off by passing `--runtime-config=api/<version>` as a
command line argument to the API server. The values for this argument are a comma-separated
list of API versions. Later values override earlier values.

The `runtime-config` command line argument also supports 2 special keys:
-->
<p>通过 API 服务器的命令行参数 <code>--runtime-config=api/&lt;version&gt;</code> ，
可以开启/关闭某个指定的 API 版本。
此参数的值是一个逗号分隔的 API 版本列表。
此列表中，后面的值可以覆盖前面的值。</p>
<p>命令行参数 <code>runtime-config</code> 支持两个特殊的值（keys）：</p>
<!-- 
- `api/all`, representing all known APIs
- `api/legacy`, representing only legacy APIs. Legacy APIs are any APIs that have been
   explicitly [deprecated](/zh/docs/reference/using-api/deprecation-policy/).

For example, to turning off all API versions except v1, pass `--runtime-config=api/all=false,api/v1=true`
to the `kube-apiserver`.
-->
<ul>
<li><code>api/all</code>：指所有已知的 API</li>
<li><code>api/legacy</code>：指过时的 API。过时的 API 就是明确地
<a href="/zh/docs/reference/using-api/deprecation-policy/">弃用</a>
的 API。</li>
</ul>
<p>例如：为了停用除去 v1 版本之外的全部其他 API 版本，
就用参数 <code>--runtime-config=api/all=false,api/v1=true</code> 启动 <code>kube-apiserver</code>。</p>
<h2 id="接下来">接下来</h2>
<!-- 
Read the [full documentation](/docs/reference/command-line-tools-reference/kube-apiserver/)
for the `kube-apiserver` component.
-->
<p>阅读<a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">完整的文档</a>,
以了解 <code>kube-apiserver</code> 组件。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c2f73ef872a65be44f4ab1e6511b8eb9">2.19 - 启用拓扑感知提示</h1>
    
	<!-- 
---
reviewers:
- robscott
title: Enabling Topology Aware Hints
content_type: task
min-kubernetes-server-version: 1.21
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>

<!-- 
_Topology Aware Hints_ enable topology aware routing with topology hints
included in <a class='glossary-tooltip' title='一种将网络端点与 Kubernetes 资源组合在一起的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/endpoint-slices/' target='_blank' aria-label='EndpointSlices'>EndpointSlices</a>.
This approach tries to keep traffic close to where it originated from;
you might do this to reduce costs, or to improve network performance.
-->
<p><em>拓扑感知提示</em> 启用具有拓扑感知能力的路由，其中拓扑感知信息包含在
<a class='glossary-tooltip' title='一种将网络端点与 Kubernetes 资源组合在一起的方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/endpoint-slices/' target='_blank' aria-label='EndpointSlices'>EndpointSlices</a> 中。
此功能尽量将流量限制在它的发起区域附近；
可以降低成本，或者提高网络性能。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 1.21.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- 
The following prerequisite is needed in order to enable topology aware hints:

* Configure the <a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a> to run in
  iptables mode or IPVS mode
* Ensure that you have not disabled EndpointSlices
-->
<p>为了启用拓扑感知提示，先要满足以下先决条件：</p>
<ul>
<li>配置 <a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='kube-proxy'>kube-proxy</a>
以 iptables 或 IPVS 模式运行</li>
<li>确保未禁用 EndpointSlices</li>
</ul>
<!-- 
## Enable Topology Aware Hints
-->
<h2 id="enable-topology-aware-hints">启动拓扑感知提示</h2>
<!-- 
To enable service topology hints, enable the `TopologyAwareHints` [feature
gate](/docs/reference/command-line-tools-reference/feature-gates/) for the
kube-apiserver, kube-controller-manager, and kube-proxy:
-->
<p>要启用服务拓扑感知，请启用 kube-apiserver、kube-controller-manager、和 kube-proxy 的
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
<code>TopologyAwareHints</code>。</p>
<pre tabindex="0"><code>--feature-gates=&quot;TopologyAwareHints=true&quot;
</code></pre><h2 id="接下来">接下来</h2>
<!-- 
* Read about [Topology Aware Hints](/docs/concepts/services-networking/topology-aware-hints) for Services
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
-->
<ul>
<li>参阅面向服务的<a href="/zh/docs/concepts/services-networking/topology-aware-hints">拓扑感知提示</a></li>
<li>参阅<a href="/zh/docs/concepts/services-networking/connect-applications-service/">用服务连通应用</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9ceed97f912df7289ed8872e290cfbad">2.20 - 在 Kubernetes 集群中使用 NodeLocal DNSCache</h1>
    
	<!--
---
reviewers:
- bowei
- zihongz
title: Using NodeLocal DNSCache in Kubernetes clusters
content_type: task
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>

<!--
This page provides an overview of NodeLocal DNSCache feature in Kubernetes.
-->
<p>本页概述了 Kubernetes 中的 NodeLocal DNSCache 功能。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
 <!-- steps -->
<!--
## Introduction
-->
<h2 id="引言">引言</h2>
<!--
NodeLocal DNSCache improves Cluster DNS performance by running a dns caching agent on cluster nodes as a DaemonSet. In today's architecture, Pods in ClusterFirst DNS mode reach out to a kube-dns serviceIP for DNS queries. This is translated to a kube-dns/CoreDNS endpoint via iptables rules added by kube-proxy. With this new architecture, Pods will reach out to the dns caching agent running on the same node, thereby avoiding iptables DNAT rules and connection tracking. The local caching agent will query kube-dns service for cache misses of cluster hostnames(cluster.local suffix by default).
-->
<p>NodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能。
在当今的体系结构中，处于 ClusterFirst DNS 模式的 Pod 可以连接到 kube-dns serviceIP 进行 DNS 查询。
通过 kube-proxy 添加的 iptables 规则将其转换为 kube-dns/CoreDNS 端点。
借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。
本地缓存代理将查询 kube-dns 服务以获取集群主机名的缓存缺失（默认为 cluster.local 后缀）。</p>
<!--
## Motivation
-->
<h2 id="动机">动机</h2>
<!--
* With the current DNS architecture, it is possible that Pods with the highest DNS QPS have to reach out to a different node, if there is no local kube-dns/CoreDNS instance.
Having a local cache will help improve the latency in such scenarios.
-->
<ul>
<li>使用当前的 DNS 体系结构，如果没有本地 kube-dns/CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须延伸到另一个节点。
在这种脚本下，拥有本地缓存将有助于改善延迟。</li>
</ul>
<!--
* Skipping iptables DNAT and connection tracking will help reduce [conntrack races](https://github.com/kubernetes/kubernetes/issues/56903) and avoid UDP DNS entries filling up conntrack table.
-->
<ul>
<li>跳过 iptables DNAT 和连接跟踪将有助于减少 <a href="https://github.com/kubernetes/kubernetes/issues/56903">conntrack 竞争</a>并避免 UDP DNS 条目填满 conntrack 表。</li>
</ul>
<!--
* Connections from local caching agent to kube-dns servie can be upgraded to TCP. TCP conntrack entries will be removed on connection close in contrast with UDP entries that have to timeout ([default](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt) `nf_conntrack_udp_timeout` is 30 seconds)
-->
<ul>
<li>从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP 。
TCP conntrack 条目将在连接关闭时被删除，相反 UDP 条目必须超时(<a href="https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt">默认</a> <code>nf_conntrack_udp_timeout</code> 是 30 秒)</li>
</ul>
<!--
* Upgrading DNS queries from UDP to TCP would reduce tail latency attributed to dropped UDP packets and DNS timeouts usually up to 30s (3 retries + 10s timeout). Since the nodelocal cache listens for UDP DNS queries, applications don't need to be changed.
-->
<ul>
<li>将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）。</li>
</ul>
<!--
* Metrics & visibility into dns requests at a node level.
-->
<ul>
<li>在节点级别对 dns 请求的度量和可见性。</li>
</ul>
<!--
* Negative caching can be re-enabled, thereby reducing number of queries to kube-dns service.
-->
<ul>
<li>可以重新启用负缓存，从而减少对 kube-dns 服务的查询数量。</li>
</ul>
<!--
## Architecture Diagram
-->
<h2 id="架构图">架构图</h2>
<!--
This is the path followed by DNS Queries after NodeLocal DNSCache is enabled:
-->
<p>启用 NodeLocal DNSCache 之后，这是 DNS 查询所遵循的路径：</p>
<!--

<figure>
    <img src="/images/docs/nodelocaldns.svg"
         alt="NodeLocal DNSCache flow"/> <figcaption>
            <h4>Nodelocal DNSCache flow</h4><p>This image shows how NodeLocal DNSCache handles DNS queries.</p>
        </figcaption>
</figure>

-->

<figure>
    <img src="/images/docs/nodelocaldns.svg"
         alt="NodeLocal DNSCache 流"/> <figcaption>
            <h4>Nodelocal DNSCache 流</h4><p>此图显示了 NodeLocal DNSCache 如何处理 DNS 查询。</p>
        </figcaption>
</figure>

<!--
## Configuration
-->
<h2 id="配置">配置</h2>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> The local listen IP address for NodeLocal DNSCache can be any address that can be guaranteed to not collide with any existing IP in your cluster. It's recommended to use an address with a local scope, per example, from the link-local range 169.254.0.0/16 for IPv4 or from the Unique Local Address range in IPv6 fd00::/8.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> NodeLocal DNSCache 的本地侦听 IP 地址可以是任何地址，只要该地址不和你的集群里现有的 IP 地址发生冲突。
推荐使用本地范围内的地址，例如，IPv4 链路本地区段 169.254.0.0/16 内的地址，
或者 IPv6 唯一本地地址区段 fd00::/8 内的地址。</div>
</blockquote>
<!--
This feature can be enabled using the following steps:
-->
<p>可以使用以下步骤启动此功能：</p>
<!--
* Prepare a manifest similar to the sample [`nodelocaldns.yaml`](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml) and save it as `nodelocaldns.yaml.`
-->
<ul>
<li>根据示例 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml"><code>nodelocaldns.yaml</code></a>
准备一个清单，把它保存为 <code>nodelocaldns.yaml</code>。</li>
</ul>
<!--
* If using IPv6, the CoreDNS configuration file need to enclose all the IPv6 addresses into square brackets if used in IP:Port format. 
If you are using the sample manifest from the previous point, this will require to modify [the configuration line L70](https://github.com/kubernetes/kubernetes/blob/b2ecd1b3a3192fbbe2b9e348e095326f51dc43dd/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml#L70) like this `health [__PILLAR__LOCAL__DNS__]:8080`
-->
<ul>
<li>如果使用 IPv6，在使用 IP:Port 格式的时候需要把 CoreDNS 配置文件里的所有 IPv6 地址用方括号包起来。
如果你使用上述的示例清单，需要把 <a href="https://github.com/kubernetes/kubernetes/blob/b2ecd1b3a3192fbbe2b9e348e095326f51dc43dd/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml#L70">配置行 L70</a>
修改为 <code>health [__PILLAR__LOCAL__DNS__]:8080</code>。</li>
</ul>
<!--
* Substitute the variables in the manifest with the right values:

     * kubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}`

     * domain=`<cluster-domain>`

     * localdns=`<node-local-address>`

     `<cluster-domain>` is "cluster.local" by default. `<node-local-address>` is the local listen IP address chosen for NodeLocal DNSCache.
-->
<ul>
<li>
<p>把清单里的变量更改为正确的值：</p>
<ul>
<li>
<p>kubedns=<code>kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}</code></p>
</li>
<li>
<p>domain=<code>&lt;cluster-domain&gt;</code></p>
</li>
<li>
<p>localdns=<code>&lt;node-local-address&gt;</code></p>
</li>
</ul>
<p><code>&lt;cluster-domain&gt;</code> 的默认值是 &quot;cluster.local&quot;。 <code>&lt;node-local-address&gt;</code> 是 NodeLocal DNSCache 选择的本地侦听 IP 地址。</p>
</li>
</ul>
<!--
   * If kube-proxy is running in IPTABLES mode:

     ``` bash
     sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g" nodelocaldns.yaml
     ```

     `__PILLAR__CLUSTER__DNS__` and `__PILLAR__UPSTREAM__SERVERS__` will be populated by the node-local-dns pods.
     In this mode, node-local-dns pods listen on both the kube-dns service IP as well as `<node-local-address>`, so pods can lookup DNS records using either IP address.
-->   
<ul>
<li>
<p>如果 kube-proxy 运行在 IPTABLES 模式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sed -i <span style="color:#b44">&#34;s/__PILLAR__LOCAL__DNS__/</span><span style="color:#b8860b">$localdns</span><span style="color:#b44">/g; s/__PILLAR__DNS__DOMAIN__/</span><span style="color:#b8860b">$domain</span><span style="color:#b44">/g; s/__PILLAR__DNS__SERVER__/</span><span style="color:#b8860b">$kubedns</span><span style="color:#b44">/g&#34;</span> nodelocaldns.yaml
</code></pre></div><p>node-local-dns Pods 会设置 <code>__PILLAR__CLUSTER__DNS__</code> 和 <code>__PILLAR__UPSTREAM__SERVERS__</code>。
在此模式下, node-local-dns Pods 会同时侦听 kube-dns 服务的 IP 地址和 <code>&lt;node-local-address&gt;</code> 的地址，
以便 Pods 可以使用其中任何一个 IP 地址来查询 DNS 记录。</p>
</li>
</ul>
<!--
  * If kube-proxy is running in IPVS mode:

    ``` bash
     sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/$kubedns/g" nodelocaldns.yaml
    ```
     In this mode, node-local-dns pods listen only on `<node-local-address>`. The node-local-dns interface cannot bind the kube-dns cluster IP since the interface used for IPVS loadbalancing already uses this address.
     `__PILLAR__UPSTREAM__SERVERS__` will be populated by the node-local-dns pods.
-->
<ul>
<li>
<p>如果 kube-proxy 运行在 IPVS 模式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sed -i <span style="color:#b44">&#34;s/__PILLAR__LOCAL__DNS__/</span><span style="color:#b8860b">$localdns</span><span style="color:#b44">/g; s/__PILLAR__DNS__DOMAIN__/</span><span style="color:#b8860b">$domain</span><span style="color:#b44">/g; s/__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/</span><span style="color:#b8860b">$kubedns</span><span style="color:#b44">/g&#34;</span> nodelocaldns.yaml
</code></pre></div><p>在此模式下，node-local-dns Pods 只会侦听 <code>&lt;node-local-address&gt;</code> 的地址。
node-local-dns 接口不能绑定 kube-dns 的集群 IP 地址，因为 IPVS 负载均衡
使用的接口已经占用了该地址。
node-local-dns Pods 会设置 <code>__PILLAR__UPSTREAM__SERVERS__</code>。</p>
</li>
</ul>
<!--
* Run `kubectl create -f nodelocaldns.yaml`
* If using kube-proxy in IPVS mode, `--cluster-dns` flag to kubelet needs to be modified to use `<node-local-address>` that NodeLocal DNSCache is listening on.
  Otherwise, there is no need to modify the value of the `--cluster-dns` flag, since NodeLocal DNSCache listens on both the kube-dns service IP as well as `<node-local-address>`.
-->
<ul>
<li>运行 <code>kubectl create -f nodelocaldns.yaml</code></li>
<li>如果 kube-proxy 运行在 IPVS 模式，需要修改 kubelet 的 <code>--cluster-dns</code> 参数为 NodeLocal DNSCache 正在侦听的 <code>&lt;node-local-address&gt;</code> 地址。
否则，不需要修改 <code>--cluster-dns</code> 参数，因为 NodeLocal DNSCache 会同时侦听 kube-dns 服务的 IP 地址和 <code>&lt;node-local-address&gt;</code> 的地址。</li>
</ul>
<!--
Once enabled, node-local-dns Pods will run in the kube-system namespace on each of the cluster nodes. This Pod runs [CoreDNS](https://github.com/coredns/coredns) in cache mode, so all CoreDNS metrics exposed by the different plugins will be available on a per-node basis.

You can disable this feature by removing the DaemonSet, using `kubectl delete -f <manifest>` . You should also revert any changes you made to the kubelet configuration.
-->
<p>启用后，node-local-dns Pods 将在每个集群节点上的 kube-system 名字空间中运行。
此 Pod 在缓存模式下运行 <a href="https://github.com/coredns/coredns">CoreDNS</a> ，因此每个节点都可以使用不同插件公开的所有 CoreDNS 指标。</p>
<p>如果要禁用该功能，你可以使用 <code>kubectl delete -f &lt;manifest&gt;</code> 来删除 DaemonSet。你还应该恢复你对 kubelet 配置所做的所有改动。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fe5ad73163d38596340536ec03a205f0">2.21 - 在 Kubernetes 集群中使用 sysctl</h1>
    
	<!--
title: Using sysctls in a Kubernetes Cluster
reviewers:
- sttts
content_type: task
--->
<!-- overview -->
<!--
This document describes how to configure and use kernel parameters within a
Kubernetes cluster using the <a class='glossary-tooltip' title='用于获取和设置 Unix 内核参数的接口' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/sysctl-cluster/' target='_blank' aria-label='sysctl'>sysctl</a>
interface.
-->
<p>本文档介绍如何通过 <a class='glossary-tooltip' title='用于获取和设置 Unix 内核参数的接口' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/sysctl-cluster/' target='_blank' aria-label='sysctl'>sysctl</a>
接口在 Kubernetes 集群中配置和使用内核参数。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Listing all Sysctl Parameters
-->
<h2 id="获取-sysctl-的参数列表">获取 Sysctl 的参数列表</h2>
<!--
In Linux, the sysctl interface allows an administrator to modify kernel
parameters at runtime. Parameters are available via the `/proc/sys/` virtual
process file system. The parameters cover various subsystems such as:
-->
<p>在 Linux 中，管理员可以通过 sysctl 接口修改内核运行时的参数。在 <code>/proc/sys/</code>
虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：</p>
<!--
- kernel (common prefix: `kernel.`)
- networking (common prefix: `net.`)
- virtual memory (common prefix: `vm.`)
- MDADM (common prefix: `dev.`)
- More subsystems are described in [Kernel docs](https://www.kernel.org/doc/Documentation/sysctl/README).
-->
<ul>
<li>内核子系统（通常前缀为: <code>kernel.</code>）</li>
<li>网络子系统（通常前缀为: <code>net.</code>）</li>
<li>虚拟内存子系统（通常前缀为: <code>vm.</code>）</li>
<li>MDADM 子系统（通常前缀为: <code>dev.</code>）</li>
<li>更多子系统请参见<a href="https://www.kernel.org/doc/Documentation/sysctl/README">内核文档</a>。</li>
</ul>
<!--
To get a list of all parameters, you can run
--->
<p>若要获取完整的参数列表，请执行以下命令</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo sysctl -a
</code></pre></div><!--
## Enabling Unsafe Sysctls

Sysctls are grouped into _safe_  and _unsafe_ sysctls. In addition to proper
namespacing a _safe_ sysctl must be properly _isolated_ between pods on the same
node. This means that setting a _safe_ sysctl for one pod
-->
<h2 id="启用非安全的-sysctl-参数">启用非安全的 Sysctl 参数</h2>
<p>sysctl 参数分为 <em>安全</em> 和 <em>非安全的</em>。
<em>安全</em> sysctl 参数除了需要设置恰当的命名空间外，在同一 node 上的不同 Pod
之间也必须是 <em>相互隔离的</em>。这意味着在 Pod 上设置 <em>安全</em> sysctl 参数</p>
<!--
- must not have any influence on any other pod on the node
- must not allow to harm the node's health
- must not allow to gain CPU or memory resources outside of the resource limits
  of a pod.
-->
<ul>
<li>必须不能影响到节点上的其他 Pod</li>
<li>必须不能损害节点的健康</li>
<li>必须不允许使用超出 Pod 的资源限制的 CPU 或内存资源。</li>
</ul>
<!--
By far, most of the _namespaced_ sysctls are not necessarily considered _safe_.
The following sysctls are supported in the _safe_ set:
-->
<p>至今为止，大多数 <em>有命名空间的</em> sysctl 参数不一定被认为是 <em>安全</em> 的。
以下几种 sysctl 参数是 <em>安全的</em>：</p>
<ul>
<li><code>kernel.shm_rmid_forced</code></li>
<li><code>net.ipv4.ip_local_port_range</code></li>
<li><code>net.ipv4.tcp_syncookies</code></li>
<li><code>net.ipv4.ping_group_range</code> （从 Kubernetes 1.18 开始）</li>
</ul>
<!--
The example `net.ipv4.tcp_syncookies` is not namespaced on Linux kernel version 4.4 or lower.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 示例中的 <code>net.ipv4.tcp_syncookies</code> 在Linux 内核 4.4 或更低的版本中是无命名空间的。</div>
</blockquote>
<!--
This list will be extended in future Kubernetes versions when the kubelet
supports better isolation mechanisms.
-->
<p>在未来的 Kubernetes 版本中，若 kubelet 支持更好的隔离机制，则上述列表中将会
列出更多 <em>安全的</em> sysctl 参数。</p>
<!--
All _safe_ sysctls are enabled by default.
-->
<p>所有 <em>安全的</em> sysctl 参数都默认启用。</p>
<!--
All _unsafe_ sysctls are disabled by default and must be allowed manually by the
cluster admin on a per-node basis. Pods with disabled unsafe sysctls will be
scheduled, but will fail to launch.
-->
<p>所有 <em>非安全的</em> sysctl 参数都默认禁用，且必须由集群管理员在每个节点上手动开启。
那些设置了不安全 sysctl 参数的 Pod 仍会被调度，但无法正常启动。</p>
<!--
With the warning above in mind, the cluster admin can allow certain _unsafe_
sysctls for very special situations like e.g. high-performance or real-time
application tuning. _Unsafe_ sysctls are enabled on a node-by-node basis with a
flag of the kubelet, e.g.:
-->
<p>参考上述警告，集群管理员只有在一些非常特殊的情况下（如：高可用或实时应用调整），
才可以启用特定的 <em>非安全的</em> sysctl 参数。
如需启用 <em>非安全的</em> sysctl 参数，请你在每个节点上分别设置 kubelet 命令行参数，例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubelet --allowed-unsafe-sysctls <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  <span style="color:#b44">&#39;kernel.msg*,net.core.somaxconn&#39;</span> ...
</code></pre></div><!--
For <a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>, this can be done via the `extra-config` flag:
-->
<p>如果你使用 <a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>，可以通过 <code>extra-config</code> 参数来配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start --extra-config<span style="color:#666">=</span><span style="color:#b44">&#34;kubelet.allowed-unsafe-sysctls=kernel.msg*,net.core.somaxconn&#34;</span>...
</code></pre></div><!--
Only _namespaced_ sysctls can be enabled this way.
-->
<p>只有 <em>有命名空间的</em> sysctl 参数可以通过该方式启用。</p>
<!--
## Setting Sysctls for a Pod

A number of sysctls are _namespaced_ in today's Linux kernels. This means that
they can be set independently for each pod on a node. Only namespaced sysctls
are configurable via the pod securityContext within Kubernetes.
-->
<h2 id="设置-pod-的-sysctl-参数">设置 Pod 的 Sysctl 参数</h2>
<p>目前，在 Linux 内核中，有许多的 sysctl 参数都是 <em>有命名空间的</em> 。
这就意味着可以为节点上的每个 Pod 分别去设置它们的 sysctl 参数。
在 Kubernetes 中，只有那些有命名空间的 sysctl 参数可以通过 Pod 的 securityContext 对其进行配置。</p>
<!--
The following sysctls are known to be namespaced. This list could change
in future versions of the Linux kernel.
-->
<p>以下列出有命名空间的 sysctl 参数，在未来的 Linux 内核版本中，此列表可能会发生变化。</p>
<ul>
<li><code>kernel.shm*</code>,</li>
<li><code>kernel.msg*</code>,</li>
<li><code>kernel.sem</code>,</li>
<li><code>fs.mqueue.*</code>,</li>
<li><code>net.*</code>（内核中可以在容器命名空间里被更改的网络配置项相关参数）。然而也有一些特例
（例如，<code>net.netfilter.nf_conntrack_max</code> 和 <code>net.netfilter.nf_conntrack_expect_max</code>
可以在容器命名空间里被更改，但它们是非命名空间的）。</li>
</ul>
<!--
Sysctls with no namespace are called _node-level_ sysctls. If you need to set
them, you must manually configure them on each node's operating system, or by
using a DaemonSet with privileged containers.
-->
<p>没有命名空间的 sysctl 参数称为 <em>节点级别的</em> sysctl 参数。
如果需要对其进行设置，则必须在每个节点的操作系统上手动地去配置它们，
或者通过在 DaemonSet 中运行特权模式容器来配置。</p>
<!--
Use the pod securityContext to configure namespaced sysctls. The securityContext
applies to all containers in the same pod.
-->
<p>可使用 Pod 的 securityContext 来配置有命名空间的 sysctl 参数，
securityContext 应用于同一个 Pod 中的所有容器。</p>
<!--
This example uses the pod securityContext to set a safe sysctl
`kernel.shm_rmid_forced` and two unsafe sysctls `net.core.somaxconn` and
`kernel.msgmax` There is no distinction between _safe_ and _unsafe_ sysctls in
the specification.
-->
<p>此示例中，使用 Pod SecurityContext 来对一个安全的 sysctl 参数
<code>kernel.shm_rmid_forced</code> 以及两个非安全的 sysctl 参数
<code>net.core.somaxconn</code> 和 <code>kernel.msgmax</code> 进行设置。
在 Pod 规约中对 <em>安全的</em> 和 <em>非安全的</em> sysctl 参数不做区分。</p>
<!--
Only modify sysctl parameters after you understand their effects, to avoid
destabilizing your operating system.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 为了避免破坏操作系统的稳定性，请你在了解变更后果之后再修改 sysctl 参数。</div>
</blockquote>

<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sysctl-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">sysctls</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kernel.shm_rmid_forced<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>net.core.somaxconn<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1024&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kernel.msgmax<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;65536&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!-- discussion -->
<!--
Due to their nature of being _unsafe_, the use of _unsafe_ sysctls
is at-your-own-risk and can lead to severe problems like wrong behavior of
containers, resource shortage or complete breakage of a node.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 由于 <em>非安全的</em> sysctl 参数其本身具有不稳定性，在使用 <em>非安全的</em> sysctl 参数
时可能会导致一些严重问题，如容器的错误行为、机器资源不足或节点被完全破坏，
用户需自行承担风险。</div>
</blockquote>

<!--
It is good practice to consider nodes with special sysctl settings as
_tainted_ within a cluster, and only schedule pods onto them which need those
sysctl settings. It is suggested to use the Kubernetes [_taints and toleration_
feature](/docs/reference/generated/kubectl/kubectl-commands/#taint) to implement this.
-->
<p>最佳实践方案是将集群中具有特殊 sysctl 设置的节点视为 <em>有污点的</em>，并且只调度
需要使用到特殊 sysctl 设置的 Pod 到这些节点上。
建议使用 Kubernetes 的
<a href="/docs/reference/generated/kubectl/kubectl-commands/#taint">污点和容忍度特性</a> 来实现它。</p>
<!--
A pod with the _unsafe_ sysctls will fail to launch on any node which has not
enabled those two _unsafe_ sysctls explicitly. As with _node-level_ sysctls it
is recommended to use
[_taints and toleration_ feature](/docs/reference/generated/kubectl/kubectl-commands/#taint) or
[taints on nodes](/docs/concepts/configuration/taint-and-toleration/)
to schedule those pods onto the right nodes.
-->
<p>设置了 <em>非安全的</em> sysctl 参数的 Pod 在禁用了这两种 <em>非安全的</em> sysctl 参数配置
的节点上启动都会失败。与 <em>节点级别的</em> sysctl 一样，建议开启
<a href="/docs/reference/generated/kubectl/kubectl-commands/#taint">污点和容忍度特性</a> 或
<a href="/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">为节点配置污点</a>
以便将 Pod 调度到正确的节点之上。</p>
<h2 id="podsecuritypolicy">PodSecurityPolicy</h2>
<!--
You can further control which sysctls can be set in pods by specifying lists of
sysctls or sysctl patterns in the `forbiddenSysctls` and/or
`allowedUnsafeSysctls` fields of the PodSecurityPolicy. A sysctl pattern ends
with a `*` character, such as `kernel.*`. A `*` character on its own matches
all sysctls.
-->
<p>你可以通过在 PodSecurityPolicy 的 <code>forbiddenSysctls</code> 和/或 <code>allowedUnsafeSysctls</code>
字段中，指定 sysctl 或填写 sysctl 匹配模式来进一步为 Pod 设置 sysctl 参数。
sysctl 参数匹配模式以 <code>*</code> 字符结尾，如 <code>kernel.*</code>。
单独的 <code>*</code>  字符匹配所有 sysctl 参数。</p>
<!--
By default, all safe sysctls are allowed.
-->
<p>所有 <em>安全的</em> sysctl 参数都默认启用。</p>
<!--
Both `forbiddenSysctls` and `allowedUnsafeSysctls` are lists of plain sysctl names
or sysctl patterns (which end with `*`). The string `*` matches all sysctls.
-->
<p><code>forbiddenSysctls</code> 和 <code>allowedUnsafeSysctls</code> 的值都是字符串列表类型，
可以添加 sysctl 参数名称，也可以添加 sysctl 参数匹配模式（以<code>*</code>结尾）。
只填写 <code>*</code> 则匹配所有的 sysctl 参数。</p>
<!--
The `forbiddenSysctls` field excludes specific sysctls. You can forbid a
combination of safe and unsafe sysctls in the list. To forbid setting any
sysctls, use `*` on its own.
-->
<p><code>forbiddenSysctls</code> 字段用于禁用特定的 sysctl 参数。
你可以在列表中禁用安全和非安全的 sysctl 参数的组合。
要禁用所有的 sysctl 参数，请设置为 <code>*</code>。</p>
<!--
If you specify any unsafe sysctl in the `allowedUnsafeSysctls` field and it is
not present in the `forbiddenSysctls` field, that sysctl can be used in Pods
using this PodSecurityPolicy. To allow all unsafe sysctls in the
PodSecurityPolicy to be set, use `*` on its own.
-->
<p>如果要在 <code>allowedUnsafeSysctls</code> 字段中指定一个非安全的 sysctl 参数，
并且它在 <code>forbiddenSysctls</code> 字段中未被禁用，则可以在 Pod 中通过
PodSecurityPolicy 启用该 sysctl 参数。
若要在 PodSecurityPolicy 中开启所有非安全的 sysctl 参数，
请设 <code>allowedUnsafeSysctls</code> 字段值为 <code>*</code>。</p>
<!--
Do not configure these two fields such that there is overlap, meaning that a
given sysctl is both allowed and forbidden.
-->
<p><code>allowedUnsafeSysctls</code> 与 <code>forbiddenSysctls</code> 两字段的配置不能重叠，
否则这就意味着存在某个 sysctl 参数既被启用又被禁用。</p>
<!--
If you whitelist unsafe sysctls via the `allowedUnsafeSysctls` field
in a PodSecurityPolicy, any pod using such a sysctl will fail to start
if the sysctl is not whitelisted via the `--allowed-unsafe-sysctls` kubelet
flag as well on that node.
--->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 如果你通过 PodSecurityPolicy 中的 <code>allowedUnsafeSysctls</code> 字段将非安全的 sysctl
参数列入白名单，但该 sysctl 参数未通过 kubelet 命令行参数
<code>--allowed-unsafe-sysctls</code> 在节点上将其列入白名单，则设置了这个 sysctl
参数的 Pod 将会启动失败。</div>
</blockquote>

<!--
This example allows unsafe sysctls prefixed with `kernel.msg` to be set and
disallows setting of the `kernel.shm_rmid_forced` sysctl.
-->
<p>以下示例设置启用了以 <code>kernel.msg</code> 为前缀的非安全的 sysctl 参数，同时禁用了
sysctl 参数 <code>kernel.shm_rmid_forced</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>policy/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PodSecurityPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sysctl-psp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">allowedUnsafeSysctls</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- kernel.msg*<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">forbiddenSysctls</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- kernel.shm_rmid_forced<span style="color:#bbb">
</span><span style="color:#bbb"> </span>...<span style="color:#bbb">
</span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eec61e72c300dbfbf7302400ca966432">2.22 - 在运行中的集群上重新配置节点的 kubelet</h1>
    
	<!--
reviewers:
- mtaufen
- dawnchen
title: Reconfigure a Node's Kubelet in a Live Cluster
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [deprecated]</code>
</div>

<!--
Caution: Dynamic Kubelet Configuration feature is deprecated and should not be used. Please switch to alternative means distributing configuration to the Nodes of your cluster.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> <a href="https://github.com/kubernetes/enhancements/issues/281">动态 kubelet 配置</a>
已经废弃不建议使用。请选择其他方法将配置分发到集群中的节点。</div>
</blockquote>

<!--
[Dynamic Kubelet Configuration](https://github.com/kubernetes/enhancements/issues/281)
allows you to change the configuration of each Kubelet in a live Kubernetes
cluster by deploying a ConfigMap and configuring each Node to use it.
-->
<p><a href="https://github.com/kubernetes/enhancements/issues/281">动态 kubelet 配置</a>
允许你通过部署一个所有节点都会使用的 ConfigMap
达到在运行中的 Kubernetes 集群中更改 kubelet 配置的目的。</p>
<!--
All kubelet configuration parameters can be changed dynamically,
but this is unsafe for some parameters. Before deciding to change a parameter
dynamically, you need a strong understanding of how that change will affect your
cluster's behavior. Always carefully test configuration changes on a small set
of nodes before rolling them out cluster-wide. Advice on configuring specific
fields is available in the inline
[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/).
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 所有 kubelet 配置参数都可以被动态更改，但对某些参数来说这类更改是不安全的。
在决定动态更改参数之前，你需要深刻理解这个改动将会如何影响集群的行为。
在将变更扩散到整个集群之前，你需要先在小规模的节点集合上仔细地测试这些配置变动。
特定字段相关的配置建议可以在文档
<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>中找到。</div>
</blockquote>

<h2 id="准备开始">准备开始</h2>
<!--
You need to have a Kubernetes cluster.
You also need kubectl v1.11 or higher, configured to communicate with your cluster.

 要获知版本信息，请输入 <code>kubectl version</code>.

Your cluster API server version (eg v1.12) must be no more than one minor
version away from the version of kubectl that you are using. For example,
if your cluster is running v1.16 then you can use kubectl v1.15, v1.16
or v1.17; other combinations
[aren't supported](/docs/setup/release/version-skew-policy/#kubectl).
-->
<p>你需要一个 Kubernetes 集群。
你需要 v1.11 或更高版本的 kubectl，并配置好与集群的通信。

 要获知版本信息，请输入 <code>kubectl version</code>.

你的集群 API 服务器版本（如 v1.12）不能和你的 kubectl
版本相差超过一个小版本号。
例如，如果你的集群在运行 v1.16，那么你可以使用 v1.15、v1.16、v1.17 的 kubectl，
所有其他的组合都是
<a href="/zh/docs/setup/release/version-skew-policy/#kubectl">不支持的</a>。</p>
<!--
Some of the examples use the command line tool
[jq](https://stedolan.github.io/jq/). You do not need `jq` to complete the task,
because there are manual alternatives.

For each node that you're reconfiguring, you must set the kubelet
`-dynamic-config-dir` flag to a writable directory.
-->
<p>在某些例子中使用了命令行工具 <a href="https://stedolan.github.io/jq/">jq</a>。
你并不一定需要 <code>jq</code> 才能完成这些任务，因为总是有一些手工替代的方式。</p>
<p>针对你重新配置的每个节点，你必须设置 kubelet 的标志
<code>-dynamic-config-dir</code>，使之指向一个可写的目录。</p>
<!-- steps -->
<!--
## Reconfiguring the kubelet on a running node in your cluster

### Basic Workflow Overview
-->
<h2 id="重配置-集群中运行节点上的-kubelet">重配置 集群中运行节点上的 kubelet</h2>
<h3 id="基本工作流程概览">基本工作流程概览</h3>
<!--
The basic workflow for configuring a Kubelet in a live cluster is as follows:

1. Write a YAML or JSON configuration file containing the
   kubelet's configuration.
2. Wrap this file in a ConfigMap and save it to the Kubernetes control plane.
3. Update the Kubelet's corresponding Node object to use this ConfigMap.
-->
<p>在运行中的集群中配置 kubelet 的基本工作流程如下：</p>
<ol>
<li>编写一个包含 kubelet 配置的 YAML 或 JSON 文件。</li>
<li>将此文件包装在 ConfigMap 中并将其保存到 Kubernetes 控制平面。</li>
<li>更新 kubelet 所在节点对象以使用此 ConfigMap。</li>
</ol>
<!--
Each kubelet watches a configuration reference on its respective Node object.
When this reference changes, the Kubelet downloads the new configuration,
updates a local reference to refer to the file, and exits.
For the feature to work correctly, you must be running an OS-level service
manager (such as systemd), which will restart the Kubelet if it exits. When the
Kubelet is restarted, it will begin using the new configuration.
-->
<p>每个 kubelet 都会在其各自的节点对象上监测（Watch）配置引用。当引用更改时，kubelet 将下载新的配置文件，
更新本地引用指向该文件，然后退出。
为了使该功能正常地工作，你必须运行操作系统级别的服务管理器（如 systemd），
它将会在 kubelet 退出后将其重启。
kubelet 重新启动时，将开始使用新配置。</p>
<!--
The new configuration completely overrides configuration provided by `--config`,
and is overridden by command-line flags. Unspecified values in the new configuration
will receive default values appropriate to the configuration version
(e.g. `kubelet.config.k8s.io/v1beta1`), unless overridden by flags.
-->
<p>新配置将会完全地覆盖 <code>--config</code> 所提供的配置，并被命令行标志覆盖。
新配置中未指定的值将收到适合配置版本的默认值
(e.g. <code>kubelet.config.k8s.io/v1beta1</code>)，除非被命令行标志覆盖。</p>
<!--
The status of the Node's Kubelet configuration is reported via
`Node.Spec.Status.Config`. Once you have updated a Node to use the new
ConfigMap, you can observe this status to confirm that the Node is using the
intended configuration.
-->
<p>节点 kubelet 配置状态可通过 <code>node.spec.status.config</code> 获取。
一旦你更新了一个节点去使用新的 ConfigMap，
就可以通过观察此状态来确认该节点是否正在使用预期配置。</p>
<!--
This document describes editing Nodes using `kubectl edit`.
There are other ways to modify a Node's spec, including `kubectl patch`, for
example, which facilitate scripted workflows.
-->
<p>本文中使用命令 <code>kubectl edit</code> 来编辑节点，还有其他的方式可以修改节点的规约，
比如更利于脚本化工作流程的 <code>kubectl patch</code>。</p>
<!--
This document only describes a single Node consuming each ConfigMap. Keep in
mind that it is also valid for multiple Nodes to consume the same ConfigMap.
-->
<p>本文仅仅讲述在单节点上使用每个 ConfigMap。请注意对于多个节点使用相同的 ConfigMap
也是合法的。</p>
<!--
While it is *possible* to change the configuration by
updating the ConfigMap in-place, this causes all Kubelets configured with
that ConfigMap to update simultaneously. It is much safer to treat ConfigMaps
as immutable by convention, aided by `kubectl`'s `-append-hash` option,
and incrementally roll out updates to `Node.Spec.ConfigSource`.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 尽管通过就地更新 ConfigMap 来更改配置是 <em>可能的</em>。
但是这样做会导致所有使用该 ConfigMap 配置的 kubelet 同时更新。
更安全的做法是按惯例将 ConfigMap 视为不可变更的，借助于
<code>kubectl</code> 的 <code>--append-hash</code> 选项逐步把更新推广到 <code>node.spec.configSource</code>。</div>
</blockquote>

<!--
### Automatic RBAC rules for Node Authorizer

Previously, you were required to manually create RBAC rules
to allow Nodes to access their assigned ConfigMaps. The Node Authorizer now
automatically configures these rules.
-->
<h3 id="节点鉴权器的自动-rbac-规则">节点鉴权器的自动 RBAC 规则</h3>
<p>以前，你需要手动创建 RBAC 规则以允许节点访问其分配的 ConfigMap。节点鉴权器现在
能够自动配置这些规则。</p>
<!--
### Generating a file that contains the current configuration

The Dynamic Kubelet Configuration feature allows you to provide an override for
the entire configuration object, rather than a per-field overlay. This is a
simpler model that makes it easier to trace the source of configuration values
and debug issues. The compromise, however, is that you must start with knowledge
of the existing configuration to ensure that you only change the fields you
intend to change.
-->
<h3 id="生成包含当前配置的文件">生成包含当前配置的文件</h3>
<p>动态 kubelet 配置特性允许你为整个配置对象提供一个重载配置，而不是靠单个字段的叠加。
这是一个更简单的模型，可以更轻松地跟踪配置值的来源，更便于调试问题。
然而，相应的代价是你必须首先了解现有配置，以确保你只更改你打算修改的字段。</p>
<!--
The kubelet loads settings from its configuration file, but you can set command
line flags to override the configuration in the file. This means that if you
only know the contents of the configuration file, and you don't know the
command line overrides, then you do not know the running configuration either.
-->
<p>组件 kubelet 从其配置文件中加载配置数据，不过你可以通过设置命令行标志
来重载文件中的一些配置。这意味着，如果你仅知道配置文件的内容，而你不知道
命令行重载了哪些配置，你就无法知道 kubelet 的运行时配置是什么。</p>
<!--
Because you need to know the running configuration in order to override it,
you can fetch the running configuration from the kubelet. You can generate a
config file containing a Node's current configuration by accessing the kubelet's
`configz` endpoint, through `kubectl proxy`. The next section explains how to
do this.
-->
<p>因为你需要知道运行时所使用的配置才能重载之，你可以从 kubelet 取回其运行时配置。
你可以通过访问 kubelet 的 <code>configz</code> 末端来生成包含节点当前配置的配置文件；
这一操作可以通过 <code>kubectl proxy</code> 来完成。
下一节解释如何完成这一操作。</p>
<!--
The kubelet's `configz` endpoint is there to help with debugging, and is not
a stable part of kubelet behavior.
Do not rely on the behavior of this endpoint for production scenarios or for
use with automated tools.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 组件 <code>kubelet</code> 上的 <code>configz</code> 末端是用来协助调试的，并非 kubelet 稳定行为的一部分。
请不要在产品环境下依赖此末端的行为，也不要在自动化工具中使用此末端。</div>
</blockquote>

<!--
For more information on configuring the kubelet via a configuration file, see
[Set kubelet parameters via a config file](/docs/tasks/administer-cluster/kubelet-config-file)).
-->
<p>关于如何使用配置文件来配置 kubelet 行为的更多信息可参见
<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file">通过配置文件设置 kubelet 参数</a>
文档。</p>
<!-- #### Generate the configuration file -->
<h4 id="生成配置文件">生成配置文件</h4>
<!--
The steps below use the `jq` command to streamline working with JSON.
To follow the tasks as written, you need to have `jq` installed. You can
adapt the steps if you prefer to extract the `kubeletconfig` subobject manually.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 下面的任务步骤中使用了 <code>jq</code> 命令以方便处理 JSON 数据。为了完成这里讲述的任务，
你需要安装 <code>jq</code>。如果你更希望手动提取 <code>kubeletconfig</code> 子对象，也可以对这里
的对应步骤做一些调整。</div>
</blockquote>
<!--
1. Choose a Node to reconfigure. In this example, the name of this Node is
   referred to as `NODE_NAME`.
2. Start the kubectl proxy in the background using the following command:
-->
<ol>
<li>
<p>选择要重新配置的节点。在本例中，此节点的名称为 <code>NODE_NAME</code>。</p>
</li>
<li>
<p>使用以下命令在后台启动 kubectl 代理：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8001</span> &amp;
</code></pre></div></li>
</ol>
<!--
3. Run the following command to download and unpack the configuration from the
   `configz` endpoint. The command is long, so be careful when copying and
   pasting. **If you use zsh**, note that common zsh configurations add backslashes
   to escape the opening and closing curly braces around the variable name in the URL.
   For example: `${NODE_NAME}` will be rewritten as `$\{NODE_NAME\}` during the paste.
   You must remove the backslashes before running the command, or the command will fail.
-->
<ol start="3">
<li>
<p>运行以下命令从 <code>configz</code> 端点中下载并解压配置。这个命令很长，因此在复制粘贴时要小心。
<strong>如果你使用 zsh</strong>，请注意常见的 zsh 配置要添加反斜杠转义 URL 中变量名称周围的大括号。
例如：在粘贴时，<code>${NODE_NAME}</code> 将被重写为 <code>$\{NODE_NAME\}</code>。
你必须在运行命令之前删除反斜杠，否则命令将失败。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">NODE_NAME</span><span style="color:#666">=</span><span style="color:#b44">&#34;the-name-of-the-node-you-are-reconfiguring&#34;</span>; curl -sSL <span style="color:#b44">&#34;http://localhost:8001/api/v1/nodes/</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/proxy/configz&#34;</span> | jq <span style="color:#b44">&#39;.kubeletconfig|.kind=&#34;KubeletConfiguration&#34;|.apiVersion=&#34;kubelet.config.k8s.io/v1beta1&#34;&#39;</span> &gt; kubelet_configz_<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div></li>
</ol>
<!--
You need to manually add the `kind` and `apiVersion` to the downloaded
object，because they are not reported by the `configz` endpoint。
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 你需要手动将 <code>kind</code> 和 <code>apiVersion</code> 添加到下载对象中，因为它们不是由 <code>configz</code> 末端
返回的。</div>
</blockquote>
<!--
#### Edit the configuration file

Using a text editor, change one of the parameters in the
file generated by the previous procedure. For example, you
might edit the QPS parameter `eventRecordQPS`.
-->
<h4 id="修改配置文件">修改配置文件</h4>
<p>使用文本编辑器，改变上述操作生成的文件中一个参数。
例如，你或许会修改 QPS 参数 <code>eventRecordQPS</code>。</p>
<!--
#### Push the configuration file to the control plane

Push the edited configuration file to the control plane with the
following command:
-->
<h4 id="把配置文件推送到控制平面">把配置文件推送到控制平面</h4>
<p>用以下命令把编辑后的配置文件推送到控制平面：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl -n kube-system create configmap my-node-config <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-file<span style="color:#666">=</span><span style="color:#b8860b">kubelet</span><span style="color:#666">=</span>kubelet_configz_<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --append-hash -o yaml
</code></pre></div><!--
This is an example of a valid response:
-->
<p>下面是合法响应的一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2017-09-14T20:23:33Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-node-config-gkt4c2m4b2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;119980&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/namespaces/kube-system/configmaps/my-node-config-gkt4c2m4b2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>946d785e-998a-11e7-a8dd-42010a800006<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kubelet</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>{...}<span style="color:#bbb">
</span></code></pre></div><!--
You created that ConfigMap inside the `kube-system` namespace because the kubelet
is a Kubernetes system component.
-->
<p>你会在 <code>kube-system</code> 命名空间中创建 ConfigMap，因为 kubelet 是 Kubernetes 的系统组件。</p>
<!--
The `-append-hash` option appends a short checksum of the ConfigMap contents
to the name. This is convenient for an edit-then-push workflow, because it
automatically, yet deterministically, generates new names for new ConfigMaps.
The name that includes this generated hash is referred to as `CONFIG_MAP_NAME`
in the following examples.
-->
<p><code>--append-hash</code> 选项给 ConfigMap 内容附加了一个简短校验和。
这对于先编辑后推送的工作流程很方便，
因为它自动并确定地为新 ConfigMap 生成新的名称。
在以下示例中，包含生成的哈希字符串的对象名被称为 <code>CONFIG_MAP_NAME</code>。</p>
<!--
#### Set the Node to use the new configuration


Edit the Node's reference to point to the new ConfigMap with the
following command:
-->
<h4 id="配置节点使用新的配置">配置节点使用新的配置</h4>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl edit node <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><!--
In your text editor, add the following YAML under `spec`:
-->
<p>在你的文本编辑器中，在 <code>spec</code> 下增添以下 YAML：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">configSource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>CONFIG_MAP_NAME<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubeletConfigKey</span>:<span style="color:#bbb"> </span>kubelet<span style="color:#bbb">
</span></code></pre></div><!--
You must specify all three of `name`, `namespace`, and `kubeletConfigKey`.
The `kubeletConfigKey` parameter shows the Kubelet which key of the ConfigMap
contains its config.
-->
<p>你必须同时指定 <code>name</code>、<code>namespace</code> 和 <code>kubeletConfigKey</code> 这三个属性。
<code>kubeletConfigKey</code> 这个参数通知 kubelet ConfigMap 中的哪个键下面包含所要的配置。</p>
<!--
#### Observe that the Node begins using the new configuration

Retrieve the Node using the `kubectl get node ${NODE_NAME} -o yaml` command and inspect
`Node.Status.Config`. The config sources corresponding to the `active`,
`assigned`, and `lastKnownGood` configurations are reported in the status.

- The `active` configuration is the version the Kubelet is currently running with.
- The `assigned` configuration is the latest version the Kubelet has resolved based on
  `Node.Spec.ConfigSource`.
- The `lastKnownGood` configuration is the version the
  Kubelet will fall back to if an invalid config is assigned in `Node.Spec.ConfigSource`.
-->
<h4 id="观察节点开始使用新配置">观察节点开始使用新配置</h4>
<p>用 <code>kubectl get node ${NODE_NAME} -o yaml</code> 命令读取节点并检查 <code>node.status.config</code> 内容。
状态部分报告了对应 <code>active</code>（使用中的）配置、<code>assigned</code>（被赋予的）配置和
<code>lastKnownGood</code>（最近已知可用的）配置的配置源。</p>
<ul>
<li><code>active</code> 是 kubelet 当前运行时所使用的版本。</li>
<li><code>assigned</code> 参数是 kubelet 基于 <code>node.spec.configSource</code> 所解析出来的最新版本。</li>
<li><code>lastKnownGood</code> 参数是 kubelet 的回退版本；如果在 <code>node.spec.configSource</code> 中
包含了无效的配置值，kubelet 可以回退到这个版本。</li>
</ul>
<!--
The`lastKnownGood` configuration might not be present if it is set to its default value,
the local config deployed with the node. The status will update `lastKnownGood` to
match a valid `assigned` config after the Kubelet becomes comfortable with the config.
The details of how the Kubelet determines a config should become the `lastKnownGood` are
not guaranteed by the API, but is currently implemented as a 10-minute grace period.
-->
<p>如果用本地配置部署节点，使其设置成默认值，这个 <code>lastKnownGood</code> 配置可能不存在。
在 kubelet 配置好后，将更新 <code>lastKnownGood</code> 为一个有效的 <code>assigned</code> 配置。
决定如何确定某配置成为 <code>lastKnownGood</code> 配置的细节并不在 API 保障范畴，
不过目前实现中采用了 10 分钟的宽限期。</p>
<!--
You can use the following command (using `jq`) to filter down
to the config status:
-->
<p>你可以使用以下命令（使用 <code>jq</code>）过滤出配置状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get no <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span> -o json | jq <span style="color:#b44">&#39;.status.config&#39;</span>
</code></pre></div><!--
The following is an example response:
-->
<p>以下是一个响应示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;active&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;configMap&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;kubeletConfigKey&#34;</span>: <span style="color:#b44">&#34;kubelet&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;kube-system&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;resourceVersion&#34;</span>: <span style="color:#b44">&#34;1326&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  },
  <span style="color:#008000;font-weight:bold">&#34;assigned&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;configMap&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;kubeletConfigKey&#34;</span>: <span style="color:#b44">&#34;kubelet&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;kube-system&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;resourceVersion&#34;</span>: <span style="color:#b44">&#34;1326&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  },
  <span style="color:#008000;font-weight:bold">&#34;lastKnownGood&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;configMap&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;kubeletConfigKey&#34;</span>: <span style="color:#b44">&#34;kubelet&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;my-node-config-9mbkccg2cc&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;kube-system&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;resourceVersion&#34;</span>: <span style="color:#b44">&#34;1326&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span>
    }
  }
}
</code></pre></div><!--
If you do not have `jq`, you can look at the whole response and find `Node.Status.Config`
by eye.
-->
<p>如果你没有安装 <code>jq</code>，你可以查看整个响应对象，查找其中的 <code>node.status.config</code>
部分。</p>
<!--
If an error occurs, the Kubelet reports it in the `Node.Status.Config.Error`
structure. Possible errors are listed in
[Understanding Node.Status.Config.Error messages](#understanding-node-config-status-errors).
You can search for the identical text in the Kubelet log for additional details
and context about the error.
-->
<p>如果发生错误，kubelet 会在 <code>Node.Status.Config.Error</code> 中显示出错误信息的结构体。
错误可能出现在列表<a href="#understanding-node-config-status-errors">理解节点状态配置错误信息</a>中。
你可以在 kubelet 日志中搜索相同的文本以获取更多详细信息和有关错误的上下文。</p>
<!--
#### Make more changes

Follow the workflow above to make more changes and push them again. Each time
you push a ConfigMap with new contents, the -append-hash kubectl option creates
the ConfigMap with a new name. The safest rollout strategy is to first create a
new ConfigMap, and then update the Node to use the new ConfigMap.
-->
<h4 id="make-more-changes">做出更多的改变  </h4>
<p>按照下面的工作流程做出更多的改变并再次推送它们。
你每次推送一个 ConfigMap 的新内容时，kubeclt 的 <code>--append-hash</code> 选项都会给
ConfigMap 创建一个新的名称。
最安全的上线策略是首先创建一个新的 ConfigMap，然后更新节点以使用新的 ConfigMap。</p>
<!--
#### Reset the Node to use its local default configuration

To reset the Node to use the configuration it was provisioned with, edit the
Node using `kubectl edit node ${NODE_NAME}` and remove the
`Node.Spec.ConfigSource` field.
-->
<h4 id="重置节点以使用其本地默认配置">重置节点以使用其本地默认配置</h4>
<p>要重置节点，使其使用节点创建时使用的配置，可以用
<code>kubectl edit node $ {NODE_NAME}</code> 命令编辑节点，并删除 <code>node.spec.configSource</code>
字段。</p>
<!-- 
#### Observe that the Node is using its local default configuration

After removing this subfield, `Node.Status.Config` eventually becomes
empty, since all config sources have been reset to `nil`, which indicates that
the local default config is `assigned`, `active`, and `lastKnownGood`, and no
error is reported.
-->
<h4 id="观察节点正在使用本地默认配置">观察节点正在使用本地默认配置</h4>
<p>在删除此字段后，<code>node.status.config</code> 最终变成空，所有配置源都已重置为 <code>nil</code>。
这表示本地默认配置成为了 <code>assigned</code>、<code>active</code> 和 <code>lastKnownGood</code> 配置，
并且没有报告错误。</p>
<!-- discussion -->
<!--
## `kubectl patch` example

You can change a Node's configSource using several different mechanisms.
This example uses `kubectl patch`:
-->
<h2 id="kubectl-patch-示例"><code>kubectl patch</code> 示例</h2>
<p>你可以使用几种不同的机制来更改节点的 configSource。</p>
<p>本例使用<code>kubectl patch</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch node <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">NODE_NAME</span><span style="color:#b68;font-weight:bold">}</span> -p <span style="color:#b44">&#34;{\&#34;spec\&#34;:{\&#34;configSource\&#34;:{\&#34;configMap\&#34;:{\&#34;name\&#34;:\&#34;</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONFIG_MAP_NAME</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">\&#34;,\&#34;namespace\&#34;:\&#34;kube-system\&#34;,\&#34;kubeletConfigKey\&#34;:\&#34;kubelet\&#34;}}}}&#34;</span>
</code></pre></div><!--
## Understanding how the Kubelet checkpoints config

When a new config is assigned to the Node, the Kubelet downloads and unpacks the
config payload as a set of files on the local disk. The Kubelet also records metadata
that locally tracks the assigned and last-known-good config sources, so that the
Kubelet knows which config to use across restarts, even if the API server becomes
unavailable. After checkpointing a config and the relevant metadata, the Kubelet
exits if it detects that the assigned config has changed. When the Kubelet is
restarted by the OS-level service manager (such as `systemd`), it reads the new
metadata and uses the new config.
-->
<h2 id="了解-kubelet-如何为配置生成检查点">了解 Kubelet 如何为配置生成检查点</h2>
<p>当为节点赋予新配置时，kubelet 会下载并解压配置负载为本地磁盘上的一组文件。
kubelet 还记录一些元数据，用以在本地跟踪已赋予的和最近已知良好的配置源，以便
kubelet 在重新启动时知道使用哪个配置，即使 API 服务器变为不可用。
在为配置信息和相关元数据生成检查点之后，如果检测到已赋予的配置发生改变，则 kubelet 退出。
当 kubelet 被 OS 级服务管理器（例如 <code>systemd</code>）重新启动时，它会读取新的元数据并使用新配置。</p>
<!--
The recorded metadata is fully resolved, meaning that it contains all necessary
information to choose a specific config version - typically a `UID` and `ResourceVersion`.
This is in contrast to `Node.Spec.ConfigSource`, where the intended config is declared
via the idempotent `namespace/name` that identifies the target ConfigMap; the Kubelet
tries to use the latest version of this ConfigMap.
-->
<p>当记录的元数据已被完全解析时，意味着它包含选择一个指定的配置版本所需的所有信息
-- 通常是 <code>UID</code> 和 <code>ResourceVersion</code>。
这与 <code>node.spec.configSource</code> 形成对比，后者通过幂等的 <code>namespace/name</code> 声明来标识
目标 ConfigMap；kubelet 尝试使用此 ConfigMap 的最新版本。</p>
<!--
When you are debugging problems on a node, you can inspect the Kubelet's config
metadata and checkpoints. The structure of the Kubelet's checkpointing directory is:
-->
<p>当你在调试节点上问题时，可以检查 kubelet 的配置元数据和检查点。kubelet 的检查点目录结构是：</p>
<!--
```none
- -dynamic-config-dir (root for managing dynamic config)
| - meta
  | - assigned (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the assigned config)
  | - last-known-good (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the last-known-good config)
| - checkpoints
  | - uid1 (dir for versions of object identified by uid1)
    | - resourceVersion1 (dir for unpacked files from resourceVersion1 of object with uid1)
    | - ...
  | - ...
```
-->
<pre tabindex="0"><code class="language-none" data-lang="none">- --dynamic-config-dir （用于管理动态配置的根目录）
|-- meta
  | - assigned （编码后的 kubeletconfig/v1beta1.SerializedNodeConfigSource 对象，对应赋予的配置）
  | - last-known-good （编码后的 kubeletconfig/v1beta1.SerializedNodeConfigSource 对象，对应最近已知可用配置）
| - checkpoints
  | - uid1 （用 uid1 来标识的对象版本目录)
    | - resourceVersion1 （uid1 对象 resourceVersion1 版本下所有解压文件的目录）
    | - ...
  | - ...
</code></pre><!-- 
## Understanding `Node.Status.Config.Error` messages {#understanding-node-config-status-errors}

The following table describes error messages that can occur
when using Dynamic Kubelet Config. You can search for the identical text
in the Kubelet log for additional details and context about the error.
-->
<h2 id="understanding-node-config-status-errors">理解 <code>Node.Status.Config.Error</code> 消息</h2>
<p>下表描述了使用动态 kubelet 配置时可能发生的错误消息。
你可以在 kubelet 日志中搜索相同的文本来获取有关错误的其他详细信息和上下文。</p>
<!--
Error Message    | Possible Causes
:----------------| :----------------
failed to load config, see Kubelet log for details | The kubelet likely could not parse the downloaded config payload, or encountered a filesystem error attempting to load the payload from disk.
failed to validate config, see Kubelet log for details | The configuration in the payload, combined with any command-line flag overrides, and the sum of feature gates from flags, the config file, and the remote payload, was determined to be invalid by the kubelet.
invalid NodeConfigSource, exactly one subfield must be non-nil, but all were nil | Since Node.Spec.ConfigSource is validated by the API server to contain at least one non-nil subfield, this likely means that the kubelet is older than the API server and does not recognize a newer source type.
failed to sync: failed to download config, see Kubelet log for details | The kubelet could not download the config. It is possible that Node.Spec.ConfigSource could not be resolved to a concrete API object, or that network errors disrupted the download attempt. The kubelet will retry the download when in this error state.
failed to sync: internal failure, see Kubelet log for details | The kubelet encountered some internal problem and failed to update its config as a result. Examples include filesystem errors and reading objects from the internal informer cache.
internal failure, see Kubelet log for details | The kubelet encountered some internal problem while manipulating config, outside of the configuration sync loop.
-->





<table><caption style="display: none;">理解 node.status.config.error 消息</caption>
<thead>
<tr>
<th style="text-align:left">错误信息</th>
<th style="text-align:left">可能的原因</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">failed to load config, see Kubelet log for details</td>
<td style="text-align:left">kubelet 可能无法解析下载配置的有效负载，或者当尝试从磁盘中加载有效负载时，遇到文件系统错误。</td>
</tr>
<tr>
<td style="text-align:left">failed to validate config, see Kubelet log for details</td>
<td style="text-align:left">有效负载中的配置，与命令行标志所产生的覆盖配置以及特行门控的组合、配置文件本身、远程负载被 kubelet 判定为无效。</td>
</tr>
<tr>
<td style="text-align:left">invalid NodeConfigSource, exactly one subfield must be non-nil, but all were nil</td>
<td style="text-align:left">由于 API 服务器负责对 node.spec.configSource 执行验证，检查其中是否包含至少一个非空子字段，这个消息可能意味着 kubelet 比 API 服务器版本低，因而无法识别更新的源类型。</td>
</tr>
<tr>
<td style="text-align:left">failed to sync: failed to download config, see Kubelet log for details</td>
<td style="text-align:left">kubelet 无法下载配置数据。可能是 node.spec.configSource 无法解析为具体的 API 对象，或者网络错误破坏了下载。处于此错误状态时，kubelet 将重新尝试下载。</td>
</tr>
<tr>
<td style="text-align:left">failed to sync: internal failure, see Kubelet log for details</td>
<td style="text-align:left">kubelet 遇到了一些内部问题，因此无法更新其配置。 例如：发生文件系统错误或无法从内部缓存中读取对象。</td>
</tr>
<tr>
<td style="text-align:left">internal failure, see Kubelet log for details</td>
<td style="text-align:left">在对配置进行同步的循环之外操作配置时，kubelet 遇到了一些内部问题。</td>
</tr>
</tbody>
</table>

<h2 id="接下来">接下来</h2>
<!--
- For more information on configuring the kubelet via a configuration file, see
[Set kubelet parameters via a config file](/docs/tasks/administer-cluster/kubelet-config-file).
- See the reference documentation for [`NodeConfigSource`](/docs/reference/generated/kubernetes-api/v1.22/#nodeconfigsource-v1-core)
- Learn more about kubelet configuration by checking the
  [`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/)
  reference.
-->
<ul>
<li>关于如何通过配置文件来配置 kubelet 的更多细节信息，可参阅
<a href="/zh/docs/tasks/administer-cluster/kubelet-config-file">使用配置文件设置 kubelet 参数</a>.</li>
<li>阅读 API 文档中 <a href="/docs/reference/generated/kubernetes-api/v1.22/#nodeconfigsource-v1-core"><code>NodeConfigSource</code></a> 说明</li>
<li>查阅<a href="/docs/reference/config-api/kubelet-config.v1beta1/"><code>KubeletConfiguration</code></a>文献进一步了解 kubelet
配置信息。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a3790dfb57271d13517e549dffa805b9">2.23 - 声明网络策略</h1>
    
	<!--
reviewers:
- caseydavenport
- danwinship
title: Declare Network Policy
min-kubernetes-server-version: v1.8
content_type: task
-->
<!-- overview -->
<!--
This document helps you get started using the Kubernetes [NetworkPolicy API](/docs/concepts/services-networking/network-policies/) to declare network policies that govern how pods communicate with each other.
-->
<p>本文可以帮助你开始使用 Kubernetes 的
<a href="/zh/docs/concepts/services-networking/network-policies/">NetworkPolicy API</a>
声明网络策略去管理 Pod 之间的通信</p>
<blockquote class="callout caution" role="alert">
  <strong>注意：</strong>
  本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循<a href="https://github.com/cncf/foundation/blob/master/website-guidelines.md" target="_blank">CNCF 网站指南</a>，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读<a href="/docs/contribute/style/content-guide/#third-party-content">内容指南</a>。
</blockquote>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.8.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Make sure you've configured a network provider with network policy support. There are a number of network providers that support NetworkPolicy, including:

* [Calico](/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/)
* [Cilium](/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/)
* [Kube-router](/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/)
* [Romana](/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/)
* [Weave Net](/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/)
-->
<p>你首先需要有一个支持网络策略的 Kubernetes 集群。已经有许多支持 NetworkPolicy 的网络提供商，包括：</p>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/">Calico</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/">Cilium</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/">Kube-router</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/">Romana</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/">Weave 网络</a></li>
</ul>
<!-- steps -->
<!--
## Create an `nginx` deployment and expose it via a service

To see how Kubernetes network policy works, start off by creating an `nginx` Deployment.
-->
<h2 id="创建一个-nginx-deployment-并且通过服务将其暴露">创建一个<code>nginx</code> Deployment 并且通过服务将其暴露</h2>
<p>为了查看 Kubernetes 网络策略是怎样工作的，可以从创建一个<code>nginx</code> Deployment 并且通过服务将其暴露开始</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment nginx --image<span style="color:#666">=</span>nginx
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/nginx created
</code></pre><!--
Expose the Deployment through a Service called `nginx`.
-->
<p>将此 Deployment 以名为 <code>nginx</code> 的 Service 暴露出来：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment nginx --port<span style="color:#666">=</span><span style="color:#666">80</span>
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">service/nginx exposed
</code></pre><!--
The above commands create a Deployment with an nginx Pod and expose the Deployment through a Service named `nginx`. The `nginx` Pod and Deployment are found in the `default` namespace.
-->
<p>上述命令创建了一个带有一个 nginx 的 Deployment，并将之通过名为 <code>nginx</code> 的
Service 暴露出来。名为 <code>nginx</code> 的 Pod 和 Deployment 都位于 <code>default</code>
名字空间内。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc,pod
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
svc/kubernetes              10.100.0.1    &lt;none&gt;        443/TCP    46m
svc/nginx                   10.100.0.16   &lt;none&gt;        80/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
po/nginx-701339712-e0qfq    1/1           Running       0          35s
</code></pre><!--
## Test the service by accessing it from another Pod

You should be able to access the new `nginx` service from other Pods. To access the `nginx` Service from another Pod in the `default` namespace, start a busybox container:
-->
<h2 id="通过从-pod-访问服务对其进行测试">通过从 Pod 访问服务对其进行测试</h2>
<p>你应该可以从其它的 Pod 访问这个新的 <code>nginx</code> 服务。
要从 default 命名空间中的其它s Pod 来访问该服务。可以启动一个 busybox 容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run busybox --rm -ti --image<span style="color:#666">=</span>busybox /bin/sh
</code></pre></div><!--
In your shell, run the following command:
-->
<p>在你的 Shell 中，运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre><!--
## Limit access to the `nginx` service

To limit the access to the `nginx` service so that only Pods with the label `access: true` can query it, create a NetworkPolicy object as follows:
-->
<h2 id="限制-nginx-服务的访问">限制 <code>nginx</code> 服务的访问</h2>
<p>如果想限制对 <code>nginx</code> 服务的访问，只让那些拥有标签 <code>access: true</code> 的 Pod 访问它，
那么可以创建一个如下所示的 NetworkPolicy 对象：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/nginx-policy.yaml" download="service/networking/nginx-policy.yaml"><code>service/networking/nginx-policy.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-nginx-policy-yaml')" title="Copy service/networking/nginx-policy.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-nginx-policy-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>NetworkPolicy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>access-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ingress</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">from</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">podSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">access</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
The name of a NetworkPolicy object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>NetworkPolicy 对象的名称必须是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.</p>
<!--
NetworkPolicy includes a `podSelector` which selects the grouping of Pods to which the policy applies. You can see this policy selects Pods with the label `app=nginx`. The label was automatically added to the Pod in the `nginx` Deployment. An empty `podSelector` selects all pods in the namespace.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> NetworkPolicy 中包含选择策略所适用的 Pods 集合的 <code>podSelector</code>。
你可以看到上面的策略选择的是带有标签 <code>app=nginx</code> 的 Pods。
此标签是被自动添加到 <code>nginx</code> Deployment 中的 Pod 上的。
如果 <code>podSelector</code> 为空，则意味着选择的是名字空间中的所有 Pods。</div>
</blockquote>
<!--
## Assign the policy to the service

Use kubectl to create a NetworkPolicy from the above `nginx-policy.yaml` file:
-->
<h2 id="为服务指定策略">为服务指定策略</h2>
<p>使用 kubectl 根据上面的 <code>nginx-policy.yaml</code> 文件创建一个 NetworkPolicy：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/networking/nginx-policy.yaml
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">networkpolicy.networking.k8s.io/access-nginx created
</code></pre><!--
## Test access to the service when access label is not defined

When you attempt to access the `nginx` Service from a Pod without the correct labels, the request times out:
-->
<h2 id="测试没有定义访问标签时访问服务">测试没有定义访问标签时访问服务</h2>
<p>如果你尝试从没有设定正确标签的 Pod 中去访问 <code>nginx</code> 服务，请求将会超时：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run busybox --rm -ti --image<span style="color:#666">=</span>busybox -- /bin/sh
</code></pre></div><!--
In your shell, run the command:
-->
<p>在 Shell 中运行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
wget: download timed out
</code></pre><!--
## Define access label and test again

You can create a Pod with the correct labels to see that the request is allowed:
-->
<h2 id="定义访问标签后再次测试">定义访问标签后再次测试</h2>
<p>创建一个拥有正确标签的 Pod，你将看到请求是被允许的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run busybox --rm -ti --labels<span style="color:#666">=</span><span style="color:#b44">&#34;access=true&#34;</span> --image<span style="color:#666">=</span>busybox -- /bin/sh
</code></pre></div><!--
In your shell, run the command:
-->
<p>在 Shell 中运行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget --spider --timeout<span style="color:#666">=</span><span style="color:#666">1</span> nginx
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Connecting to nginx (10.100.0.16:80)
remote file exists
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b35b8ddb9bbc15620ce9636f4346c05c">2.24 - 安全地清空一个节点</h1>
    
	<!--
reviewers:
- davidopp
- mml
- foxish
- kow3ns
title: Safely Drain a Node
content_type: task
min-kubernetes-server-version: 1.5
-->
<!-- overview -->
<!-- 
This page shows how to safely drain a node, respecting the PodDisruptionBudget you have defined.
 -->
<p>本页展示了如何在确保 PodDisruptionBudget 的前提下，安全地清空一个<a class='glossary-tooltip' title='Kubernetes 中的工作机器称作节点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/nodes/' target='_blank' aria-label='节点'>节点</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p>您的 Kubernetes 服务器版本必须不低于版本 1.5.
要获知版本信息，请输入 <code>kubectl version</code>.</p>
<!-- 
This task assumes that you have met the following prerequisites:

* You are using Kubernetes release >= 1.5.
* Either:
  1. You do not require your applications to be highly available during the
     node drain, or
  2. You have read about the [PodDisruptionBudget concept](/docs/concepts/workloads/pods/disruptions/)
     and [Configured PodDisruptionBudgets](/docs/tasks/run-application/configure-pdb/) for
     applications that need them.
-->
<p>此任务假定你已经满足了以下先决条件：</p>
<ul>
<li>使用的 Kubernetes 版本 &gt;= 1.5。</li>
<li>以下两项，具备其一：
<ol>
<li>在节点清空期间，不要求应用程序具有高可用性</li>
<li>你已经了解了 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget 的概念</a>，
并为需要它的应用程序<a href="/zh/docs/tasks/run-application/configure-pdb/">配置了 PodDisruptionBudget</a>。</li>
</ol>
</li>
</ul>
<!-- steps -->
<!--
## (Optional) Configure a disruption budget {#configure-poddisruptionbudget}

To endure that your workloads remain available during maintenance, you can
configure a [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/).

If availability is important for any applications that run or could run on the node(s)
that you are draining, [configure a PodDisruptionBudgets](/docs/tasks/run-application/configure-pdb/)
first and the continue following this guide.
-->
<h2 id="configure-poddisruptionbudget">（可选） 配置干扰预算</h2>
<p>为了确保你的负载在维护期间仍然可用，你可以配置一个 <a href="/zh/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>。
如果可用性对于正在清空的该节点上运行或可能在该节点上运行的任何应用程序很重要，
首先 <a href="/zh/docs/tasks/run-application/configure-pdb/">配置一个 PodDisruptionBudgets</a> 并继续遵循本指南。</p>
<!-- 
## Use `kubectl drain` to remove a node from service

You can use `kubectl drain` to safely evict all of your pods from a
node before you perform maintenance on the node (e.g. kernel upgrade,
hardware maintenance, etc.). Safe evictions allow the pod's containers
to [gracefully terminate](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)
and will respect the `PodDisruptionBudgets` you have specified.
-->
<h2 id="use-kubectl-drain-to-remove-a-node-from-service">使用 <code>kubectl drain</code> 从服务中删除一个节点</h2>
<p>在对节点执行维护（例如内核升级、硬件维护等）之前，
可以使用 <code>kubectl drain</code> 从节点安全地逐出所有 Pods。
安全的驱逐过程允许 Pod 的容器
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">体面地终止</a>，
并确保满足指定的 PodDisruptionBudgets。</p>
<!-- 
By default `kubectl drain` will ignore certain system pods on the node
that cannot be killed; see
the [kubectl drain](/docs/reference/generated/kubectl/kubectl-commands/#drain)
documentation for more details.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 默认情况下， <code>kubectl drain</code> 将忽略节点上不能杀死的特定系统 Pod；
有关更多细节，请参阅
<a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a> 文档。</div>
</blockquote>
<!-- 
When `kubectl drain` returns successfully, that indicates that all of
the pods (except the ones excluded as described in the previous paragraph)
have been safely evicted (respecting the desired graceful termination period,
and respecting the PodDisruptionBudget you have defined). It is then safe to
bring down the node by powering down its physical machine or, if running on a
cloud platform, deleting its virtual machine.

First, identify the name of the node you wish to drain. You can list all of the nodes in your cluster with
-->
<p><code>kubectl drain</code> 的成功返回，表明所有的 Pods（除了上一段中描述的被排除的那些），
已经被安全地逐出（考虑到期望的终止宽限期和你定义的 PodDisruptionBudget）。
然后就可以安全地关闭节点，
比如关闭物理机器的电源，如果它运行在云平台上，则删除它的虚拟机。</p>
<p>首先，确定想要清空的节点的名称。可以用以下命令列出集群中的所有节点:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!-- 
Next, tell Kubernetes to drain the node:
-->
<p>接下来，告诉 Kubernetes 清空节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain &lt;node name&gt;
</code></pre></div><!-- 
Once it returns (without giving an error), you can power down the node
(or equivalently, if on a cloud platform, delete the virtual machine backing the node).
If you leave the node in the cluster during the maintenance operation, you need to run
-->
<p>一旦它返回（没有报错），
你就可以下线此节点（或者等价地，如果在云平台上，删除支持该节点的虚拟机）。
如果要在维护操作期间将节点留在集群中，则需要运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl uncordon &lt;node name&gt;
</code></pre></div><!-- 
afterwards to tell Kubernetes that it can resume scheduling new pods onto the node.
-->
<p>然后告诉 Kubernetes，它可以继续在此节点上调度新的 Pods。</p>
<!-- 
## Draining multiple nodes in parallel

The `kubectl drain` command should only be issued to a single node at a
time. However, you can run multiple `kubectl drain` commands for
different nodes in parallel, in different terminals or in the
background. Multiple drain commands running concurrently will still
respect the `PodDisruptionBudget` you specify.
-->
<h2 id="draining-multiple-nodes-in-parallel">并行清空多个节点 </h2>
<p><code>kubectl drain</code> 命令一次只能发送给一个节点。
但是，你可以在不同的终端或后台为不同的节点并行地运行多个 <code>kubectl drain</code> 命令。
同时运行的多个 drain 命令仍然遵循你指定的 PodDisruptionBudget 。</p>
<!-- 
For example, if you have a StatefulSet with three replicas and have
set a PodDisruptionBudget for that set specifying `minAvailable: 2`,
`kubectl drain` only evicts a pod from the StatefulSet if all three
replicas pods are ready; if then you issue multiple drain commands in
parallel, Kubernetes respects the PodDisruptionBudget and ensure
that only 1 (calculated as `replicas - minAvailable`) Pod is unavailable
at any given time. Any drains that would cause the number of ready
replicas to fall below the specified budget are blocked.
-->
<p>例如，如果你有一个三副本的 StatefulSet，
并设置了一个 <code>PodDisruptionBudget</code>，指定 <code>minAvailable: 2</code>。
如果所有的三个 Pod 均就绪，并且你并行地发出多个 drain 命令，
那么 <code>kubectl drain</code> 只会从 StatefulSet 中逐出一个 Pod，
因为 Kubernetes 会遵守 PodDisruptionBudget 并确保在任何时候只有一个 Pod 不可用
（最多不可用 Pod 个数的计算方法：<code>replicas - minAvailable</code>）。
任何会导致就绪副本数量低于指定预算的清空操作都将被阻止。</p>
<!-- 
## The Eviction API

If you prefer not to use [kubectl drain](/docs/reference/generated/kubectl/kubectl-commands/#drain) (such as
to avoid calling to an external command, or to get finer control over the pod
eviction process), you can also programmatically cause evictions using the eviction API.
-->
<h2 id="the-eviction-api">驱逐 API</h2>
<p>如果你不喜欢使用
<a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">kubectl drain</a>
（比如避免调用外部命令，或者更细化地控制 pod 驱逐过程），
你也可以用驱逐 API 通过编程的方式达到驱逐的效果。</p>
<!-- 
You should first be familiar with using [Kubernetes language clients](/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api).

The eviction subresource of a
pod can be thought of as a kind of policy-controlled DELETE operation on the pod
itself. To attempt an eviction (perhaps more REST-precisely, to attempt to
*create* an eviction), you POST an attempted operation. Here's an example:
-->
<p>首先应该熟悉使用
<a href="/zh/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api">Kubernetes 语言客户端</a>。</p>
<p>Pod 的 Eviction 子资源可以看作是一种策略控制的 DELETE 操作，作用于 Pod 本身。
要尝试驱逐（更准确地说，尝试 <em>创建</em> 一个 Eviction），需要用 POST 发出所尝试的操作。这里有一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;policy/v1beta1&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Eviction&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;quux&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;default&#34;</span>
  }
}
</code></pre></div><!-- 
You can attempt an eviction using `curl`:
-->
<p>你可以使用 <code>curl</code> 尝试驱逐：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -v -H <span style="color:#b44">&#39;Content-type: application/json&#39;</span> http://127.0.0.1:8080/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</code></pre></div><!-- 
The API can respond in one of three ways:

- If the eviction is granted, then the pod is deleted just as if you had sent
  a `DELETE` request to the pod's URL and you get back `200 OK`.
- If the current state of affairs wouldn't allow an eviction by the rules set
  forth in the budget, you get back `429 Too Many Requests`. This is
  typically used for generic rate limiting of *any* requests, but here we mean
  that this request isn't allowed *right now* but it may be allowed later.
  Currently, callers do not get any `Retry-After` advice, but they may in
  future versions.
- If there is some kind of misconfiguration, like multiple budgets pointing at
  the same pod, you will get `500 Internal Server Error`.
-->
<p>API 可以通过以下三种方式之一进行响应：</p>
<ul>
<li>如果驱逐被授权，那么 Pod 将被删掉，并且你会收到 <code>200 OK</code>，
就像你向 Pod 的 URL 发送了 <code>DELETE</code> 请求一样。</li>
<li>如果按照预算中规定，目前的情况不允许的驱逐，你会收到 <code>429 Too Many Requests</code>。
这通常用于对 <em>一些</em> 请求进行通用速率限制，
但这里我们的意思是：此请求 <em>现在</em> 不允许，但以后可能会允许。
目前，调用者不会得到任何 <code>Retry-After</code> 的提示，但在将来的版本中可能会得到。</li>
<li>如果有一些错误的配置，比如多个预算指向同一个 Pod，你将得到 <code>500 Internal Server Error</code>。</li>
</ul>
<!-- 
For a given eviction request, there are two cases:

- There is no budget that matches this pod. In this case, the server always
  returns `200 OK`.
- There is at least one budget. In this case, any of the three above responses may
   apply.
-->
<p>对于一个给定的驱逐请求，有两种情况：</p>
<ul>
<li>没有匹配这个 Pod 的预算。这种情况，服务器总是返回 <code>200 OK</code>。</li>
<li>至少匹配一个预算。在这种情况下，上述三种回答中的任何一种都可能适用。</li>
</ul>
<!-- 
## Stuck evictions

In some cases, an application may reach a broken state, one where unless you intervene the
eviction API will never return anything other than 429 or 500.

For example: this can happen if ReplicaSet is creating Pods for your application but
the replacement Pods do not become `Ready`. You can also see similar symptoms if the
last Pod evicted has a very long termination grace period.
-->
<h2 id="驱逐阻塞">驱逐阻塞</h2>
<p>在某些情况下，应用程序可能会到达一个中断状态，除了 429 或 500 之外，它将永远不会返回任何内容。
例如 ReplicaSet 创建的替换 Pod 没有变成就绪状态，或者被驱逐的最后一个
Pod 有很长的终止宽限期，就会发生这种情况。</p>
<!--
In this case, there are two potential solutions:

- Abort or pause the automated operation. Investigate the reason for the stuck application,
  and restart the automation.
- After a suitably long wait, `DELETE` the Pod from your cluster's control plane, instead
  of using the eviction API.

Kubernetes does not specify what the behavior should be in this case; it is up to the
application owners and cluster owners to establish an agreement on behavior in these cases.
-->
<p>在这种情况下，有两种可能的解决方案：</p>
<ul>
<li>中止或暂停自动操作。调查应用程序卡住的原因，并重新启动自动化。</li>
<li>经过适当的长时间等待后，从集群中删除 Pod 而不是使用驱逐 API。</li>
</ul>
<p>Kubernetes 并没有具体说明在这种情况下应该采取什么行为，
这应该由应用程序所有者和集群所有者紧密沟通，并达成对行动一致意见。</p>
<h2 id="接下来">接下来</h2>
<!-- 
* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).
-->
<ul>
<li>执行<a href="/zh/docs/tasks/run-application/configure-pdb/">配置 PDB</a>中的各个步骤，
保护你的应用</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a24171610b6ea75a142cb9c8c7882390">2.25 - 将重复的控制平面迁至云控制器管理器</h1>
    
	<!--
---
reviewers:
- jpbetz
- cheftako
title: "Migrate Replicated Control Plane To Use Cloud Controller Manager"
linkTitle: "Migrate Replicated Control Plane To Use Cloud Controller Manager"
content_type: task
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>

<!--
title: Cloud Controller Manager
id: cloud-controller-manager
date: 2018-04-12
full_link: /docs/concepts/architecture/cloud-controller/
short_description: >
  Control plane component that integrates Kubernetes with third-party cloud providers.

aka: 
tags:
- core-object
- architecture
- operation
-->
<!--
 A Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.
-->
<p><p>云管理控制器是 云控制器管理器是指嵌入特定云的控制逻辑的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>组件。
云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上，
并将与该云平台交互的组件同与你的集群交互的组件分离开来。</p></p>
<!--
By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.
-->
<p>通过分离 Kubernetes 和底层云基础设置之间的互操作性逻辑，
云控制器管理器组件使云提供商能够以不同于 Kubernetes 主项目的
步调发布新特征。</p>
<!--
## Background

As part of the [cloud provider extraction effort](https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/),
all cloud specific controllers must be moved out of the `kube-controller-manager`. 
All existing clusters that run cloud controllers in the `kube-controller-manager` must migrate to instead run the controllers in a cloud provider specific `cloud-controller-manager`.

Leader Migration provides a mechanism in which HA clusters can safely migrate "cloud specific" controllers between 
the `kube-controller-manager` and the `cloud-controller-manager` via a shared resource lock between the two components while upgrading the replicated control plane. 
For a single-node control plane, or if unavailability of controller managers can be tolerated during the upgrade, Leader Migration is not needed and this guide can be ignored.
-->
<h2 id="背景">背景</h2>
<p>作为<a href="https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/">云驱动提取工作</a>
的一部分，所有特定于云的控制器都必须移出 <code>kube-controller-manager</code>。
所有在 <code>kube-controller-manager</code> 中运行云控制器的现有集群必须迁移到云驱动特定的 <code>cloud-controller-manager</code> 中运行控制器。</p>
<p>领导者迁移提供了一种机制，使得 HA 集群可以通过两个组件之间的共享资源锁定，
安全地将“特定于云”的控制器从 <code>kube-controller-manager</code> 和迁移到<code>cloud-controller-manager</code>，
同时升级复制的控制平面。
对于单节点控制平面，或者在升级过程中可以容忍控制器管理器不可用的情况，则不需要领导者迁移，并且可以忽略本指南。</p>
<!--
Leader Migration can be enabled by setting `--enable-leader-migration` on `kube-controller-manager` or `cloud-controller-manager`.
Leader Migration only applies during the upgrade and can be safely disabled or left enabled after the upgrade is complete.

This guide walks you through the manual process of upgrading the control plane from `kube-controller-manager` with 
built-in cloud provider to running both `kube-controller-manager` and `cloud-controller-manager`. 
If you use a tool to administrator the cluster, please refer to the documentation of the tool and the cloud provider for more details.
-->
<p>领导者迁移可以通过在 <code>kube-controller-manager</code> 或 <code>cloud-controller-manager</code> 上设置 <code>--enable-leader-migration</code> 来启用。
领导者迁移仅在升级期间适用，并且可以安全地禁用，也可以在升级完成后保持启用状态。</p>
<p>本指南将引导你手动将控制平面从内置的云驱动的 <code>kube-controller-manager</code> 升级为
同时运行 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code>。
如果使用工具来管理群集，请参阅对应工具和云驱动的文档以获取更多详细信息。</p>
<h2 id="准备开始">准备开始</h2>
<!--
It is assumed that the control plane is running Kubernetes version N and to be upgraded to version N + 1. 
Although it is possible to migrate within the same version, ideally the migration should be performed as part of a upgrade so that changes of configuration can be aligned to each release. 
The exact versions of N and N + 1 depend on each cloud provider. For example, if a cloud provider builds a `cloud-controller-manager` to work with Kubernetes 1.22, then N can be 1.21 and N + 1 can be 1.22.

The control plane nodes should run `kube-controller-manager` with Leader Election enabled through `--leader-elect=true`. 
As of version N, an in-tree cloud privider must be set with `--cloud-provider` flag and `cloud-controller-manager` should not yet be deployed.
-->
<p>假定控制平面正在运行 Kubernetes N 版本，并且要升级到 N+1 版本。
尽管可以在同一版本中进行迁移，但理想情况下，迁移应作为升级的一部分执行，以便可以更改配置与每个发布版本保持一致。
N 和 N+1的确切版本取决于各个云驱动。例如，如果云驱动构建了一个可与 Kubernetes 1.22 配合使用的 <code>cloud-controller-manager</code>，
则 N 可以为 1.21，N+1 可以为 1.22。</p>
<p>控制平面节点应运行 <code>kube-controller-manager</code>，并通过 <code>--leader-elect=true</code> 启用领导者选举。
从版本 N 开始，树内云驱动必须设置 <code>--cloud-provider</code> 标志，而且 <code>cloud-controller-manager</code> 尚未部署。</p>
<!--
The out-of-tree cloud provider must have built a `cloud-controller-manager` with Leader Migration implementation. 
If the cloud provider imports `k8s.io/cloud-provider` and `k8s.io/controller-manager` of version v0.21.0 or later, Leader Migration will be available.
However, for version before v0.22.0, Leader Migration is alpha and requires feature gate `ControllerManagerLeaderMigration` to be enabled.

This guide assumes that kubelet of each control plane node starts `kube-controller-manager` 
and `cloud-controller-manager` as static pods defined by their manifests. 
If the components run in a different setting, please adjust the steps accordingly.

For authorization, this guide assumes that the cluster uses RBAC. 
If another authorization mode grants permissions to `kube-controller-manager` and `cloud-controller-manager` components, 
please grant the needed access in a way that matches the mode.
-->
<p>树外云驱动必须已经构建了一个实现领导者迁移的 <code>cloud-controller-manager</code>。
如果云驱动导入了 v0.21.0 或更高版本的 <code>k8s.io/cloud-provider</code> 和 <code>k8s.io/controller-manager</code>，
则可以进行领导者迁移。
但是，对 v0.22.0 以下的版本，领导者迁移是一项 Alpha 阶段功能，它需要启用特性门控 <code>ControllerManagerLeaderMigration</code>。</p>
<p>本指南假定每个控制平面节点的 kubelet 以静态 pod 的形式启动 <code>kube-controller-manager</code>
和 <code>cloud-controller-manager</code>，静态 pod 的定义在清单文件中。
如果组件以其他设置运行，请相应地调整步骤。</p>
<p>为了获得授权，本指南假定集群使用 RBAC。
如果其他授权模式授予 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code> 组件权限，
请以与该模式匹配的方式授予所需的访问权限。</p>
<!-- steps -->
<!--
### Grant access to Migration Lease

The default permissions of the controller manager allow only accesses to their main Lease.
In order for the migration to work, accesses to another Lease are required.

You can grant `kube-controller-manager` full access to the leases API by modifying 
the `system::leader-locking-kube-controller-manager` role. 
This task guide assumes that the name of the migration lease is `cloud-provider-extraction-migration`.

`kubectl patch -n kube-system role 'system::leader-locking-kube-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge`

Do the same to the `system::leader-locking-cloud-controller-manager` role.

`kubectl patch -n kube-system role 'system::leader-locking-cloud-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge`
-->
<h3 id="授予访问迁移-lease-的权限">授予访问迁移 Lease 的权限</h3>
<p>控制器管理器的默认权限仅允许访问其主 Lease 对象。为了使迁移正常进行，需要访问其他 Lease 对象。</p>
<p>你可以通过修改 <code>system::leader-locking-kube-controller-manager</code> 角色来授予
<code>kube-controller-manager</code> 对 Lease API 的完全访问权限。
本任务指南假定迁移 Lease 的名称为 <code>cloud-provider-extraction-migration</code>。</p>
<p><code>kubectl patch -n kube-system role 'system::leader-locking-kube-controller-manager' -p '{&quot;rules&quot;: [ {&quot;apiGroups&quot;:[ &quot;coordination.k8s.io&quot;], &quot;resources&quot;: [&quot;leases&quot;], &quot;resourceNames&quot;: [&quot;cloud-provider-extraction-migration&quot;], &quot;verbs&quot;: [&quot;create&quot;, &quot;list&quot;, &quot;get&quot;, &quot;update&quot;] } ]}' --type=merge</code></p>
<p>对 <code>system::leader-locking-cloud-controller-manager</code> 角色执行相同的操作。</p>
<p><code>kubectl patch -n kube-system role 'system::leader-locking-cloud-controller-manager' -p '{&quot;rules&quot;: [ {&quot;apiGroups&quot;:[ &quot;coordination.k8s.io&quot;], &quot;resources&quot;: [&quot;leases&quot;], &quot;resourceNames&quot;: [&quot;cloud-provider-extraction-migration&quot;], &quot;verbs&quot;: [&quot;create&quot;, &quot;list&quot;, &quot;get&quot;, &quot;update&quot;] } ]}' --type=merge</code></p>
<!--
### Initial Leader Migration configuration

Leader Migration optionally takes a configuration file representing the state of controller-to-manager assignment. At this moment, with in-tree cloud provider, `kube-controller-manager` runs `route`, `service`, and `cloud-node-lifecycle`. The following example configuration shows the assignment.

Leader Migration can be enabled without a configuration. Please see [Default Configuration](#default-configuration) for details.
-->
<h3 id="初始领导者迁移配置">初始领导者迁移配置</h3>
<p>领导者迁移可以选择使用一个表示控制器到管理器分配状态的配置文件。
目前，对于树内云驱动，<code>kube-controller-manager</code> 运行 <code>route</code>、<code>service</code> 和 <code>cloud-node-lifecycle</code>。
以下示例配置显示了分配。</p>
<p>领导者迁移可以不指定配置来启用。请参阅 <a href="#default-configuration">默认配置</a> 以获取更多详细信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerLeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>kube-controller-manager<span style="color:#bbb">
</span></code></pre></div><!--
On each control plane node, save the content to `/etc/leadermigration.conf`, 
and update the manifest of `kube-controller-manager` so that the file is mounted inside the container at the same location. 
Also, update the same manifest to add the following arguments:

- `--enable-leader-migration` to enable Leader Migration on the controller manager
- `--leader-migration-config=/etc/leadermigration.conf` to set configuration file

Restart `kube-controller-manager` on each node. At this moment, `kube-controller-manager` has leader migration enabled and is ready for the migration.
-->
<p>在每个控制平面节点上，将内容保存到 <code>/etc/leadermigration.conf</code> 中，
并更新 <code>kube-controller-manager</code> 清单，以便将文件安装在容器内的同一位置。
另外，更新相同的清单，添加以下参数：</p>
<ul>
<li><code>--enable-leader-migration</code> 在控制器管理器上启用领导者迁移</li>
<li><code>--leader-migration-config=/etc/leadermigration.conf</code> 设置配置文件</li>
</ul>
<p>在每个节点上重新启动 <code>kube-controller-manager</code>。这时，<code>kube-controller-manager</code>
已启用领导者迁移，并准备进行迁移。</p>
<!--
### Deploy Cloud Controller Manager

In version N + 1, the desired state of controller-to-manager assignment can be represented by a new configuration file, shown as follows. 
Please note `component` field of each `controllerLeaders` changing from `kube-controller-manager` to `cloud-controller-manager`.
-->
<h3 id="部署云控制器管理器">部署云控制器管理器</h3>
<p>在 N+1 版本中，控制器到管理器分配的期望状态可以由新的配置文件表示，如下所示。
请注意，每个 <code>controllerLeaders</code> 的 <code>component</code> 字段从 <code>kube-controller-manager</code> 更改为 <code>cloud-controller-manager</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LeaderMigrationConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>controllermanager.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">leaderName</span>:<span style="color:#bbb"> </span>cloud-provider-extraction-migration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resourceLock</span>:<span style="color:#bbb"> </span>leases<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">controllerLeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>route<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>service<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cloud-node-lifecycle<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>cloud-controller-manager<span style="color:#bbb">
</span></code></pre></div><!--
When creating control plane nodes of version N + 1, the content should be deploy to `/etc/leadermigration.conf`. 
The manifest of `cloud-controller-manager` should be updated to mount the configuration file in 
the same manner as `kube-controller-manager` of version N. Similarly, add `--feature-gates=ControllerManagerLeaderMigration=true`,
`--enable-leader-migration`, and `--leader-migration-config=/etc/leadermigration.conf` to the arguments of `cloud-controller-manager`.

Create a new control plane node of version N + 1 with the updated `cloud-controller-manager` manifest, 
and with the `--cloud-provider` flag unset for `kube-controller-manager`.
`kube-controller-manager` of version N + 1 MUST NOT have Leader Migration enabled because, 
with an external cloud provider, it does not run the migrated controllers anymore and thus it is not involved in the migration.

Please refer to [Cloud Controller Manager Administration](/docs/tasks/administer-cluster/running-cloud-controller/)
for more detail on how to deploy `cloud-controller-manager`.
-->
<p>当创建 N+1 版本的控制平面节点时，应将内容部署到 <code>/etc/leadermigration.conf</code>。
应该更新 <code>cloud-controller-manager</code> 清单，以与 N 版本的 <code>kube-controller-manager</code> 相同的方式挂载配置文件。
类似地，添加 <code>--feature-gates=ControllerManagerLeaderMigration=true</code>、<code>--enable-leader-migration</code>
和 <code>--leader-migration-config=/etc/leadermigration.conf</code> 到 <code>cloud-controller-manager</code> 的参数中。</p>
<p>使用已更新的 <code>cloud-controller-manager</code> 清单创建一个新的 N+1 版本的控制平面节点。
并且没有设置 <code>kube-controller-manager</code> 的 <code>--cloud-provider</code> 标志。
N+1 版本的 <code>kube-controller-manager</code> 不能启用领导者迁移，
因为在使用外部云驱动的情况下，它不再运行已迁移的控制器，因此不参与迁移。</p>
<p>请参阅<a href="/zh/docs/tasks/administer-cluster/running-cloud-controller/">云控制器管理器管理</a>
了解有关如何部署 <code>cloud-controller-manager</code> 的更多细节。</p>
<!--
### Upgrade Control Plane

The control plane now contains nodes of both version N and N + 1. 
The nodes of version N run `kube-controller-manager` only, 
and these of version N + 1 run both `kube-controller-manager` and `cloud-controller-manager`. 
The migrated controllers, as specified in the configuration, are running under either `kube-controller-manager` of 
version N or `cloud-controller-manager` of version N + 1 depending on which controller manager holds the migration lease.
No controller will ever be running under both controller managers at any time.

In a rolling manner, create a new control plane node of version N + 1 and bring down one of version N + 1 until the control plane contains only nodes of version N + 1.
If a rollback from version N + 1 to N is required, add nodes of version N with Leader Migration enabled for `kube-controller-manager` back to the control plane, replacing one of version N + 1 each time until there are only nodes of version N.
-->
<h3 id="升级控制平面">升级控制平面</h3>
<p>现在，控制平面包含 N 和 N+1 版本的节点。
N 版本的节点仅运行 <code>kube-controller-manager</code>，而 N+1 版本的节点同时运行
<code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code>。
根据配置所指定，已迁移的控制器在 N 版本的 <code>kube-controller-manager</code> 或 N+1 版本的
<code>cloud-controller-manager</code> 下运行，
具体取决于哪个控制器管理器拥有迁移  Lease 对象。任何时候都不存在一个控制器在两个控制器管理器下运行。</p>
<p>以滚动的方式创建一个新的版本为 N+1 的控制平面节点，并将 N+1 版本中的一个关闭，
直到控制平面仅包含版本为 N+1 的节点。
如果需要从 N+1 版本回滚到 N 版本，则将启用了领导者迁移的 <code>kube-controller-manager</code>
且版本为 N 的节点添加回控制平面，每次替换 N+1 版本的一个，直到只有 N 版本的节点为止。</p>
<!--
### (Optional) Disable Leader Migration {#disable-leader-migration}

Now that the control plane has been upgraded to run both `kube-controller-manager` and `cloud-controller-manager` of version N + 1, 
Leader Migration has finished its job and can be safely disabled to save one Lease resource. 
It is safe to re-enable Leader Migration for the rollback in the future.

In a rolling manager, update manifest of `cloud-controller-manager` to unset both 
`--enable-leader-migration` and `--leader-migration-config=` flag, 
also remove the mount of `/etc/leadermigration.conf`, and finally remove `/etc/leadermigration.conf`. 
To re-enable Leader Migration, recreate the configuration file and add its mount and the flags that enable Leader Migration back to `cloud-controller-manager`.
-->
<h3 id="disable-leader-migration">（可选）禁用领导者迁移</h3>
<p>现在，控制平面已经升级，可以同时运行 N+1 版本的 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code> 了。
领导者迁移已经完成工作，可以安全地禁用以节省一个 Lease 资源。
在将来可以安全地重新启用领导者迁移以完成回滚。</p>
<p>在滚动管理器中，更新 <code>cloud-controller-manager</code> 的清单以同时取消设置 <code>--enable-leader-migration</code>
和 <code>--leader-migration-config=</code> 标志，并删除 <code>/etc/leadermigration.conf</code> 的挂载。
最后删除 <code>/etc/leadermigration.conf</code>。
要重新启用领导者迁移，请重新创建配置文件，并将其挂载和启用领导者迁移的标志添加回到 <code>cloud-controller-manager</code>。</p>
<!--
### Default Configuration

Starting Kubernetes 1.22, Leader Migration provides a default configuration suitable for the default controller-to-manager assignment.
The default configuration can be enabled by setting `--enable-leader-migration` but without `--leader-migration-config=`.

For `kube-controller-manager` and `cloud-controller-manager`, if there are no flags that enable any in-tree cloud provider or change ownership of controllers, the default configuration can be used to avoid manual creation of the configuration file.
-->
<h3 id="default-configuration">默认配置</h3>
<p>从 Kubernetes 1.22 开始，领导者迁移提供了一个默认配置，它适用于默认的控制器到管理器分配。
可以通过设置 <code>--enable-leader-migration</code>，但不设置 <code>--leader-migration-config=</code> 来启用默认配置。</p>
<p>对于 <code>kube-controller-manager</code> 和 <code>cloud-controller-manager</code>，如果没有用参数来启用树内云驱动或者改变控制器属主，
则可以使用默认配置来避免手动创建配置文件。</p>
<h2 id="接下来">接下来</h2>
<!--
- Read the [Controller Manager Leader Migration](https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2436-controller-manager-leader-migration) enhancement proposal
-->
<ul>
<li>阅读<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cloud-provider/2436-controller-manager-leader-migration">领导者迁移控制器管理器</a>改进建议</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9585dc0efb0450fd68728e7511754717">2.26 - 开发云控制器管理器</h1>
    
	<!--
reviewers:
- luxas
- thockin
- wlan0
title: Developing Cloud Controller Manager
content_type: concept
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code>
</div>

<!--
title: Cloud Controller Manager
id: cloud-controller-manager
date: 2018-04-12
full_link: /docs/concepts/architecture/cloud-controller/
short_description: >
  Control plane component that integrates Kubernetes with third-party cloud providers.

aka: 
tags:
- core-object
- architecture
- operation
-->
<!--
 A Kubernetes <a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.
-->
<p><p>组件 cloud-controller-manager 是 云控制器管理器是指嵌入特定云的控制逻辑的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制平面'>控制平面</a>组件。
云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上，
并将与该云平台交互的组件同与你的集群交互的组件分离开来。</p></p>
<!--
By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.
-->
<p>通过分离 Kubernetes 和底层云基础设置之间的互操作性逻辑，
云控制器管理器组件使云提供商能够以不同于 Kubernetes 主项目的
步调发布新特征。</p>
<!-- body -->
<!--
## Background

Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the `cloud-controller-manager` binary allows cloud vendors to evolve independently from the core Kubernetes code.
-->
<h2 id="背景">背景</h2>
<p>由于云驱动的开发和发布与 Kubernetes 项目本身步调不同，将特定于云环境
的代码抽象到 <code>cloud-controller-manager</code> 二进制组件有助于云厂商独立于
Kubernetes 核心代码推进其驱动开发。</p>
<!--
The Kubernetes project provides skeleton cloud-controller-manager code with Go interfaces to allow you (or your cloud provider) to plug in your own implementations. This means that a cloud provider can implement a cloud-controller-manager by importing packages from Kubernetes core; each cloudprovider will register their own code by calling `cloudprovider.RegisterCloudProvider` to update a global variable of available cloud providers.
-->
<p>Kubernetes 项目提供 cloud-controller-manager 的框架代码，其中包含 Go
语言的接口，便于你（或者你的云驱动提供者）接驳你自己的实现。
这意味着每个云驱动可以通过从 Kubernetes 核心代码导入软件包来实现一个
cloud-controller-manager；每个云驱动会通过调用
<code>cloudprovider.RegisterCloudProvider</code> 接口来注册其自身实现代码，从而更新
记录可用云驱动的全局变量。</p>
<!--
## Developing
-->
<h2 id="开发">开发</h2>
<h3 id="out-of-tree">Out of Tree</h3>
<!--
To build an out-of-tree cloud-controller-manager for your cloud, follow these steps:
-->
<p>要为你的云环境构建一个 out-of-tree 云控制器管理器：</p>
<!--
1. Create a go package with an implementation that satisfies [cloudprovider.Interface](https://git.k8s.io/kubernetes/pkg/cloudprovider/cloud.go).
2. Use [main.go in cloud-controller-manager](https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/main.go) from Kubernetes core as a template for your main.go. As mentioned above, the only difference should be the cloud package that will be imported.
3. Import your cloud package in `main.go`, ensure your package has an `init` block to run [cloudprovider.RegisterCloudProvider](https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/plugins.go#L42-L52).
-->
<ol>
<li>使用满足 <a href="https://git.k8s.io/kubernetes/pkg/cloudprovider/cloud.go">cloudprovider.Interface</a>
的实现创建一个 Go 语言包。</li>
<li>使用来自 Kubernetes 核心代码库的
<a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/main.go">cloud-controller-manager 中的 main.go</a>
作为 main.go 的模板。如上所述，唯一的区别应该是将导入的云包。</li>
<li>在 <code>main.go</code> 中导入你的云包，确保你的包有一个 <code>init</code> 块来运行
<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/plugins.go#L42-L52">cloudprovider.RegisterCloudProvider</a>。</li>
</ol>
<!--
Many cloud providers publish their controller manager code as open source. If you are creating
a new cloud-controller-manager from scratch, you could take an existing out-of-tree cloud
controller manager as your starting point.
-->
<p>很多云驱动都将其控制器管理器代码以开源代码的形式公开。
如果你在开发一个新的 cloud-controller-manager，你可以选择某个 out-of-tree
云控制器管理器作为出发点。</p>
<h3 id="in-tree">In Tree</h3>
<!--
For in-tree cloud providers, you can run the in-tree cloud controller manager as a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a> in your cluster. See [Cloud Controller Manager Administration](/docs/tasks/administer-cluster/running-cloud-controller/) for more details.
-->
<p>对于 in-tree 驱动，你可以将 in-tree 云控制器管理器作为群集中的
<a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='Daemonset'>Daemonset</a> 来运行。
有关详细信息，请参阅<a href="/zh/docs/tasks/administer-cluster/running-cloud-controller/">云控制器管理器管理</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-00733cc3747eb3f5fe1c9e0439262967">2.27 - 开启服务拓扑</h1>
    
	<!-- overview -->
<!--
This page provides an overview of enabling Service Topology in Kubernetes.
-->
<p>本页面提供了在 Kubernetes 中启用服务拓扑的概述。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Introduction

_Service Topology_ enables a service to route traffic based upon the Node
topology of the cluster. For example, a service can specify that traffic be
preferentially routed to endpoints that are on the same Node as the client, or
in the same availability zone.
-->
<h2 id="介绍">介绍</h2>
<p><em>服务拓扑（Service Topology）</em> 使服务能够根据集群中的 Node 拓扑来路由流量。
比如，服务可以指定将流量优先路由到与客户端位于同一节点或者同一可用区域的端点上。</p>
<!--
## Prerequisites

The following prerequisites are needed in order to enable topology aware service
routing:

   * Kubernetes 1.17 or later
   * <a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='Kube-proxy'>Kube-proxy</a> running in iptables mode or IPVS mode
   * Enable [Endpoint Slices](/docs/concepts/services-networking/endpoint-slices/)
-->
<h2 id="先决条件">先决条件</h2>
<p>需要下面列的先决条件，才能启用拓扑感知的服务路由：</p>
<ul>
<li>Kubernetes 1.17 或更新版本</li>
<li><a class='glossary-tooltip' title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-proxy/' target='_blank' aria-label='Kube-proxy'>Kube-proxy</a> 以 iptables 或者 IPVS 模式运行</li>
<li>启用<a href="/zh/docs/concepts/services-networking/endpoint-slices/">端点切片</a></li>
</ul>
<!--
## Enable Service Topology






<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.17 [alpha]</code>
</div>


To enable service topology, enable the `ServiceTopology` and `EndpointSlice` feature gate for all Kubernetes components:
-->
<h2 id="启用服务拓扑">启用服务拓扑</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.17 [alpha]</code>
</div>

<p>要启用服务拓扑功能，需要为所有 Kubernetes 组件启用 <code>ServiceTopology</code> 和 <code>EndpointSlice</code> 特性门控：</p>
<pre tabindex="0"><code>--feature-gates=&quot;ServiceTopology=true,EndpointSlice=true&quot;
</code></pre><h2 id="接下来">接下来</h2>
<!--
* Read about the [Service Topology](/docs/concepts/services-networking/service-topology) concept
* Read about [Endpoint Slices](/docs/concepts/services-networking/endpoint-slices)
* Read [Connecting Applications with Services](/docs/concepts/services-networking/connect-applications-service/)
-->
<ul>
<li>阅读<a href="/zh/docs/concepts/services-networking/service-topology">服务拓扑</a>概念</li>
<li>阅读<a href="/zh/docs/concepts/services-networking/endpoint-slices">端点切片</a></li>
<li>阅读<a href="/zh/docs/concepts/services-networking/connect-applications-service/">通过服务来连接应用</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7127e6b7344b315b30b1ce8c4d8bfc55">2.28 - 控制节点上的 CPU 管理策略</h1>
    
	<!--
title: Control CPU Management Policies on the Node
reviewers:
- sjenning
- ConnorDoyle
- balajismaniam
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code>
</div>

<!--
Kubernetes keeps many aspects of how pods execute on nodes abstracted
from the user. This is by design.  However, some workloads require
stronger guarantees in terms of latency and/or performance in order to operate
acceptably. The kubelet provides methods to enable more complex workload
placement policies while keeping the abstraction free from explicit placement
directives.
-->
<p>按照设计，Kubernetes 对 pod 执行相关的很多方面进行了抽象，使得用户不必关心。
然而，为了正常运行，有些工作负载要求在延迟和/或性能方面有更强的保证。
为此，kubelet 提供方法来实现更复杂的负载放置策略，同时保持抽象，避免显式的放置指令。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## CPU Management Policies

By default, the kubelet uses [CFS quota](https://en.wikipedia.org/wiki/Completely_Fair_Scheduler)
to enforce pod CPU limits.  When the node runs many CPU-bound pods,
the workload can move to different CPU cores depending on
whether the pod is throttled and which CPU cores are available at
scheduling time.  Many workloads are not sensitive to this migration and thus
work fine without any intervention.
-->
<h2 id="cpu-管理策略">CPU 管理策略</h2>
<p>默认情况下，kubelet 使用 <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS 配额</a>
来执行 Pod 的 CPU 约束。
当节点上运行了很多 CPU 密集的 Pod 时，工作负载可能会迁移到不同的 CPU 核，
这取决于调度时 Pod 是否被扼制，以及哪些 CPU 核是可用的。
许多工作负载对这种迁移不敏感，因此无需任何干预即可正常工作。</p>
<!--
However, in workloads where CPU cache affinity and scheduling latency
significantly affect workload performance, the kubelet allows alternative CPU
management policies to determine some placement preferences on the node.
-->
<p>然而，有些工作负载的性能明显地受到 CPU 缓存亲和性以及调度延迟的影响。
对此，kubelet 提供了可选的 CPU 管理策略，来确定节点上的一些分配偏好。</p>
<!--
### Configuration

The CPU Manager policy is set with the `-cpu-manager-policy` kubelet
option. There are two supported policies:
-->
<h3 id="配置">配置</h3>
<p>CPU 管理策略通过 kubelet 参数 <code>--cpu-manager-policy</code> 来指定。支持两种策略：</p>
<!--
* `none`: the default, which represents the existing scheduling behavior.
* `static`: allows pods with certain resource characteristics to be
  granted increased CPU affinity and exclusivity on the node.
-->
<ul>
<li><code>none</code>: 默认策略，表示现有的调度行为。</li>
<li><code>static</code>: 允许为节点上具有某些资源特征的 pod 赋予增强的 CPU 亲和性和独占性。</li>
</ul>
<!--
The CPU manager periodically writes resource updates through the CRI in
order to reconcile in-memory CPU assignments with cgroupfs. The reconcile
frequency is set through a new Kubelet configuration value
`-cpu-manager-reconcile-period`. If not specified, it defaults to the same
duration as `-node-status-update-frequency`.
-->
<p>CPU 管理器定期通过 CRI 写入资源更新，以保证内存中 CPU 分配与 cgroupfs 一致。
同步频率通过新增的 Kubelet 配置参数 <code>--cpu-manager-reconcile-period</code> 来设置。
如果不指定，默认与 <code>--node-status-update-frequency</code> 的周期相同。</p>
<!--
The behavior of the static policy can be fine-tuned using the `--cpu-manager-policy-options` flag.
The flag takes a comma-separated list of `key=value` policy options.
-->
<p>Static 策略的行为可以使用 <code>--cpu-manager-policy-options</code> 参数来微调。
该参数采用一个逗号分隔的 <code>key=value</code> 策略选项列表。</p>
<!--
### None policy

The `none` policy explicitly enables the existing default CPU
affinity scheme, providing no affinity beyond what the OS scheduler does
automatically.  Limits on CPU usage for
[Guaranteed pods](/docs/tasks/configure-pod-container/quality-service-pod/) and
[Burstable pods](/docs/tasks/configure-pod-container/quality-service-pod/)
are enforced using CFS quota.
-->
<h3 id="none-策略">none 策略</h3>
<p><code>none</code> 策略显式地启用现有的默认 CPU 亲和方案，不提供操作系统调度器默认行为之外的亲和性策略。
通过 CFS 配额来实现 <a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">Guaranteed pods</a>
和 <a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">Burstable pods</a>
的 CPU 使用限制。</p>
<!--
### Static policy

The `static` policy allows containers in `Guaranteed` pods with integer CPU
`requests` access to exclusive CPUs on the node. This exclusivity is enforced
using the [cpuset cgroup controller](https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt).
-->
<h3 id="static-策略">static 策略</h3>
<p><code>static</code> 策略针对具有整数型 CPU <code>requests</code> 的 <code>Guaranteed</code> Pod ，它允许该类 Pod
中的容器访问节点上的独占 CPU 资源。这种独占性是使用
<a href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt">cpuset cgroup 控制器</a> 来实现的。</p>
<!--
System services such as the container runtime and the kubelet itself can continue to run on these exclusive CPUs.  The exclusivity only extends to other pods.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 诸如容器运行时和 kubelet 本身的系统服务可以继续在这些独占 CPU 上运行。独占性仅针对其他 Pod。</div>
</blockquote>
<!--
CPU Manager doesn't support offlining and onlining of
CPUs at runtime. Also, if the set of online CPUs changes on the node,
the node must be drained and CPU manager manually reset by deleting the
state file `cpu_manager_state` in the kubelet root directory.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> CPU 管理器不支持运行时下线和上线 CPUs。此外，如果节点上的在线 CPUs 集合发生变化，
则必须驱逐节点上的 Pod，并通过删除 kubelet 根目录中的状态文件 <code>cpu_manager_state</code>
来手动重置 CPU 管理器。</div>
</blockquote>
<!--
This policy manages a shared pool of CPUs that initially contains all CPUs in the
node. The amount of exclusively allocatable CPUs is equal to the total
number of CPUs in the node minus any CPU reservations by the kubelet `--kube-reserved` or
`--system-reserved` options. From 1.17, the CPU reservation list can be specified
explicitly by kubelet `--reserved-cpus` option. The explicit CPU list specified by
`--reserved-cpus` takes precedence over the CPU reservation specified by
`--kube-reserved` and `--system-reserved`. CPUs reserved by these options are taken, in
integer quantity, from the initial shared pool in ascending order by physical
core ID.  This shared pool is the set of CPUs on which any containers in
`BestEffort` and `Burstable` pods run. Containers in `Guaranteed` pods with fractional
CPU `requests` also run on CPUs in the shared pool. Only containers that are
both part of a `Guaranteed` pod and have integer CPU `requests` are assigned
exclusive CPUs.
--->
<p>该策略管理一个共享 CPU 资源池，最初，该资源池包含节点上所有的 CPU 资源。可用
的独占性 CPU 资源数量等于节点的 CPU 总量减去通过 <code>--kube-reserved</code> 或 <code>--system-reserved</code> 参数保留的 CPU 。从1.17版本开始，CPU保留列表可以通过 kublet 的 '--reserved-cpus' 参数显式地设置。
通过 '--reserved-cpus' 指定的显式CPU列表优先于使用 '--kube-reserved' 和 '--system-reserved' 参数指定的保留CPU。 通过这些参数预留的 CPU 是以整数方式，按物理内
核 ID 升序从初始共享池获取的。 共享池是 <code>BestEffort</code> 和 <code>Burstable</code> pod 运行
的 CPU 集合。<code>Guaranteed</code> pod 中的容器，如果声明了非整数值的 CPU <code>requests</code> ，也将运行在共享池的 CPU 上。只有 <code>Guaranteed</code> pod 中，指定了整数型 CPU <code>requests</code> 的容器，才会被分配独占 CPU 资源。</p>
<!--
The kubelet requires a CPU reservation greater than zero be made
using either `--kube-reserved` and/or `--system-reserved`  or `--reserved-cpus` when the static
policy is enabled. This is because zero CPU reservation would allow the shared
pool to become empty.
--->
<blockquote class="note callout">
  <div><strong>说明：</strong> 当启用 static 策略时，要求使用 <code>--kube-reserved</code> 和/或 <code>--system-reserved</code> 或
<code>--reserved-cpus</code> 来保证预留的 CPU 值大于零。
这是因为零预留 CPU 值可能使得共享池变空。</div>
</blockquote>
<!--
As `Guaranteed` pods whose containers fit the requirements for being statically
assigned are scheduled to the node, CPUs are removed from the shared pool and
placed in the cpuset for the container. CFS quota is not used to bound
the CPU usage of these containers as their usage is bound by the scheduling domain
itself. In others words, the number of CPUs in the container cpuset is equal to the integer
CPU `limit` specified in the pod spec. This static assignment increases CPU
affinity and decreases context switches due to throttling for the CPU-bound
workload.

Consider the containers in the following pod specs:
-->
<p>当 <code>Guaranteed</code> Pod 调度到节点上时，如果其容器符合静态分配要求，
相应的 CPU 会被从共享池中移除，并放置到容器的 cpuset 中。
因为这些容器所使用的 CPU 受到调度域本身的限制，所以不需要使用 CFS 配额来进行 CPU 的绑定。
换言之，容器 cpuset  中的 CPU 数量与 Pod 规约中指定的整数型 CPU <code>limit</code> 相等。
这种静态分配增强了 CPU 亲和性，减少了 CPU 密集的工作负载在节流时引起的上下文切换。</p>
<p>考虑以下 Pod 规格的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `BestEffort` QoS class because no resource `requests` or
`limits` are specified. It runs in the shared pool.
-->
<p>该 Pod 属于 <code>BestEffort</code> QoS 类型，因为其未指定 <code>requests</code> 或 <code>limits</code> 值。
所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Burstable` QoS class because resource `requests` do not
equal `limits` and the `cpu` quantity is not specified. It runs in the shared
pool.
-->
<p>该 Pod 属于 <code>Burstable</code> QoS 类型，因为其资源 <code>requests</code> 不等于 <code>limits</code>，且未指定 <code>cpu</code> 数量。
所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Burstable` QoS class because resource `requests` do not
equal `limits`. It runs in the shared pool.
-->
<p>该 pod 属于 <code>Burstable</code> QoS 类型，因为其资源 <code>requests</code> 不等于 <code>limits</code>。
所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Guaranteed` QoS class because `requests` are equal to `limits`.
And the container's resource limit for the CPU resource is an integer greater than
or equal to one. The `nginx` container is granted 2 exclusive CPUs.
-->
<p>该 Pod 属于 <code>Guaranteed</code> QoS 类型，因为其 <code>requests</code> 值与 <code>limits</code>相等。
同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。
所以，该 <code>nginx</code> 容器被赋予 2 个独占 CPU。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1.5&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Guaranteed` QoS class because `requests` are equal to `limits`.
But the container's resource limit for the CPU resource is a fraction. It runs in
the shared pool.
-->
<p>该 Pod 属于 <code>Guaranteed</code> QoS 类型，因为其 <code>requests</code> 值与 <code>limits</code>相等。
但是容器对 CPU 资源的限制值是一个小数。所以该容器运行在共享 CPU 池中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Guaranteed` QoS class because only `limits` are specified
and `requests` are set equal to `limits` when not explicitly specified. And the
container's resource limit for the CPU resource is an integer greater than or
equal to one. The `nginx` container is granted 2 exclusive CPUs.
-->
<p>该 Pod 属于 <code>Guaranteed</code> QoS 类型，因其指定了 <code>limits</code> 值，同时当未显式指定时，
<code>requests</code> 值被设置为与 <code>limits</code> 值相等。
同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。
所以，该 <code>nginx</code> 容器被赋予 2 个独占 CPU。</p>
<!--
#### Static policy options

If the `full-pcpus-only` policy option is specified, the static policy will always allocate full physical cores.
You can enable this option by adding `full-pcups-only=true` to the CPUManager policy options.
-->
<h4 id="static-策略选项">Static 策略选项</h4>
<p>如果使用 <code>full-pcpus-only</code> 策略选项，static 策略总是会分配完整的物理核心。
你可以通过在 CPUManager 策略选项里加上 <code>full-pcups-only=true</code> 来启用该选项。</p>
<!--
By default, without this option, the static policy allocates CPUs using a topology-aware best-fit allocation.
On SMT enabled systems, the policy can allocate individual virtual cores, which correspond to hardware threads.
This can lead to different containers sharing the same physical cores; this behaviour in turn contributes
to the [noisy neighbours problem](https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors).
-->
<p>默认情况下，如果不使用该选项，static 策略会使用拓扑感知最适合的分配方法来分配 CPU。
在启用了 SMT 的系统上，此策略所分配是与硬件线程对应的、独立的虚拟核。
这会导致不同的容器共享相同的物理核心，该行为进而会导致
<a href="https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors">吵闹的邻居问题</a>。</p>
<!--
With the option enabled, the pod will be admitted by the kubelet only if the CPU request of all its containers
can be fulfilled by allocating full physical cores.
If the pod does not pass the admission, it will be put in Failed state with the message `SMTAlignmentError`.
-->
<p>启用该选项之后，只有当一个 Pod 里所有容器的 CPU 请求都能够分配到完整的物理核心时，kubelet 才会接受该 Pod。
如果 Pod 没有被准入，它会被置于 Failed 状态，错误消息是 <code>SMTAlignmentError</code>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8060aed5bf1172fa62199a4c306a4cd1">2.29 - 控制节点上的拓扑管理策略</h1>
    
	<!--
title: Control Topology Management Policies on a node
reviewers:
- ConnorDoyle
- klueska
- lmdaly
- nolancon
- bg-chun
content_type: task
min-kubernetes-server-version: v1.18
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code>
</div>

<!--
An increasing number of systems leverage a combination of CPUs and hardware accelerators to support latency-critical execution and high-throughput parallel computation. These include workloads in fields such as telecommunications, scientific computing, machine learning, financial services and data analytics. Such hybrid systems comprise a high performance environment.
-->
<p>越来越多的系统利用 CPU 和硬件加速器的组合来支持对延迟要求较高的任务和高吞吐量的并行计算。
这类负载包括电信、科学计算、机器学习、金融服务和数据分析等。
此类混合系统即用于构造这些高性能环境。</p>
<!--
In order to extract the best performance, optimizations related to CPU isolation, memory and device locality are required. However, in Kubernetes, these optimizations are handled by a disjoint set of components.
-->
<p>为了获得最佳性能，需要进行与 CPU 隔离、内存和设备局部性有关的优化。
但是，在 Kubernetes 中，这些优化由各自独立的组件集合来处理。</p>
<!--
_Topology Manager_ is a Kubelet component that aims to co-ordinate the set of components that are responsible for these optimizations.
-->
<p><em>拓扑管理器（Topology Manager）</em> 是一个 kubelet 的一部分，旨在协调负责这些优化的一组组件。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.18.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## How Topology Manager Works
-->
<h2 id="拓扑管理器如何工作">拓扑管理器如何工作</h2>
<!--
Prior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make resource allocation decisions independently of each other.
This can result in undesirable allocations on multiple-socketed systems, performance/latency sensitive applications will suffer due to these undesirable allocations. 
 Undesirable in this case meaning for example, CPUs and devices being allocated from different NUMA Nodes thus, incurring additional latency.
-->
<p>在引入拓扑管理器之前， Kubernetes 中的 CPU 和设备管理器相互独立地做出资源分配决策。
这可能会导致在多处理系统上出现并非期望的资源分配；由于这些与期望相左的分配，对性能或延迟敏感的应用将受到影响。
这里的不符合期望意指，例如， CPU 和设备是从不同的 NUMA 节点分配的，因此会导致额外的延迟。</p>
<!--
The Topology Manager is a Kubelet component, which acts as a source of truth so that other Kubelet components can make topology aligned resource allocation choices.
-->
<p>拓扑管理器是一个 Kubelet 组件，扮演信息源的角色，以便其他 Kubelet 组件可以做出与拓扑结构相对应的资源分配决定。</p>
<!--
The Topology Manager provides an interface for components, called *Hint Providers*, to send and receive topology information. Topology Manager has a set of node level policies which are explained below.
-->
<p>拓扑管理器为组件提供了一个称为 <em>建议供应者（Hint Providers）</em> 的接口，以发送和接收拓扑信息。
拓扑管理器具有一组节点级策略，具体说明如下。</p>
<!--
The Topology manager receives Topology information from the *Hint Providers* as a bitmask denoting NUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform a set of operations on the hints provided and converge on the hint determined by the policy to give the optimal result, if an undesirable hint is stored the preferred field for the hint will be set to false. In the current policies preferred is the narrowest preferred mask.
The selected hint is stored as part of the Topology Manager. Depending on the policy configured the pod can be accepted or rejected from the node based on the selected hint.
The hint is then stored in the Topology Manager for use by the *Hint Providers* when making the resource allocation decisions.
-->
<p>拓扑管理器从 <em>建议提供者</em> 接收拓扑信息，作为表示可用的 NUMA 节点和首选分配指示的位掩码。
拓扑管理器策略对所提供的建议执行一组操作，并根据策略对提示进行约减以得到最优解；如果存储了与预期不符的建议，则该建议的优选字段将被设置为 false。
在当前策略中，首选的是最窄的优选掩码。
所选建议将被存储为拓扑管理器的一部分。
取决于所配置的策略，所选建议可用来决定节点接受或拒绝 Pod 。
之后，建议会被存储在拓扑管理器中，供 <em>建议提供者</em> 进行资源分配决策时使用。</p>
<!--
### Enable the Topology Manager feature

Support for the Topology Manager requires `TopologyManager` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled. It is enabled by default starting with Kubernetes 1.18.
-->
<h3 id="启用拓扑管理器功能特性">启用拓扑管理器功能特性</h3>
<p>对拓扑管理器的支持要求启用 <code>TopologyManager</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>。
从 Kubernetes 1.18 版本开始，这一特性默认是启用的。</p>
<!--
### Topology Manager Scopes and Policies

The Topology Manager currently:
 - Aligns Pods of all QoS classes.
 - Aligns the requested resources that Hint Provider provides topology hints for.
-->
<h3 id="拓扑管理器作用域和策略">拓扑管理器作用域和策略</h3>
<p>拓扑管理器目前：</p>
<ul>
<li>对所有 QoS 类的 Pod 执行对齐操作</li>
<li>针对建议提供者所提供的拓扑建议，对请求的资源进行对齐</li>
</ul>
<!--
If these conditions are met, the Topology Manager will align the requested resources.

In order to customise how this alignment is carried out, the Topology Manager provides two distinct knobs: `scope` and `policy`.
-->
<p>如果满足这些条件，则拓扑管理器将对齐请求的资源。</p>
<p>为了定制如何进行对齐，拓扑管理器提供了两种不同的方式：<code>scope</code> 和 <code>policy</code>。</p>
<!--
The `scope` defines the granularity at which you would like resource alignment to be performed (e.g. at the `pod` or `container` level). And the `policy` defines the actual strategy used to carry out the alignment (e.g. `best-effort`, `restricted`, `single-numa-node`, etc.).

Details on the various `scopes` and `policies` available today can be found below.
-->
<p><code>scope</code> 定义了资源对齐时你所希望使用的粒度（例如，是在 <code>pod</code> 还是 <code>container</code> 级别）。
<code>policy</code> 定义了对齐时实际使用的策略（例如，<code>best-effort</code>、<code>restricted</code>、<code>single-numa-node</code> 等等）。</p>
<p>可以在下文找到现今可用的各种 <code>scopes</code> 和 <code>policies</code> 的具体信息。</p>
<!--
To align CPU resources with other requested resources in a Pod Spec, the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node. See [control CPU Management Policies](/docs/tasks/administer-cluster/cpu-management-policies/).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 为了将 Pod 规约中的 CPU 资源与其他请求资源对齐，CPU 管理器需要被启用并且
节点上应配置了适当的 CPU 管理器策略。
参看<a href="/zh/docs/tasks/administer-cluster/cpu-management-policies/">控制 CPU 管理策略</a>.</div>
</blockquote>
<!--
### Topology Manager Scopes

The Topology Manager can deal with the alignment of resources in a couple of distinct scopes:

* `container` (default)
* `pod`

Either option can be selected at a time of the kubelet startup, with `--topology-manager-scope` flag.
-->
<h3 id="拓扑管理器作用域">拓扑管理器作用域</h3>
<p>拓扑管理器可以在以下不同的作用域内进行资源对齐：</p>
<ul>
<li><code>container</code> （默认）</li>
<li><code>pod</code></li>
</ul>
<p>在 kubelet 启动时，可以使用 <code>--topology-manager-scope</code> 标志来选择其中任一选项。</p>
<!--
### container scope

The `container` scope is used by default.
-->
<h3 id="容器作用域">容器作用域</h3>
<p>默认使用的是 <code>container</code> 作用域。</p>
<!--
Within this scope, the Topology Manager performs a number of sequential resource alignments, i.e., for each container (in a pod) a separate alignment is computed. In other words, there is no notion of grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect, the Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.
-->
<p>在该作用域内，拓扑管理器依次进行一系列的资源对齐，
也就是，对每一个容器（包含在一个 Pod 里）计算单独的对齐。
换句话说，在该特定的作用域内，没有根据特定的 NUMA 节点集来把容器分组的概念。
实际上，拓扑管理器会把单个容器任意地对齐到 NUMA 节点上。</p>
<!--
The notion of grouping the containers was endorsed and implemented on purpose in the following scope, for example the `pod` scope.
-->
<p>容器分组的概念是在以下的作用域内特别实现的，也就是 <code>pod</code> 作用域。</p>
<!--
### pod scope

To select the `pod` scope, start the kubelet with the command line option `--topology-manager-scope=pod`.
-->
<h3 id="pod-作用域">Pod 作用域</h3>
<p>使用命令行选项 <code>--topology-manager-scope=pod</code> 来启动 kubelet，就可以选择 <code>pod</code> 作用域。</p>
<!--
This scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers) to either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the alignments produced by the Topology Manager on different occasions:
-->
<p>该作用域允许把一个 Pod 里的所有容器作为一个分组，分配到一个共同的 NUMA 节点集。
也就是，拓扑管理器会把一个 Pod 当成一个整体，
并且试图把整个 Pod（所有容器）分配到一个单个的 NUMA 节点或者一个共同的 NUMA 节点集。
以下的例子说明了拓扑管理器在不同的场景下使用的对齐方式：</p>
<!--
* all containers can be and are allocated to a single NUMA node;
* all containers can be and are allocated to a shared set of NUMA nodes.
-->
<ul>
<li>所有容器可以被分配到一个单一的 NUMA 节点；</li>
<li>所有容器可以被分配到一个共享的 NUMA 节点集。</li>
</ul>
<!--
The total amount of particular resource demanded for the entire pod is calculated according to [effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resources) formula, and thus, this total value is equal to the maximum of:
* the sum of all app container requests,
* the maximum of init container requests,
for a resource.
-->
<p>整个 Pod 所请求的某种资源总量是根据
<a href="/zh/docs/concepts/workloads/pods/init-containers/#resources">有效 request/limit</a>
公式来计算的，
因此，对某一种资源而言，该总量等于以下数值中的最大值：</p>
<ul>
<li>所有应用容器请求之和；</li>
<li>初始容器请求的最大值。</li>
</ul>
<!--
Using the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically valuable for workloads that are latency sensitive or for high-throughput applications that perform IPC. By combining both options, you are able to place all containers in a pod onto a single NUMA node; hence, the inter-NUMA communication overhead can be eliminated for that pod.
-->
<p><code>pod</code> 作用域与 <code>single-numa-node</code> 拓扑管理器策略一起使用，
对于延时敏感的工作负载，或者对于进行 IPC 的高吞吐量应用程序，都是特别有价值的。
把这两个选项组合起来，你可以把一个 Pod 里的所有容器都放到一个单个的 NUMA 节点，
使得该 Pod 消除了 NUMA 之间的通信开销。</p>
<!--
In the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes is present among possible allocations. Reconsider the example above:
-->
<p>在 <code>single-numa-node</code> 策略下，只有当可能的分配方案中存在合适的 NUMA 节点集时，Pod 才会被接受。
重新考虑上述的例子：</p>
<!--
* a set containing only a single NUMA node - it leads to pod being admitted,
* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one NUMA node, two or more NUMA nodes are required to satisfy the allocation).
-->
<ul>
<li>节点集只包含单个 NUMA 节点时，Pod 就会被接受，</li>
<li>然而，节点集包含多个 NUMA 节点时，Pod 就会被拒绝
（因为满足该分配方案需要两个或以上的 NUMA 节点，而不是单个 NUMA 节点）。</li>
</ul>
<!--
To recap, Topology Manager first computes a set of NUMA nodes and then tests it against Topology Manager policy, which either leads to the rejection or admission of the pod.
-->
<p>简要地说，拓扑管理器首先计算出 NUMA 节点集，然后使用拓扑管理器策略来测试该集合，
从而决定拒绝或者接受 Pod。</p>
<!--
### Topology Manager Policies
-->
<h3 id="拓扑管理器策略">拓扑管理器策略</h3>
<!--
Topology Manager supports four allocation policies. You can set a policy via a Kubelet flag, `--topology-manager-policy`.
There are four supported policies:

* `none` (default)
* `best-effort`
* `restricted`
* `single-numa-node`
-->
<p>拓扑管理器支持四种分配策略。
你可以通过 Kubelet 标志 <code>--topology-manager-policy</code> 设置策略。
所支持的策略有四种：</p>
<ul>
<li><code>none</code> (默认)</li>
<li><code>best-effort</code></li>
<li><code>restricted</code></li>
<li><code>single-numa-node</code></li>
</ul>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> If Topology Manager is configured with the <strong>pod</strong> scope, the container, which is considered by the policy, is reflecting requirements of the entire pod, and thus each container from the pod will result with <strong>the same</strong> topology alignment decision.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果拓扑管理器配置使用 <strong>Pod</strong> 作用域，
那么在策略考量一个容器时，该容器反映的是整个 Pod 的要求，
于是该 Pod 里的每个容器都会得到 <strong>相同的</strong> 拓扑对齐决定。</div>
</blockquote>
<!--
### none policy {#policy-none}

This is the default policy and does not perform any topology alignment.
-->
<h3 id="policy-none">none 策略</h3>
<p>这是默认策略，不执行任何拓扑对齐。</p>
<!--
### best-effort policy {#policy-best-effort}

For each container in a Guaranteed Pod, kubelet, with `best-effort` topology 
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager stores the 
preferred NUMA Node affinity for that container. If the affinity is not preferred, 
Topology Manager will store this and admit the pod to the node anyway.
-->
<h3 id="policy-best-effort">best-effort 策略</h3>
<p>对于 Guaranteed 类的 Pod 中的每个容器，具有 <code>best-effort</code> 拓扑管理策略的
kubelet 将调用每个建议提供者以确定资源可用性。
使用此信息，拓扑管理器存储该容器的首选 NUMA 节点亲和性。
如果亲和性不是首选，则拓扑管理器将存储该亲和性，并且无论如何都将  pod 接纳到该节点。</p>
<!--
The *Hint Providers* can then use this information when making the 
resource allocation decision.
-->
<p>之后 <em>建议提供者</em> 可以在进行资源分配决策时使用这个信息。</p>
<!--
### restricted policy {#policy-restricted}

For each container in a Guaranteed Pod, kubelet, with `restricted` topology 
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager stores the 
preferred NUMA Node affinity for that container. If the affinity is not preferred, 
Topology Manager will reject this pod from the node. This will result in a pod in a `Terminated` state with a pod admission failure.
-->
<h3 id="policy-restricted">restricted 策略</h3>
<p>对于 Guaranteed 类 Pod 中的每个容器， 配置了 <code>restricted</code> 拓扑管理策略的 kubelet
调用每个建议提供者以确定其资源可用性。。
使用此信息，拓扑管理器存储该容器的首选 NUMA 节点亲和性。
如果亲和性不是首选，则拓扑管理器将从节点中拒绝此 Pod 。
这将导致 Pod 处于 <code>Terminated</code> 状态，且 Pod 无法被节点接纳。</p>
<!--
Once the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to reschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeploy of the pod.
An external control loop could be also implemented to trigger a redeployment of pods that have the `Topology Affinity` error.
-->
<p>一旦 Pod 处于 <code>Terminated</code> 状态，Kubernetes 调度器将不会尝试重新调度该 Pod。
建议使用 ReplicaSet 或者 Deployment 来重新部署 Pod。
还可以通过实现外部控制环，以启动对具有 <code>Topology Affinity</code> 错误的 Pod 的重新部署。</p>
<!--
If the pod is admitted, the *Hint Providers* can then use this information when making the 
resource allocation decision.
-->
<p>如果 Pod 被允许运行在某节点，则 <em>建议提供者</em> 可以在做出资源分配决定时使用此信息。</p>
<!--
### single-numa-node policy {#policy-single-numa-node}

For each container in a Guaranteed Pod, kubelet, with `single-numa-node` topology 
management policy, calls each Hint Provider to discover their resource availability.
Using this information, the Topology Manager determines if a single NUMA Node affinity is possible.
If it is, Topology Manager will store this and the *Hint Providers* can then use this information when making the 
resource allocation decision.
If, however, this is not possible then the Topology Manager will reject the pod from the node. This will result in a pod in a `Terminated` state with a pod admission failure.
-->
<h3 id="policy-single-numa-node">single-numa-node 策略</h3>
<p>对于 Guaranteed 类 Pod 中的每个容器， 配置了 <code>single-numa-nodde</code> 拓扑管理策略的
kubelet 调用每个建议提供者以确定其资源可用性。
使用此信息，拓扑管理器确定单 NUMA 节点亲和性是否可能。
如果是这样，则拓扑管理器将存储此信息，然后 <em>建议提供者</em> 可以在做出资源分配决定时使用此信息。
如果不可能，则拓扑管理器将拒绝 Pod 运行于该节点。
这将导致 Pod 处于 <code>Terminated</code> 状态，且 Pod 无法被节点接受。</p>
<!--
Once the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to reschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeploy of the Pod.
An external control loop could be also implemented to trigger a redeployment of pods that have the `Topology Affinity` error.
-->
<p>一旦 Pod 处于 <code>Terminated</code> 状态，Kubernetes 调度器将不会尝试重新调度该 Pod。
建议使用 ReplicaSet 或者 Deployment 来重新部署 Pod。
还可以通过实现外部控制环，以触发具有 <code>Topology Affinity</code> 错误的 Pod 的重新部署。</p>
<!--
### Pod Interactions with Topology Manager Policies

Consider the containers in the following pod specs:
-->
<h3 id="pod-与拓扑管理器策略的交互">Pod 与拓扑管理器策略的交互</h3>
<p>考虑以下 pod 规范中的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `BestEffort` QoS class because no resource `requests` or
`limits` are specified.
-->
<p>该 Pod 以 <code>BestEffort</code> QoS 类运行，因为没有指定资源 <code>requests</code> 或 <code>limits</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `Burstable` QoS class because requests are less than limits.
-->
<p>由于 requests 数少于 limits，因此该 Pod 以 <code>Burstable</code> QoS 类运行。</p>
<!--
If the selected policy is anything other than `none`, Topology Manager would consider these Pod specifications. The Topology Manager would consult the Hint Providers to get topology hints. In the case of the `static`, the CPU Manager policy would return default topology hint, because these Pods do not have explicity request CPU resources.
-->
<p>如果选择的策略是 <code>none</code> 以外的任何其他策略，拓扑管理器都会评估这些 Pod 的规范。
拓扑管理器会咨询建议提供者，获得拓扑建议。
若策略为 <code>static</code>，则 CPU 管理器策略会返回默认的拓扑建议，因为这些 Pod
并没有显式地请求 CPU 资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/device</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod with integer CPU request runs in the `Guaranteed` QoS class because
`requests` are equal to `limits`.
-->
<p>此 Pod 以 <code>Guaranteed</code> QoS 类运行，因为其 <code>requests</code> 值等于 <code>limits</code> 值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceA</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceB</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceA</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/deviceB</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This pod runs in the `BestEffort` QoS class because there are no CPU and memory requests.
-->
<p>因为未指定 CPU 和内存请求，所以 Pod 以 <code>BestEffort</code> QoS 类运行。</p>
<!--
The Topology Manager would consider both of the above pods. The Topology Manager would consult the Hint Providers, which are CPU and Device Manager to get topology hints for the pods. 

In the case of the `Guaranteed` pod with integer request, the `static` CPU Manager policy would return hints relating to the CPU request and the Device Manager would send back hints for the requested device.
-->
<p>拓扑管理器将考虑以上两个 Pod。拓扑管理器将咨询建议提供者即 CPU 和设备管理器，以获取 Pod 的拓扑提示。
对于 <code>Guaranteed</code> 类的 CPU 请求数为整数的 Pod，<code>static</code> CPU 管理器策略将返回与 CPU 请求有关的提示，
而设备管理器将返回有关所请求设备的提示。</p>
<!--
In the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy would return default topology hint as there is no exclusive CPU request and the Device Manager would send back hints for the requested device.

In the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default topology hint.
-->
<p>对于 <code>Guaranteed</code> 类的 CPU 请求可共享的 Pod，<code>static</code> CPU
管理器策略将返回默认的拓扑提示，因为没有排他性的 CPU 请求；而设备管理器
则针对所请求的设备返回有关提示。</p>
<p>在上述两种 <code>Guaranteed</code> Pod 的情况中，<code>none</code> CPU 管理器策略会返回默认的拓扑提示。</p>
<!--
In the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default topology hint as there is no CPU request and the Device Manager would send back the hints for each of the requested devices.
-->
<p>对于 <code>BestEffort</code> Pod，由于没有 CPU 请求，<code>static</code> CPU 管理器策略将发送默认提示，
而设备管理器将为每个请求的设备发送提示。</p>
<!--
Using this information the Topology Manager calculates the optimal hint for the pod and stores this information, which will be used by the Hint Providers when they are making their resource assignments.
-->
<p>基于此信息，拓扑管理器将为 Pod 计算最佳提示并存储该信息，并且供
提示提供程序在进行资源分配时使用。</p>
<!--
### Known Limitations

1. The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes there will be a state explosion when trying to enumerate the possible NUMA affinities and generating their hints.

2. The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail on the node due to the Topology Manager.

3. The Device Manager and the CPU Manager are the only components to adopt the Topology Manager's HintProvider interface. This means that NUMA alignment can only be achieved for resources managed by the CPU Manager and the Device Manager. Memory or Hugepages are not considered by the Topology Manager for NUMA alignment.
-->
<h3 id="已知的局限性">已知的局限性</h3>
<ol>
<li>拓扑管理器所能处理的最大 NUMA 节点个数是 8。若 NUMA 节点数超过 8，
枚举可能的 NUMA 亲和性并为之生成提示时会发生状态爆炸。</li>
<li>调度器不支持拓扑功能，因此可能会由于拓扑管理器的原因而在节点上进行调度，然后在该节点上调度失败。</li>
<li>设备管理器和 CPU 管理器时能够采纳拓扑管理器 HintProvider 接口的唯一两个组件。
这意味着 NUMA 对齐只能针对 CPU 管理器和设备管理器所管理的资源实现。
内存和大页面在拓扑管理器决定 NUMA 对齐时都还不会被考虑在内。</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0b17e83b6049e53b8ffa864bdfa07c87">2.30 - 搭建高可用的 Kubernetes Masters</h1>
    
	<!--
reviewers:
- jszczepkowski
title: Set up High-Availability Kubernetes Masters
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes 1.5 [alpha]</code>
</div>

<!--
You can replicate Kubernetes masters in `kube-up` or `kube-down` scripts for Google Compute Engine.
This document describes how to use kube-up/down scripts to manage highly available (HA) masters and how HA masters are implemented for use with GCE.
-->
<p>你可以在谷歌计算引擎（GCE）的 <code>kubeup</code> 或 <code>kube-down</code> 脚本中复制 Kubernetes Master。
本文描述了如何使用 kube-up/down 脚本来管理高可用（HA）的 Master，
以及如何使用 GCE 实现高可用控制节点。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Starting an HA-compatible cluster

To create a new HA-compatible cluster, you must set the following flags in your `kube-up` script:
-->
<h2 id="启动一个兼容高可用的集群">启动一个兼容高可用的集群</h2>
<p>要创建一个新的兼容高可用的集群，你必须在 <code>kubeup</code> 脚本中设置以下标志:</p>
<!--
* `MULTIZONE=true` - to prevent removal of master replicas kubelets from zones different than server's default zone.
Required if you want to run master replicas in different zones, which is recommended.

* `ENABLE_ETCD_QUORUM_READ=true` - to ensure that reads from all API servers will return most up-to-date data.
If true, reads will be directed to leader etcd replica.
Setting this value to true is optional: reads will be more reliable but will also be slower.

Optionally, you can specify a GCE zone where the first master replica is to be created.
Set the following flag:

* `KUBE_GCE_ZONE=zone` - zone where the first master replica will run.

The following sample command sets up a HA-compatible cluster in the GCE zone europe-west1-b:

```shell
MULTIZONE=true KUBE_GCE_ZONE=europe-west1-b  ENABLE_ETCD_QUORUM_READS=true ./cluster/kube-up.sh
```

Note that the commands above create a cluster with one master;
however, you can add new master replicas to the cluster with subsequent commands.
-->
<ul>
<li>
<p><code>MULTIZONE=true</code> - 为了防止从不同于服务器的默认区域的区域中删除 kubelets 副本。
如果你希望在不同的区域运行副本，那么这一项是必需并且推荐的。</p>
</li>
<li>
<p><code>ENABLE_ETCD_QUORUM_READ=true</code> - 确保从所有 API 服务器读取数据时将返回最新的数据。
如果为 true，读操作将被定向到主 etcd 副本。可以选择将这个值设置为 true，
那么读取将更可靠，但也会更慢。</p>
</li>
</ul>
<p>你还可以指定一个 GCE 区域，在这里创建第一个主节点副本。设置以下标志:</p>
<ul>
<li><code>KUBE_GCE_ZONE=zone</code> - 将运行第一个主节点副本的区域。</li>
</ul>
<p>下面的命令演示在 GCE  europe-west1-b 区域中设置一个兼容高可用的集群:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">MULTIZONE</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>europe-west1-b  <span style="color:#b8860b">ENABLE_ETCD_QUORUM_READS</span><span style="color:#666">=</span><span style="color:#a2f">true</span> ./cluster/kube-up.sh
</code></pre></div><p>注意，上面的命令创建一个只有单一主节点的集群;
但是，你可以使用后续命令将新的主节点副本添加到集群中。</p>
<!--
## Adding a new master replica

After you have created an HA-compatible cluster, you can add master replicas to it.
You add master replicas by using a `kube-up` script with the following flags:

* `KUBE_REPLICATE_EXISTING_MASTER=true` - to create a replica of an existing
master.

* `KUBE_GCE_ZONE=zone` - zone where the master replica will run.
Must be in the same region as other replicas' zones.

You don't need to set the `MULTIZONE` or `ENABLE_ETCD_QUORUM_READS` flags,
as those are inherited from when you started your HA-compatible cluster.

The following sample command replicates the master on an existing HA-compatible cluster:

```shell
KUBE_GCE_ZONE=europe-west1-c KUBE_REPLICATE_EXISTING_MASTER=true ./cluster/kube-up.sh
```
-->
<h2 id="增加一个新的主节点副本">增加一个新的主节点副本</h2>
<p>在创建了兼容高可用的集群之后，可以向其中添加主节点副本。
你可以使用带有如下标记的 <code>kubeup</code> 脚本添加主节点副本:</p>
<ul>
<li>
<p><code>KUBE_REPLICATE_EXISTING_MASTER=true</code> - 创建一个已经存在的主节点的副本。</p>
</li>
<li>
<p><code>KUBE_GCE_ZONE=zone</code> -主节点副本将运行的区域。必须与其他副本位于同一区域。</p>
</li>
</ul>
<p>你无需设置 <code>MULTIZONE</code> 或 <code>ENABLE_ETCD_QUORUM_READS</code> 标志，因为他们可以从兼容高可用的集群中继承。</p>
<p>使用下面的命令可以复制现有兼容高可用的集群上的 Master:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>europe-west1-c <span style="color:#b8860b">KUBE_REPLICATE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> ./cluster/kube-up.sh
</code></pre></div><!--
## Removing a master replica

You can remove a master replica from an HA cluster by using a `kube-down` script with the following flags:

* `KUBE_DELETE_NODES=false` - to restrain deletion of kubelets.

* `KUBE_GCE_ZONE=zone` - the zone from where master replica will be removed.

* `KUBE_REPLICA_NAME=replica_name` - (optional) the name of master replica to remove.
If empty: any replica from the given zone will be removed.

The following sample command removes a master replica from an existing HA cluster:

```shell
KUBE_DELETE_NODES=false KUBE_GCE_ZONE=europe-west1-c ./cluster/kube-down.sh
```
-->
<h2 id="删除主节点副本">删除主节点副本</h2>
<p>你可以使用一个 <code>kube-down</code> 脚本从高可用集群中删除一个主节点副本，并可以使用以下标记:</p>
<ul>
<li>
<p><code>KUBE_DELETE_NODES=false</code> - 限制删除 kubelets。</p>
</li>
<li>
<p><code>KUBE_GCE_ZONE=zone</code> - 将移除主节点副本的区域。</p>
</li>
<li>
<p><code>KUBE_REPLICA_NAME=replica_name</code> - （可选）要删除的主节点副本的名称。
如果为空：将删除给定区域中的所有副本。</p>
</li>
</ul>
<p>使用下面的命令可以从一个现有的高可用集群中删除一个 Master副本:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_DELETE_NODES</span><span style="color:#666">=</span><span style="color:#a2f">false</span> <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>europe-west1-c ./cluster/kube-down.sh
</code></pre></div><!--
## Handling master replica failures

If one of the master replicas in your HA cluster fails,
the best practice is to remove the replica from your cluster and add a new replica in the same zone.
The following sample commands demonstrate this process:

1. Remove the broken replica:

```shell
KUBE_DELETE_NODES=false KUBE_GCE_ZONE=replica_zone KUBE_REPLICA_NAME=replica_name ./cluster/kube-down.sh
```

<ol start="2"><li>Add a new replica in place of the old one:</li></ol>

```shell
KUBE_GCE_ZONE=replica-zone KUBE_REPLICATE_EXISTING_MASTER=true ./cluster/kube-up.sh
```
-->
<h2 id="处理主节点副本失败">处理主节点副本失败</h2>
<p>如果高可用集群中的一个主节点副本失败，最佳实践是从集群中删除副本，
并在相同的区域中添加一个新副本。
下面的命令演示了这个过程:</p>
<ol>
<li>删除失败的副本:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_DELETE_NODES</span><span style="color:#666">=</span><span style="color:#a2f">false</span> <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>replica_zone <span style="color:#b8860b">KUBE_REPLICA_NAME</span><span style="color:#666">=</span>replica_name ./cluster/kube-down.sh
</code></pre></div><ol start="2"><li>在原有位置增加一个新副本：</li></ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>replica-zone <span style="color:#b8860b">KUBE_REPLICATE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> ./cluster/kube-up.sh
</code></pre></div><!--
## Best practices for replicating masters for HA clusters

* Try to place master replicas in different zones. During a zone failure, all masters placed inside the zone will fail.
To survive zone failure, also place nodes in multiple zones
(see [multiple-zones](/docs/setup/best-practices/multiple-zones/) for details).

* Do not use a cluster with two master replicas. Consensus on a two-replica cluster requires both replicas running when changing persistent state.
As a result, both replicas are needed and a failure of any replica turns cluster into majority failure state.
A two-replica cluster is thus inferior, in terms of HA, to a single replica cluster.

* When you add a master replica, cluster state (etcd) is copied to a new instance.
If the cluster is large, it may take a long time to duplicate its state.
This operation may be sped up by migrating etcd data directory, as described [here](https://coreos.com/etcd/docs/latest/admin_guide.html#member-migration)
(we are considering adding support for etcd data dir migration in future).
-->
<h2 id="高可用集群复制主节点的最佳实践">高可用集群复制主节点的最佳实践</h2>
<ul>
<li>
<p>尝试将主节点副本放置在不同的区域。在某区域故障时，放置在该区域内的所有主机都将失败。
为了在区域故障中幸免，请同样将工作节点放置在多区域中
（详情请见<a href="/zh/docs/setup/best-practices/multiple-zones/">多区域</a>）。</p>
</li>
<li>
<p>不要使用具有两个主节点副本的集群。在双副本集群上达成一致需要在更改持久状态时
两个副本都处于运行状态。
因此，两个副本都是需要的，任一副本的失败都会将集群带入多数失败状态。
因此，就高可用而言，双副本集群不如单个副本集群。</p>
</li>
<li>
<p>添加主节点副本时，集群状态（etcd）会被复制到一个新实例。如果集群很大，
可能需要很长时间才能复制它的状态。
这个操作可以通过迁移 etcd 数据存储来加速, 详情参见
<a href="https://coreos.com/etcd/docs/latest/admin_guide.html#member-migration">这里</a>
（我们正在考虑在未来添加对迁移 etcd 数据存储的支持）。</p>
</li>
</ul>
<!-- discussion -->
<!--
## Implementation notes

![ha-master-gce](/images/docs/ha-master-gce.png)
-->
<h2 id="实现说明">实现说明</h2>
<p><img src="/images/docs/ha-master-gce.png" alt="ha-master-gce"></p>
<!--
### Overview

Each of master replicas will run the following components in the following mode:

* etcd instance: all instances will be clustered together using consensus;

* API server: each server will talk to local etcd - all API servers in the cluster will be available;

* controllers, scheduler, and cluster auto-scaler: will use lease mechanism - only one instance of each of them will be active in the cluster;

* add-on manager: each manager will work independently trying to keep add-ons in sync.

In addition, there will be a load balancer in front of API servers that will route external and internal traffic to them.
-->
<h3 id="概述">概述</h3>
<p>每个主节点副本将以以下模式运行以下组件:</p>
<ul>
<li>
<p>etcd 实例： 所有实例将会以共识方式组建集群；</p>
</li>
<li>
<p>API 服务器： 每个服务器将与本地 etcd 通信——集群中的所有 API 服务器都可用;</p>
</li>
<li>
<p>控制器、调度器和集群自动扩缩器：将使用租约机制 —— 每个集群中只有一个实例是可用的；</p>
</li>
<li>
<p>插件管理器：每个管理器将独立工作，试图保持插件同步。</p>
</li>
</ul>
<p>此外，在 API 服务器前面将有一个负载均衡器，用于将外部和内部通信路由到他们。</p>
<!--
### Load balancing

When starting the second master replica, a load balancer containing the two replicas will be created
and the IP address of the first replica will be promoted to IP address of load balancer.
Similarly, after removal of the penultimate master replica, the load balancer will be removed and its IP address will be assigned to the last remaining replica.
Please note that creation and removal of load balancer are complex operations and it may take some time (~20 minutes) for them to propagate.
-->
<h3 id="负载均衡">负载均衡</h3>
<p>启动第二个主节点副本时，将创建一个包含两个副本的负载均衡器，
并将第一个副本的 IP 地址提升为负载均衡器的 IP 地址。
类似地，在删除倒数第二个主节点副本之后，将删除负载均衡器，
并将其 IP 地址分配给最后一个剩余的副本。
请注意，创建和删除负载均衡器是复杂的操作，可能需要一些时间（约20分钟）来同步。</p>
<!--
###主节点service & kubelets

Instead of trying to keep an up-to-date list of Kubernetes apiserver in the Kubernetes service,
the system directs all traffic to the external IP:

* in one master cluster the IP points to the single master,

* in multi-master cluster the IP points to the load balancer in-front of the masters.

Similarly, the external IP will be used by kubelets to communicate with master.
-->
<h3 id="主节点服务-kubelets">主节点服务 &amp; kubelets</h3>
<p>Kubernetes 并不试图在其服务中保持 apiserver 的列表为最新，
相反，它将将所有访问请求指向外部 IP：</p>
<ul>
<li>在拥有一个主节点的集群中，IP 指向单一的主节点，</li>
<li>在拥有多个主节点的集群中，IP 指向主节点前面的负载均衡器。</li>
</ul>
<p>类似地，kubelets 将使用外部 IP 与主节点通信。</p>
<!--
### Master certificates

Kubernetes generates主节点TLS certificates for the external public IP and local IP for each replica.
There are no certificates for the ephemeral public IP for replicas;
to access a replica via its ephemeral public IP, you must skip TLS verification.
-->
<h3 id="主节点证书">主节点证书</h3>
<p>Kubernetes 为每个副本的外部公共 IP 和本地 IP 生成主节点 TLS 证书。
副本的临时公共 IP 没有证书；
要通过其临时公共 IP 访问副本，必须跳过 TLS 检查。</p>
<!--
### Clustering etcd

To allow etcd clustering, ports needed to communicate between etcd instances will be opened (for inside cluster communication).
To make such deployment secure, communication between etcd instances is authorized using SSL.
-->
<h3 id="etcd-集群">etcd 集群</h3>
<p>为了允许 etcd 组建集群，需开放 etcd 实例之间通信所需的端口（用于集群内部通信）。
为了使这种部署安全，etcd 实例之间的通信使用 SSL 进行鉴权。</p>
<!--
### API server identity
-->
<h3 id="api-服务器标识">API 服务器标识</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>

<!--
The API Server Identity feature is controlled by a
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
and is not enabled by default. You can activate API Server Identity by enabling
the feature gate named `APIServerIdentity` when you start the
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API Server'>API Server</a>:
-->
<p>使用 API 服务器标识功能需要启用<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
该功能默认不启用。
你可以在启动 <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a> 的时候启用特性门控 <code>APIServerIdentity</code> 来激活 API 服务器标识：</p>
<!--
```shell
kube-apiserver \
--feature-gates=APIServerIdentity=true \
 # …and other flags as usual
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kube-apiserver <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>--feature-gates<span style="color:#666">=</span><span style="color:#b8860b">APIServerIdentity</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span> <span style="color:#080;font-style:italic"># …其他标记照常</span>
</code></pre></div><!--
During bootstrap, each kube-apiserver assigns a unique ID to itself. The ID is
in the format of `kube-apiserver-{UUID}`. Each kube-apiserver creates a
[Lease](/docs/reference/generated/kubernetes-api/v1.22//#lease-v1-coordination-k8s-io)
in the _kube-system_ <a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespaces'>namespaces</a>.
-->
<p>在启动引导过程中，每个 kube-apiserver 会给自己分配一个唯一 ID。
该 ID 的格式是 <code>kube-apiserver-{UUID}</code>。
每个 kube-apiserver 会在 <em>kube-system</em> <a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a> 里创建一个 <a href="/docs/reference/generated/kubernetes-api/v1.22//#lease-v1-coordination-k8s-io"><code>Lease</code> 对象</a>。</p>
<!--
The Lease name is the unique ID for the kube-apiserver. The Lease contains a
label `k8s.io/component=kube-apiserver`. Each kube-apiserver refreshes its
Lease every `IdentityLeaseRenewIntervalSeconds` (defaults to 10s). Each
kube-apiserver also checks all the kube-apiserver identity Leases every
`IdentityLeaseDurationSeconds` (defaults to 3600s), and deletes Leases that
hasn't got refreshed for more than `IdentityLeaseDurationSeconds`.
`IdentityLeaseRenewIntervalSeconds` and `IdentityLeaseDurationSeconds` can be
configured by kube-apiserver flags `identity-lease-renew-interval-seconds`
and `identity-lease-duration-seconds`.
-->
<p><code>Lease</code> 对象的名字是 kube-apiserver 的唯一 ID。
<code>Lease</code> 对象包含一个标签 <code>k8s.io/component=kube-apiserver</code>。
每个 kube-apiserver 每过 <code>IdentityLeaseRenewIntervalSeconds</code>（默认是 10 秒）就会刷新它的 <code>Lease</code> 对象。
每个 kube-apiserver 每过 <code>IdentityLeaseDurationSeconds</code>（默认是 3600 秒）也会检查所有 kube-apiserver 的标识 <code>Lease</code> 对象，
并且会删除超过 <code>IdentityLeaseDurationSeconds</code> 时间还没被刷新的 <code>Lease</code> 对象。
可以在 kube-apiserver 的 <code>identity-lease-renew-interval-seconds</code>
和 <code>identity-lease-duration-seconds</code> 标记里配置 <code>IdentityLeaseRenewIntervalSeconds</code> 和 <code>IdentityLeaseDurationSeconds</code>。</p>
<!--
Enabling this feature is a prerequisite for using features that involve HA API
server coordination (for example, the `StorageVersionAPI` feature gate).
-->
<p>启用该功能是使用 HA API 服务器协调相关功能（例如，<code>StorageVersionAPI</code> 特性门控）的前提条件。</p>
<!--
## Additional reading

[Automated HA master deployment - design doc](https://git.k8s.io/community/contributors/design-proposals/cluster-lifecycle/ha_master.md)
-->
<h2 id="拓展阅读">拓展阅读</h2>
<p><a href="https://git.k8s.io/community/contributors/design-proposals/cluster-lifecycle/ha_master.md">自动化高可用集群部署 - 设计文档</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2bffd7f3571cdd609bd97fb2e1bdb2fe">2.31 - 改变默认 StorageClass</h1>
    
	<!-- overview -->
<!--
This page shows how to change the default Storage Class that is used to
provision volumes for PersistentVolumeClaims that have no special requirements.
-->
<p>本文展示了如何改变默认的 Storage Class，它用于为没有特殊需求的 PersistentVolumeClaims 配置 volumes。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Why change the default storage class?

Depending on the installation method, your Kubernetes cluster may be deployed with
an existing StorageClass that is marked as default. This default StorageClass
is then used to dynamically provision storage for PersistentVolumeClaims
that do not require any specific storage class. See
[PersistentVolumeClaim documentation](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
for details.
-->
<h2 id="为什么要改变默认存储类">为什么要改变默认存储类？</h2>
<p>取决于安装模式，你的 Kubernetes 集群可能和一个被标记为默认的已有 StorageClass 一起部署。
这个默认的 StorageClass 以后将被用于动态的为没有特定存储类需求的 PersistentVolumeClaims
配置存储。更多细节请查看
<a href="/zh/docs/concepts/storage/persistent-volumes/#perspersistentvolumeclaims">PersistentVolumeClaim 文档</a>。</p>
<!--
The pre-installed default StorageClass may not fit well with your expected workload;
for example, it might provision storage that is too expensive. If this is the case,
you can either change the default StorageClass or disable it completely to avoid
dynamic provisioning of storage.
-->
<p>预先安装的默认 StorageClass 可能不能很好的适应你期望的工作负载；例如，它配置的存储可能太过昂贵。
如果是这样的话，你可以改变默认 StorageClass，或者完全禁用它以防止动态配置存储。</p>
<!--
Deleting the default StorageClass may not work, as it may be re-created
automatically by the addon manager running in your cluster. Please consult the docs for your installation
for details about addon manager and how to disable individual addons.
-->
<p>删除默认 StorageClass 可能行不通，因为它可能会被你集群中的扩展管理器自动重建。
请查阅你的安装文档中关于扩展管理器的细节，以及如何禁用单个扩展。</p>
<!--
## Changing the default StorageClass
-->
<h2 id="改变默认-storageclass">改变默认 StorageClass</h2>
<!--
1. List the StorageClasses in your cluster: 
-->
<ol>
<li>
<p>列出你的集群中的 StorageClasses：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get storageclass
</code></pre></div><p>输出类似这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">NAME                 PROVISIONER               AGE
standard <span style="color:#666">(</span>default<span style="color:#666">)</span>   kubernetes.io/gce-pd      1d
gold                 kubernetes.io/gce-pd      1d
</code></pre></div><p>默认 StorageClass 以 <code>(default)</code> 标记。</p>
</li>
</ol>
<!--
1. Mark the default StorageClass as non-default:
-->
<ol start="2">
<li>
<p>标记默认 StorageClass  非默认：</p>
<!--
The default StorageClass has an annotation
`storageclass.kubernetes.io/is-default-class` set to `true`. Any other value
or absence of the annotation is interpreted as `false`.

To mark a StorageClass as non-default, you need to change its value to `false`:
-->
<p>默认 StorageClass 的注解 <code>storageclass.kubernetes.io/is-default-class</code> 设置为 <code>true</code>。
注解的其它任意值或者缺省值将被解释为 <code>false</code>。</p>
<p>要标记一个 StorageClass 为非默认的，你需要改变它的值为 <code>false</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch storageclass standard -p <span style="color:#b44">&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;false&#34;}}}&#39;</span>
</code></pre></div><!--
where `standard` is the name of your chosen StorageClass.
-->
<p>这里的 <code>standard</code> 是你选择的 StorageClass 的名字。</p>
</li>
</ol>
<!--
1. Mark a StorageClass as default:
-->
<ol start="3">
<li>
<p>标记一个 StorageClass 为默认的：</p>
<!--
Similar to the previous step, you need to add/set the annotation
`storageclass.kubernetes.io/is-default-class=true`.
-->
<p>和前面的步骤类似，你需要添加/设置注解 <code>storageclass.kubernetes.io/is-default-class=true</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch storageclass &lt;your-class-name&gt; -p <span style="color:#b44">&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;true&#34;}}}&#39;</span>
</code></pre></div><!--
Please note that at most one StorageClass can be marked as default. If two
or more of them are marked as default, a `PersistentVolumeClaim` without
`storageClassName` explicitly specified cannot be created.
-->
<p>请注意，最多只能有一个 StorageClass 能够被标记为默认。
如果它们中有两个或多个被标记为默认，Kubernetes 将忽略这个注解，
也就是它将表现为没有默认 StorageClass。</p>
</li>
</ol>
<!--
1. Verify that your chosen StorageClass is default:
-->
<ol start="4">
<li>
<p>验证你选用的 StorageClass 为默认的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get storageclass
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似这样：</p>
<pre tabindex="0"><code>NAME             PROVISIONER               AGE
standard         kubernetes.io/gce-pd      1d
gold (default)   kubernetes.io/gce-pd      1d
</code></pre></li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [PersistentVolumes](/docs/concepts/storage/persistent-volumes/).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fbc9136f53eccd6eb8c80f4bbea3b8f4">2.32 - 更改 PersistentVolume 的回收策略</h1>
    
	<!-- overview -->
<!--
This page shows how to change the reclaim policy of a Kubernetes
PersistentVolume.
-->
<p>本文展示了如何更改 Kubernetes PersistentVolume 的回收策略。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Why change reclaim policy of a PersistentVolume

PersistentVolumes can have various reclaim policies, including "Retain",
"Recycle", and "Delete". For dynamically provisioned PersistentVolumes,
the default reclaim policy is "Delete". This means that a dynamically provisioned
volume is automatically deleted when a user deletes the corresponding
PersistentVolumeClaim. This automatic behavior might be inappropriate if the volume
contains precious data. In that case, it is more appropriate to use the "Retain"
policy. With the "Retain" policy, if a user deletes a PersistentVolumeClaim,
the corresponding PersistentVolume will not be deleted. Instead, it is moved to the
Released phase, where all of its data can be manually recovered.
-->
<h2 id="为什么要更改-persistentvolume-的回收策略">为什么要更改 PersistentVolume 的回收策略</h2>
<p>PersistentVolumes 可以有多种回收策略，包括 &quot;Retain&quot;、&quot;Recycle&quot; 和  &quot;Delete&quot;。
对于动态配置的 PersistentVolumes 来说，默认回收策略为 &quot;Delete&quot;。
这表示当用户删除对应的 PersistentVolumeClaim 时，动态配置的 volume 将被自动删除。
如果 volume 包含重要数据时，这种自动行为可能是不合适的。
那种情况下，更适合使用 &quot;Retain&quot; 策略。
使用 &quot;Retain&quot; 时，如果用户删除 PersistentVolumeClaim，对应的 PersistentVolume 不会被删除。
相反，它将变为 Released 状态，表示所有的数据可以被手动恢复。</p>
<!--
## Changing the reclaim policy of a PersistentVolume
-->
<h2 id="更改-persistentvolume-的回收策略">更改 PersistentVolume 的回收策略</h2>
<!--
1. List the PersistentVolumes in your cluster:
-->
<ol>
<li>
<p>列出你集群中的 PersistentVolumes</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pv
</code></pre></div><p>输出类似于这样：</p>
<pre tabindex="0"><code>NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                  REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1                   10s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2                   6s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim3                   3s
</code></pre><!--
This list also includes the name of the claims that are bound to each volume
for easier identification of dynamically provisioned volumes.
-->
<p>这个列表同样包含了绑定到每个卷的 claims 名称，以便更容易的识别动态配置的卷。</p>
</li>
</ol>
<!--
1. Choose one of your PersistentVolumes and change its reclaim policy:
-->
<ol start="2">
<li>
<p>选择你的 PersistentVolumes 中的一个并更改它的回收策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch pv &lt;your-pv-name&gt; -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;persistentVolumeReclaimPolicy&#34;:&#34;Retain&#34;}}&#39;</span>
</code></pre></div><!--
where `<your-pv-name>` is the name of your chosen PersistentVolume.
-->
<p>这里的 <code>&lt;your-pv-name&gt;</code> 是你选择的 PersistentVolume 的名字。</p>
<!--
On Windows, you must _double_ quote any JSONPath template that contains spaces
(not single quote as shown above for bash). This in turn means that you must
use a single quote or escaped double quote around any literals in the template. For example:
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>在 Windows 系统上，你必须对包含空格的 JSONPath 模板加双引号（而不是像上面
一样为 Bash 环境使用的单引号）。这也意味着你必须使用单引号或者转义的双引号
来处理模板中的字面值。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cmd" data-lang="cmd">kubectl patch pv &lt;your-pv-name&gt; -p <span style="color:#b44">&#34;{\&#34;</span>spec\<span style="color:#b44">&#34;:{\&#34;</span>persistentVolumeReclaimPolicy\<span style="color:#b44">&#34;:\&#34;</span>Retain\<span style="color:#b44">&#34;}}&#34;</span>
</code></pre></div></div>
</blockquote>
</li>
</ol>
<!--
1. Verify that your chosen PersistentVolume has the right policy:
-->
<ol start="3">
<li>
<p>验证你选择的 PersistentVolume 拥有正确的策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pv
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于这样：</p>
<pre tabindex="0"><code>NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                  REASON    AGE
pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1                   40s
pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2                   36s
pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Retain          Bound     default/claim3                   33s
</code></pre><!--
In the preceding output, you can see that the volume bound to claim
`default/claim3` has reclaim policy `Retain`. It will not be automatically
deleted when a user deletes claim `default/claim3`.
-->
<p>在前面的输出中，你可以看到绑定到申领 <code>default/claim3</code> 的卷的回收策略为 <code>Retain</code>。
当用户删除申领 <code>default/claim3</code> 时，它不会被自动删除。</p>
</li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [PersistentVolumes](/docs/concepts/storage/persistent-volumes/).
* Learn more about [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
<li>进一步了解 <a href="/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a></li>
</ul>
<h3 id="参考">参考</h3>
<ul>
<li><a href="/docs/api-reference/v1.22/#persistentvolume-v1-core">PersistentVolume</a></li>
<li><a href="/docs/api-reference/v1.22/#persistentvolumeclaim-v1-core">PersistentVolumeClaim</a></li>
<li>参阅 <a href="/docs/api-reference/v1.22/#persistentvolumeclaim-v1-core">PersistentVolumeSpec</a> 的 <code>persistentVolumeReclaimPolicy</code> 字段</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-966cd1cc69c69410d8698b3ac74abce2">2.33 - 自动扩缩集群 DNS 服务</h1>
    
	<!--
title: Autoscale the DNS Service in a Cluster
content_type: task
-->
<!-- overview -->
<!--
This page shows how to enable and configure autoscaling of the DNS service in a
Kubernetes cluster.
-->
<p>本页展示了如何在集群中启用和配置 DNS 服务的自动扩缩功能。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!--
* This guide assumes your nodes use the AMD64 or Intel 64 CPU architecture

* Make sure the [DNS feature](/docs/concepts/services-networking/dns-pod-service/) itself is enabled.

* Kubernetes version 1.4.0 or later is recommended.
-->
<ul>
<li>
<p>本指南假设你的节点使用 AMD64 或 Intel 64 CPU 架构</p>
</li>
<li>
<p>确保已启用 <a href="/zh/docs/concepts/services-networking/dns-pod-service/">DNS 功能</a>本身。</p>
</li>
<li>
<p>建议使用 Kubernetes 1.4.0 或更高版本。</p>
</li>
</ul>
<!-- steps -->
<!--
## Determining whether DNS horizontal autoscaling is already enabled

List the <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a>
in your cluster in the kube-system namespace:

```shell
kubectl get deployment --namespace=kube-system
```

The output is similar to this:

    NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
    ...
    dns-autoscaler        1         1         1            1           ...
    ...

If you see "dns-autoscaler" in the output, DNS horizontal autoscaling is
already enabled, and you can skip to
[Tuning autoscaling parameters](#tuning-autoscaling-parameters).
-->
<h2 id="determining-whether-dns-horizontal-autoscaling-is-already-enabled">确定是否 DNS 水平 水平自动扩缩特性已经启用</h2>
<p>在 kube-system 命名空间中列出集群中的 <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a> ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><p>输出类似如下这样：</p>
<pre tabindex="0"><code>NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
...
dns-autoscaler        1         1         1            1           ...
...
</code></pre><p>如果在输出中看到 “dns-autoscaler”，说明 DNS 水平自动扩缩已经启用，可以跳到
<a href="#tuning-autoscaling-parameters">调优自动扩缩参数</a>。</p>
<!--
## Getting the name of your DNS Deployment {#find-scaling-target}

List the Deployments in your cluster in the kube-system namespace:

```shell
kubectl get deployment --namespace=kube-system
```

The output is similar to this:
-->
<h2 id="find-scaling-target">获取 DNS Deployment 的名称</h2>
<p>列出集群内 kube-system 名字空间中的 DNS Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment -l k8s-app<span style="color:#666">=</span>kube-dns --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><p>输出类似如下这样：</p>
<pre tabindex="0"><code>NAME      READY   UP-TO-DATE   AVAILABLE   AGE
...
coredns   2/2     2            2           ...
...
</code></pre><!--
If you don't see a Deployment for DNS services, you can also look for it by name:
-->
<p>如果看不到 DNS 服务的 Deployment，你也可以通过名字来查找：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
and look for a deployment named `coredns` or `kube-dns`.
-->
<p>并在输出中寻找名称为 <code>coredns</code> 或 <code>kube-dns</code> 的 Deployment。</p>
<!--
Your scale target is:
-->
<p>你的扩缩目标为：</p>
<pre tabindex="0"><code>Deployment/&lt;your-deployment-name&gt;
</code></pre><!--
where `<your-deployment-name>` is the name of your DNS Deployment. For example, if
your DNS Deployment name is coredns, your scale target is Deployment/coredns.
-->
<p>其中 <code>&lt;your-deployment-name&gt;</code> 是 DNS Deployment 的名称。
例如，如果你的 DNS Deployment 名称是 <code>coredns</code>，则你的扩展目标是 Deployment/coredns。</p>
<!--
CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label
`k8s-app=kube-dns` so that it can work in clusters that originally used
kube-dns.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> CoreDNS 是 Kubernetes 的默认 DNS 服务。CoreDNS 设置标签 <code>k8s-app=kube-dns</code>，
以便能够在原来使用 <code>kube-dns</code> 的集群中工作。</div>
</blockquote>
<!--
## Enabling DNS horizontal autoscaling      {#enablng-dns-horizontal-autoscaling}

In this section, you create a Deployment. The Pods in the Deployment run a
container based on the `cluster-proportional-autoscaler-amd64` image.

Create a file named `dns-horizontal-autoscaler.yaml` with this content:
-->
<h2 id="enablng-dns-horizontal-autoscaling">启用 DNS 水平自动扩缩  </h2>
<p>在本节，我们创建一个 Deployment。Deployment 中的 Pod 运行一个基于
<code>cluster-proportional-autoscaler-amd64</code> 镜像的容器。</p>
<p>创建文件 <code>dns-horizontal-autoscaler.yaml</code>，内容如下所示：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/dns/dns-horizontal-autoscaler.yaml" download="admin/dns/dns-horizontal-autoscaler.yaml"><code>admin/dns/dns-horizontal-autoscaler.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-dns-dns-horizontal-autoscaler-yaml')" title="Copy admin/dns/dns-horizontal-autoscaler.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-dns-dns-horizontal-autoscaler-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>20m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>10Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- /cluster-proportional-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --namespace=kube-system<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --configmap=dns-autoscaler<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --target=&lt;SCALE_TARGET&gt;<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># When cluster is using large nodes(with more cores), &#34;coresPerReplica&#34; should dominate.</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># If using small nodes, &#34;nodesPerReplica&#34; should dominate.</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --default-params={&#34;linear&#34;:{&#34;coresPerReplica&#34;:256,&#34;nodesPerReplica&#34;:16,&#34;min&#34;:1}}<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --logtostderr=true<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --v=2<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the file, replace `<SCALE_TARGET>` with your scale target.

Go to the directory that contains your configuration file, and enter this
command to create the Deployment:
-->
<p>在文件中，将 <code>&lt;SCALE_TARGET&gt;</code> 替换成扩缩目标。</p>
<p>进入到包含配置文件的目录中，输入如下命令创建 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f dns-horizontal-autoscaler.yaml
</code></pre></div><!--
The output of a successful command is:
-->
<p>一个成功的命令输出是：</p>
<pre tabindex="0"><code>deployment.apps/dns-autoscaler created
</code></pre><!--
DNS horizontal autoscaling is now enabled.
-->
<p>DNS 水平自动扩缩在已经启用了。</p>
<!--
## Tuning autoscaling parameters  {#tuning-autoscaling-parameters}

Verify that the dns-autoscaler <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a> exists:
-->
<h2 id="tuning-autoscaling-parameters">调优自动扩缩参数  </h2>
<p>验证 dns-autoscaler <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a> 是否存在：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmap --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME                  DATA      AGE
...
dns-autoscaler        1         ...
...
</code></pre><!--
Modify the data in the ConfigMap:
-->
<p>修改该 ConfigMap 中的数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit configmap dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
Look for this line:
-->
<p>找到如下这行内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">linear</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;{&#34;coresPerReplica&#34;:256,&#34;min&#34;:1,&#34;nodesPerReplica&#34;:16}&#39;</span><span style="color:#bbb">
</span></code></pre></div><!--
Modify the fields according to your needs. The "min" field indicates the
minimal number of DNS backends. The actual number of backends number is
calculated using this equation:
-->
<p>根据需要修改对应的字段。“min” 字段表明 DNS 后端的最小数量。
实际后端的数量通过使用如下公式来计算：</p>
<pre tabindex="0"><code>replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )
</code></pre><!--
Note that the values of both `coresPerReplica` and `nodesPerReplica` are
integers.

The idea is that when a cluster is using nodes that have many cores,
`coresPerReplica` dominates. When a cluster is using nodes that have fewer
cores, `nodesPerReplica` dominates.

There are other supported scaling patterns. For details, see
[cluster-proportional-autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler).
-->
<p>注意 <code>coresPerReplica</code> 和 <code>nodesPerReplica</code> 的值都是整数。</p>
<p>背后的思想是，当一个集群使用具有很多核心的节点时，由 <code>coresPerReplica</code> 来控制。
当一个集群使用具有较少核心的节点时，由 <code>nodesPerReplica</code> 来控制。</p>
<p>其它的扩缩模式也是支持的，详情查看
<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler</a>。</p>
<!--
## Disable DNS horizontal autoscaling

There are a few options for tuning DNS horizontal autoscaling. Which option to
use depends on different conditions.
-->
<h2 id="禁用-dns-水平自动扩缩">禁用 DNS 水平自动扩缩</h2>
<p>有几个可供调优的 DNS 水平自动扩缩选项。具体使用哪个选项因环境而异。</p>
<!--
### Option 1: Scale down the dns-autoscaler deployment to 0 replicas

This option works for all situations. Enter this command:
-->
<h3 id="选项-1-缩容-dns-autoscaler-deployment-至-0-个副本">选项 1：缩容 dns-autoscaler Deployment 至 0 个副本</h3>
<p>该选项适用于所有场景。运行如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment --replicas<span style="color:#666">=</span><span style="color:#666">0</span> dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!-- The output is: -->
<p>输出如下所示：</p>
<pre tabindex="0"><code>deployment.apps/dns-autoscaler scaled
</code></pre><!--
Verify that the replica count is zero:
-->
<p>验证当前副本数为 0：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get rs --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!--
The output displays 0 in the DESIRED and CURRENT columns:
-->
<p>输出内容中，在 DESIRED 和 CURRENT 列显示为 0：</p>
<pre tabindex="0"><code>NAME                                 DESIRED   CURRENT   READY   AGE
...
dns-autoscaler-6b59789fc8            0         0         0       ...
...
</code></pre><!--
### Option 2: Delete the dns-autoscaler deployment

This option works if dns-autoscaler is under your own control, which means
no one will re-create it:
-->
<h3 id="选项-2-删除-dns-autoscaler-deployment">选项 2：删除 dns-autoscaler Deployment</h3>
<p>如果 dns-autoscaler 为你所控制，也就说没有人会去重新创建它，可以选择此选项：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment dns-autoscaler --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><!-- The output is:-->
<p>输出内容如下所示：</p>
<pre tabindex="0"><code>deployment.apps &quot;dns-autoscaler&quot; deleted
</code></pre><!--
### Option 3: Delete the dns-autoscaler manifest file from the master node

This option works if dns-autoscaler is under control of the (deprecated)
[Addon Manager](https://git.k8s.io/kubernetes/cluster/addons/README.md),
and you have write access to the master node.
-->
<h3 id="选项-3-从主控节点删除-dns-autoscaler-清单文件">选项 3：从主控节点删除 dns-autoscaler 清单文件</h3>
<p>如果 dns-autoscaler 在<a href="https://git.k8s.io/kubernetes/cluster/addons/README.md">插件管理器</a>
的控制之下，并且具有操作 master 节点的写权限，可以使用此选项。</p>
<!--
Sign in to the master node and delete the corresponding manifest file.
The common path for this dns-autoscaler is:
-->
<p>登录到主控节点，删除对应的清单文件。
dns-autoscaler 对应的路径一般为：</p>
<pre tabindex="0"><code>/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
</code></pre><!--
After the manifest file is deleted, the Addon Manager will delete the
dns-autoscaler Deployment.
-->
<p>当清单文件被删除后，插件管理器将删除 dns-autoscaler Deployment。</p>
<!-- discussion -->
<!--
## Understanding how DNS horizontal autoscaling works

* The cluster-proportional-autoscaler application is deployed separately from
the DNS service.

* An autoscaler Pod runs a client that polls the Kubernetes API server for the
number of nodes and cores in the cluster.
-->
<h2 id="理解-dns-水平自动扩缩工作原理">理解 DNS 水平自动扩缩工作原理</h2>
<ul>
<li>
<p>cluster-proportional-autoscaler 应用独立于 DNS 服务部署。</p>
</li>
<li>
<p>autoscaler Pod 运行一个客户端，它通过轮询 Kubernetes API 服务器获取集群中节点和核心的数量。</p>
</li>
</ul>
<!--
* A desired replica count is calculated and applied to the DNS backends based on
the current schedulable nodes and cores and the given scaling parameters.

* The scaling parameters and data points are provided via a ConfigMap to the
autoscaler, and it refreshes its parameters table every poll interval to be up
to date with the latest desired scaling parameters.
-->
<ul>
<li>
<p>系统会基于当前可调度的节点个数、核心数以及所给的扩缩参数，计算期望的副本数并应用到 DNS 后端。</p>
</li>
<li>
<p>扩缩参数和数据点会基于一个 ConfigMap 来提供给 autoscaler，它会在每次轮询时刷新它的参数表，
以与最近期望的扩缩参数保持一致。</p>
</li>
</ul>
<!--
* Changes to the scaling parameters are allowed without rebuilding or restarting
the autoscaler Pod.

* The autoscaler provides a controller interface to support two control
patterns: *linear* and *ladder*.
-->
<ul>
<li>
<p>扩缩参数是可以被修改的，而且不需要重建或重启 autoscaler Pod。</p>
</li>
<li>
<p>autoscaler 提供了一个控制器接口来支持两种控制模式：<em>linear</em> 和 <em>ladder</em>。</p>
</li>
</ul>
<h2 id="接下来">接下来</h2>
<!--
* Read about [Guaranteed Scheduling For Critical Add-On Pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).
* Learn more about the
[implementation of cluster-proportional-autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler).

-->
<ul>
<li>阅读<a href="/zh/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">为关键插件 Pod 提供的调度保障</a></li>
<li>进一步了解 <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler 实现</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3d0cd7d2f13d4759094f281504cf57b8">2.34 - 自定义 DNS 服务</h1>
    
	<!-- 
reviewers:
- bowei
- zihongz
title: Customizing DNS Service
content_type: task
min-kubernetes-server-version: v1.12
-->
<!-- overview -->
<!-- 
This page explains how to configure your DNS
<a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod(s)'>Pod(s)</a> and customize the
DNS resolution process in your cluster.
-->
<p>本页说明如何配置 DNS <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod(s)'>Pod(s)</a>，以及定制集群中 DNS 解析过程。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.12.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- 
Your cluster must be running the CoreDNS add-on.
[Migrating to CoreDNS](/docs/tasks/administer-cluster/coredns/#migrating-to-coredns)
explains how to use `kubeadm` to migrate from `kube-dns`.
-->
<p>你的集群必须运行 CoreDNS 插件。
文档<a href="/zh/docs/tasks/administer-cluster/coredns/#migrating-to-coredns">迁移到 CoreDNS</a>
解释了如何使用 <code>kubeadm</code> 从 <code>kube-dns</code> 迁移到 CoreDNS。</p>
<!-- steps -->
<!-- 
## Introduction 

DNS is a built-in Kubernetes service launched automatically
using the addon manager
[cluster add-on](https://releases.k8s.io/main/cluster/addons/README.md). 
-->
<h2 id="介绍">介绍</h2>
<p>DNS 是使用<a href="https://releases.k8s.io/main/cluster/addons/README.md">集群插件</a>
管理器自动启动的内置的 Kubernetes 服务。</p>
<!-- 
As of Kubernetes v1.12, CoreDNS is the recommended DNS Server, replacing kube-dns.  If your cluster
originally used kube-dns, you may still have `kube-dns` deployed rather than CoreDNS.
-->
<p>从 Kubernetes v1.12 开始，CoreDNS 是推荐的 DNS 服务器，取代了 kube-dns。 如果
你的集群原来使用 kube-dns，你可能部署的仍然是 <code>kube-dns</code> 而不是 CoreDNS。</p>
<!--
Both the CoreDNS and kube-dns Service are named `kube-dns` in the `metadata.name` field.  
This is so that there is greater interoperability with workloads that relied on the legacy `kube-dns` Service name to resolve addresses internal to the cluster. Using a Service named `kube-dns` abstracts away the implementation detail of which DNS provider is running behind that common name.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> CoreDNS 和 kube-dns 的 Service 都在其 <code>metadata.name</code> 字段使用名字 <code>kube-dns</code>。
这是为了能够与依靠传统 <code>kube-dns</code> 服务名称来解析集群内部地址的工作负载具有更好的互操作性。
使用 <code>kube-dns</code> 作为服务名称可以抽离共有名称之后运行的是哪个 DNS 提供程序这一实现细节。</div>
</blockquote>
<!--
If you are running CoreDNS as a Deployment, it will typically be exposed as a Kubernetes Service with a static IP address.
The kubelet passes DNS resolver information to each container with the `-cluster-dns=<dns-service-ip>` flag.
-->
<p>如果你在使用 Deployment 运行 CoreDNS，则该 Deployment 通常会向外暴露为一个具有
静态 IP 地址 Kubernetes 服务。
kubelet 使用 <code>--cluster-dns=&lt;DNS 服务 IP&gt;</code> 标志将 DNS 解析器的信息传递给每个容器。</p>
<!-- 
DNS names also need domains. You configure the local domain in the kubelet
with the flag `-cluster-domain=<default-local-domain>`. 
-->
<p>DNS 名称也需要域名。 你可在 kubelet 中使用 <code>--cluster-domain=&lt;默认本地域名&gt;</code>
标志配置本地域名。</p>
<!-- 
The DNS server supports forward lookups (A and AAAA records), port lookups (SRV records), reverse IP address lookups (PTR records),
and more. For more information, see [DNS for Services and Pods](/docs/concepts/services-networking/dns-pod-service/).
-->
<p>DNS 服务器支持正向查找（A 和 AAAA 记录）、端口发现（SRV 记录）、反向 IP 地址发现（PTR 记录）等。
更多信息，请参见<a href="/zh/docs/concepts/services-networking/dns-pod-service/">Pod 和 服务的 DNS</a>。</p>
<!-- 
If a Pod's `dnsPolicy` is set to "`default`", it inherits the name resolution
configuration from the node that the Pod runs on. The Pod's DNS resolution
should behave the same as the node.
But see [Known issues](/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues). 
-->
<p>如果 Pod 的 <code>dnsPolicy</code> 设置为 &quot;<code>default</code>&quot;，则它将从 Pod 运行所在节点继承名称解析配置。
Pod 的 DNS 解析行为应该与节点相同。
但请参阅<a href="/zh/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues">已知问题</a>。</p>
<!-- 
If you don't want this, or if you want a different DNS config for pods, you can
use the kubelet's `-resolv-conf` flag.  Set this flag to "" to prevent Pods from
inheriting DNS. Set it to a valid file path to specify a file other than
`/etc/resolv.conf` for DNS inheritance. 
-->
<p>如果你不想这样做，或者想要为 Pod 使用其他 DNS 配置，则可以
使用 kubelet 的 <code>--resolv-conf</code> 标志。 将此标志设置为 &quot;&quot; 可以避免 Pod 继承 DNS。
将其设置为有别于 <code>/etc/resolv.conf</code> 的有效文件路径可以设定 DNS 继承不同的配置。</p>
<h2 id="coredns">CoreDNS</h2>
<!-- 
CoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS, complying with the [dns specifications]
(https://github.com/kubernetes/dns/blob/master/docs/specification.md). 
-->
<p>CoreDNS 是通用的权威 DNS 服务器，可以用作集群 DNS，符合
<a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">DNS 规范</a>。</p>
<!-- 
### CoreDNS ConfigMap options 

CoreDNS is a DNS server that is modular and pluggable, and each plugin adds new functionality to CoreDNS. 
This can be configured by maintaining a [Corefile](https://coredns.io/2017/07/23/corefile-explained/), which is the CoreDNS
configuration file. A cluster administrator can modify the ConfigMap for the CoreDNS Corefile to change how service discovery works.  
-->
<h3 id="coredns-configmap-options">CoreDNS ConfigMap 选项 </h3>
<p>CoreDNS 是模块化且可插拔的 DNS 服务器，每个插件都为 CoreDNS 添加了新功能。
可以通过维护 <a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile</a>，即 CoreDNS 配置文件，
来定制其行为。 集群管理员可以修改 CoreDNS Corefile 的 ConfigMap，以更改服务发现的工作方式。</p>
<!-- 
In Kubernetes, CoreDNS is installed with the following default Corefile configuration. 
-->
<p>在 Kubernetes 中，CoreDNS 安装时使用如下默认 Corefile 配置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    .:53 {
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        health {
</span><span style="color:#b44;font-style:italic">            lameduck 5s
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        ready
</span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style="color:#b44;font-style:italic">           pods insecure
</span><span style="color:#b44;font-style:italic">           fallthrough in-addr.arpa ip6.arpa
</span><span style="color:#b44;font-style:italic">           ttl 30
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        prometheus :9153
</span><span style="color:#b44;font-style:italic">        forward . /etc/resolv.conf
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        loop
</span><span style="color:#b44;font-style:italic">        reload
</span><span style="color:#b44;font-style:italic">        loadbalance
</span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></code></pre></div><!-- 
The Corefile configuration includes the following [plugins](https://coredns.io/plugins/) of CoreDNS: 
-->
<p>Corefile 配置包括以下 CoreDNS <a href="https://coredns.io/plugins/">插件</a>：</p>
<!-- 
* [errors](https://coredns.io/plugins/errors/): Errors are logged to stdout.
* [health](https://coredns.io/plugins/health/): Health of CoreDNS is reported to http://localhost:8080/health.
* [kubernetes](https://coredns.io/plugins/kubernetes/): CoreDNS will reply to DNS queries based on IP of the services and pods of Kubernetes. You can find more details [here](https://coredns.io/plugins/kubernetes/).  
-->
<ul>
<li>
<p><a href="https://coredns.io/plugins/errors/">errors</a>：错误记录到标准输出。</p>
</li>
<li>
<p><a href="https://coredns.io/plugins/health/">health</a>：在 http://localhost:8080/health 处提供 CoreDNS 的健康报告。</p>
</li>
<li>
<p><a href="https://coredns.io/plugins/ready/">ready</a>：在端口 8181 上提供的一个 HTTP 末端，当所有能够
表达自身就绪的插件都已就绪时，在此末端返回 200 OK。</p>
</li>
<li>
<p><a href="https://coredns.io/plugins/kubernetes/">kubernetes</a>：CoreDNS 将基于 Kubernetes 的服务和 Pod 的
IP 答复 DNS 查询。你可以在 CoreDNS 网站阅读<a href="https://coredns.io/plugins/kubernetes/">更多细节</a>。
你可以使用 <code>ttl</code> 来定制响应的 TTL。默认值是 5 秒钟。TTL 的最小值可以是 0 秒钟，
最大值为 3600 秒。将 TTL 设置为 0 可以禁止对 DNS 记录进行缓存。</p>
<!-- 
The `pods insecure` option is provided for backward compatibility with kube-dns. You can use the
`pods verified` option, which returns an A record only if there exists a pod in same namespace
with matching IP. The `pods disabled` option can be used if you don't use pod records. 
-->
<p><code>pods insecure</code> 选项是为了与 kube-dns 向后兼容。你可以使用 <code>pods verified</code> 选项，该选项使得
仅在相同名称空间中存在具有匹配 IP 的 Pod 时才返回 A 记录。如果你不使用 Pod 记录，则可以使用
<code>pods disabled</code> 选项。</p>
</li>
</ul>
<!-- 
* [prometheus](https://coredns.io/plugins/prometheus/): Metrics of CoreDNS are available at http://localhost:9153/metrics in [Prometheus](https://prometheus.io/) format.
* [forward](https://coredns.io/plugins/forward/): Any queries that are not within the cluster domain of Kubernetes will be forwarded to predefined resolvers (/etc/resolv.conf).
* [cache](https://coredns.io/plugins/cache/): This enables a frontend cache.
* [loop](https://coredns.io/plugins/loop/): Detects simple forwarding loops and halts the CoreDNS process if a loop is found.
* [reload](https://coredns.io/plugins/reload): Allows automatic reload of a changed Corefile. After you edit the ConfigMap configuration, allow two minutes for your changes to take effect.
* [loadbalance](https://coredns.io/plugins/loadbalance): This is a round-robin DNS loadbalancer that randomizes the order of A, AAAA, and MX records in the answer. 
-->
<ul>
<li><a href="https://coredns.io/plugins/prometheus/">prometheus</a>：CoreDNS 的度量指标值以
<a href="https://prometheus.io/">Prometheus</a> 格式在 http://localhost:9153/metrics 上提供。</li>
<li><a href="https://coredns.io/plugins/forward/">forward</a>: 不在 Kubernetes 集群域内的任何查询都将转发到
预定义的解析器 (/etc/resolv.conf).</li>
<li><a href="https://coredns.io/plugins/cache/">cache</a>：启用前端缓存。</li>
<li><a href="https://coredns.io/plugins/loop/">loop</a>：检测到简单的转发环，如果发现死循环，则中止 CoreDNS 进程。</li>
<li><a href="https://coredns.io/plugins/reload">reload</a>：允许自动重新加载已更改的 Corefile。
编辑 ConfigMap 配置后，请等待两分钟，以使更改生效。</li>
<li><a href="https://coredns.io/plugins/loadbalance">loadbalance</a>：这是一个轮转式 DNS 负载均衡器，
它在应答中随机分配 A、AAAA 和 MX 记录的顺序。</li>
</ul>
<!-- 
You can modify the default CoreDNS behavior by modifying the ConfigMap. 
-->
<p>你可以通过修改 ConfigMap 来更改默认的 CoreDNS 行为。</p>
<!-- 
### Configuration of Stub-domain and upstream nameserver using CoreDNS 

CoreDNS has the ability to configure stubdomains and upstream nameservers using the [forward plugin](https://coredns.io/plugins/forward/).
-->
<h3 id="使用-coredns-配置存根域和上游域名服务器">使用 CoreDNS 配置存根域和上游域名服务器</h3>
<p>CoreDNS 能够使用 <a href="https://coredns.io/plugins/forward/">forward 插件</a>配置存根域和上游域名服务器。</p>
<!-- 
#### Example

If a cluster operator has a [Consul](https://www.consul.io/) domain server located at 10.150.0.1, and all Consul names have the suffix .consul.local. To configure it in CoreDNS, the cluster administrator creates the following stanza in the CoreDNS ConfigMap. 
-->
<h4 id="示例">示例</h4>
<p>如果集群操作员在 10.150.0.1 处运行了 <a href="https://www.consul.io/">Consul</a> 域服务器，
且所有 Consul 名称都带有后缀 <code>.consul.local</code>。要在 CoreDNS 中对其进行配置，
集群管理员可以在 CoreDNS 的 ConfigMap 中创建加入以下字段。</p>
<pre tabindex="0"><code>consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
</code></pre><!-- 
To explicitly force all non-cluster DNS lookups to go through a specific nameserver at 172.16.0.1, point the  `forward` to the nameserver instead of `/etc/resolv.conf` 
-->
<p>要显式强制所有非集群 DNS 查找通过特定的域名服务器（位于 172.16.0.1），可将 <code>forward</code>
指向该域名服务器，而不是 <code>/etc/resolv.conf</code>。</p>
<pre tabindex="0"><code>forward .  172.16.0.1
</code></pre><!-- 
The final ConfigMap along with the default `Corefile` configuration looks like: 
-->
<p>最终的包含默认的 <code>Corefile</code> 配置的 ConfigMap 如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    .:53 {
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        health
</span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style="color:#b44;font-style:italic">           pods insecure
</span><span style="color:#b44;font-style:italic">           fallthrough in-addr.arpa ip6.arpa
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        prometheus :9153
</span><span style="color:#b44;font-style:italic">        forward . 172.16.0.1
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        loop
</span><span style="color:#b44;font-style:italic">        reload
</span><span style="color:#b44;font-style:italic">        loadbalance
</span><span style="color:#b44;font-style:italic">    }
</span><span style="color:#b44;font-style:italic">    consul.local:53 {
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        forward . 10.150.0.1
</span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></code></pre></div><!-- 
The `kubeadm` supports automatic translation of the CoreDNS ConfigMap from the kube-dns ConfigMap.
-->
<p>工具 <code>kubeadm</code> 支持将 kube-dns ConfigMap 自动转换为 CoreDNS ConfigMap。</p>
<!--
While kube-dns accepts an FQDN for stubdomain and nameserver (eg: ns.foo.com), CoreDNS does not support this feature. 
During translation, all FQDN nameservers will be omitted from the CoreDNS config.*** 
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 尽管 kube-dns 接受 FQDN（例如：ns.foo.com）作为存根域和名字服务器，CoreDNS 不支持此功能。
转换期间，CoreDNS 配置中将忽略所有的 FQDN 域名服务器。</div>
</blockquote>
<!-- 
## CoreDNS configuration equivalent to kube-dns

CoreDNS supports the features of kube-dns and more.
A ConfigMap created for kube-dns to support `StubDomains`and `upstreamNameservers` translates to the `proxy` plugin in CoreDNS.
Similarly, the `Federations` plugin in kube-dns translates to the `federation` plugin in CoreDNS.

### Example

This example ConfigMap for kubedns specifies federations, stubdomains and upstreamnameservers: 
-->
<h2 id="coredns-配置等同于-kube-dns">CoreDNS 配置等同于 kube-dns</h2>
<p>CoreDNS 不仅仅提供 kube-dns 的功能。
为 kube-dns 创建的 ConfigMap 支持 <code>StubDomains</code> 和 <code>upstreamNameservers</code> 转换为 CoreDNS 中的 <code>forward</code> 插件。
同样，kube-dns 中的 <code>Federations</code> 插件会转换为 CoreDNS 中的 <code>federation</code> 插件。</p>
<h3 id="示例-1">示例</h3>
<p>用于 kubedns 的此示例 ConfigMap 描述了 federations、stubdomains and upstreamnameservers：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">federations</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>{<span style="color:#b44">&#34;foo&#34;</span><span style="color:#bbb"> </span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;foo.feddomain.com&#34;</span>}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">stubDomains</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>{<span style="color:#b44">&#34;abc.com&#34;</span><span style="color:#bbb"> </span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;1.2.3.4&#34;</span>],<span style="color:#bbb"> </span><span style="color:#b44">&#34;my.cluster.local&#34;</span><span style="color:#bbb"> </span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;2.3.4.5&#34;</span>]}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">upstreamNameservers</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>[<span style="color:#b44">&#34;8.8.8.8&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;8.8.4.4&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span></code></pre></div><!-- 
The equivalent configuration in CoreDNS creates a Corefile: 
-->
<p>CoreDNS 中的等效配置将创建一个 Corefile：</p>
<ul>
<li>
<p>针对 federations:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">federation cluster.local {<span style="color:#bbb">
</span><span style="color:#bbb">    </span>foo foo.feddomain.com<span style="color:#bbb">
</span><span style="color:#bbb"></span>}<span style="color:#bbb">
</span></code></pre></div></li>
<li>
<p>针对 stubDomains:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">abc.com:53 {<span style="color:#bbb">
</span><span style="color:#bbb">     </span>errors<span style="color:#bbb">
</span><span style="color:#bbb">     </span>cache 30<span style="color:#bbb">
</span><span style="color:#bbb">     </span>proxy . 1.2.3.4<span style="color:#bbb">
</span><span style="color:#bbb"> </span>}<span style="color:#bbb">
</span><span style="color:#bbb"> </span>my.cluster.local:53 {<span style="color:#bbb">
</span><span style="color:#bbb">     </span>errors<span style="color:#bbb">
</span><span style="color:#bbb">     </span>cache 30<span style="color:#bbb">
</span><span style="color:#bbb">     </span>proxy . 2.3.4.5<span style="color:#bbb">
</span><span style="color:#bbb"> </span>}<span style="color:#bbb">
</span></code></pre></div></li>
</ul>
<!-- 
The complete Corefile with the default plugins: 
-->
<p>带有默认插件的完整 Corefile：</p>
<pre tabindex="0"><code>.:53 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
    }
    federation cluster.local {
       foo foo.feddomain.com
    }
    prometheus :9153
    forward .  8.8.8.8 8.8.4.4
    cache 30
}
abc.com:53 {
    errors
    cache 30
    forward . 1.2.3.4
}
my.cluster.local:53 {
    errors
    cache 30
    forward . 2.3.4.5
}
</code></pre><!-- 
## Migration to CoreDNS

To migrate from kube-dns to CoreDNS, [a detailed blog](https://coredns.io/2018/05/21/migration-from-kube-dns-to-coredns/) is available to help users adapt CoreDNS in place of kube-dns.
A cluster administrator can also migrate using [the deploy script](https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh).

-->
<h2 id="迁移到-coredns">迁移到 CoreDNS</h2>
<p>要从 kube-dns 迁移到 CoreDNS，<a href="https://coredns.io/2018/05/21/migration-from-kube-dns-to-coredns/">此博客</a>
提供了帮助用户将 kube-dns 替换为 CoreDNS。
集群管理员还可以使用<a href="https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh">部署脚本</a>
进行迁移。</p>
<h2 id="接下来">接下来</h2>
<!--
- Read [Debugging DNS Resolution](/docs/tasks/administer-cluster/dns-debugging-resolution/).
-->
<ul>
<li>阅读<a href="/zh/docs/tasks/administer-cluster/dns-debugging-resolution/">调试 DNS 解析</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bc6e50c405a620aab43b40d41d6375df">2.35 - 访问集群上运行的服务</h1>
    
	<!-- overview -->
<!--
This page shows how to connect to services running on the Kubernetes cluster.
-->
<p>本文展示了如何连接 Kubernetes 集群上运行的服务。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Accessing services running on the cluster

In Kubernetes, [nodes](/docs/concepts/architecture/nodes/),
[pods](/docs/concepts/workloads/pods/) and [services](/docs/concepts/services-networking/service/) all have
their own IPs.  In many cases, the node IPs, pod IPs, and some service IPs on a cluster will not be
routable, so they will not be reachable from a machine outside the cluster,
such as your desktop machine.
-->
<h2 id="访问集群上运行的服务">访问集群上运行的服务</h2>
<p>在 Kubernetes 里，<a href="/zh/docs/concepts/architecture/nodes/">节点</a>、
<a href="/zh/docs/concepts/workloads/pods/">Pod</a> 和
<a href="/zh/docs/concepts/services-networking/service/">服务</a> 都有自己的 IP。
许多情况下，集群上的节点 IP、Pod IP 和某些服务 IP 是路由不可达的，
所以不能从集群之外访问它们，例如从你自己的台式机。</p>
<!--
### Ways to connect

You have several options for connecting to nodes, pods and services from outside the cluster:
-->
<h3 id="ways-to-connect">连接方式  </h3>
<p>你有多种可选方式从集群外连接节点、Pod 和服务：</p>
<!--
  - Access services through public IPs.
    - Use a service with type `NodePort` or `LoadBalancer` to make the service reachable outside
      the cluster.  See the [services](/docs/concepts/services-networking/service/) and
      [kubectl expose](/docs/reference/generated/kubectl/kubectl-commands/#expose) documentation.
    - Depending on your cluster environment, this may just expose the service to your corporate network,
      or it may expose it to the internet.  Think about whether the service being exposed is secure.
      Does it do its own authentication?
    - Place pods behind services.  To access one specific pod from a set of replicas, such as for debugging,
      place a unique label on the pod and create a new service which selects this label.
    - In most cases, it should not be necessary for application developer to directly access
      nodes via their nodeIPs.
-->
<ul>
<li>通过公网 IP 访问服务
<ul>
<li>使用类型为 <code>NodePort</code> 或 <code>LoadBalancer</code> 的服务，可以从外部访问它们。
请查阅<a href="/zh/docs/concepts/services-networking/service/">服务</a> 和
<a href="/docs/reference/generated/kubectl/kubectl-commands/#expose">kubectl expose</a> 文档。</li>
<li>取决于你的集群环境，你可以仅把服务暴露在你的企业网络环境中，也可以将其暴露在
因特网上。需要考虑暴露的服务是否安全，它是否有自己的用户认证？</li>
<li>将 Pod 放置于服务背后。如果要访问一个副本集合中特定的 Pod，例如用于调试目的，
请给 Pod 指定一个独特的标签并创建一个新服务选择该标签。</li>
<li>大部分情况下，都不需要应用开发者通过节点 IP 直接访问节点。</li>
</ul>
</li>
</ul>
<!--
  - Access services, nodes, or pods using the Proxy Verb.
    - Does apiserver authentication and authorization prior to accessing the remote service.
      Use this if the services are not secure enough to expose to the internet, or to gain
      access to ports on the node IP, or for debugging.
    - Proxies may cause problems for some web applications.
    - Only works for HTTP/HTTPS.
    - Described [here](#manually-constructing-apiserver-proxy-urls).
-->
<ul>
<li>通过 Proxy 动词访问服务、节点或者 Pod
<ul>
<li>在访问远程服务之前，利用 API 服务器执行身份认证和鉴权。
如果你的服务不够安全，无法暴露到因特网中，或者需要访问节点 IP 上的端口，
又或者出于调试目的，可使用这种方式。</li>
<li>代理可能给某些应用带来麻烦</li>
<li>此方式仅适用于 HTTP/HTTPS</li>
<li>进一步的描述在<a href="#manually-constructing-apiserver-proxy-urls">这里</a></li>
<li>从集群中的 node 或者 pod 访问。</li>
</ul>
</li>
</ul>
<!--
  - Access from a node or pod in the cluster.
    - Run a pod, and then connect to a shell in it using [kubectl exec](/docs/reference/generated/kubectl/kubectl-commands/#exec).
      Connect to other nodes, pods, and services from that shell.
    - Some clusters may allow you to ssh to a node in the cluster. From there you may be able to
      access cluster services. This is a non-standard method, and will work on some clusters but
      not others. Browsers and other tools may or may not be installed. Cluster DNS may not work.
-->
<ul>
<li>从集群中的一个节点或 Pod 访问
<ul>
<li>运行一个 Pod，然后使用
<a href="/docs/reference/generated/kubectl/kubectl-commands/#exec">kubectl exec</a>
连接到它的 Shell。从那个 Shell 连接其他的节点、Pod 和 服务</li>
<li>某些集群可能允许你 SSH 到集群中的节点。你可能可以从那儿访问集群服务。
这是一个非标准的方式，可能在一些集群上能工作，但在另一些上却不能。
浏览器和其他工具可能已经安装也可能没有安装。集群 DNS 可能不会正常工作。</li>
</ul>
</li>
</ul>
<!--
### Discovering builtin services

Typically, there are several services which are started on a cluster by kube-system. Get a list of these
with the `kubectl cluster-info` command:
-->
<h3 id="discovering-builtin-services">发现内置服务  </h3>
<p>典型情况下，kube-system 名字空间中会启动集群的几个服务。
使用 <code>kubectl cluster-info</code> 命令获取这些服务的列表：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>Kubernetes master is running at https://104.197.5.247
elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy
kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy
grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
</code></pre><!--
This shows the proxy-verb URL for accessing each service.
For example, this cluster has cluster-level logging enabled (using Elasticsearch), which can be reached
at `https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/` if suitable credentials are passed, or through a kubectl proxy at, for example:
`http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/`.
-->
<p>这一输出显示了用 proxy 动词访问每个服务时可用的 URL。例如，此集群
（使用 Elasticsearch）启用了集群层面的日志。如果提供合适的凭据，可以通过
<code>https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>
访问，或通过一个 <code>kubectl proxy</code> 来访问：
<code>http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>。</p>
<!--
See [Access Clusters Using the Kubernetes API](/docs/tasks/administer-cluster/access-cluster-api/#accessing-the-cluster-api) for how to pass credentials or use kubectl proxy.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 请参阅<a href="/zh/docs/tasks/administer-cluster/access-cluster-api/#accessing-the-cluster-api">使用 Kubernets API 访问集群</a>
了解如何传递凭据或如何使用 <code>kubectl proxy</code>。</div>
</blockquote>
<!--
#### Manually constructing apiserver proxy URLs

As mentioned above, you use the `kubectl cluster-info` command to retrieve the service's proxy URL. To create proxy URLs that include service endpoints, suffixes, and parameters, you simply append to the service's proxy URL:
`http://`*`kubernetes_master_address`*`/api/v1/namespaces/`*`namespace_name`*`/services/`*`[https:]service_name[:port_name]`*`/proxy`

If you haven't specified a name for your port, you don't have to specify *port_name* in the URL.
-->
<h4 id="manually-constructing-apiserver-proxy-urls">手动构建 API 服务器代理 URLs  </h4>
<p>如前所述，你可以使用 <code>kubectl cluster-info</code> 命令取得服务的代理 URL。
为了创建包含服务末端、后缀和参数的代理 URLs，你可以简单地在服务的代理 URL 中添加：
<code>http://</code><em><code>kubernetes_master_address</code></em><code>/api/v1/namespaces/</code><em><code>namespace_name</code></em><code>/services/</code><em><code>service_name[:port_name]</code></em><code>/proxy</code></p>
<p>如果还没有为你的端口指定名称，你可以不用在 URL 中指定 <em>port_name</em>。</p>
<!--
##### Examples

* To access the Elasticsearch service endpoint `_search?q=user:kimchy`, you would use:
-->
<h5 id="示例">示例</h5>
<ul>
<li>
<p>如要访问 Elasticsearch 服务末端 <code>_search?q=user:kimchy</code>，你可以使用：</p>
<pre tabindex="0"><code>http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy
</code></pre></li>
</ul>
<!--
* To access the Elasticsearch cluster health information `_cluster/health?pretty=true`, you would use:
-->
<ul>
<li>
<p>如要访问 Elasticsearch 集群健康信息<code>_cluster/health?pretty=true</code>，你会使用：</p>
<pre tabindex="0"><code>https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true` 
</code></pre><!--
The health information is similar to this:
-->
<p>健康信息与下面的例子类似：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;cluster_name&#34;</span> : <span style="color:#b44">&#34;kubernetes_logging&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;status&#34;</span> : <span style="color:#b44">&#34;yellow&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;timed_out&#34;</span> : <span style="color:#a2f;font-weight:bold">false</span>,
  <span style="color:#008000;font-weight:bold">&#34;number_of_nodes&#34;</span> : <span style="color:#666">1</span>,
  <span style="color:#008000;font-weight:bold">&#34;number_of_data_nodes&#34;</span> : <span style="color:#666">1</span>,
  <span style="color:#008000;font-weight:bold">&#34;active_primary_shards&#34;</span> : <span style="color:#666">5</span>,
  <span style="color:#008000;font-weight:bold">&#34;active_shards&#34;</span> : <span style="color:#666">5</span>,
  <span style="color:#008000;font-weight:bold">&#34;relocating_shards&#34;</span> : <span style="color:#666">0</span>,
  <span style="color:#008000;font-weight:bold">&#34;initializing_shards&#34;</span> : <span style="color:#666">0</span>,
  <span style="color:#008000;font-weight:bold">&#34;unassigned_shards&#34;</span> : <span style="color:#666">5</span>
}
</code></pre></div></li>
</ul>
<!--
* To access the *https* Elasticsearch service health information `_cluster/health?pretty=true`, you would use:
-->
<ul>
<li>
<p>要访问 <em>https</em> Elasticsearch 服务健康信息 <code>_cluster/health?pretty=true</code>，你会使用：</p>
<pre tabindex="0"><code>https://104.197.5.247/api/v1/namespaces/kube-system/services/https:elasticsearch-logging/proxy/_cluster/health?pretty=true
</code></pre></li>
</ul>
<!--
#### Using web browsers to access services running on the cluster

You may be able to put an apiserver proxy URL into the address bar of a browser. However:
-->
<h4 id="通过-web-浏览器访问集群中运行的服务">通过 Web 浏览器访问集群中运行的服务</h4>
<p>你或许能够将 API 服务器代理的 URL 放入浏览器的地址栏，然而：</p>
<!--
- Web browsers cannot usually pass tokens, so you may need to use basic (password) auth. Apiserver can be configured to accept basic auth,
    but your cluster may not be configured to accept basic auth.
- Some web apps may not work, particularly those with client side javascript that construct URLs in a
    way that is unaware of the proxy path prefix.
-->
<ul>
<li>Web 服务器通常不能传递令牌，所以你可能需要使用基本（密码）认证。
API 服务器可以配置为接受基本认证，但你的集群可能并没有这样配置。</li>
<li>某些 Web 应用可能无法工作，特别是那些使用客户端 Javascript 构造 URL 的
应用，所构造的 URL 可能并不支持代理路径前缀。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8bcf4aeb5bbb6d6969a146e5ab97557b">2.36 - 调试 DNS 问题</h1>
    
	<!-- overview -->
<!--
This page provides hints on diagnosing DNS problems.
-->
<p>这篇文章提供了一些关于 DNS 问题诊断的方法。</p>
<!-- steps -->
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- 
Your cluster must be configured to use the CoreDNS
<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='addon'>addon</a> or its precursor,
kube-dns.  
-->
<p>你的集群必须使用了 CoreDNS <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>
或者其前身，<code>kube-dns</code>。</p>


您的 Kubernetes 服务器版本必须不低于版本 v1.6.
 要获知版本信息，请输入 <code>kubectl version</code>.

<!--
### Create a simple Pod to use as a test environment



 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/dns/dnsutils.yaml" download="admin/dns/dnsutils.yaml"><code>admin/dns/dnsutils.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-dns-dnsutils-yaml')" title="Copy admin/dns/dnsutils.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-dns-dnsutils-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/kubernetes-e2e-test-images/dnsutils:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;3600&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>



<blockquote class="note callout">
  <div><strong>说明：</strong> This example creates a pod in the <code>default</code> namespace. DNS name resolution for
services depends on the namespace of the pod. For more information, review
<a href="/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">DNS for Services and Pods</a>.</div>
</blockquote>

Use that manifest to create a Pod:

```shell
kubectl create -f https://k8s.io/examples/admin/dns/busybox.yaml
pod/busybox created

kubectl get pods busybox
NAME      READY     STATUS    RESTARTS   AGE
busybox   1/1       Running   0          <some-time>
```
-->
<h3 id="创建一个简单的-pod-作为测试环境">创建一个简单的 Pod 作为测试环境</h3>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/dns/dnsutils.yaml" download="admin/dns/dnsutils.yaml"><code>admin/dns/dnsutils.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-dns-dnsutils-yaml')" title="Copy admin/dns/dnsutils.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-dns-dnsutils-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dnsutils<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/kubernetes-e2e-test-images/dnsutils:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;3600&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<blockquote class="note callout">
  <div><strong>说明：</strong> 此示例在 <code>default</code> 命名空间创建 pod。 服务的 DNS 名字解析取决于 pod 的命名空间。 详细信息请查阅
<a href="/zh/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">服务和 Pod 的 DNS</a>。</div>
</blockquote>
<p>使用上面的清单来创建一个 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
</code></pre></div><pre tabindex="0"><code>pod/dnsutils created
</code></pre><!--
…and verify its status:
-->
<p>验证其状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods dnsutils
</code></pre></div><pre tabindex="0"><code>NAME      READY     STATUS    RESTARTS   AGE
dnsutils   1/1       Running   0          &lt;some-time&gt;
</code></pre><!--
Once that pod is running, you can exec `nslookup` in that environment.
If you see something like the following, DNS is working correctly.
-->
<p>一旦 Pod 处于运行状态，你就可以在该环境里执行 <code>nslookup</code>。
如果你看到类似下列的内容，则表示 DNS 是正常运行的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup kubernetes.default
</code></pre></div><pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
</code></pre><!--
If the `nslookup` command fails, check the following:
-->
<p>如果 <code>nslookup</code> 命令执行失败，请检查下列内容：</p>
<!--
### Check the local DNS configuration first

Take a look inside the resolv.conf file.
(See [Inheriting DNS from the node](/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node) and
[Known issues](#known-issues) below for more information)
-->
<h3 id="先检查本地的-dns-配置">先检查本地的 DNS 配置</h3>
<p>查看 resolv.conf 文件的内容
（阅读<a href="/zh/docs/tasks/administer-cluster/dns-custom-nameservers/">从节点继承 DNS 配置</a> 和
后文的<a href="#known-issues">已知问题</a> ，获取更多信息)</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -ti dnsutils -- cat /etc/resolv.conf
</code></pre></div><!--
Verify that the search path and name server are set up like the following
(note that search path may vary for different cloud providers):
-->
<p>验证 search 和 nameserver 的配置是否与下面的内容类似
（注意 search 根据不同的云提供商可能会有所不同)：</p>
<pre tabindex="0"><code>search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5
</code></pre><!--
Errors such as the following indicate a problem with the CoreDNS (or kube-dns)
add-on or with associated Services:
-->
<p>下列错误表示 CoreDNS （或 kube-dns）插件或者相关服务出现了问题：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup kubernetes.default
</code></pre></div><p>输出为：</p>
<pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'
</code></pre><p>或者</p>
<pre tabindex="0"><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'
</code></pre><!--
### Check if the DNS pod is running

Use the `kubectl get pods` command to verify that the DNS pod is running.
-->
<h3 id="check-if-the-dns-pod-is-running">检查 DNS Pod 是否运行  </h3>
<p>使用 <code>kubectl get pods</code> 命令来验证 DNS Pod 是否运行。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>kube-dns
</code></pre></div><pre tabindex="0"><code>NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
</code></pre><!--
The value for label `k8s-app` is `kube-dns` for both CoreDNS and kube-dns deployments.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 对于 CoreDNS 和 kube-dns 部署而言，标签 <code>k8s-app</code> 的值都应该是 <code>kube-dns</code>。</div>
</blockquote>
<!--
If you see that no CoreDNS pod is running or that the pod has failed/completed,
the DNS add-on may not be deployed by default in your current environment and you
will have to deploy it manually.
-->
<p>如果你发现没有 CoreDNS Pod 在运行，或者该 Pod 的状态是 failed 或者 completed，
那可能这个 DNS 插件在您当前的环境里并没有成功部署，你将需要手动去部署它。</p>
<!--
### Check for Errors in the DNS pod

Use `kubectl logs` command to see logs for the DNS containers.
-->
<h3 id="check-for-errors-in-the-dns-pod">检查 DNS Pod 里的错误   </h3>
<p>使用 <code>kubectl logs</code> 命令来查看 DNS 容器的日志信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs --namespace<span style="color:#666">=</span>kube-system -l k8s-app<span style="color:#666">=</span>kube-dns
</code></pre></div><!--
Here is an example of a healthy CoreDNS log:
-->
<p>下列是一个正常运行的 CoreDNS 日志信息：</p>
<pre tabindex="0"><code>.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c
</code></pre><!--
See if there are any suspicious or unexpected messages in the logs.
-->
<p>查看是否日志中有一些可疑的或者意外的消息。</p>
<!--
### Is DNS service up?

Verify that the DNS service is up by using the `kubectl get service` command.
-->
<h3 id="is-dns-service-up">检查是否启用了 DNS 服务  </h3>
<p>使用 <code>kubectl get service</code> 命令来检查 DNS 服务是否已经启用。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><pre tabindex="0"><code>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      &lt;none&gt;        53/UDP,53/TCP        1h
...
</code></pre><!--
The service name is `kube-dns` for both CoreDNS and kube-dns deployments.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 不管是 CoreDNS 还是 kube-dns，这个服务的名字都会是 <code>kube-dns</code> 。</div>
</blockquote>
<!--
If you have created the Service or in the case it should be created by default
but it does not appear, see
[debugging Services](/docs/tasks/debug-application-cluster/debug-service/) for
more information.
-->
<p>如果你已经创建了 DNS 服务，或者该服务应该是默认自动创建的但是它并没有出现，
请阅读<a href="/zh/docs/tasks/debug-application-cluster/debug-service/">调试服务</a>
来获取更多信息。</p>
<!--
### Are DNS endpoints exposed?

You can verify that DNS endpoints are exposed by using the `kubectl get endpoints`
command.
-->
<h3 id="are-dns-endpoints-exposed">DNS 的端点公开了吗？   </h3>
<p>你可以使用 <code>kubectl get endpoints</code> 命令来验证 DNS 的端点是否公开了。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ep kube-dns --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><pre tabindex="0"><code>NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
</code></pre><!--
If you do not see the endpoints, see endpoints section in the
[debugging services](/docs/tasks/debug-application-cluster/debug-service/) documentation.

For additional Kubernetes DNS examples, see the
[cluster-dns examples](https://github.com/kubernetes/examples/tree/master/staging/cluster-dns)
in the Kubernetes GitHub repository.
-->
<p>如果你没看到对应的端点，请阅读
<a href="/zh/docs/tasks/debug-application-cluster/debug-service/">调试服务</a>的端点部分。</p>
<p>若需要了解更多的 Kubernetes DNS 例子，请在 Kubernetes GitHub 仓库里查看
<a href="https://github.com/kubernetes/examples/tree/master/staging/cluster-dns">cluster-dns 示例</a>。</p>
<!--
### Are DNS queries being received/processed?

You can verify if queries are being received by CoreDNS by adding the `log` plugin to the CoreDNS configuration (aka Corefile).
The CoreDNS Corefile is held in a ConfigMap named `coredns`. To edit it, use the command ...
-->
<h3 id="are-dns-queries-bing-received-processed">DNS 查询有被接收或者执行吗？  </h3>
<p>你可以通过给 CoreDNS 的配置文件（也叫 Corefile）添加 <code>log</code> 插件来检查查询是否被正确接收。
CoreDNS 的 Corefile 被保存在一个叫 <code>coredns</code> 的 ConfigMap 里，使用下列命令来编辑它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl -n kube-system edit configmap coredns
</code></pre></div><!--
Then add `log` in the Corefile section per the example below.
-->
<p>然后按下面的例子给 Corefile 添加 <code>log</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>coredns<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">Corefile</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    .:53 {
</span><span style="color:#b44;font-style:italic">        log
</span><span style="color:#b44;font-style:italic">        errors
</span><span style="color:#b44;font-style:italic">        health
</span><span style="color:#b44;font-style:italic">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style="color:#b44;font-style:italic">          pods insecure
</span><span style="color:#b44;font-style:italic">          upstream
</span><span style="color:#b44;font-style:italic">          fallthrough in-addr.arpa ip6.arpa
</span><span style="color:#b44;font-style:italic">        }
</span><span style="color:#b44;font-style:italic">        prometheus :9153
</span><span style="color:#b44;font-style:italic">        forward . /etc/resolv.conf
</span><span style="color:#b44;font-style:italic">        cache 30
</span><span style="color:#b44;font-style:italic">        loop
</span><span style="color:#b44;font-style:italic">        reload
</span><span style="color:#b44;font-style:italic">        loadbalance
</span><span style="color:#b44;font-style:italic">    }</span><span style="color:#bbb">    
</span></code></pre></div><!--
After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.
-->
<p>保存这些更改后，你可能会需要等待一到两分钟让 Kubernetes 把这些更改应用到
CoreDNS 的 Pod 里。</p>
<!--
Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.

Here is an example of a query in the log.
-->
<p>接下来，发起一些查询并依照前文所述查看日志信息，如果 CoreDNS 的 Pod 接收到这些查询，
你将可以在日志信息里看到它们。</p>
<p>下面是日志信息里的查询例子：</p>
<pre tabindex="0"><code>.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd,ra 106 0.000066649s

</code></pre><!--
### Are you in the right namespace for the service?

DNS queries that don't specify a namespace are limited to the pod's 
namespace. 

If the namespace of the pod and service differ, the DNS query must include 
the namespace of the service.

This query is limited to the pod's namespace:
```shell
kubectl exec -i -t dnsutils -- nslookup <service-name>
```
-->
<h3 id="你的服务在正确的命名空间中吗">你的服务在正确的命名空间中吗？</h3>
<p>未指定命名空间的 DNS 查询仅作用于 pod 所在的命名空间。</p>
<p>如果 pod 和服务的命名空间不相同，则 DNS 查询必须指定服务所在的命名空间。</p>
<p>该查询仅限于 pod 所在的名称空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;
</code></pre></div><!--
This query specifies the namespace:
```shell
kubectl exec -i -t dnsutils -- nslookup <service-name>.<namespace>
```
-->
<p>指定命名空间的查询：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t dnsutils -- nslookup &lt;service-name&gt;.&lt;namespace&gt;
</code></pre></div><!--
To learn more about name resolution, see 
[DNS for Services and Pods](/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names). 
-->
<p>要进一步了解名字解析，请查看
<a href="/zh/docs/concepts/services-networking/dns-pod-service/#what-things-get-dns-names">服务和 Pod 的 DNS</a>。</p>
<!--
## Known issues

Some Linux distributions (e.g. Ubuntu), use a local DNS resolver by default (systemd-resolved).
Systemd-resolved moves and replaces `/etc/resolv.conf` with a stub file that can cause a fatal forwarding
loop when resolving names in upstream servers. This can be fixed manually by using kubelet's `-resolv-conf` flag
to point to the correct `resolv.conf` (With `systemd-resolved`, this is `/run/systemd/resolve/resolv.conf`).
kubeadm automatically detects `systemd-resolved`, and adjusts the kubelet flags accordingly.
-->
<h2 id="known-issues">已知问题</h2>
<p>有些 Linux 发行版本（比如 Ubuntu）默认使用一个本地的 DNS 解析器（systemd-resolved）。
<code>systemd-resolved</code> 会用一个存根文件（Stub File）来覆盖 <code>/etc/resolv.conf</code> 内容，
从而可能在上游服务器中解析域名产生转发环（forwarding loop）。 这个问题可以通过手动指定
kubelet 的 <code>--resolv-conf</code> 标志为正确的 <code>resolv.conf</code>（如果是 <code>systemd-resolved</code>，
则这个文件路径为 <code>/run/systemd/resolve/resolv.conf</code>）来解决。
kubeadm 会自动检测 <code>systemd-resolved</code> 并对应的更改 kubelet 的命令行标志。</p>
<!--
Kubernetes installs do not configure the nodes' `resolv.conf` files to use the
cluster DNS by default, because that process is inherently distribution-specific.
This should probably be implemented eventually.
-->
<p>Kubernetes 的安装并不会默认配置节点的 <code>resolv.conf</code> 文件来使用集群的 DNS 服务，因为这个配置对于不同的发行版本是不一样的。这个问题应该迟早会被解决的。</p>
<!--
Linux's libc (a.k.a. glibc) has a limit for the DNS `nameserver` records to 3
by default. What's more, for the glibc versions which are older than
glibc-2.17-222 ([the new versions update see this
issue](https://access.redhat.com/solutions/58028)), the allowed number of DNS
`search` records has been limited to 6 ([see this bug from
2005](https://bugzilla.redhat.com/show_bug.cgi?id=168253)). Kubernetes needs
to consume 1 `nameserver` record and 3 `search` records. This means that if a
local installation already uses 3 `nameserver`s or uses more than 3 `search`es
while your glibc version is in the affected list, some of those settings will
be lost. To work around the DNS `nameserver` records limit, the node can run
`dnsmasq`, which will provide more `nameserver` entries. You can also use
kubelet's `--resolv-conf` flag. To fix the DNS `search` records limit,
consider upgrading your linux distribution or upgrading to an unaffected
version of glibc.
-->
<p>Linux 的 libc 限制 <code>nameserver</code> 只能有三个记录。不仅如此，对于 glibc-2.17-222
之前的版本（<a href="https://access.redhat.com/solutions/58028">参见此 Issue 了解新版本的更新</a>），<code>search</code> 的记录不能超过 6 个
（ <a href="https://bugzilla.redhat.com/show_bug.cgi?id=168253">详情请查阅这个 2005 年的 bug</a>）。
Kubernetes 需要占用一个 <code>nameserver</code> 记录和三个<code>search</code>记录。
这意味着如果一个本地的安装已经使用了三个 <code>nameserver</code> 或者使用了超过三个
<code>search</code> 记录，而你的 glibc 版本也在有问题的版本列表中，那么有些配置很可能会丢失。
为了绕过 DNS <code>nameserver</code> 个数限制，节点可以运行 <code>dnsmasq</code>，以提供更多的
<code>nameserver</code> 记录。你也可以使用kubelet 的 <code>--resolv-conf</code> 标志来解决这个问题。
要想修复 DNS <code>search</code> 记录个数限制问题，可以考虑升级你的 Linux 发行版本，或者
升级 glibc 到一个不再受此困扰的版本。</p>
<!--
If you are using Alpine version 3.3 or earlier as your base image, DNS may not
work properly owing to a known issue with Alpine.
Check [here](https://github.com/kubernetes/kubernetes/issues/30215)
for more information.
-->
<p>如果你使用 Alpine  3.3 或更早版本作为你的基础镜像，DNS 可能会由于 Alpine 中
一个已知的问题导致无法正常工作。
请查看<a href="https://github.com/kubernetes/kubernetes/issues/30215">这里</a>获取更多信息。</p>
<h2 id="接下来">接下来</h2>
<!--
- [Autoscaling the DNS Service in a Cluster](/docs/tasks/administer-cluster/dns-horizontal-autoscaling/).
- [DNS for Services and Pods](/docs/concepts/services-networking/dns-pod-service/)
-->
<ul>
<li>参阅<a href="/zh/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">自动扩缩集群中的 DNS 服务</a>.</li>
<li>阅读<a href="/zh/docs/concepts/services-networking/dns-pod-service/">服务和 Pod 的 DNS</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1e966f5d0540bbee0876f9d0d08d54dc">2.37 - 通过名字空间共享集群</h1>
    
	<!--
reviewers:
- derekwaynecarr
- janetkuo
title: Share a Cluster with Namespaces
content_type: task
-->
<!-- overview -->
<!--
This page shows how to view, work in, and delete <a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='namespaces'>namespaces</a>. The page also shows how to use Kubernetes namespaces to subdivide your cluster.
-->
<p>本页展示如何查看、使用和删除<a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>。
本页同时展示如何使用 Kubernetes 名字空间来划分集群。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* Have an [existing Kubernetes cluster](/docs/setup/).
* You have a basic understanding of Kubernetes <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a>, <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a>, and <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a>.
-->
<ul>
<li>你已拥有一个<a href="/zh/docs/setup/">配置好的 Kubernetes 集群</a>。</li>
<li>你已对 Kubernetes 的 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pods'>Pods</a> ,
<a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='Services'>Services</a> , 和
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployments'>Deployments</a> 有基本理解。</li>
</ul>
<!-- steps -->
<!-- ## Viewing namespaces -->
<h2 id="查看名字空间">查看名字空间</h2>
<!-- 1. List the current namespaces in a cluster using: -->
<ol>
<li>列出集群中现有的名字空间：</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces
</code></pre></div><pre tabindex="0"><code>NAME          STATUS    AGE
default       Active    11d
kube-system   Active    11d
kube-public   Active    11d
</code></pre><!-- Kubernetes starts with three initial namespaces: -->
<p>初始状态下，Kubernetes 具有三个名字空间：</p>
<!--
* `default` The default namespace for objects with no other namespace
* `kube-system` The namespace for objects created by the Kubernetes system
* `kube-public` This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. -->
<ul>
<li><code>default</code> 无名字空间对象的默认名字空间</li>
<li><code>kube-system</code> 由 Kubernetes 系统创建的对象的名字空间</li>
<li><code>kube-public</code> 自动创建且被所有用户可读的名字空间（包括未经身份认证的）。此名字空间通常在某些资源在整个集群中可见且可公开读取时被集群使用。此名字空间的公共方面只是一个约定，而不是一个必要条件。</li>
</ul>
<!-- You can also get the summary of a specific namespace using: -->
<p>你还可以通过下列命令获取特定名字空间的摘要：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces &lt;name&gt;
</code></pre></div><!-- Or you can get detailed information with: -->
<p>或用下面的命令获取详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe namespaces &lt;name&gt;
</code></pre></div><pre tabindex="0"><code>Name:           default
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;
Status:         Active

No resource quota.

Resource Limits
 Type       Resource    Min Max Default
 ----               --------    --- --- ---
 Container          cpu         -   -   100m
</code></pre><!-- Note that these details show both resource quota (if present) as well as resource limit ranges. -->
<p>请注意，这些详情同时显示了资源配额（如果存在）以及资源限制区间。</p>
<!-- Resource quota tracks aggregate usage of resources in the *Namespace* and allows cluster operators
to define *Hard* resource usage limits that a *Namespace* may consume. -->
<p>资源配额跟踪并聚合 <em>Namespace</em> 中资源的使用情况，并允许集群运营者定义 <em>Namespace</em> 可能消耗的 <em>Hard</em> 资源使用限制。</p>
<!-- A limit range defines min/max constraints on the amount of resources a single entity can consume in
a *Namespace*.

See [Admission control: Limit Range](https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md) -->
<p>限制区间定义了单个实体在一个 <em>Namespace</em> 中可使用的最小/最大资源量约束。</p>
<p>参阅 <a href="https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md">准入控制: 限制区间</a></p>
<!--
A namespace can be in one of two phases:

* `Active` the namespace is in use
* `Terminating` the namespace is being deleted, and can not be used for new objects

See the [design doc](https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#phases) for more details. -->
<p>名字空间可以处于下列两个阶段中的一个:</p>
<ul>
<li><code>Active</code> 名字空间正在被使用中</li>
<li><code>Terminating</code> 名字空间正在被删除，且不能被用于新对象。</li>
</ul>
<p>参见<a href="https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#phases">设计文档</a> 查看更多细节。</p>
<!-- ## Creating a new namespace -->
<h2 id="创建名字空间">创建名字空间</h2>
<!--
Avoid creating namespace with prefix `kube-`, since it is reserved for Kubernetes system namespaces.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 避免使用前缀 <code>kube-</code> 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。</div>
</blockquote>
<!-- 1. Create a new YAML file called `my-namespace.yaml` with the contents: -->
<ol>
<li>
<p>新建一个名为 <code>my-namespace.yaml</code> 的 YAML 文件，并写入下列内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Namespace<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>&lt;insert-namespace-name-here&gt;<span style="color:#bbb">
</span></code></pre></div><!-- Then run: -->
<p>然后运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./my-namespace.yaml
</code></pre></div></li>
</ol>
<!--
2. Alternatively, you can create namespace using below command:
-->
<ol start="2">
<li>
<p>或者，你可以使用下面的命令创建名字空间：</p>
<pre tabindex="0"><code>kubectl create namespace &lt;insert-namespace-name-here&gt;
</code></pre></li>
</ol>
<!--
The name of your namespace must be a valid
[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).
-->
<p>请注意，名字空间的名称必须是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-label-names">DNS 标签</a>。</p>
<!--
There's an optional field `finalizers`, which allows observables to purge resources whenever the namespace is deleted. Keep in mind that if you specify a nonexistent finalizer, the namespace will be created but will get stuck in the `Terminating` state if the user tries to delete it.

More information on `finalizers` can be found in the namespace [design doc](https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#finalizers).
-->
<p>可选字段 <code>finalizers</code> 允许观察者们在名字空间被删除时清除资源。记住如果指定了一个不存在的终结器，名字空间仍会被创建，但如果用户试图删除它，它将陷入 <code>Terminating</code> 状态。</p>
<p>更多有关 <code>finalizers</code> 的信息请查阅 <a href="https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#finalizers">设计文档</a> 中名字空间部分。</p>
<!-- ## Deleting a namespace -->
<h2 id="删除名字空间">删除名字空间</h2>
<!--
Delete a namespace with
-->
<p>删除名字空间使用命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespaces &lt;insert-some-namespace-name&gt;
</code></pre></div><!-- This deletes _everything_ under the namespace! -->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 这会删除名字空间下的 <em>所有内容</em> ！</div>
</blockquote>

<!-- This delete is asynchronous, so for a time you will see the namespace in the `Terminating` state. -->
<p>删除是异步的，所以有一段时间你会看到名字空间处于 <code>Terminating</code> 状态。</p>
<!--
## Subdividing your cluster using Kubernetes namespaces
-->
<h2 id="使用-kubernetes-名字空间细分你的集群">使用 Kubernetes 名字空间细分你的集群</h2>
<!--
1. Understand the default namespace

   By default, a Kubernetes cluster will instantiate a default namespace when provisioning
   the cluster to hold the default set of Pods, Services, and Deployments used by the cluster.
-->
<ol>
<li>
<p>理解 default 名字空间</p>
<p>默认情况下，Kubernetes 集群会在配置集群时实例化一个 default 名字空间，用以存放集群所使用的默认
Pods、Services 和 Deployments 集合。</p>
<!--
Assuming you have a fresh cluster, you can introspect the available namespace's by doing the following:
-->
<p>假设你有一个新的集群，你可以通过执行以下操作来内省可用的名字空间</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces
</code></pre></div><pre tabindex="0"><code>NAME      STATUS    AGE
default   Active    13m
</code></pre></li>
</ol>
<!--
2. Create new namespaces
-->
<ol start="2">
<li>
<p>创建新的名字空间</p>
<!--
For this exercise, we will create two additional Kubernetes namespaces to hold our content.
-->
<p>在本练习中，我们将创建两个额外的 Kubernetes 名字空间来保存我们的内容。</p>
<!--
In a scenario where an organization is using a shared Kubernetes cluster for development and
production use cases:
-->
<p>在某组织使用共享的 Kubernetes 集群进行开发和生产的场景中：</p>
<!--
The development team would like to maintain a space in the cluster where they can
get a view on the list of Pods, Services, and Deployments
they use to build and run their application.  In this space, Kubernetes resources come
and go, and the restrictions on who can or cannot modify resources
are relaxed to enable agile development.
-->
<p>开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pods、Services
和 Deployments 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，
对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。</p>
<!--
The operations team would like to maintain a space in the cluster where they can enforce
strict procedures on who can or cannot manipulate the set of
Pods, Services, and Deployments that run the production site.
-->
<p>运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，
对谁可以或不可以操作运行生产站点的 Pods、Services 和 Deployments 集合进行控制。</p>
<!--
One pattern this organization could follow is to partition the Kubernetes cluster into
two namespaces: `development` and `production`.
-->
<p>该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个名字空间：development 和 production。</p>
<!-- Let's create two new namespaces to hold our work. -->
<p>让我们创建两个新的名字空间来保存我们的工作。</p>
<!-- Create the `development` namespace using kubectl. -->
<p>使用 kubectl 创建 <code>development</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</code></pre></div><!-- And then let's create the `production` namespace using kubectl. -->
<p>让我们使用 kubectl 创建 <code>production</code> 名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</code></pre></div><!-- To be sure things are right, list all of the namespaces in our cluster. -->
<p>为了确保一切正常，列出集群中的所有名字空间。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get namespaces --show-labels
</code></pre></div><pre tabindex="0"><code>NAME          STATUS    AGE       LABELS
default       Active    32m       &lt;none&gt;
development   Active    29s       name=development
production    Active    23s       name=production
</code></pre></li>
</ol>
<!-- 3. Create pods in each namespace -->
<ol start="3">
<li>
<p>在每个名字空间中创建 pod</p>
<!--
A Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster.

Users interacting with one namespace do not see the content in another namespace.
-->
<p>Kubernetes 名字空间为集群中的 Pods、Services 和 Deployments 提供了作用域。</p>
<p>与一个名字空间交互的用户不会看到另一个名字空间中的内容。</p>
<!-- To demonstrate this, let's spin up a simple Deployment and Pods in the `development` namespace. -->
<p>为了演示这一点，让我们在 <code>development</code> 名字空间中启动一个简单的 Deployment 和 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment snowflake --image<span style="color:#666">=</span>k8s.gcr.io/serve_hostname -n<span style="color:#666">=</span>development
kubectl scale deployment snowflake --replicas<span style="color:#666">=</span><span style="color:#666">2</span> -n<span style="color:#666">=</span>development
</code></pre></div><!--
We have just created a deployment whose replica size is 2 that is running the pod
called `snowflake` with a basic container that just serves the hostname.
-->
<p>我们刚刚创建了一个副本个数为 2 的 Deployment，运行名为 <code>snowflake</code> 的
Pod，其中包含一个仅负责提供主机名的基本容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment -n<span style="color:#666">=</span>development
</code></pre></div><pre tabindex="0"><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
snowflake    2/2     2            2           2m
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>snowflake -n<span style="color:#666">=</span>development
</code></pre></div><pre tabindex="0"><code>NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
</code></pre><!--
And this is great, developers are able to do what they want, and they do not have to worry about affecting content in the `production` namespace.

Let's switch to the `production` namespace and show how resources in one namespace are hidden from the other.

The `production` namespace should be empty, and the following commands should return nothing.
-->
<p>看起来还不错，开发人员能够做他们想做的事，而且他们不必担心会影响到
<code>production</code> 名字空间下面的内容。</p>
<p>让我们切换到 <code>production</code> 名字空间，展示一下一个名字空间中的资源是如何对
另一个名字空间隐藏的。</p>
<p>名字空间 <code>production</code> 应该是空的，下面的命令应该不会返回任何东西。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment -n<span style="color:#666">=</span>production
kubectl get pods -n<span style="color:#666">=</span>production
</code></pre></div><!--
Production likes to run cattle, so let's create some cattle pods.
-->
<p>生产环境下一般以养牛的方式运行负载，所以让我们创建一些 Cattle（牛）Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment cattle --image<span style="color:#666">=</span>k8s.gcr.io/serve_hostname -n<span style="color:#666">=</span>production
kubectl scale deployment cattle --replicas<span style="color:#666">=</span><span style="color:#666">5</span> -n<span style="color:#666">=</span>production

kubectl get deployment -n<span style="color:#666">=</span>production
</code></pre></div><pre tabindex="0"><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
cattle       5/5     5            5           10s
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>cattle -n<span style="color:#666">=</span>production
</code></pre></div><pre tabindex="0"><code>NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
</code></pre></li>
</ol>
<!--
At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.
-->
<p>此时，应该很清楚的展示了用户在一个名字空间中创建的资源对另一个名字空间是隐藏的。</p>
<!--
As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different
authorization rules for each namespace.
-->
<p>随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个名字空间提供不同的授权规则。</p>
<!-- discussion -->
<!--
## Understanding the motivation for using namespaces
-->
<h2 id="理解使用名字空间的动机">理解使用名字空间的动机</h2>
<!--
A single cluster should be able to satisfy the needs of multiple users or groups of users (henceforth a 'user community').
-->
<p>单个集群应该能满足多个用户及用户组的需求（以下称为 “用户社区”）。</p>
<!-- Kubernetes _namespaces_ help different projects, teams, or customers to share a Kubernetes cluster. -->
<p>Kubernetes <em>名字空间</em> 帮助不同的项目、团队或客户去共享 Kubernetes 集群。</p>
<!--
It does this by providing the following:

1. A scope for [Names](/docs/concepts/overview/working-with-objects/names/).
2. A mechanism to attach authorization and policy to a subsection of the cluster.
-->
<p>名字空间通过以下方式实现这点：</p>
<ol>
<li>为<a href="/zh/docs/concepts/overview/working-with-objects/names/">名字</a>设置作用域.</li>
<li>为集群中的部分资源关联鉴权和策略的机制。</li>
</ol>
<!--
Use of multiple namespaces is optional.
-->
<p>使用多个名字空间是可选的。</p>
<!--
Each user community wants to be able to work in isolation from other communities.
-->
<p>每个用户社区都希望能够与其他社区隔离开展工作。</p>
<!--
Each user community has its own:

1. resources (pods, services, replication controllers, etc.)
2. policies (who can or cannot perform actions in their community)
3. constraints (this community is allowed this much quota, etc.)
-->
<p>每个用户社区都有自己的：</p>
<ol>
<li>资源（pods、服务、 副本控制器等等）</li>
<li>策略（谁能或不能在他们的社区里执行操作）</li>
<li>约束（该社区允许多少配额，等等）</li>
</ol>
<!--
A cluster operator may create a Namespace for each unique user community.
-->
<p>集群运营者可以为每个唯一用户社区创建名字空间。</p>
<!--
The Namespace provides a unique scope for:

1. named resources (to avoid basic naming collisions)
2. delegated management authority to trusted users
3. ability to limit community resource consumption
-->
<p>名字空间为下列内容提供唯一的作用域：</p>
<ol>
<li>命名资源（避免基本的命名冲突）</li>
<li>将管理权限委派给可信用户</li>
<li>限制社区资源消耗的能力</li>
</ol>
<!--
Use cases include:

1.  As a cluster operator, I want to support multiple user communities on a single cluster.
2.  As a cluster operator, I want to delegate authority to partitions of the cluster to trusted users
    in those communities.
3.  As a cluster operator, I want to limit the amount of resources each community can consume in order
    to limit the impact to other communities using the cluster.
4.  As a cluster user, I want to interact with resources that are pertinent to my user community in
    isolation of what other user communities are doing on the cluster.
-->
<p>用例包括:</p>
<ol>
<li>作为集群运营者, 我希望能在单个集群上支持多个用户社区。</li>
<li>作为集群运营者，我希望将集群分区的权限委派给这些社区中的受信任用户。</li>
<li>作为集群运营者，我希望能限定每个用户社区可使用的资源量，以限制对使用同一集群的其他用户社区的影响。</li>
<li>作为群集用户，我希望与我的用户社区相关的资源进行交互，而与其他用户社区在该集群上执行的操作无关。</li>
</ol>
<!--
## Understanding namespaces and DNS
-->
<h2 id="理解名字空间和-dns">理解名字空间和 DNS</h2>
<!--
When you create a [Service](/docs/concepts/services-networking/service/), it creates a corresponding [DNS entry](/docs/concepts/services-networking/dns-pod-service/).
This entry is of the form `<service-name>.<namespace-name>.svc.cluster.local`, which means
that if a container just uses `<service-name>` it will resolve to the service which
is local to a namespace.  This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production.  If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).
-->
<p>当你创建<a href="/zh/docs/concepts/services-networking/service/">服务</a>时，Kubernetes
会创建相应的 <a href="/zh/docs/concepts/services-networking/dns-pod-service/">DNS 条目</a>。
此条目的格式为 <code>&lt;服务名称&gt;.&lt;名字空间名称&gt;.svc.cluster.local</code>。
这意味着如果容器只使用 <code>&lt;服务名称&gt;</code>，它将解析为名字空间本地的服务。
这对于在多个名字空间（如开发、暂存和生产）中使用相同的配置非常有用。
如果要跨名字空间访问，则需要使用完全限定的域名（FQDN）。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [setting the namespace preference](/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference).
* Learn more about [setting the namespace for a request](/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request)
* See [namespaces design](https://github.com/kubernetes/community/blob/main/contributors/design-proposals/architecture/namespaces.md).
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference">设置名字空间偏好</a></li>
<li>进一步了解<a href="/zh/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request">设置请求的名字空间</a></li>
<li>参阅<a href="https://github.com/kubernetes/community/blob/main/contributors/design-proposals/architecture/namespaces.md">名字空间的设计文档</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f58763cc9447491b6c40f939a02d441d">2.38 - 通过配置文件设置 Kubelet 参数</h1>
    
	<!--
reviewers:
- mtaufen
- dawnchen
title: Set Kubelet parameters via a config file
content_type: task
--->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code>
</div>

<!--
A subset of the Kubelet's configuration parameters may be
set via an on-disk config file, as a substitute for command-line flags.
This functionality is considered beta in v1.10.
--->
<p>通过保存在硬盘的配置文件设置 kubelet 的部分配置参数，这可以作为命令行参数的替代。
此功能在 v1.10 中为 beta 版。</p>
<!--
Providing parameters via a config file is the recommended approach because
it simplifies node deployment and configuration management.
--->
<p>建议通过配置文件的方式提供参数，因为这样可以简化节点部署和配置管理。</p>
<h2 id="准备开始">准备开始</h2>
<!--
- A v1.10 or higher Kubelet binary must be installed for beta functionality.
-->
<ul>
<li>需要安装 1.10 或更高版本的 kubelet 可执行文件，才能使用此 beta 功能。</li>
</ul>
<!-- steps -->
<!--
## Create the config file

The subset of the Kubelet's configuration that can be configured via a file
is defined by the `KubeletConfiguration` struct
[here (v1beta1)](https://github.com/kubernetes/kubernetes/blob/main/staging/src/k8s.io/kubelet/config/v1beta1/types.go).
-->
<h2 id="创建配置文件">创建配置文件</h2>
<p><code>KubeletConfiguration</code> 结构体定义了可以通过文件配置的 Kubelet 配置子集，
该结构体在 <a href="https://github.com/kubernetes/kubernetes/blob/main/staging/src/k8s.io/kubelet/config/v1beta1/types.go">这里（v1beta1）</a>
可以找到。</p>
<!--
The configuration file must be a JSON or YAML representation of the parameters
in this struct. Make sure the Kubelet has read permissions on the file.

Here is an example of what this file might look like:
-->
<p>配置文件必须是这个结构体中参数的 JSON 或 YAML 表现形式。
确保 kubelet 可以读取该文件。</p>
<p>下面是一个 Kubelet 配置文件示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>KubeletConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">evictionHard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">memory.available</span>:<span style="color:#bbb">  </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
In the example, the Kubelet is configured to evict Pods when available memory drops below 200Mi.
All other Kubelet configuration values are left at their built-in defaults, unless overridden
by flags. Command line flags which target the same value as a config file will override that value.

For a trick to generate a configuration file from a live node, see
[Reconfigure a Node's Kubelet in a Live Cluster](/docs/tasks/administer-cluster/reconfigure-kubelet).
-->
<p>在这个示例中, 当可用内存低于 200Mi 时, kubelet 将会开始驱逐 Pods。
没有声明的其余配置项都将使用默认值，除非使用命令行参数来重载。
命令行中的参数将会覆盖配置文件中的对应值。</p>
<p>作为一个小技巧，你可以从活动节点生成配置文件，相关方法请查看
<a href="/zh/docs/tasks/administer-cluster/reconfigure-kubelet">重新配置活动集群节点的 kubelet</a>。</p>
<!--
## Start a Kubelet process configured via the config file

Start the Kubelet with the `--config` flag set to the path of the Kubelet's config file.
The Kubelet will then load its config from this file.
--->
<h2 id="启动通过配置文件配置的-kubelet-进程">启动通过配置文件配置的 Kubelet 进程</h2>
<p>启动 Kubelet 需要将 <code>--config</code> 参数设置为 Kubelet 配置文件的路径。Kubelet 将从此文件加载其配置。</p>
<!--
Note that command line flags which target the same value as a config file will override that value.
This helps ensure backwards compatibility with the command-line API.
-->
<p>请注意，命令行参数与配置文件有相同的值时，就会覆盖配置文件中的该值。
这有助于确保命令行 API 的向后兼容性。</p>
<!--
Note that relative file paths in the Kubelet config file are resolved relative to the
location of the Kubelet config file, whereas relative paths in command line flags are resolved
relative to the Kubelet's current working directory.
-->
<p>请注意，kubelet 配置文件中的相对文件路径是相对于 kubelet 配置文件的位置解析的，
而命令行参数中的相对路径是相对于 kubelet 的当前工作目录解析的。</p>
<!--
Note that some default values differ between command-line flags and the Kubelet config file.
If `--config` is provided and the values are not specified via the command line, the
defaults for the `KubeletConfiguration` version apply.
In the above example, this version is `kubelet.config.k8s.io/v1beta1`.
--->
<p>请注意，命令行参数和 Kubelet 配置文件的某些默认值不同。
如果设置了 <code>--config</code>，并且没有通过命令行指定值，则 <code>KubeletConfiguration</code>
版本的默认值生效。在上面的例子中，version 是 <code>kubelet.config.k8s.io/v1beta1</code>。</p>
<!-- discussion -->
<!--
## Relationship to Dynamic Kubelet Config

If you are using the [Dynamic Kubelet Configuration](/docs/tasks/administer-cluster/reconfigure-kubelet)
feature, the combination of configuration provided via `--config` and any flags which override these values
is considered the default "last known good" configuration by the automatic rollback mechanism.
--->
<h2 id="与动态-kubelet-配置的关系">与动态 Kubelet 配置的关系</h2>
<p>如果你正在使用<a href="/zh/docs/tasks/administer-cluster/reconfigure-kubelet">动态 kubelet 配置</a>特性，
那么自动回滚机制将认为通过 <code>--config</code> 提供的配置与覆盖这些值的任何参数的组合是
&quot;最后已知正常（last known good）&quot; 的配置。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5e59f5575dce11fdaed640afdbeedfc1">2.39 - 配置 API 对象配额</h1>
    
	<!--
title: Configure Quotas for API Objects
content_type: task
-->
<!-- overview -->
<!--
This page shows how to configure quotas for API objects, including
PersistentVolumeClaims and Services. A quota restricts the number of
objects, of a particular type, that can be created in a namespace.
You specify quotas in a
[ResourceQuota](/docs/reference/generated/kubernetes-api/v1.22/#resourcequota-v1-core)
object.
-->
<p>本文讨论如何为 API 对象配置配额，包括 PersistentVolumeClaim 和 Service。
配额限制了可以在命名空间中创建的特定类型对象的数量。
你可以在 <a href="/docs/reference/generated/kubernetes-api/v1.22/#resourcequota-v1-core">ResourceQuota</a> 对象中指定配额。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间以便本例中创建的资源和集群中的其余部分相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace quota-object-example
</code></pre></div><!--
## Create a ResourceQuota

Here is the configuration file for a ResourceQuota object:
-->
<h2 id="创建-resourcequota">创建 ResourceQuota</h2>
<p>下面是一个 ResourceQuota 对象的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-objects.yaml" download="admin/resource/quota-objects.yaml"><code>admin/resource/quota-objects.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-objects-yaml')" title="Copy admin/resource/quota-objects.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-objects-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>object-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the ResourceQuota:
-->
<p>创建 ResourceQuota：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
View detailed information about the ResourceQuota:
-->
<p>查看 ResourceQuota 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get resourcequota object-quota-demo --namespace<span style="color:#666">=</span>quota-object-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that in the quota-object-example namespace, there can be at most
one PersistentVolumeClaim, at most two Services of type LoadBalancer, and no Services
of type NodePort.
-->
<p>输出结果表明在 quota-object-example 命名空间中，至多只能有一个 PersistentVolumeClaim，
最多两个 LoadBalancer 类型的服务，不能有 NodePort 类型的服务。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">used</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.loadbalancers</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">services.nodeports</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
## Create a PersistentVolumeClaim

Here is the configuration file for a PersistentVolumeClaim object:
-->
<h2 id="创建-persistentvolumeclaim">创建 PersistentVolumeClaim</h2>
<p>下面是一个 PersistentVolumeClaim 对象的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-objects-pvc.yaml" download="admin/resource/quota-objects-pvc.yaml"><code>admin/resource/quota-objects-pvc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-objects-pvc-yaml')" title="Copy admin/resource/quota-objects-pvc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-objects-pvc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pvc-quota-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>3Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the PersistentVolumeClaim:
-->
<p>创建 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
Verify that the PersistentVolumeClaim was created:
-->
<p>确认已创建完 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get persistentvolumeclaims --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
The output shows that the PersistentVolumeClaim exists and has status Pending:
-->
<p>输出信息表明 PersistentVolumeClaim 存在并且处于 Pending 状态：</p>
<pre tabindex="0"><code>NAME             STATUS
pvc-quota-demo   Pending
</code></pre><!--
## Attempt to create a second PersistentVolumeClaim

Here is the configuration file for a second PersistentVolumeClaim:
-->
<h2 id="尝试创建第二个-persistentvolumeclaim">尝试创建第二个 PersistentVolumeClaim</h2>
<p>下面是第二个 PersistentVolumeClaim 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/resource/quota-objects-pvc-2.yaml" download="admin/resource/quota-objects-pvc-2.yaml"><code>admin/resource/quota-objects-pvc-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-resource-quota-objects-pvc-2-yaml')" title="Copy admin/resource/quota-objects-pvc-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-resource-quota-objects-pvc-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pvc-quota-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>4Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Attempt to create the second PersistentVolumeClaim:
-->
<p>尝试创建第二个 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace<span style="color:#666">=</span>quota-object-example
</code></pre></div><!--
The output shows that the second PersistentVolumeClaim was not created,
because it would have exceeded the quota for the namespace.
-->
<p>输出信息表明第二个 PersistentVolumeClaim 没有创建成功，因为这会超出命名空间的配额。</p>
<pre tabindex="0"><code>persistentvolumeclaims &quot;pvc-quota-demo-2&quot; is forbidden:
exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1,
used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1
</code></pre><!--
## Notes

These are the strings used to identify API resources that can be constrained
by quotas:
-->
<h2 id="说明">说明</h2>
<p>下面这些字符串可被用来标识那些能被配额限制的 API 资源：</p>
<table>
<tr><th>String</th><th>API Object</th></tr>
<tr><td>"pods"</td><td>Pod</td></tr>
<tr><td>"services"</td><td>Service</td></tr>
<tr><td>"replicationcontrollers"</td><td>ReplicationController</td></tr>
<tr><td>"resourcequotas"</td><td>ResourceQuota</td></tr>
<tr><td>"secrets"</td><td>Secret</td></tr>
<tr><td>"configmaps"</td><td>ConfigMap</td></tr>
<tr><td>"persistentvolumeclaims"</td><td>PersistentVolumeClaim</td></tr>
<tr><td>"services.nodeports"</td><td>Service of type NodePort</td></tr>
<tr><td>"services.loadbalancers"</td><td>Service of type LoadBalancer</td></tr>
</table>
<!--
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除你的命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace quota-object-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)

* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认的内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为命名空间配置默认的 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置内存的最小和最大限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置 CPU 的最小和最大限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置 CPU 和内存配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></li>
</ul>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)

* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)

* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">为 Pod 配置服务质量</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6f3658d05bf8864be1d96b1d1287cffb">2.40 - 配置资源不足时的处理方式</h1>
    
	<!--
reviewers:
- derekwaynecarr
- vishh
- timstclair
title: Configure Out Of Resource Handling
content_type: concept
-->
<!-- overview -->
<!--
This page explains how to configure out of resource handling with `kubelet`.

The `kubelet` needs to preserve node stability when available compute resources
are low. This is especially important when dealing with incompressible
compute resources, such as memory or disk space. If such resources are exhausted,
nodes become unstable.
-->
<p>本页介绍如何使用 <code>kubelet</code> 配置资源不足时的处理方式。</p>
<p>当可用计算资源较少时，<code>kubelet</code>需要保证节点稳定性。
这在处理如内存和硬盘之类的不可压缩资源时尤为重要。
如果任意一种资源耗尽，节点将会变得不稳定。</p>
<!-- body -->
<!--
### Eviction Signals

The `kubelet` supports eviction decisions based on the signals described in the following
table. The value of each signal is described in the Description column, which is based on
the `kubelet` summary API.
-->
<h3 id="驱逐信号">驱逐信号</h3>
<p><code>kubelet</code> 支持按照以下表格中描述的信号触发驱逐决定。
每个信号的值在 description 列描述，基于 <code>kubelet</code> 摘要 API。</p>
<!--
| Eviction Signal            | Description                                                         |
|----------------------------|---------------------------------------------------------------------|
| `memory.available` | `memory.available` := `node.status.capacity[memory]` - `node.stats.memory.workingSet` |
| `nodefs.available` | `nodefs.available` := `node.stats.fs.available` |
| `nodefs.inodesFree` | `nodefs.inodesFree` := `node.stats.fs.inodesFree` |
| `imagefs.available` | `imagefs.available` := `node.stats.runtime.imagefs.available` |
| `imagefs.inodesFree` | `imagefs.inodesFree` := `node.stats.runtime.imagefs.inodesFree` |
| `pid.available`      | `pid.available` := `node.stats.rlimit.maxpid` - `node.stats.rlimit.curproc`           |
-->
<table>
<thead>
<tr>
<th>驱逐信号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>memory.available</code></td>
<td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td>
</tr>
<tr>
<td><code>nodefs.available</code></td>
<td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td>
</tr>
<tr>
<td><code>nodefs.inodesFree</code></td>
<td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td>
</tr>
<tr>
<td><code>imagefs.available</code></td>
<td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td>
</tr>
<tr>
<td><code>imagefs.inodesFree</code></td>
<td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td>
</tr>
<tr>
<td><code>pid.available</code></td>
<td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td>
</tr>
</tbody>
</table>
<!--
Each of the above signals supports either a literal or percentage based value.
The percentage based value is calculated relative to the total capacity
associated with each signal.
-->
<p>上面的每个信号都支持字面值或百分比的值。基于百分比的值的计算与每个信号对应的总容量相关。</p>
<!--
The value for `memory.available` is derived from the cgroupfs instead of tools
like `free -m`. This is important because `free -m` does not work in a
container, and if users use the [node
allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable) feature, out of resource decisions
are made local to the end user Pod part of the cgroup hierarchy as well as the
root node. This
[script](/examples/admin/resource/memory-available.sh)
reproduces the same set of steps that the `kubelet` performs to calculate
`memory.available`. The `kubelet` excludes inactive_file (i.e. # of bytes of
file-backed memory on inactive LRU list) from its calculation as it assumes that
memory is reclaimable under pressure.
-->
<p><code>memory.available</code> 的值从 cgroupfs 获取，而不是通过类似 <code>free -m</code> 的工具。
这很重要，因为 <code>free -m</code> 不能在容器中工作，并且如果用户使用了
<a href="/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">节点可分配资源</a>
特性，资源不足的判定将同时在本地 cgroup 层次结构的终端用户 Pod 部分和根节点做出。
这个<a href="/zh/examples/admin/resource/memory-available.sh">脚本</a>
复现了与 <code>kubelet</code> 计算 <code>memory.available</code> 相同的步骤。
<code>kubelet</code> 将 <code>inactive_file</code>（意即活动 LRU 列表上基于文件后端的内存字节数）从计算中排除，
因为它假设内存在出现压力时将被回收。</p>
<!--
`kubelet` supports only two filesystem partitions.

1. The `nodefs` filesystem that kubelet uses for volumes, daemon logs, etc.
2. The `imagefs` filesystem that container runtimes uses for storing images and
   container writable layers.
-->
<p><code>kubelet</code> 只支持两种文件系统分区。</p>
<ol>
<li><code>nodefs</code> 文件系统，kubelet 将其用于卷和守护程序日志等。</li>
<li><code>imagefs</code> 文件系统，容器运行时用于保存镜像和容器可写层。</li>
</ol>
<!--
`imagefs` is optional. `kubelet` auto-discovers these filesystems using
cAdvisor. `kubelet` does not care about any other filesystems. Any other types
of configurations are not currently supported by the kubelet. For example, it is
*not OK* to store volumes and logs in a dedicated `filesystem`.

In future releases, the `kubelet` will deprecate the existing [garbage
collection](/docs/concepts/cluster-administration/kubelet-garbage-collection/)
support in favor of eviction in response to disk pressure.
-->
<p><code>imagefs</code> 可选。<code>kubelet</code> 使用 cAdvisor 自动发现这些文件系统。
<code>kubelet</code> 不关心其它文件系统。当前不支持配置任何其它类型。
例如，在专用 <code>filesytem</code> 中存储卷和日志是 <em>不可以</em> 的。</p>
<p>在将来的发布中，<code>kubelet</code>将废除当前存在的
<a href="/zh/docs/concepts/cluster-administration/kubelet-garbage-collection/">垃圾回收</a>
机制，这种机制目前支持将驱逐操作作为对磁盘压力的响应。</p>
<!--
### Eviction Thresholds

The `kubelet` supports the ability to specify eviction thresholds that trigger the `kubelet` to reclaim resources.

Each threshold has the following form:

`[eviction-signal][operator][quantity]`

where:

* `eviction-signal` is an eviction signal token as defined in the previous table.
* `operator` is the desired relational operator, such as `<` (less than).
* `quantity` is the eviction threshold quantity, such as `1Gi`. These tokens must
match the quantity representation used by Kubernetes. An eviction threshold can also
be expressed as a percentage using the `%` token.

For example, if a node has `10Gi` of total memory and you want trigger eviction if
the available memory falls below `1Gi`, you can define the eviction threshold as
either `memory.available<10%` or `memory.available<1Gi`. You cannot use both.
-->
<h3 id="驱逐阈值">驱逐阈值</h3>
<p><code>kubelet</code>支持指定驱逐阈值，用于触发 <code>kubelet</code> 回收资源。</p>
<p>每个阈值形式如下：</p>
<p><code>[eviction-signal][operator][quantity]</code></p>
<ul>
<li>合法的 <code>eviction-signal</code> 标志如上所示。</li>
<li><code>operator</code> 是所需的关系运算符，例如 <code>&lt;</code>。</li>
<li><code>quantity</code> 是驱逐阈值值标志，例如 <code>1Gi</code>。合法的标志必须匹配 Kubernetes 使用的数量表示。
驱逐阈值也可以使用 <code>%</code> 标记表示百分比。</li>
</ul>
<p>举例说明，如果一个节点有 <code>10Gi</code> 内存，希望在可用内存下降到 <code>1Gi</code> 以下时引起驱逐操作，
则驱逐阈值可以使用下面任意一种方式指定（但不是两者同时）。</p>
<ul>
<li><code>memory.available&lt;10%</code></li>
<li><code>memory.available&lt;1Gi</code></li>
</ul>
<!--
#### Soft Eviction Thresholds

A soft eviction threshold pairs an eviction threshold with a required
administrator-specified grace period. No action is taken by the `kubelet`
to reclaim resources associated with the eviction signal until that grace
period has been exceeded. If no grace period is provided, the `kubelet`
returns an error on startup.
-->
<h4 id="软驱逐阈值">软驱逐阈值</h4>
<p>软驱逐阈值使用一对由驱逐阈值和管理员必须指定的宽限期组成的配置对。在超过宽限期前，<code>kubelet</code>不会采取任何动作回收和驱逐信号关联的资源。如果没有提供宽限期，<code>kubelet</code>启动时将报错。</p>
<!--
In addition, if a soft eviction threshold has been met, an operator can
specify a maximum allowed Pod termination grace period to use when evicting
pods from the node. If specified, the `kubelet` uses the lesser value among
the `pod.Spec.TerminationGracePeriodSeconds` and the max allowed grace period.
If not specified, the `kubelet` kills Pods immediately with no graceful
termination.
-->
<p>此外，如果达到了软驱逐阈值，操作员可以指定从节点驱逐 pod 时，在宽限期内允许结束的 pod 的最大数量。
如果指定了 <code>pod.Spec.TerminationGracePeriodSeconds</code> 值，
<code>kubelet</code> 将使用它和宽限期二者中较小的一个。
如果没有指定，<code>kubelet</code>将立即终止 pod，而不会优雅结束它们。</p>
<!--
To configure soft eviction thresholds, the following flags are supported:

* `eviction-soft` describes a set of eviction thresholds (e.g. `memory.available<1.5Gi`) that if met over a
corresponding grace period would trigger a Pod eviction.
* `eviction-soft-grace-period` describes a set of eviction grace periods (e.g. `memory.available=1m30s`) that
correspond to how long a soft eviction threshold must hold before triggering a Pod eviction.
* `eviction-max-pod-grace-period` describes the maximum allowed grace period (in seconds) to use when terminating
pods in response to a soft eviction threshold being met.
-->
<p>软驱逐阈值的配置支持下列标记：</p>
<ul>
<li><code>eviction-soft</code> 描述了驱逐阈值的集合（例如 <code>memory.available&lt;1.5Gi</code>），如果在宽限期之外满足条件将触发 pod 驱逐。</li>
<li><code>eviction-soft-grace-period</code> 描述了驱逐宽限期的集合（例如 <code>memory.available=1m30s</code>），对应于在驱逐 pod 前软驱逐阈值应该被控制的时长。</li>
<li><code>eviction-max-pod-grace-period</code> 描述了当满足软驱逐阈值并终止 pod 时允许的最大宽限期值（秒数）。</li>
</ul>
<!--
#### Hard Eviction Thresholds

A hard eviction threshold has no grace period, and if observed, the `kubelet`
will take immediate action to reclaim the associated starved resource. If a
hard eviction threshold is met, the `kubelet` kills the Pod immediately
with no graceful termination.

To configure hard eviction thresholds, the following flag is supported:

* `eviction-hard` describes a set of eviction thresholds (e.g. `memory.available<1Gi`) that if met
would trigger a Pod eviction.

The `kubelet` has the following default hard eviction threshold:

* `memory.available<100Mi`
* `nodefs.available<10%`
* `imagefs.available<15%`

On a Linux node, the default value also includes `nodefs.inodesFree<5%`.

-->
<h4 id="硬驱逐阈值">硬驱逐阈值</h4>
<p>硬驱逐阈值没有宽限期，一旦察觉，<code>kubelet</code> 将立即采取行动回收关联的短缺资源。
如果满足硬驱逐阈值，<code>kubelet</code> 将立即结束 Pod 而不是体面地终止它们。</p>
<p>硬驱逐阈值的配置支持下列标记：</p>
<ul>
<li><code>eviction-hard</code> 描述了驱逐阈值的集合（例如 <code>memory.available&lt;1Gi</code>），如果满足条件将触发 Pod 驱逐。</li>
</ul>
<p><code>kubelet</code> 有如下所示的默认硬驱逐阈值：</p>
<ul>
<li><code>memory.available&lt;100Mi</code></li>
<li><code>nodefs.available&lt;10%</code></li>
<li><code>imagefs.available&lt;15%</code></li>
</ul>
<p>在Linux节点上，默认值还包括 <code>nodefs.inodesFree&lt;5％</code>。</p>
<!--
### Eviction Monitoring Interval

The `kubelet` evaluates eviction thresholds per its configured housekeeping interval.

* `housekeeping-interval` is the interval between container housekeepings.
-->
<h3 id="驱逐监控时间间隔">驱逐监控时间间隔</h3>
<p><code>kubelet</code> 根据其配置的整理时间间隔计算驱逐阈值。</p>
<ul>
<li><code>housekeeping-interval</code> 是容器管理时间间隔。</li>
</ul>
<!--
### Node Conditions

The `kubelet` maps one or more eviction signals to a corresponding node condition.

If a hard eviction threshold has been met, or a soft eviction threshold has been met
independent of its associated grace period, the `kubelet` reports a condition that
reflects the node is under pressure.
-->
<h3 id="节点状态">节点状态</h3>
<p><code>kubelet</code> 会将一个或多个驱逐信号映射到对应的节点状态。</p>
<p>如果满足硬驱逐阈值，或者满足独立于其关联宽限期的软驱逐阈值时，<code>kubelet</code>将报告节点处于压力下的状态。</p>
<!--
The following node conditions are defined that correspond to the specified eviction signal.

| Node Condition | Eviction Signal  | Description                                                      |
|-------------------------|-------------------------------|--------------------------------------------|
| `MemoryPressure` | `memory.available` | Available memory on the node has satisfied an eviction threshold |
| `DiskPressure` | `nodefs.available`, `nodefs.inodesFree`, `imagefs.available`, or `imagefs.inodesFree` | Available disk space and inodes on either the node's root filesystem or image filesystem has satisfied an eviction threshold |
| `PIDPressure`     | `pid.available`                                                                       | Available processes identifiers on the (Linux) node has fallen below an eviction threshold                                   |   

The `kubelet` continues to report node status updates at the frequency specified by
`--node-status-update-frequency` which defaults to `10s`.
-->
<p>下列节点状态根据相应的驱逐信号定义。</p>
<table>
<thead>
<tr>
<th>节点状态</th>
<th>驱逐信号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MemoryPressure</code></td>
<td><code>memory.available</code></td>
<td>节点上可用内存量达到逐出阈值</td>
</tr>
<tr>
<td><code>DiskPressure</code></td>
<td><code>nodefs.available</code>, <code>nodefs.inodesFree</code>, <code>imagefs.available</code>, 或 <code>imagefs.inodesFree</code></td>
<td>节点或者节点的根文件系统或镜像文件系统上可用磁盘空间和 i 节点个数达到逐出阈值</td>
</tr>
<tr>
<td><code>PIDPressure</code></td>
<td><code>pid.available</code></td>
<td>在（Linux）节点上的可用进程标识符已降至驱逐阈值以下</td>
</tr>
</tbody>
</table>
<p><code>kubelet</code> 将以 <code>--node-status-update-frequency</code> 指定的频率连续报告节点状态更新，其默认值为 <code>10s</code>。</p>
<!--
### Oscillation of node conditions

If a node is oscillating above and below a soft eviction threshold, but not exceeding
its associated grace period, it would cause the corresponding node condition to
constantly oscillate between true and false, and could cause poor scheduling decisions
as a consequence.

To protect against this oscillation, the following flag is defined to control how
long the `kubelet` must wait before transitioning out of a pressure condition.

* `eviction-pressure-transition-period` is the duration for which the `kubelet` has
to wait before transitioning out of an eviction pressure condition.

The `kubelet` would ensure that it has not observed an eviction threshold being met
for the specified pressure condition for the period specified before toggling the
condition back to `false`.
-->
<h3 id="节点状态振荡">节点状态振荡</h3>
<p>如果节点在软驱逐阈值的上下振荡，但没有超过关联的宽限期时，将引起对应节点的状态持续在
true 和 false 间跳变，并导致不好的调度结果。</p>
<p>为了防止这种振荡，可以定义下面的标志，用于控制 <code>kubelet</code> 从压力状态中退出之前必须等待的时间。</p>
<ul>
<li><code>eviction-pressure-transition-period</code> 是 <code>kubelet</code> 从压力状态中退出之前必须等待的时长。</li>
</ul>
<p><code>kubelet</code> 将确保在设定的时间段内没有发现和指定压力条件相对应的驱逐阈值被满足时，才会将状态变回 <code>false</code>。</p>
<!--
### Reclaiming node level resources

If an eviction threshold has been met and the grace period has passed,
the `kubelet` initiates the process of reclaiming the pressured resource
until it has observed the signal has gone below its defined threshold.

The `kubelet` attempts to reclaim node level resources prior to evicting end-user Pods. If
disk pressure is observed, the `kubelet` reclaims node level resources differently if the
machine has a dedicated `imagefs` configured for the container runtime.
-->
<h3 id="回收节点层级资源">回收节点层级资源</h3>
<p>如果满足驱逐阈值并超过了宽限期，<code>kubelet</code>将启动回收压力资源的过程，直到它发现低于设定阈值的信号为止。</p>
<p><code>kubelet</code> 将尝试在驱逐终端用户 pod 前回收节点层级资源。
发现磁盘压力时，如果节点针对容器运行时配置有独占的 <code>imagefs</code>，<code>kubelet</code>回收节点层级资源的方式将会不同。</p>
<!--
#### With `imagefs`

If `nodefs` filesystem has met eviction thresholds, `kubelet` frees up disk space by deleting the dead Pods and their containers.

If `imagefs` filesystem has met eviction thresholds, `kubelet` frees up disk space by deleting all unused images.

#### Without `imagefs`

If `nodefs` filesystem has met eviction thresholds, `kubelet` frees up disk space in the following order:

1. Delete dead Pods and their containers
2. Delete all unused images
-->
<h4 id="使用-imagefs">使用 <code>imagefs</code></h4>
<p>如果 <code>nodefs</code> 文件系统满足驱逐阈值，<code>kubelet</code>通过驱逐 pod 及其容器来释放磁盘空间。</p>
<p>如果 <code>imagefs</code> 文件系统满足驱逐阈值，<code>kubelet</code>通过删除所有未使用的镜像来释放磁盘空间。</p>
<h4 id="未使用-imagefs">未使用 <code>imagefs</code></h4>
<p>如果 <code>nodefs</code> 满足驱逐阈值，<code>kubelet</code>将以下面的顺序释放磁盘空间：</p>
<ol>
<li>删除停止运行的 pod/container</li>
<li>删除全部没有使用的镜像</li>
</ol>
<!--
### Evicting end-user Pods

If the `kubelet` is unable to reclaim sufficient resource on the node, `kubelet` begins evicting Pods.

The `kubelet` ranks Pods for eviction first by whether or not their usage of the starved resource exceeds requests,
then by [Priority](/docs/concepts/configuration/pod-priority-preemption/), and then by the consumption of the starved compute resource relative to the Pods' scheduling requests.

-->
<h3 id="驱逐最终用户的-pod">驱逐最终用户的 pod</h3>
<p>如果 <code>kubelet</code> 在节点上无法回收足够的资源，<code>kubelet</code>将开始驱逐 pod。</p>
<p><code>kubelet</code> 首先根据他们对短缺资源的使用是否超过请求来排除 pod 的驱逐行为，
然后通过<a href="/zh/docs/concepts/configuration/pod-priority-preemption/">优先级</a>，
然后通过相对于 pod 的调度请求消耗急需的计算资源。</p>
<!--
As a result, `kubelet` ranks and evicts Pods in the following order:

* `BestEffort` or `Burstable` Pods whose usage of a starved resource exceeds its request.
Such pods are ranked by Priority, and then usage above request.
* `Guaranteed` pods and `Burstable` pods whose usage is beneath requests are evicted last.
`Guaranteed` Pods are guaranteed only when requests and limits are specified for all
the containers and they are equal. Such pods are guaranteed to never be evicted because
of another Pod's resource consumption. If a system daemon (such as `kubelet`, `docker`,
and `journald`) is consuming more resources than were reserved via `system-reserved` or
`kube-reserved` allocations, and the node only has `Guaranteed` or `Burstable` Pods using
less than requests remaining, then the node must choose to evict such a Pod in order to
preserve node stability and to limit the impact of the unexpected consumption to other Pods.
In this case, it will choose to evict pods of Lowest Priority first.
-->
<p><code>kubelet</code> 按以下顺序对要驱逐的 pod 排名：</p>
<ul>
<li><code>BestEffort</code> 或 <code>Burstable</code>，其对短缺资源的使用超过了其请求，此类 pod 按优先级排序，然后使用高于请求。</li>
<li><code>Guaranteed</code> pod 和 <code>Burstable</code> pod，其使用率低于请求，最后被驱逐。
<code>Guaranteed</code> Pod 只有为所有的容器指定了要求和限制并且它们相等时才能得到保证。
由于另一个 Pod 的资源消耗，这些 Pod 保证永远不会被驱逐。
如果系统守护进程（例如 <code>kubelet</code>、<code>docker</code>、和 <code>journald</code>）消耗的资源多于通过
<code>system-reserved</code> 或 <code>kube-reserved</code> 分配保留的资源，并且该节点只有 <code>Guaranteed</code> 或
<code>Burstable</code> Pod 使用少于剩余的请求，然后节点必须选择驱逐这样的 Pod
以保持节点的稳定性并限制意外消耗对其他 pod 的影响。
在这种情况下，它将首先驱逐优先级最低的 pod。</li>
</ul>
<!--
If necessary, `kubelet` evicts Pods one at a time to reclaim disk when `DiskPressure`
is encountered. If the `kubelet` is responding to `inode` starvation, it reclaims
`inodes` by evicting Pods with the lowest quality of service first. If the `kubelet`
is responding to lack of available disk, it ranks Pods within a quality of service
that consumes the largest amount of disk and kill those first.
-->
<p>必要时，<code>kubelet</code>会在遇到 <code>DiskPressure</code> 时逐个驱逐 Pod 来回收磁盘空间。
如果 <code>kubelet</code> 响应 <code>inode</code> 短缺，它会首先驱逐服务质量最低的 Pod 来回收 <code>inodes</code>。
如果 <code>kubelet</code> 响应缺少可用磁盘，它会将 Pod 排在服务质量范围内，该服务会消耗大量的磁盘并首先结束这些磁盘。</p>
<!--
#### With `imagefs`

If `nodefs` is triggering evictions, `kubelet` sorts Pods based on the usage on `nodefs`
- local volumes + logs of all its containers.

If `imagefs` is triggering evictions, `kubelet` sorts Pods based on the writable layer usage of all its containers.

#### Without `imagefs`

If `nodefs` is triggering evictions, `kubelet` sorts Pods based on their total disk usage
- local volumes + logs & writable layer of all its containers.
-->
<h4 id="使用-imagefs-1">使用 <code>imagefs</code></h4>
<p>如果是 <code>nodefs</code> 触发驱逐，<code>kubelet</code>将按 <code>nodefs</code> 用量 - 本地卷 + pod 的所有容器日志的总和对其排序。</p>
<p>如果是 <code>imagefs</code> 触发驱逐，<code>kubelet</code>将按 pod 所有可写层的用量对其进行排序。</p>
<h4 id="未使用-imagefs-1">未使用 <code>imagefs</code></h4>
<p>如果是 <code>nodefs</code> 触发驱逐，<code>kubelet</code>会根据磁盘的总使用情况对 pod 进行排序 - 本地卷 + 所有容器的日志及其可写层。</p>
<!--
### Minimum eviction reclaim

In certain scenarios, eviction of Pods could result in reclamation of small amount of resources. This can result in
`kubelet` hitting eviction thresholds in repeated successions. In addition to that, eviction of resources like `disk`,
 is time consuming.
-->
<h3 id="最小驱逐回收">最小驱逐回收</h3>
<p>在某些场景，驱逐 pod 会导致回收少量资源。这将导致 <code>kubelet</code> 反复碰到驱逐阈值。除此之外，对如 <code>disk</code> 这类资源的驱逐时比较耗时的。</p>
<!--
To mitigate these issues, `kubelet` can have a per-resource `minimum-reclaim`. Whenever `kubelet` observes
resource pressure, `kubelet` attempts to reclaim at least `minimum-reclaim` amount of resource below
the configured eviction threshold.

For example, with the following configuration:
-->
<p>为了减少这类问题，<code>kubelet</code>可以为每个资源配置一个 <code>minimum-reclaim</code>。
当 <code>kubelet</code> 发现资源压力时，<code>kubelet</code>将尝试至少回收驱逐阈值之下 <code>minimum-reclaim</code> 数量的资源。</p>
<p>例如使用下面的配置：</p>
<pre tabindex="0"><code>--eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi,imagefs.available&lt;100Gi
--eviction-minimum-reclaim=&quot;memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi&quot;`
</code></pre><!--
If an eviction threshold is triggered for `memory.available`, the `kubelet` works to ensure
that `memory.available` is at least `500Mi`. For `nodefs.available`, the `kubelet` works
to ensure that `nodefs.available` is at least `1.5Gi`, and for `imagefs.available` it
works to ensure that `imagefs.available` is at least `102Gi` before no longer reporting pressure
on their associated resources.

The default `eviction-minimum-reclaim` is `0` for all resources.
-->
<p>如果 <code>memory.available</code> 驱逐阈值被触发，<code>kubelet</code> 将保证 <code>memory.available</code> 至少为 <code>500Mi</code>。
对于 <code>nodefs.available</code>，<code>kubelet</code> 将保证 <code>nodefs.available</code> 至少为 <code>1.5Gi</code>。
对于 <code>imagefs.available</code>，<code>kubelet</code> 将保证 <code>imagefs.available</code> 至少为 <code>102Gi</code>，
直到不再有相关资源报告压力为止。</p>
<p>所有资源的默认 <code>eviction-minimum-reclaim</code> 值为 <code>0</code>。</p>
<!--
### Scheduler

The node reports a condition when a compute resource is under pressure. The
scheduler views that condition as a signal to dissuade placing additional
pods on the node.

| Node Condition    | Scheduler Behavior                               |
| ---------------- | ------------------------------------------------ |
| `MemoryPressure` | No new `BestEffort` Pods are scheduled to the node. |
| `DiskPressure` | No new Pods are scheduled to the node. |
-->
<h3 id="调度器">调度器</h3>
<p>当资源处于压力之下时，节点将报告状态。调度器将那种状态视为一种信号，阻止更多 pod 调度到这个节点上。</p>
<table>
<thead>
<tr>
<th>节点状态</th>
<th>调度器行为</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MemoryPressure</code></td>
<td>新的 <code>BestEffort</code> Pod 不会被调度到该节点</td>
</tr>
<tr>
<td><code>DiskPressure</code></td>
<td>没有新的 Pod 会被调度到该节点</td>
</tr>
</tbody>
</table>
<!--
## Node OOM Behavior

If the node experiences a system OOM (out of memory) event prior to the `kubelet` is able to reclaim memory,
the node depends on the [oom_killer](https://lwn.net/Articles/391222/) to respond.

The `kubelet` sets a `oom_score_adj` value for each container based on the quality of service for the Pod.
-->
<h2 id="节点-oom-行为">节点 OOM 行为</h2>
<p>如果节点在 <code>kubelet</code> 回收内存之前经历了系统 OOM（内存不足）事件，它将基于
<a href="https://lwn.net/Articles/391222/">oom-killer</a> 做出响应。</p>
<p><code>kubelet</code> 基于 pod 的 service 质量为每个容器设置一个 <code>oom_score_adj</code> 值。</p>
<!--
| Quality of Service | oom_score_adj |
|----------------------------|-----------------------------------------------------------------------|
| `Guaranteed` | -998 |
| `BestEffort` | 1000 |
| `Burstable` | min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999) |
-->
<table>
<thead>
<tr>
<th>Service 质量</th>
<th>oom_score_adj</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Guaranteed</code></td>
<td>-998</td>
</tr>
<tr>
<td><code>BestEffort</code></td>
<td>1000</td>
</tr>
<tr>
<td><code>Burstable</code></td>
<td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td>
</tr>
</tbody>
</table>
<!--
If the `kubelet` is unable to reclaim memory prior to a node experiencing system OOM, the `oom_killer` calculates
an `oom_score` based on the percentage of memory it's using on the node, and then add the `oom_score_adj` to get an
effective `oom_score` for the container, and then kills the container with the highest score.

The intended behavior should be that containers with the lowest quality of service that
are consuming the largest amount of memory relative to the scheduling request should be killed first in order
to reclaim memory.

Unlike Pod eviction, if a Pod container is OOM killed, it may be restarted by the `kubelet` based on its `RestartPolicy`.
-->
<p>如果 <code>kubelet</code> 在节点经历系统 OOM 之前无法回收内存，<code>oom_killer</code> 将基于它在节点上
使用的内存百分比算出一个 <code>oom_score</code>，并加上 <code>oom_score_adj</code> 得到容器的有效
<code>oom_score</code>，然后结束得分最高的容器。</p>
<p>预期的行为应该是拥有最低服务质量并消耗和调度请求相关内存量最多的容器第一个被结束，以回收内存。</p>
<p>和 pod 驱逐不同，如果一个 Pod 的容器是被 OOM 结束的，基于其 <code>RestartPolicy</code>，
它可能会被 <code>kubelet</code> 重新启动。</p>
<!--
## Best Practices

The following sections describe best practices for out of resource handling.

### Schedulable resources and eviction policies

Consider the following scenario:

* Node memory capacity: `10Gi`
* Operator wants to reserve 10% of memory capacity for system daemons (kernel, `kubelet`, etc.)
* Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.

To facilitate this scenario, the `kubelet` would be launched as follows:
-->
<h2 id="最佳实践">最佳实践</h2>
<p>以下部分描述了资源外处理的最佳实践。</p>
<h3 id="可调度资源和驱逐策略">可调度资源和驱逐策略</h3>
<p>考虑以下场景：</p>
<ul>
<li>节点内存容量：<code>10Gi</code></li>
<li>操作员希望为系统守护进程保留 10% 内存容量（内核、<code>kubelet</code>等）。</li>
<li>操作员希望在内存用量达到 95% 时驱逐 pod，以减少对系统的冲击并防止系统 OOM 的发生。</li>
</ul>
<p>为了促成这个场景，<code>kubelet</code>将像下面这样启动：</p>
<pre tabindex="0"><code>--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre><!--
Implicit in this configuration is the understanding that "System reserved" should include the amount of memory
covered by the eviction threshold.

To reach that capacity, either some Pod is using more than its request, or the system is using more than `1.5Gi - 500Mi = 1Gi`.

This configuration ensures that the scheduler does not place Pods on a node that immediately induce memory pressure
and trigger eviction assuming those Pods use less than their configured request.
-->
<p>这个配置的暗示是理解系统保留应该包含被驱逐阈值覆盖的内存数量。</p>
<p>要达到这个容量，要么某些 pod 使用了超过它们请求的资源，要么系统使用的内存超过 <code>1.5Gi - 500Mi = 1Gi</code>。</p>
<p>这个配置将保证在 pod 使用量都不超过它们配置的请求值时，如果可能立即引起内存压力并触发驱逐时，调度器不会将 pod 放到这个节点上。</p>
<!--
### DaemonSet

It is never desired for `kubelet` to evict a `DaemonSet` Pod, since the Pod is
immediately recreated and rescheduled back to the same node.

At the moment, the `kubelet` has no ability to distinguish a Pod created
from `DaemonSet` versus any other object. If/when that information is
available, the `kubelet` could pro-actively filter those Pods from the
candidate set of Pods provided to the eviction strategy.

In general, it is strongly recommended that `DaemonSet` not
create `BestEffort` Pods to avoid being identified as a candidate Pod
for eviction. Instead `DaemonSet` should ideally launch `Guaranteed` Pods.
-->
<h3 id="daemonset">DaemonSet</h3>
<p>我们永远都不希望 <code>kubelet</code> 驱逐一个从 <code>DaemonSet</code> 派生的 Pod，因为这个 Pod 将立即被重建并调度回相同的节点。</p>
<p>目前，<code>kubelet</code>没有办法区分一个 Pod 是由 <code>DaemonSet</code> 还是其他对象创建。
如果/当这个信息可用时，<code>kubelet</code> 可能会预先将这些 pod 从提供给驱逐策略的候选集合中过滤掉。</p>
<p>总之，强烈推荐 <code>DaemonSet</code> 不要创建 <code>BestEffort</code> 的 Pod，防止其被识别为驱逐的候选 Pod。
相反，理想情况下 <code>DaemonSet</code> 应该启动 <code>Guaranteed</code> 的 pod。</p>
<!--
## Deprecation of existing feature flags to reclaim disk

`kubelet` has been freeing up disk space on demand to keep the node stable.

As disk based eviction matures, the following `kubelet` flags are marked for deprecation
in favor of the simpler configuration supported around eviction.
-->
<h2 id="现有的回收磁盘特性标签已被弃用">现有的回收磁盘特性标签已被弃用</h2>
<p><code>kubelet</code> 已经按需求清空了磁盘空间以保证节点稳定性。</p>
<p>当磁盘驱逐成熟时，下面的 <code>kubelet</code> 标志将被标记为废弃的，以简化支持驱逐的配置。</p>
<!--
| Existing Flag | New Flag |
| ------------- | -------- |
| `--image-gc-high-threshold` | `--eviction-hard` or `eviction-soft` |
| `--image-gc-low-threshold` | `--eviction-minimum-reclaim` |
| `--maximum-dead-containers` | deprecated |
| `--maximum-dead-containers-per-container` | deprecated |
| `--minimum-container-ttl-duration` | deprecated |
| `--low-diskspace-threshold-mb` | `--eviction-hard` or `eviction-soft` |
| `--outofdisk-transition-frequency` | `--eviction-pressure-transition-period` |
-->
<table>
<thead>
<tr>
<th>现有标签</th>
<th>新标签</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--image-gc-high-threshold</code></td>
<td><code>--eviction-hard</code> or <code>eviction-soft</code></td>
</tr>
<tr>
<td><code>--image-gc-low-threshold</code></td>
<td><code>--eviction-minimum-reclaim</code></td>
</tr>
<tr>
<td><code>--maximum-dead-containers</code></td>
<td>deprecated</td>
</tr>
<tr>
<td><code>--maximum-dead-containers-per-container</code></td>
<td>deprecated</td>
</tr>
<tr>
<td><code>--minimum-container-ttl-duration</code></td>
<td>deprecated</td>
</tr>
<tr>
<td><code>--low-diskspace-threshold-mb</code></td>
<td><code>--eviction-hard</code> or <code>eviction-soft</code></td>
</tr>
<tr>
<td><code>--outofdisk-transition-frequency</code></td>
<td><code>--eviction-pressure-transition-period</code></td>
</tr>
</tbody>
</table>
<!--
## Known issues

The following sections describe known issues related to out of resource handling.
-->
<h2 id="已知问题">已知问题</h2>
<p>以下部分描述了与资源外处理有关的已知问题。</p>
<!--
### kubelet may not observe memory pressure right away

The `kubelet` currently polls `cAdvisor` to collect memory usage stats at a regular interval. If memory usage
increases within that window rapidly, the `kubelet` may not observe `MemoryPressure` fast enough, and the `OOMKiller`
will still be invoked. We intend to integrate with the `memcg` notification API in a future release to reduce this
latency, and instead have the kernel tell us when a threshold has been crossed immediately.
-->
<h3 id="kubelet-可能无法立即发现内存压力">kubelet 可能无法立即发现内存压力</h3>
<p><code>kubelet</code>当前通过以固定的时间间隔轮询 <code>cAdvisor</code> 来收集内存使用数据。如果内存使用在那个时间窗口内迅速增长，<code>kubelet</code>可能不能足够快的发现 <code>MemoryPressure</code>，<code>OOMKiller</code>将不会被调用。我们准备在将来的发行版本中通过集成 <code>memcg</code> 通知 API 来减小这种延迟。当超过阈值时，内核将立即告诉我们。</p>
<!--
If you are not trying to achieve extreme utilization, but a sensible measure of overcommit, a viable workaround for
this issue is to set eviction thresholds at approximately 75% capacity. This increases the ability of this feature
to prevent system OOMs, and promote eviction of workloads so cluster state can rebalance.
-->
<p>如果您想处理可察觉的超量使用而不要求极端精准，可以设置驱逐阈值为大约 75% 容量作为这个问题的变通手段。这将增强这个特性的能力，防止系统 OOM，并提升负载卸载能力，以再次平衡集群状态。</p>
<!--
### kubelet may evict more Pods than needed

The Pod eviction may evict more Pods than needed due to stats collection timing gap. This can be mitigated by adding
the ability to get root container stats on an on-demand basis [(https://github.com/google/cadvisor/issues/1247)](https://github.com/google/cadvisor/issues/1247) in the future.
-->
<h3 id="kubelet-可能会驱逐超过需求数量的-pod">kubelet 可能会驱逐超过需求数量的 pod</h3>
<p>由于状态采集的时间差，驱逐操作可能驱逐比所需的更多的 pod。将来可通过添加从根容器获取所需状态的能力
<a href="https://github.com/google/cadvisor/issues/1247">https://github.com/google/cadvisor/issues/1247</a>
来减缓这种状况。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a02f35804917d7a269c38d7e2c475005">2.41 - 限制存储消耗</h1>
    
	<!--
title: Limit Storage Consumption
content_type: task
-->
<!-- overview -->
<!--
This example demonstrates how to limit the amount of storage consumed in a namespace
-->
<p>此示例演示了如何限制一个名字空间中的存储使用量。</p>
<!--
The following resources are used in the demonstration: [ResourceQuota](/docs/concepts/policy/resource-quotas/),
[LimitRange](/docs/tasks/administer-cluster/memory-default-namespace/),
and [PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/).
-->
<p>演示中用到了以下资源：<a href="/zh/docs/concepts/policy/resource-quotas/">ResourceQuota</a>，
<a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">LimitRange</a> 和
<a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaim</a>。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!-- steps -->
<!--
## Scenario: Limiting Storage Consumption
-->
<h2 id="场景-限制存储消耗">场景：限制存储消耗</h2>
<!--
The cluster-admin is operating a cluster on behalf of a user population and the admin wants to control
how much storage a single namespace can consume in order to control cost.
-->
<p>集群管理员代表用户群操作集群，管理员希望控制单个名称空间可以消耗多少存储空间以控制成本。</p>
<!--
The admin would like to limit:
-->
<p>管理员想要限制：</p>
<!--
1. The number of persistent volume claims in a namespace
2. The amount of storage each claim can request
3. The amount of cumulative storage the namespace can have
-->
<ol>
<li>名字空间中持久卷申领（persistent volume claims）的数量</li>
<li>每个申领（claim）可以请求的存储量</li>
<li>名字空间可以具有的累计存储量</li>
</ol>
<!--
## LimitRange to limit requests for storage
-->
<h2 id="使用-limitrange-限制存储请求">使用 LimitRange 限制存储请求</h2>
<!--
Adding a `LimitRange` to a namespace enforces storage request sizes to a minimum and maximum. Storage is requested via `PersistentVolumeClaim`. The admission controller that enforces limit ranges will reject any PVC that is above or below the values set by the admin.
-->
<p>将 <code>LimitRange</code> 添加到名字空间会为存储请求大小强制设置最小值和最大值。
存储是通过 <code>PersistentVolumeClaim</code> 来发起请求的。
执行限制范围控制的准入控制器会拒绝任何高于或低于管理员所设阈值的 PVC。</p>
<!--
In this example, a PVC requesting 10Gi of storage would be rejected because it exceeds the 2Gi max.
-->
<p>在此示例中，请求 10Gi 存储的 PVC 将被拒绝，因为它超过了最大 2Gi。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>storagelimits<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">max</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>2Gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span></code></pre></div><!--
Minimum storage requests are used when the underlying storage provider requires certain minimums. For example,
AWS EBS volumes have a 1Gi minimum requirement.
-->
<p>当底层存储提供程序需要某些最小值时，将会用到所设置最小存储请求值。
例如，AWS EBS volumes 的最低要求为 1Gi。</p>
<!--
## StorageQuota to limit PVC count and cumulative storage capacity
-->
<h2 id="使用-storagequota-限制-pvc-数目和累计存储容量">使用 StorageQuota 限制 PVC 数目和累计存储容量</h2>
<!--
Admins can limit the number of PVCs in a namespace as well as the cumulative capacity of those PVCs. New PVCs that exceed
either maximum value will be rejected.
-->
<p>管理员可以限制某个名字空间中的 PVCs 个数以及这些 PVCs 的累计容量。
新 PVCs 请求如果超过任一上限值将被拒绝。</p>
<!--
In this example, a 6th PVC in the namespace would be rejected because it exceeds the maximum count of 5. Alternatively,
a 5Gi maximum quota when combined with the 2Gi max limit above, cannot have 3 PVCs where each has 2Gi. That would be 6Gi requested
 for a namespace capped at 5Gi.
-->
<p>在此示例中，名字空间中的第 6 个 PVC 将被拒绝，因为它超过了最大计数 5。
或者，当与上面的 2Gi 最大容量限制结合在一起时，意味着 5Gi 的最大配额
不能支持 3 个都是 2Gi 的 PVC。
后者实际上是向名字空间请求 6Gi 容量，而该命令空间已经设置上限为 5Gi。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>storagequota<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">persistentvolumeclaims</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests.storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5Gi&#34;</span><span style="color:#bbb">
</span></code></pre></div><!-- discussion -->
<!--
## Summary

A limit range can put a ceiling on how much storage is requested while a resource quota can effectively cap the storage consumed by a namespace through claim counts and cumulative storage capacity. The allows a cluster-admin to plan their
cluster's storage budget without risk of any one project going over their allotment.
-->
<h2 id="小结">小结</h2>
<p>限制范围对象可以用来设置可请求的存储量上限，而资源配额对象则可以通过申领计数和
累计存储容量有效地限制名字空间耗用的存储量。
这两种机制使得集群管理员能够规划其集群存储预算而不会发生任一项目超量分配的风险。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6b4e7ca6586f448c8533a120c29bdd25">2.42 - 静态加密 Secret 数据</h1>
    
	<!--
reviewers:
- smarterclayton
title: Encrypting Secret Data at Rest
content_type: task
-->
<!-- overview -->
<!--
This page shows how to enable and configure encryption of secret data at rest.
-->
<p>本文展示如何启用和配置静态 Secret 数据的加密</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!--
* etcd v3 or later is required
-->
<ul>
<li>需要 etcd v3 或者更高版本</li>
</ul>
<!-- steps -->
<!--
## Configuration and determining whether encryption at rest is already enabled

The `kube-apiserver` process accepts an argument `-experimental-encryption-provider-config`
that controls how API data is encrypted in etcd. An example configuration
is provided below.

## Understanding the encryption at rest configuration.
-->
<h2 id="配置并确定是否已启用静态数据加密">配置并确定是否已启用静态数据加密</h2>
<p><code>kube-apiserver</code> 的参数 <code>--experimental-encryption-provider-config</code> 控制 API 数据在 etcd 中的加密方式。
下面提供一个配置示例。</p>
<h2 id="理解静态数据加密">理解静态数据加密</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aesgcm</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key2<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>dGhpcyBpcyBwYXNzd29yZA==<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>c2VjcmV0IGlzIHNlY3VyZQ==<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key2<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>dGhpcyBpcyBwYXNzd29yZA==<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">secretbox</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=<span style="color:#bbb">
</span></code></pre></div><!--
Each `resources` array item is a separate config and contains a complete configuration. The
`resources.resources` field is an array of Kubernetes resource names (`resource` or `resource.group`)
that should be encrypted. The `providers` array is an ordered list of the possible encryption
providers. Only one provider type may be specified per entry (`identity` or `aescbc` may be provided,
but not both in the same item).
-->
<p>每个 <code>resources</code> 数组项目是一个单独的完整的配置。
<code>resources.resources</code> 字段是要加密的 Kubernetes 资源名称（<code>resource</code> 或 <code>resource.group</code>）的数组。
<code>providers</code> 数组是可能的加密 provider 的有序列表。
每个条目只能指定一个 provider 类型（可以是 <code>identity</code> 或 <code>aescbc</code>，但不能在同一个项目中同时指定）。</p>
<!--
The first provider in the list is used to encrypt resources going into storage. When reading
resources from storage each provider that matches the stored data attempts to decrypt the data in
order. If no provider can read the stored data due to a mismatch in format or secret key, an error
is returned which prevents clients from accessing that resource.
-->
<p>列表中的第一个 provider 用于加密进入存储的资源。
当从存储器读取资源时，与存储的数据匹配的所有 provider 将按顺序尝试解密数据。
如果由于格式或密钥不匹配而导致没有 provider 能够读取存储的数据，则会返回一个错误，以防止客户端访问该资源。</p>
<!--
**IMPORTANT:** If any resource is not readable via the encryption config (because keys were changed),
the only recourse is to delete that key from the underlying etcd directly. Calls that attempt to
read that resource will fail until it is deleted or a valid decryption key is provided.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> <strong>重要：</strong> 如果通过加密配置无法读取资源（因为密钥已更改），唯一的方法是直接从底层 etcd 中删除该密钥。
任何尝试读取资源的调用将会失败，直到它被删除或提供有效的解密密钥。</div>
</blockquote>

<h3 id="providers">Providers:</h3>
<!--
Name | Encryption | Strength | Speed | Key Length | Other Considerations
-----|------------|----------|-------|------------|---------------------
`identity` | None | N/A | N/A | N/A | Resources written as-is without encryption. When set as the first provider, the resource will be decrypted as new values are written.
`aescbc` | AES-CBC with PKCS#7 padding | Strongest | Fast | 32-byte | The recommended choice for encryption at rest but may be slightly slower than `secretbox`.
`secretbox` | XSalsa20 and Poly1305 | Strong | Faster | 32-byte | A newer standard and may not be considered acceptable in environments that require high levels of review.
`aesgcm` | AES-GCM with random nonce | Must be rotated every 200k writes | Fastest | 16, 24, or 32-byte | Is not recommended for use except when an automated key rotation scheme is implemented.
`kms` | Uses envelope encryption scheme: Data is encrypted by data encryption keys (DEKs) using AES-CBC with PKCS#7 padding, DEKs are encrypted by key encryption keys (KEKs) according to configuration in Key Management Service (KMS) | Strongest | Fast | 32-bytes |  The recommended choice for using a third party tool for key management. Simplifies key rotation, with a new DEK generated for each encryption, and KEK rotation controlled by the user. [Configure the KMS provider](/docs/tasks/administer-cluster/kms-provider/)

Each provider supports multiple keys - the keys are tried in order for decryption, and if the provider
is the first provider, the first key is used for encryption.
-->





<table><caption style="display: none;">Kubernetes 静态数据加密的 Providers</caption>
<thead>
<tr>
<th>名称</th>
<th>加密类型</th>
<th>强度</th>
<th>速度</th>
<th>密钥长度</th>
<th>其它事项</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>identity</code></td>
<td>无</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>不加密写入的资源。当设置为第一个 provider 时，资源将在新值写入时被解密。</td>
</tr>
<tr>
<td><code>aescbc</code></td>
<td>填充 PKCS#7 的 AES-CBC</td>
<td>最强</td>
<td>快</td>
<td>32字节</td>
<td>建议使用的加密项，但可能比 <code>secretbox</code> 稍微慢一些。</td>
</tr>
<tr>
<td><code>secretbox</code></td>
<td>XSalsa20 和 Poly1305</td>
<td>强</td>
<td>更快</td>
<td>32字节</td>
<td>较新的标准，在需要高度评审的环境中可能不被接受。</td>
</tr>
<tr>
<td><code>aesgcm</code></td>
<td>带有随机数的 AES-GCM</td>
<td>必须每 200k 写入一次</td>
<td>最快</td>
<td>16, 24 或者 32字节</td>
<td>建议不要使用，除非实施了自动密钥循环方案。</td>
</tr>
<tr>
<td><code>kms</code></td>
<td>使用信封加密方案：数据使用带有 PKCS#7 填充的 AES-CBC 通过数据加密密钥（DEK）加密，DEK 根据 Key Management Service（KMS）中的配置通过密钥加密密钥（Key Encryption Keys，KEK）加密</td>
<td>最强</td>
<td>快</td>
<td>32字节</td>
<td>建议使用第三方工具进行密钥管理。为每个加密生成新的 DEK，并由用户控制 KEK 轮换来简化密钥轮换。<a href="/zh/docs/tasks/administer-cluster/kms-provider/">配置 KMS 提供程序</a></td>
</tr>
</tbody>
</table>
<p>每个 provider 都支持多个密钥 - 在解密时会按顺序使用密钥，如果是第一个 provider，则第一个密钥用于加密。</p>
<!--
__Storing the raw encryption key in the EncryptionConfig only moderately improves your security posture, compared to no encryption.
Please use `kms` provider for additional security.__ By default, the `identity` provider is used to protect secrets in etcd, which
provides no encryption. `EncryptionConfiguration` was introduced to encrypt secrets locally, with a locally managed key.
-->
<p><strong>在 EncryptionConfig 中保存原始的加密密钥与不加密相比只会略微地提升安全级别。
请使用 <code>kms</code> 驱动以获得更强的安全性。</strong>
默认情况下，<code>identity</code> 驱动被用来对 etcd 中的 Secret 提供保护，
而这个驱动不提供加密能力。
<code>EncryptionConfiguration</code> 的引入是为了能够使用本地管理的密钥来在本地加密 Secret 数据。</p>
<!--
Encrypting secrets with a locally managed key protects against an etcd compromise, but it fails to protect against a host compromise.
Since the encryption keys are stored on the host in the EncryptionConfig YAML file, a skilled attacker can access that file and
extract the encryption keys.
-->
<p>使用本地管理的密钥来加密 Secret 能够保护数据免受 etcd 破坏的影响，不过无法针对
主机被侵入提供防护。
这是因为加密的密钥保存在主机上的 EncryptionConfig YAML 文件中，有经验的入侵者
仍能访问该文件并从中提取出加密密钥。</p>
<!--
Envelope encryption creates dependence on a separate key, not stored in Kubernetes. In this case, an attacker would need to compromise etcd, the kubeapi-server, and the third-party KMS provider to retrieve the plaintext values, providing a higher level of security than locally-stored encryption keys.
-->
<p>封套加密（Envelope Encryption）引入了对独立密钥的依赖，而这个密钥并不保存在 Kubernetes 中。
在这种情况下下，入侵者需要攻破 etcd、kube-apiserver 和第三方的 KMS
驱动才能获得明文数据，因而这种方案提供了比本地保存加密密钥更高的安全级别。</p>
<!--
## Encrypting your data

Create a new encryption config file:
-->
<h2 id="加密你的数据">加密你的数据</h2>
<p>创建一个新的加密配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div><!--
To create a new secret perform the following steps:

1. Generate a 32 byte random key and base64 encode it. If you're on Linux or Mac OS X, run the following command:
-->
<p>遵循如下步骤来创建一个新的 secret：</p>
<ol>
<li>
<p>生成一个 32 字节的随机密钥并进行 base64 编码。如果你在 Linux 或 Mac OS X 上，请运行以下命令：</p>
<pre tabindex="0"><code>head -c 32 /dev/urandom | base64
</code></pre></li>
</ol>
<!--
2. Place that value in the secret field.
3. Set the `--experimental-encryption-provider-config` flag on the `kube-apiserver` to point to the location of the config file.
4. Restart your API server.

**IMPORTANT:** Your config file contains keys that can decrypt content in etcd, so you must properly restrict permissions on your masters so only the user who runs the kube-apiserver can read it.
-->
<ol start="2">
<li>将这个值放入到 secret 字段中。</li>
<li>设置 <code>kube-apiserver</code> 的 <code>--experimental-encryption-provider-config</code> 参数，将其指向
配置文件所在位置。</li>
<li>重启你的 API server。</li>
</ol>
<blockquote class="caution callout">
  <div><strong>注意：</strong> 你的配置文件包含可以解密 etcd 内容的密钥，因此你必须正确限制主控节点的访问权限，
以便只有能运行 kube-apiserver 的用户才能读取它。</div>
</blockquote>
<!--
## Verifying that data is encrypted

Data is encrypted when written to etcd. After restarting your `kube-apiserver`, any newly created or
updated secret should be encrypted when stored. To check, you can use the `etcdctl` command line
program to retrieve the contents of your secret.

1. Create a new secret called `secret1` in the `default` namespace:
-->
<h2 id="验证数据已被加密">验证数据已被加密</h2>
<p>数据在写入 etcd 时会被加密。重新启动你的 <code>kube-apiserver</code> 后，任何新创建或更新的密码在存储时都应该被加密。
如果想要检查，你可以使用 <code>etcdctl</code> 命令行程序来检索你的加密内容。</p>
<ol>
<li>
<p>创建一个新的 secret，名称为 <code>secret1</code>，命名空间为 <code>default</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic secret1 -n default --from-literal<span style="color:#666">=</span><span style="color:#b8860b">mykey</span><span style="color:#666">=</span>mydata
</code></pre></div></li>
</ol>
<!--
2. Using the etcdctl commandline, read that secret out of etcd:
-->
<ol start="2">
<li>
<p>使用 etcdctl 命令行，从 etcd 中读取 secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">ETCDCTL_API</span><span style="color:#666">=</span><span style="color:#666">3</span> etcdctl get /registry/secrets/default/secret1 <span style="color:#666">[</span>...<span style="color:#666">]</span> | hexdump -C
</code></pre></div><!--
where `[...]` must be the additional arguments for connecting to the etcd server.
-->
<p>这里的 <code>[...]</code> 是用来连接 etcd 服务的额外参数。</p>
</li>
</ol>
<!--
3. Verify the stored secret is prefixed with `k8s:enc:aescbc:v1:` which indicates the `aescbc` provider has encrypted the resulting data.
4. Verify the secret is correctly decrypted when retrieved via the API:
-->
<ol start="3">
<li>
<p>验证存储的密钥前缀是否为 <code>k8s:enc:aescbc:v1:</code>，这表明 <code>aescbc</code> provider 已加密结果数据。</p>
</li>
<li>
<p>通过 API 检索，验证 secret 是否被正确解密：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe secret secret1 -n default
</code></pre></div><!--
should match `mykey: mydata`, mydata is encoded, check [decoding a secret](/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret) to
completely decode the secret.
-->
<p>其输出应该是 <code>mykey: bXlkYXRh</code>，<code>mydata</code> 数据是被加密过的，请参阅
<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret">解密 Secret</a>
了解如何完全解码 Secret 内容。</p>
</li>
</ol>
<!--
## Ensure all secrets are encrypted

Since secrets are encrypted on write, performing an update on a secret will encrypt that content.
-->
<h2 id="确保所有-secret-都被加密">确保所有 Secret 都被加密</h2>
<p>由于 Secret 是在写入时被加密，因此对 Secret 执行更新也会加密该内容。</p>
<pre tabindex="0"><code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -
</code></pre><!--
The command above reads all secrets and then updates them to apply server side encryption.
-->
<p>上面的命令读取所有 Secret，然后使用服务端加密来更新其内容。</p>
<!--
If an error occurs due to a conflicting write, retry the command.
For larger clusters, you may wish to subdivide the secrets by namespace or script an update.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果由于冲突写入而发生错误，请重试该命令。
对于较大的集群，你可能希望通过命名空间或更新脚本来对 Secret 进行划分。</div>
</blockquote>
<!--
## Rotating a decryption key

Changing the secret without incurring downtime requires a multi step operation, especially in
the presence of a highly available deployment where multiple `kube-apiserver` processes are running.

1. Generate a new key and add it as the second key entry for the current provider on all servers
2. Restart all `kube-apiserver` processes to ensure each server can decrypt using the new key
3. Make the new key the first entry in the `keys` array so that it is used for encryption in the config
4. Restart all `kube-apiserver` processes to ensure each server now encrypts using the new key
5. Run `kubectl get secrets -all-namespaces -o json | kubectl replace -f -` to encrypt all existing secrets with the new key
6. Remove the old decryption key from the config after you back up etcd with the new key in use and update all secrets

With a single `kube-apiserver`, step 2 may be skipped.
-->
<h2 id="轮换解密密钥">轮换解密密钥</h2>
<p>在不发生停机的情况下更改 Secret 需要多步操作，特别是在有多个 <code>kube-apiserver</code> 进程正在运行的
高可用环境中。</p>
<ol>
<li>生成一个新密钥并将其添加为所有服务器上当前提供程序的第二个密钥条目</li>
<li>重新启动所有 <code>kube-apiserver</code> 进程以确保每台服务器都可以使用新密钥进行解密</li>
<li>将新密钥设置为 <code>keys</code> 数组中的第一个条目，以便在配置中使用其进行加密</li>
<li>重新启动所有 <code>kube-apiserver</code> 进程以确保每个服务器现在都使用新密钥进行加密</li>
<li>运行 <code>kubectl get secrets --all-namespaces -o json | kubectl replace -f -</code> 以用新密钥加密所有现有的秘密</li>
<li>在使用新密钥备份 etcd 后，从配置中删除旧的解密密钥并更新所有密钥</li>
</ol>
<p>如果只有一个 <code>kube-apiserver</code>，第 2 步可能可以忽略。</p>
<!--
## Decrypting all data

To disable encryption at rest place the `identity` provider as the first entry in the config:
-->
<h2 id="解密所有数据">解密所有数据</h2>
<p>要禁用 rest 加密，请将 <code>identity</code> provider 作为配置中的第一个条目：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.config.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EncryptionConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- secrets<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">identity</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">aescbc</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">keys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>key1<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb"> </span>&lt;BASE 64 ENCODED SECRET&gt;<span style="color:#bbb">
</span></code></pre></div><!--
and restart all `kube-apiserver` processes. Then run
-->
<p>并重新启动所有 <code>kube-apiserver</code> 进程。然后运行：</p>
<pre tabindex="0"><code>kubectl get secrets -all-namespaces -o json | kubectl replace -f -`
</code></pre><!--
to force all secrets to be decrypted.
-->
<p>以强制解密所有 secret。</p>


</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f5da33b976758a9183018c421eb83f58">3 - 配置 Pods 和容器</h1>
    <div class="lead">对 Pod 和容器执行常见的配置任务。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-e6dd9300cf3a955f7cdfe77fb5d15292">3.1 - 为容器和 Pod 分配内存资源</h1>
    
	<!--
title: Assign Memory Resources to Containers and Pods
content_type: task
weight: 10
-->
<!-- overview -->
<!--
This page shows how to assign a memory *request* and a memory *limit* to a
Container. A Container is guaranteed to have as much memory as it requests,
but is not allowed to use more memory than its limit.
-->
<p>此页面展示如何将内存 <em>请求</em> （request）和内存 <em>限制</em> （limit）分配给一个容器。
我们保障容器拥有它请求数量的内存，但不允许使用超过限制数量的内存。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 300 MiB of memory.
-->
<p>你集群中的每个节点必须拥有至少 300 MiB 的内存。</p>
<!--
A few of the steps on this page require you to run the
[metrics-server](https://github.com/kubernetes-sigs/metrics-server)
service in your cluster. If you have the metrics-server
running, you can skip those steps.
-->
<p>该页面上的一些步骤要求你在集群中运行
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a> 服务。
如果你已经有在运行中的 metrics-server，则可以跳过这些步骤。</p>
<!--
If you are running Minikube, run the following command to enable the
metrics-server:
-->
<p>如果你运行的是 Minikube，可以运行下面的命令启用 metrics-server：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube addons <span style="color:#a2f">enable</span> metrics-server
</code></pre></div><!--
To see whether the metrics-server is running, or another provider of the resource metrics
API (`metrics.k8s.io`), run the following command:
-->
<p>要查看 metrics-server 或资源指标 API (<code>metrics.k8s.io</code>) 是否已经运行，请运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get apiservices
</code></pre></div><!--
If the resource metrics API is available, the output includes a
reference to `metrics.k8s.io`.
-->
<p>如果资源指标 API 可用，则输出结果将包含对 <code>metrics.k8s.io</code> 的引用信息。</p>
<pre tabindex="0"><code>NAME
v1beta1.metrics.k8s.io
</code></pre><!-- steps -->
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便将本练习中创建的资源与集群的其余部分隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace mem-example
</code></pre></div><!--
## Specify a memory request and a memory limit

To specify a memory request for a Container, include the `resources:requests` field
in the Container's resource manifest. To specify a memory limit, include `resources:limits`.

In this exercise, you create a Pod that has one Container. The Container has a memory
request of 100 MiB and a memory limit of 200 MiB. Here's the configuration file
for the Pod:
-->
<h2 id="指定内存请求和限制">指定内存请求和限制</h2>
<p>要为容器指定内存请求，请在容器资源清单中包含 <code>resources：requests</code> 字段。
同理，要指定内存限制，请包含 <code>resources：limits</code>。</p>
<p>在本练习中，你将创建一个拥有一个容器的 Pod。
容器将会请求 100 MiB 内存，并且内存会被限制在 200 MiB 以内。
这是 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/memory-request-limit.yaml" download="pods/resource/memory-request-limit.yaml"><code>pods/resource/memory-request-limit.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-memory-request-limit-yaml')" title="Copy pods/resource/memory-request-limit.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-memory-request-limit-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>mem-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>polinux/stress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;stress&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;--vm&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;--vm-bytes&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;150M&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;--vm-hang&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
The `args` section in the configuration file provides arguments for the Container when it starts.
The `"--vm-bytes", "150M"` arguments tell the Container to attempt to allocate 150 MiB of memory.

Create the Pod:
-->
<p>配置文件的 <code>args</code> 部分提供了容器启动时的参数。
<code>&quot;--vm-bytes&quot;, &quot;150M&quot;</code> 参数告知容器尝试分配 150 MiB 内存。</p>
<p>开始创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit.yaml --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
Verify that the Pod Container is running:
-->
<p>验证 Pod 中的容器是否已运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod memory-demo --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 相关的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod memory-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
The output shows that the one Container in the Pod has a memory request of 100 MiB
and a memory limit of 200 MiB.
-->
<p>输出结果显示：该 Pod 中容器的内存请求为 100 MiB，内存限制为 200 MiB。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
Run `kubectl top` to fetch the metrics for the pod:
-->
<p>运行 <code>kubectl top</code> 命令，获取该 Pod 的指标数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl top pod memory-demo --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
The output shows that the Pod is using about 162,900,000 bytes of memory, which
is about 150 MiB. This is greater than the Pod's 100 MiB request, but within the
Pod's 200 MiB limit.
-->
<p>输出结果显示：Pod 正在使用的内存大约为 162,900,000 字节，约为 150 MiB。
这大于 Pod 请求的 100 MiB，但在 Pod 限制的 200 MiB之内。</p>
<pre tabindex="0"><code>NAME                        CPU(cores)   MEMORY(bytes)
memory-demo                 &lt;something&gt;  162856960
</code></pre><!--
Delete your Pod:
-->
<p>删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod memory-demo --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
## Exceed a Container's memory limit

A Container can exceed its memory request if the Node has memory available. But a Container
is not allowed to use more than its memory limit. If a Container allocates more memory than
its limit, the Container becomes a candidate for termination. If the Container continues to
consume memory beyond its limit, the Container is terminated. If a terminated Container can be
restarted, the kubelet restarts it, as with any other type of runtime failure.
-->
<h2 id="超过容器限制的内存">超过容器限制的内存</h2>
<p>当节点拥有足够的可用内存时，容器可以使用其请求的内存。
但是，容器不允许使用超过其限制的内存。
如果容器分配的内存超过其限制，该容器会成为被终止的候选容器。
如果容器继续消耗超出其限制的内存，则终止容器。
如果终止的容器可以被重启，则 kubelet 会重新启动它，就像其他任何类型的运行时失败一样。</p>
<!--
In this exercise, you create a Pod that attempts to allocate more memory than its limit.
Here is the configuration file for a Pod that has one Container with a
memory request of 50 MiB and a memory limit of 100 MiB:
-->
<p>在本练习中，你将创建一个 Pod，尝试分配超出其限制的内存。
这是一个 Pod 的配置文件，其拥有一个容器，该容器的内存请求为 50 MiB，内存限制为 100 MiB：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/memory-request-limit-2.yaml" download="pods/resource/memory-request-limit-2.yaml"><code>pods/resource/memory-request-limit-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-memory-request-limit-2-yaml')" title="Copy pods/resource/memory-request-limit-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-memory-request-limit-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory-demo-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>mem-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>polinux/stress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;50Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;stress&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;--vm&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;--vm-bytes&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;250M&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;--vm-hang&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the `args` section of the configuration file, you can see that the Container
will attempt to allocate 250 MiB of memory, which is well above the 100 MiB limit.

Create the Pod:
-->
<p>在配置文件的 <code>args</code> 部分中，你可以看到容器会尝试分配 250 MiB 内存，这远高于 100 MiB 的限制。</p>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-2.yaml --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 相关的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod memory-demo-2 --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
At this point, the Container might be running or killed. Repeat the preceding command until the Container is killed:
-->
<p>此时，容器可能正在运行或被杀死。重复前面的命令，直到容器被杀掉：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME            READY     STATUS      RESTARTS   AGE
memory-demo-2   0/1       OOMKilled   <span style="color:#666">1</span>          24s
</code></pre></div><!--
Get a more detailed view of the Container status:
-->
<p>获取容器更详细的状态信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod memory-demo-2 --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
The output shows that the Container was killed because it is out of memory (OOM):
-->
<p>输出结果显示：由于内存溢出（OOM），容器已被杀掉：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">lastState:
   terminated:
     containerID: docker://65183c1877aaec2e8427bc95609cc52677a454b56fcb24340dbd22917c23b10f
     exitCode: <span style="color:#666">137</span>
     finishedAt: 2017-06-20T20:52:19Z
     reason: OOMKilled
     startedAt: null
</code></pre></div><!--
The Container in this exercise can be restarted, so the kubelet restarts it. Repeat
this command several times to see that the Container is repeatedly killed and restarted:
-->
<p>本练习中的容器可以被重启，所以 kubelet 会重启它。
多次运行下面的命令，可以看到容器在反复的被杀死和重启：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod memory-demo-2 --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
The output shows that the Container is killed, restarted, killed again, restarted again, and so on:
-->
<p>输出结果显示：容器被杀掉、重启、再杀掉、再重启……：</p>
<pre tabindex="0"><code>kubectl get pod memory-demo-2 --namespace=mem-example
NAME            READY     STATUS      RESTARTS   AGE
memory-demo-2   0/1       OOMKilled   1          37s
</code></pre><pre tabindex="0"><code>
kubectl get pod memory-demo-2 --namespace=mem-example
NAME            READY     STATUS    RESTARTS   AGE
memory-demo-2   1/1       Running   2          40s
</code></pre><!--
View detailed information about the Pod history:
-->
<p>查看关于该 Pod 历史的详细信息：</p>
<pre tabindex="0"><code>kubectl describe pod memory-demo-2 --namespace=mem-example
</code></pre><!--
The output shows that the Container starts and fails repeatedly:
-->
<p>输出结果显示：该容器反复的在启动和失败：</p>
<pre tabindex="0"><code>... Normal  Created   Created container with id 66a3a20aa7980e61be4922780bf9d24d1a1d8b7395c09861225b0eba1b1f8511
... Warning BackOff   Back-off restarting failed container
</code></pre><!--
View detailed information about your cluster's Nodes:
-->
<p>查看关于集群节点的详细信息：</p>
<pre tabindex="0"><code>kubectl describe nodes
</code></pre><!--
The output includes a record of the Container being killed because of an out-of-memory condition:
-->
<p>输出结果包含了一条练习中的容器由于内存溢出而被杀掉的记录：</p>
<pre tabindex="0"><code>Warning OOMKilling Memory cgroup out of memory: Kill process 4481 (stress) score 1994 or sacrifice child
</code></pre><!--
Delete your Pod:
-->
<p>删除 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod memory-demo-2 --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
## Specify a memory request that is too big for your Nodes

Memory requests and limits are associated with Containers, but it is useful to think
of a Pod as having a memory request and limit. The memory request for the Pod is the
sum of the memory requests for all the Containers in the Pod. Likewise, the memory
limit for the Pod is the sum of the limits of all the Containers in the Pod.
-->
<h2 id="超过整个节点容量的内存">超过整个节点容量的内存</h2>
<p>内存请求和限制是与容器关联的，但将 Pod 视为具有内存请求和限制，也是很有用的。
Pod 的内存请求是 Pod 中所有容器的内存请求之和。
同理，Pod 的内存限制是 Pod 中所有容器的内存限制之和。</p>
<!--
Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if the Node
has enough available memory to satisfy the Pod's memory request.

In this exercise, you create a Pod that has a memory request so big that it exceeds the
capacity of any Node in your cluster. Here is the configuration file for a Pod that has one
Container with a request for 1000 GiB of memory, which likely exceeds the capacity
of any Node in your cluster.
-->
<p>Pod 的调度基于请求。只有当节点拥有足够满足 Pod 内存请求的内存时，才会将 Pod 调度至节点上运行。</p>
<p>在本练习中，你将创建一个 Pod，其内存请求超过了你集群中的任意一个节点所拥有的内存。
这是该 Pod 的配置文件，其拥有一个请求 1000 GiB 内存的容器，这应该超过了你集群中任何节点的容量。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/memory-request-limit-3.yaml" download="pods/resource/memory-request-limit-3.yaml"><code>pods/resource/memory-request-limit-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-memory-request-limit-3-yaml')" title="Copy pods/resource/memory-request-limit-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-memory-request-limit-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory-demo-3<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>mem-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>memory-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>polinux/stress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1000Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1000Gi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;stress&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;--vm&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;--vm-bytes&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;150M&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;--vm-hang&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-3.yaml --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
View the Pod status:
-->
<p>查看 Pod 状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod memory-demo-3 --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
The output shows that the Pod status is PENDING. That is, the Pod is not scheduled to run on any Node, and it will remain in the PENDING state indefinitely:
-->
<p>输出结果显示：Pod 处于 PENDING 状态。
这意味着，该 Pod 没有被调度至任何节点上运行，并且它会无限期的保持该状态：</p>
<pre tabindex="0"><code>kubectl get pod memory-demo-3 --namespace=mem-example
NAME            READY     STATUS    RESTARTS   AGE
memory-demo-3   0/1       Pending   0          25s
</code></pre><!--
View detailed information about the Pod, including events:
-->
<p>查看关于 Pod 的详细信息，包括事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod memory-demo-3 --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
The output shows that the Container cannot be scheduled because of insufficient memory on the Nodes:
-->
<p>输出结果显示：由于节点内存不足，该容器无法被调度：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Events:
  ...  Reason            Message
       ------            -------
  ...  FailedScheduling  No nodes are available that match all of the following predicates:: Insufficient memory <span style="color:#666">(</span>3<span style="color:#666">)</span>.
</code></pre></div><!--
## Memory units

The memory resource is measured in bytes. You can express memory as a plain integer or a
fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki.
For example, the following represent approximately the same value:
-->
<h2 id="内存单位">内存单位</h2>
<p>内存资源的基本单位是字节（byte）。你可以使用这些后缀之一，将内存表示为
纯整数或定点整数：E、P、T、G、M、K、Ei、Pi、Ti、Gi、Mi、Ki。
例如，下面是一些近似相同的值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">128974848, 129e6, 129M , 123Mi
</code></pre></div><!--
Delete your Pod:
-->
<p>删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod memory-demo-3 --namespace<span style="color:#666">=</span>mem-example
</code></pre></div><!--
## If you do not specify a memory limit

If you do not specify a memory limit for a Container, one of the following situations applies:
-->
<h2 id="如果你没有指定内存限制">如果你没有指定内存限制</h2>
<p>如果你没有为一个容器指定内存限制，则自动遵循以下情况之一：</p>
<!--
* The Container has no upper bound on the amount of memory it uses. The Container
could use all of the memory available on the Node where it is running which in turn could invoke the OOM Killer. Further, in case of an OOM Kill, a container with no resource limits will have a greater chance of being killed.

* The Container is running in a namespace that has a default memory limit, and the
Container is automatically assigned the default limit. Cluster administrators can use a
[LimitRange](/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core)
to specify a default value for the memory limit.
-->
<ul>
<li>
<p>容器可无限制地使用内存。容器可以使用其所在节点所有的可用内存，
进而可能导致该节点调用 OOM Killer。
此外，如果发生 OOM Kill，没有资源限制的容器将被杀掉的可行性更大。</p>
</li>
<li>
<p>运行的容器所在命名空间有默认的内存限制，那么该容器会被自动分配默认限制。
集群管理员可用使用 <a href="/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core">LimitRange</a>
来指定默认的内存限制。</p>
</li>
</ul>
<!--
## Motivation for memory requests and limits

By configuring memory requests and limits for the Containers that run in your
cluster, you can make efficient use of the memory resources available on your cluster's
Nodes. By keeping a Pod's memory request low, you give the Pod a good chance of being
scheduled. By having a memory limit that is greater than the memory request, you accomplish two things:
-->
<h2 id="内存请求和限制的目的">内存请求和限制的目的</h2>
<p>通过为集群中运行的容器配置内存请求和限制，你可以有效利用集群节点上可用的内存资源。
通过将 Pod 的内存请求保持在较低水平，你可以更好地安排 Pod 调度。
通过让内存限制大于内存请求，你可以完成两件事：</p>
<!--
* The Pod can have bursts of activity where it makes use of memory that happens to be available.
* The amount of memory a Pod can use during a burst is limited to some reasonable amount.
-->
<ul>
<li>Pod 可以进行一些突发活动，从而更好的利用可用内存。</li>
<li>Pod 在突发活动期间，可使用的内存被限制为合理的数量。</li>
</ul>
<!--
## Clean up

Delete your namespace. This deletes all the Pods that you created for this task:
-->
<h2 id="清理">清理</h2>
<p>删除命名空间。下面的命令会删除你根据这个任务创建的所有 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace mem-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For app developers

* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)

* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)
-->
<h3 id="应用开发者扩展阅读">应用开发者扩展阅读</h3>
<ul>
<li>
<p><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">配置 Pod 的服务质量</a></p>
</li>
</ul>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/)

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/)

* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/)

* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)
-->
<h3 id="集群管理员扩展阅读">集群管理员扩展阅读</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认的内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为命名空间配置默认的 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">配置命名空间的最小和最大内存约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">配置命名空间的最小和最大 CPU 约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">配置命名空间下 Pod 总数</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">配置 API 对象配额</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-aa522472483f900008124a2809f2114b">3.2 - 为 Windows Pod 和容器配置 GMSA</h1>
    
	<!--
title: Configure GMSA for Windows Pods and containers
content_type: task
weight: 20
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>

<!--
This page shows how to configure [Group Managed Service Accounts](https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/group-managed-service-accounts-overview) (GMSA) for Pods and containers that will run on Windows nodes. Group Managed Service Accounts are a specific type of Active Directory account that provides automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.
-->
<p>本页展示如何为将运行在 Windows 节点上的 Pod 和容器配置
<a href="https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/group-managed-service-accounts-overview">组管理的服务账号（Group Managed Service Accounts，GMSA）</a>。
组管理的服务账号是活动目录（Active Directory）的一种特殊类型，提供自动化的
密码管理、简化的服务主体名称（Service Principal Name，SPN）管理以及跨多个
服务器将管理操作委派给其他管理员等能力。</p>
<!--
In Kubernetes, GMSA credential specs are configured at a Kubernetes cluster-wide scope as Custom Resources. Windows Pods, as well as individual containers within a Pod, can be configured to use a GMSA for domain based functions (e.g. Kerberos authentication) when interacting with other Windows services. As of v1.16, the Docker runtime supports GMSA for Windows workloads.
-->
<p>在 Kubernetes 环境中，GMSA 凭据规约配置为 Kubernetes 集群范围的自定义资源
（Custom Resources）形式。Windows Pod 以及各 Pod 中的每个容器可以配置为
使用 GMSA 来完成基于域（Domain）的操作（例如，Kerberos 身份认证），以便
与其他 Windows 服务相交互。自 Kubernetes 1.16 版本起，Docker 运行时为
Windows 负载支持 GMSA。</p>
<h2 id="准备开始">准备开始</h2>
<!--
You need to have a Kubernetes cluster and the `kubectl` command-line tool must be configured to communicate with your cluster. The cluster is expected to have Windows worker nodes. This section covers a set of initial steps required once for each cluster:
-->
<p>你需要一个 Kubernetes 集群，以及 <code>kubectl</code> 命令行工具，且工具必须已配置
为能够与你的集群通信。集群预期包含 Windows 工作节点。
本节讨论需要为每个集群执行一次的初始操作。</p>
<!--
### Install the GMSACredentialSpec CRD

A [CustomResourceDefinition](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)(CRD) for GMSA credential spec resources needs to be configured on the cluster to define the custom resource type `GMSACredentialSpec`. Download the GMSA CRD [YAML](https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/gmsa-crd.yml) and save it as gmsa-crd.yaml.
Next, install the CRD with `kubectl apply -f gmsa-crd.yaml`
-->
<h3 id="安装-gmsacredentialspec-crd">安装 GMSACredentialSpec CRD</h3>
<p>你需要在集群上配置一个用于 GMSA 凭据规约资源的
<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CustomResourceDefinition</a>(CRD)，
以便定义类型为 <code>GMSACredentialSpec</code> 的自定义资源。
首先下载 GMSA CRD <a href="https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/gmsa-crd.yml">YAML</a>
并将其保存为 <code>gmsa-crd.yaml</code>。接下来执行 <code>kubectl apply -f gmsa-crd.yaml</code>
安装 CRD。</p>
<!--
### Install webhooks to validate GMSA users
Two webhooks need to be configured on the Kubernetes cluster to populate and validate GMSA credential spec references at the Pod or container level:

1. A mutating webhook that expands references to GMSAs (by name from a Pod specification) into the full credential spec in JSON form within the Pod spec.

1. A validating webhook ensures all references to GMSAs are authorized to be used by the Pod service account.
-->
<h3 id="安装-webhook-来验证-gmsa-用户">安装 Webhook 来验证 GMSA 用户</h3>
<p>你需要为 Kubernetes 集群配置两个 Webhook，在 Pod 或容器级别填充和检查
GMSA 凭据规约引用。</p>
<ol>
<li>
<p>一个修改模式（Mutating）的 Webhook，将对 GMSA 的引用（在 Pod 规约中体现为名字）
展开为完整凭据规约的 JSON 形式，并保存回 Pod 规约中。</p>
</li>
<li>
<p>一个验证模式（Validating）的 Webhook，确保对 GMSA 的所有引用都是已经授权
给 Pod 的服务账号使用的。</p>
</li>
</ol>
<!--
Installing the above webhooks and associated objects require the steps below:

1. Create a certificate key pair (that will be used to allow the webhook container to communicate to the cluster)

1. Install a secret with the certificate from above.

1. Create a deployment for the core webhook logic. 

1. Create the validating and mutating webhook configurations referring to the deployment. 
-->
<p>安装以上 Webhook 及其相关联的对象需要执行以下步骤：</p>
<ol>
<li>
<p>创建一个证书密钥对（用于允许 Webhook 容器与集群通信）</p>
</li>
<li>
<p>安装一个包含如上证书的 Secret</p>
</li>
<li>
<p>创建一个包含核心 Webhook 逻辑的 Deployment</p>
</li>
<li>
<p>创建引用该 Deployment 的 Validating Webhook 和 Mutating Webhook 配置</p>
</li>
</ol>
<!--
A [script](https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/deploy-gmsa-webhook.sh) can be used to deploy and configure the GMSA webhooks and associated objects mentioned above. The script can be run with a `-dry-run=server` option to allow you to review the changes that would be made to your cluster.

The [YAML template](https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/gmsa-webhook.yml.tpl) used by the script may also be used to deploy the webhooks and associated objects manually (with appropriate substitutions for the parameters)
-->
<p>你可以使用<a href="https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/deploy-gmsa-webhook.sh">这个脚本</a>
来部署和配置上述 GMSA Webhook 及相关联的对象。你还可以在运行脚本时设置 <code>--dry-run=server</code>
选项以便审查脚本将会对集群做出的变更。</p>
<p>脚本所使用的<a href="https://github.com/kubernetes-sigs/windows-gmsa/blob/master/admission-webhook/deploy/gmsa-webhook.yml.tpl">YAML 模板</a>
也可用于手动部署 Webhook 及相关联的对象，不过需要对其中的参数作适当替换。</p>
<!-- steps -->
<!--
## Configure GMSAs and Windows nodes in Active Directory

Before Pods in Kubernetes can be configured to use GMSAs, the desired GMSAs need to be provisioned in Active Directory as described in the [Windows GMSA documentation](https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/getting-started-with-group-managed-service-accounts#BKMK_Step1). Windows worker nodes (that are part of the Kubernetes cluster) need to be configured in Active Directory to access the secret credentials associated with the desired GMSA as described in the [Windows GMSA documentation](https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/getting-started-with-group-managed-service-accounts#to-add-member-hosts-using-the-set-adserviceaccount-cmdlet)
-->
<h2 id="在活动目录中配置-gmsa-和-windows-节点">在活动目录中配置 GMSA 和 Windows 节点</h2>
<p>在配置 Kubernetes 中的 Pod 以使用 GMSA 之前，需要按
<a href="https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/getting-started-with-group-managed-service-accounts#BKMK_Step1">Windows GMSA 文档</a>
中描述的那样先在活动目录中准备好期望的 GMSA。
Windows 工作节点（作为 Kubernetes 集群的一部分）需要被配置到活动目录中，以便
访问与期望的 GSMA 相关联的秘密凭据数据。这一操作的描述位于
<a href="https://docs.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/getting-started-with-group-managed-service-accounts#to-add-member-hosts-using-the-set-adserviceaccount-cmdlet">Windows GMSA 文档</a>
中。</p>
<!--
## Create GMSA credential spec resources
With the GMSACredentialSpec CRD installed (as described earlier), custom resources containing GMSA credential specs can be configured. The GMSA credential spec does not contain secret or sensitive data. It is information that a container runtime can use to describe the desired GMSA of a container to Windows. GMSA credential specs can be generated in YAML format with a utility [PowerShell script](https://github.com/kubernetes-sigs/windows-gmsa/tree/master/scripts/GenerateCredentialSpecResource.ps1). 
-->
<h2 id="创建-gmsa-凭据规约资源">创建 GMSA 凭据规约资源</h2>
<p>当（如前所述）安装了 GMSACredentialSpec CRD 之后，你就可以配置包含 GMSA 凭据
规约的自定义资源了。GMSA 凭据规约中并不包含秘密或敏感数据。
其中包含的信息主要用于容器运行时，便于后者向 Windows 描述容器所期望的 GMSA。
GMSA 凭据规约可以使用
<a href="https://github.com/kubernetes-sigs/windows-gmsa/tree/master/scripts/GenerateCredentialSpecResource.ps1">PowerShell 脚本</a>
以 YAML 格式生成。</p>
<!--
Following are the steps for generating a GMSA credential spec YAML manually in JSON format and then converting it:

1. Import the CredentialSpec [module](https://github.com/MicrosoftDocs/Virtualization-Documentation/blob/live/windows-server-container-tools/ServiceAccounts/CredentialSpec.psm1): `ipmo CredentialSpec.psm1`

1. Create a credential spec in JSON format using `New-CredentialSpec`. To create a GMSA credential spec named WebApp1, invoke `New-CredentialSpec -Name WebApp1 -AccountName WebApp1 -Domain $(Get-ADDomain -Current LocalComputer)`

1. Use `Get-CredentialSpec` to show the path of the JSON file. 

1. Convert the credspec file from JSON to YAML format and apply the necessary header fields `apiVersion`, `kind`, `metadata` and `credspec` to make it a GMSACredentialSpec custom resource that can be configured in Kubernetes. 
-->
<p>下面是手动以 JSON 格式生成 GMSA 凭据规约并对其进行 YAML 转换的步骤：</p>
<ol>
<li>
<p>导入 CredentialSpec <a href="https://github.com/MicrosoftDocs/Virtualization-Documentation/blob/live/windows-server-container-tools/ServiceAccounts/CredentialSpec.psm1">模块</a>: <code>ipmo CredentialSpec.psm1</code></p>
</li>
<li>
<p>使用 <code>New-CredentialSpec</code> 来创建一个 JSON 格式的凭据规约。
要创建名为 <code>WebApp1</code> 的 GMSA 凭据规约，调用
<code>New-CredentialSpec -Name WebApp1 -AccountName WebApp1 -Domain $(Get-ADDomain -Current LocalComputer)</code>。</p>
</li>
<li>
<p>使用 <code>Get-CredentialSpec</code> 来显示 JSON 文件的路径。</p>
</li>
<li>
<p>将凭据规约从 JSON 格式转换为 YAML 格式，并添加必要的头部字段
<code>apiVersion</code>、<code>kind</code>、<code>metadata</code> 和 <code>credspec</code>，使其成为一个可以在
Kubernetes 中配置的 GMSACredentialSpec 自定义资源。</p>
</li>
</ol>
<!--
The following YAML configuration describes a GMSA credential spec named `gmsa-WebApp1`:

```yaml
apiVersion: windows.k8s.io/v1alpha1
kind: GMSACredentialSpec
metadata:
  name: gmsa-WebApp1  #This is an arbitrary name but it will be used as a reference
credspec:
  ActiveDirectoryConfig:
    GroupManagedServiceAccounts:
    - Name: WebApp1   #Username of the GMSA account
      Scope: CONTOSO  #NETBIOS Domain Name
    - Name: WebApp1   #Username of the GMSA account
      Scope: contoso.com #DNS Domain Name
  CmsPlugins:
  - ActiveDirectory
  DomainJoinConfig:
    DnsName: contoso.com  #DNS Domain Name
    DnsTreeName: contoso.com #DNS Domain Name Root
    Guid: 244818ae-87ac-4fcd-92ec-e79e5252348a  #GUID
    MachineAccountName: WebApp1 #Username of the GMSA account
    NetBiosName: CONTOSO  #NETBIOS Domain Name
    Sid: S-1-5-21-2126449477-2524075714-3094792973 #SID of GMSA
```
-->
<p>下面的 YAML 配置描述的是一个名为 <code>gmsa-WebApp1</code> 的 GMSA 凭据规约：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>windows.k8s.io/v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>GMSACredentialSpec<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>gmsa-WebApp1 <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 这是随意起的一个名字，将用作引用</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">credspec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ActiveDirectoryConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">GroupManagedServiceAccounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">Name</span>:<span style="color:#bbb"> </span>WebApp1  <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># GMSA 账号的用户名</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">Scope</span>:<span style="color:#bbb"> </span>CONTOSO <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># NETBIOS 域名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">Name</span>:<span style="color:#bbb"> </span>WebApp1  <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># GMSA 账号的用户名</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">Scope</span>:<span style="color:#bbb"> </span>contoso.com<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># DNS 域名</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">CmsPlugins</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- ActiveDirectory<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">DomainJoinConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">DnsName</span>:<span style="color:#bbb"> </span>contoso.com <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># DNS 域名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">DnsTreeName</span>:<span style="color:#bbb"> </span>contoso.com<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># DNS 域名根</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">Guid</span>:<span style="color:#bbb"> </span>244818ae-87ac-4fcd-92ec-e79e5252348a <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># GUID</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">MachineAccountName</span>:<span style="color:#bbb"> </span>WebApp1<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># GMSA 账号的用户名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">NetBiosName</span>:<span style="color:#bbb"> </span>CONTOSO <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># NETBIOS 域名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">Sid</span>:<span style="color:#bbb"> </span>S-1-5-21-2126449477-2524075714-3094792973<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># GMSA 的 SID</span><span style="color:#bbb">
</span></code></pre></div><!--
The above credential spec resource may be saved as `gmsa-Webapp1-credspec.yaml` and applied to the cluster using: `kubectl apply -f gmsa-Webapp1-credspec.yml`
-->
<p>上面的凭据规约资源可以保存为 <code>gmsa-Webapp1-credspec.yaml</code>，之后使用
<code>kubectl apply -f gmsa-Webapp1-credspec.yml</code> 应用到集群上。</p>
<!--
## Configure cluster role to enable RBAC on specific GMSA credential specs

A cluster role needs to be defined for each GMSA credential spec resource. This authorizes the `use` verb on a specific GMSA resource by a subject which is typically a service account. The following example shows a cluster role that authorizes usage of the `gmsa-WebApp1` credential spec from above. Save the file as gmsa-webapp1-role.yaml and apply using `kubectl apply -f gmsa-webapp1-role.yaml`
-->
<h2 id="配置集群角色以启用对特定-gmsa-凭据规约的-rbac">配置集群角色以启用对特定 GMSA 凭据规约的 RBAC</h2>
<p>你需要为每个 GMSA 凭据规约资源定义集群角色。
该集群角色授权某主体（通常是一个服务账号）对特定的 GMSA 资源执行 <code>use</code> 动作。
下面的示例显示的是一个集群角色，对前文创建的凭据规约 <code>gmsa-WebApp1</code> 执行鉴权。
将此文件保存为 <code>gmsa-webapp1-role.yaml</code> 并执行 <code>kubectl apply -f gmsa-webapp1-role.yaml</code>。</p>
<!--
```yaml
#Create the Role to read the credspec
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: webapp1-role
rules:
- apiGroups: ["windows.k8s.io"]
  resources: ["gmsacredentialspecs"]
  verbs: ["use"]
  resourceNames: ["gmsa-WebApp1"]
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 创建集群角色读取凭据规约</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>webapp1-role<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;windows.k8s.io&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;gmsacredentialspecs&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;use&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceNames</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;gmsa-WebApp1&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
## Assign role to service accounts to use specific GMSA credspecs
A service account (that Pods will be configured with) needs to be bound to the cluster role create above. This authorizes the service account to use the desired GMSA credential spec resource. The following shows the default service account being bound to a cluster role `webapp1-role` to use `gmsa-WebApp1` credential spec resource created above.
-->
<h2 id="将角色指派给要使用特定-gmsa-凭据规约的服务账号">将角色指派给要使用特定 GMSA 凭据规约的服务账号</h2>
<p>你需要将某个服务账号（Pod 配置所对应的那个）绑定到前文创建的集群角色上。
这一绑定操作实际上授予该服务账号使用所指定的 GMSA 凭据规约资源的访问权限。
下面显示的是一个绑定到集群角色 <code>webapp1-role</code> 上的 default 服务账号，使之
能够使用前面所创建的 <code>gmsa-WebApp1</code> 凭据规约资源。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>RoleBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>allow-default-svc-account-read-on-gmsa-WebApp1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">roleRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>webapp1-role<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span></code></pre></div><!--
## Configure GMSA credential spec reference in Pod spec
The Pod spec field `securityContext.windowsOptions.gmsaCredentialSpecName` is used to specify references to desired GMSA credential spec custom resources in Pod specs. This configures all containers in the Pod spec to use the specified GMSA. A sample Pod spec with the annotation populated to refer to `gmsa-WebApp1`:
-->
<h2 id="在-pod-规约中配置-gmsa-凭据规约引用">在 Pod 规约中配置 GMSA 凭据规约引用</h2>
<p>Pod 规约字段 <code>securityContext.windowsOptions.gmsaCredentialSpecName</code> 可用来
设置对指定 GMSA 凭据规约自定义资源的引用。
设置此引用将会配置 Pod 中的所有容器使用所给的 GMSA。
下面是一个 Pod 规约示例，其中包含了对 <code>gmsa-WebApp1</code> 凭据规约的引用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">windowsOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">gmsaCredentialSpecName</span>:<span style="color:#bbb"> </span>gmsa-webapp1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>iis<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></code></pre></div><!--
Individual containers in a Pod spec can also specify the desired GMSA credspec using a per-container `securityContext.windowsOptions.gmsaCredentialSpecName` field. For example:
-->
<p>Pod 中的各个容器也可以使用对应容器的 <code>securityContext.windowsOptions.gmsaCredentialSpecName</code>
字段来设置期望使用的 GMSA 凭据规约。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>with-creds<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>iis<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">windowsOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">gmsaCredentialSpecName</span>:<span style="color:#bbb"> </span>gmsa-Webapp1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></code></pre></div><!--
As Pod specs with GMSA fields populated (as described above) are applied in a cluster, the following sequence of events take place:

1. The mutating webhook resolves and expands all references to GMSA credential spec resources to the contents of the GMSA credential spec.

1. The validating webhook ensures the service account associated with the Pod is authorized for the `use` verb on the specified GMSA credential spec.

1. The container runtime configures each Windows container with the specified GMSA credential spec so that the container can assume the identity of the GMSA in Active Directory and access services in the domain using that identity.
-->
<p>当 Pod 规约中填充了 GMSA 相关字段（如上所述），在集群中应用 Pod 规约时会依次
发生以下事件：</p>
<ol>
<li>
<p>Mutating Webhook 解析对 GMSA 凭据规约资源的引用，并将其全部展开，
得到 GMSA 凭据规约的实际内容。</p>
</li>
<li>
<p>Validating Webhook 确保与 Pod 相关联的服务账号有权在所给的 GMSA 凭据规约
上执行 <code>use</code> 动作。</p>
</li>
<li>
<p>容器运行时为每个 Windows 容器配置所指定的 GMSA 凭据规约，这样容器就可以以
活动目录中该 GMSA 所代表的身份来执行操作，使用该身份来访问域中的服务。</p>
</li>
</ol>
<!--
## Troubleshooting

If you are having difficulties getting GMSA to work in your environment, there are a few troubleshooting steps you can take.
-->
<h2 id="故障排查">故障排查</h2>
<p>如果在你的环境中配置 GMSA 时遇到了困难，你可以采取若干步骤来排查可能
的故障。</p>
<!--
First, make sure the credspec has been passed to the Pod.  To do this you will need to `exec` into one of your Pods and check the output of the `nltest.exe /parentdomain` command.  In the example below the Pod did not get the credspec correctly:
-->
<p>首先，确保凭据规约已经被传递到 Pod。要实现这点，你需要先通过 <code>exec</code> 进入到
你的 Pod 之一，检查 <code>nltest.exe /parentdomain</code> 命令的输出。
在下面的例子中，Pod 未能正确地获得凭据规约：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it iis-auth-7776966999-n5nzr powershell.exe

Windows PowerShell
Copyright <span style="color:#666">(</span>C<span style="color:#666">)</span> Microsoft Corporation. All rights reserved.

PS C:<span style="color:#b62;font-weight:bold">\&gt;</span> nltest.exe /parentdomain
Getting parent domain failed: <span style="color:#b8860b">Status</span> <span style="color:#666">=</span> <span style="color:#666">1722</span> 0x6ba RPC_S_SERVER_UNAVAILABLE
PS C:<span style="color:#b62;font-weight:bold">\&gt;</span>
</code></pre></div><!--
If your Pod did get the credspec correctly, then next check communication with the domain.  First, from inside of your Pod, quickly do an nslookup to find the root of your domain.

This will tell us 3 things:

1. The Pod can reach the DC
1. The DC can reach the Pod
1. DNS is working correctly.
-->
<p>如果 Pod 未能正确获得凭据规约，则下一步就要检查与域之间的通信。
首先，从 Pod 内部快速执行一个 nslookup 操作，找到域根。</p>
<p>这一操作会告诉我们三件事情：</p>
<ol>
<li>Pod 能否访问域控制器（DC）</li>
<li>DC 能否访问 Pod</li>
<li>DNS 是否正常工作</li>
</ol>
<!--
If the DNS and communication test passes, next you will need to check if the Pod has established secure channel communication with the domain.  To do this, again, `exec` into your Pod and run the `nltest.exe /query` command.
-->
<p>如果 DNS 和通信测试通过，接下来你需要检查是否 Pod 已经与域之间建立了
安全通信通道。要执行这一检查，你需要再次通过 <code>exec</code> 进入到你的 Pod 中
并执行 <code>nltest.exe /query</code> 命令。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">PS C:<span style="color:#b62;font-weight:bold">\&gt;</span> nltest.exe /query
I_NetLogonControl failed: <span style="color:#b8860b">Status</span> <span style="color:#666">=</span> <span style="color:#666">1722</span> 0x6ba RPC_S_SERVER_UNAVAILABLE
</code></pre></div><!--
This tells us that for some reason, the Pod was unable to logon to the domain using the account specified in the credspec.  You can try to repair the secure channel by running the `nltest.exe /sc_reset:domain.example` command.
-->
<p>这一输出告诉我们，由于某些原因，Pod 无法使用凭据规约中的账号登录到域。
你可以通过运行 <code>nltest.exe /sc_reset:domain.example</code> 命令尝试修复安全通道。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">PS C:<span style="color:#b62;font-weight:bold">\&gt;</span> nltest /sc_reset:domain.example
Flags: <span style="color:#666">30</span> HAS_IP  HAS_TIMESERV
Trusted DC Name <span style="color:#b62;font-weight:bold">\\</span>dc10.domain.example
Trusted DC Connection Status <span style="color:#b8860b">Status</span> <span style="color:#666">=</span> <span style="color:#666">0</span> 0x0 NERR_Success
The <span style="color:#a2f">command</span> completed successfully
PS C:<span style="color:#b62;font-weight:bold">\&gt;</span>
</code></pre></div><!--
If the above command corrects the error, you can automate the step by adding the following lifecycle hook to your Pod spec.  If it did not correct the error, you will need to examine your credspec again and confirm that it is correct and complete.
-->
<p>如果上述命令修复了错误，你就可以通过向你的 Pod 规约添加生命周期回调来将此操作
自动化。如果上述命令未能奏效，你就需要再次检查凭据规约，以确保其数据时正确的
而且是完整的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>registry.domain.example/iis-auth:1809v1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">lifecycle</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">postStart</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;powershell.exe&#34;</span>,<span style="color:#b44">&#34;-command&#34;</span>,<span style="color:#b44">&#34;do { Restart-Service -Name netlogon } while ( $($Result = (nltest.exe /query); if ($Result -like &#39;*0x0 NERR_Success*&#39;) {return $true} else {return $false}) -eq $false)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></code></pre></div><!--
If you add the `lifecycle` section show above to your Pod spec, the Pod will execute the commands listed to restart the `netlogon` service until the `nltest.exe /query` command exits without error.
-->
<p>如果你向你的 Pod 规约中添加如上所示的 <code>lifecycle</code> 节，则 Pod 会自动执行所
列举的命令来重启 <code>netlogon</code> 服务，直到 <code>nltest.exe /query</code>
命令返回时没有错误信息。</p>
<!--
## GMSA limitations
When using the [ContainerD runtime for Windows](/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#cri-containerd) accessing restricted network shares via the GMSA domain identity fails. The container will receive the identity of and calls from `nltest.exe /query` will work.  It is recommended to use the [Docker EE runtime](/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#docker-ee) if access to network shares is required.  The Windows Server team is working on resolving the issue in the Windows Kernel and will release a patch to resolve this issue in the future. Look for updates on the [Microsoft Windows Containers issue tracker](https://github.com/microsoft/Windows-Containers/issues/44).
-->
<h2 id="gmsa-limitations">GMSA 的局限 </h2>
<p>在使用 <a href="/zh/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#cri-containerd">Windows 版本的 ContainerD 运行时</a>
时，通过 GMSA 域身份标识访问受限制的网络共享资源时会出错。
容器会收到身份标识且 <code>nltest.exe /query</code> 调用也能正常工作。
当需要访问网络共享资源时，建议使用
<a href="/zh/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#docker-ee">Docker EE 运行时</a>。
Windows Server 团队正在 Windows 内核中解决这一问题，并在将来发布解决此问题的补丁。
你可以在 <a href="https://github.com/microsoft/Windows-Containers/issues/44">Microsoft Windows Containers 问题跟踪列表</a>
中查找这类更新。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f5da7517bee8a8807431d9fc65263b39">3.3 - 为 Windows 的 Pod 和容器配置 RunAsUserName</h1>
    
	<!--
title: Configure RunAsUserName for Windows pods and containers
content_type: task
weight: 20
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code>
</div>

<!--
This page shows how to use the `runAsUserName` setting for Pods and containers that will run on Windows nodes. This is roughly equivalent of the Linux-specific `runAsUser` setting, allowing you to run applications in a container as a different username than the default.
-->
<p>本页展示如何为运行为在 Windows 节点上运行的 Pod 和容器配置 <code>RunAsUserName</code> 。
大致相当于 Linux 上的 <code>runAsUser</code>，允许在容器中以与默认值不同的用户名运行应用。</p>
<h2 id="准备开始">准备开始</h2>
<!--
You need to have a Kubernetes cluster and the kubectl command-line tool must be configured to communicate with your cluster. The cluster is expected to have Windows worker nodes where pods with containers running Windows workloads will get scheduled.
-->
<p>你必须有一个 Kubernetes 集群，并且 kubectl 必须能和集群通信。
集群应该要有 Windows 工作节点，将在其中调度运行 Windows 工作负载的 pod 和容器。</p>
<!--
## Set the Username for a Pod

To specify the username with which to execute the Pod's container processes, include the `securityContext` field ([PodSecurityContext](/docs/reference/generated/kubernetes-api/v1.22/#podsecuritycontext-v1-core) in the Pod specification, and within it, the `windowsOptions` ([WindowsSecurityContextOptions](/docs/reference/generated/kubernetes-api/v1.22/#windowssecuritycontextoptions-v1-core) field containing the `runAsUserName` field.
-->
<h2 id="为-pod-设置-username">为 Pod 设置 Username</h2>
<p>要指定运行 Pod 容器时所使用的用户名，请在 Pod 声明中包含 <code>securityContext</code>
（<a href="/zh/docs/reference/generated/kubernetes-api/v1.22/#podsecuritycontext-v1-core">PodSecurityContext</a>）字段，
并在其内部包含 <code>windowsOptions</code>
（<a href="/zh/docs/reference/generated/kubernetes-api/v1.22/#windowssecuritycontextoptions-v1-core">WindowsSecurityContextOptions</a>）
字段的 <code>runAsUserName</code> 字段。</p>
<!--
The Windows security context options that you specify for a Pod apply to all Containers and init Containers in the Pod.

Here is a configuration file for a Windows Pod that has the `runAsUserName` field set:
-->
<p>你为 Pod 指定的 Windows SecurityContext 选项适用于该 Pod 中（包括 init 容器）的所有容器。</p>
<p>这儿有一个已经设置了 <code>runAsUserName</code> 字段的 Windows Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/windows/run-as-username-pod.yaml" download="windows/run-as-username-pod.yaml"><code>windows/run-as-username-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('windows-run-as-username-pod-yaml')" title="Copy windows/run-as-username-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="windows-run-as-username-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>run-as-username-pod-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">windowsOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">runAsUserName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ContainerUser&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>run-as-username-demo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;ping&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-t&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;localhost&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/windows/run-as-username-pod.yaml
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>验证 Pod 容器是否在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod run-as-username-pod-demo
</code></pre></div><!--
Get a shell to the running Container:
-->
<p>获取该容器的 shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it run-as-username-pod-demo -- powershell
</code></pre></div><!--
Check that the shell is running user the correct username:
-->
<p>检查运行 shell 的用户的用户名是否正确：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">echo </span><span style="color:#b8860b">$env:USERNAME</span>
</code></pre></div><!--
The output should be:
-->
<p>输出结果应该是这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ContainerUser
</code></pre></div><!--
## Set the Username for a Container

To specify the username with which to execute a Container's processes, include the `securityContext` field ([SecurityContext](/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core)) in the Container manifest, and within it, the `windowsOptions` ([WindowsSecurityContextOptions](/docs/reference/generated/kubernetes-api/v1.22/#windowssecuritycontextoptions-v1-core) field containing the `runAsUserName` field.
-->
<h2 id="为容器设置-username">为容器设置 Username</h2>
<p>要指定运行容器时所使用的用户名，请在容器清单中包含 <code>securityContext</code>
（<a href="/zh/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core">SecurityContext</a>）
字段，并在其内部包含 <code>windowsOptions</code>
（<a href="/zh/docs/reference/generated/kubernetes-api/v1.22/#windowssecuritycontextoptions-v1-core">WindowsSecurityContextOptions</a>）
字段的 <code>runAsUserName</code> 字段。</p>
<!--
The Windows security context options that you specify for a Container apply only to that individual Container, and they override the settings made at the Pod level.

Here is the configuration file for a Pod that has one Container, and the `runAsUserName` field is set at the Pod level and the Container level:
-->
<p>你为容器指定的 Windows SecurityContext 选项仅适用于该容器，并且它会覆盖 Pod 级别设置。</p>
<p>这里有一个 Pod 的配置文件，其中只有一个容器，并且在 Pod 级别和容器级别都设置了 <code>runAsUserName</code>：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/windows/run-as-username-container.yaml" download="windows/run-as-username-container.yaml"><code>windows/run-as-username-container.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('windows-run-as-username-container-yaml')" title="Copy windows/run-as-username-container.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="windows-run-as-username-container-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>run-as-username-container-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">windowsOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">runAsUserName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ContainerUser&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>run-as-username-demo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;ping&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-t&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;localhost&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">windowsOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">runAsUserName</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;ContainerAdministrator&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/os</span>:<span style="color:#bbb"> </span>windows<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/windows/run-as-username-container.yaml
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>验证 Pod 容器是否在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod run-as-username-container-demo
</code></pre></div><!--
Get a shell to the running Container:
-->
<p>获取该容器的 shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it run-as-username-container-demo -- powershell
</code></pre></div><!--
Check that the shell is running user the correct username (the one set at the Container level):
-->
<p>检查运行 shell 的用户的用户名是否正确（应该是容器级别设置的那个）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#a2f">echo </span><span style="color:#b8860b">$env:USERNAME</span>
</code></pre></div><!--
The output should be:
-->
<p>输出结果应该是这样：</p>
<pre tabindex="0"><code>ContainerAdministrator
</code></pre><!--
## Windows Username limitations

In order to use this feature, the value set in the `runAsUserName` field must be a valid username. It must have the following format: `DOMAIN\USER`, where `DOMAIN\` is optional. Windows user names are case insensitive. Additionally, there are some restrictions regarding the `DOMAIN` and `USER`:
-->
<h2 id="windows-username-的局限性">Windows Username 的局限性</h2>
<p>想要使用此功能，在 <code>runAsUserName</code> 字段中设置的值必须是有效的用户名。
它必须是 <code>DOMAIN\USER</code> 这种格式，其中 <code>DOMAIN\</code> 是可选的。
Windows 用户名不区分大小写。此外，关于 <code>DOMAIN</code> 和 <code>USER</code> 还有一些限制：</p>
<!--
- The `runAsUserName` field cannot be empty, and it cannot contain control characters (ASCII values: `0x00-0x1F`, `0x7F`)
- The `DOMAIN` must be either a NetBios name, or a DNS name, each with their own restrictions:
  - NetBios names: maximum 15 characters, cannot start with `.` (dot), and cannot contain the following characters: `\ / : * ? " < > |`
  - DNS names: maximum 255 characters, contains only alphanumeric characters, dots, and dashes, and it cannot start or end with a `.` (dot) or `-` (dash).
- The `USER` must have at most 20 characters, it cannot contain *only* dots or spaces, and it cannot contain the following characters: `" / \ [ ] : ; | = , + * ? < > @`.
-->
<ul>
<li><code>runAsUserName</code> 字段不能为空，并且不能包含控制字符（ASCII 值：<code>0x00-0x1F</code>、<code>0x7F</code>）</li>
<li><code>DOMAIN</code> 必须是 NetBios 名称或 DNS 名称，每种名称都有各自的局限性：
<ul>
<li>NetBios 名称：最多 15 个字符，不能以 <code>.</code>（点）开头，并且不能包含以下字符：<code>\ / : * ? &quot; &lt; &gt; |</code></li>
<li>DNS 名称：最多 255 个字符，只能包含字母、数字、点和中划线，并且不能以 <code>.</code>（点）或 <code>-</code>（中划线）开头和结尾。</li>
</ul>
</li>
<li><code>USER</code> 最多不超过 20 个字符，不能 <strong>只</strong> 包含点或空格，并且不能包含以下字符：<code>&quot; / \ [ ] : ; | = , + * ? &lt; &gt; @</code></li>
</ul>
<!--
Examples of acceptable values for the `runAsUserName` field: `ContainerAdministrator`, `ContainerUser`, `NT AUTHORITY\NETWORK SERVICE`, `NT AUTHORITY\LOCAL SERVICE`.

For more information about these limtations, check [here](https://support.microsoft.com/en-us/help/909264/naming-conventions-in-active-directory-for-computers-domains-sites-and) and [here](https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.localaccounts/new-localuser?view=powershell-5.1).
-->
<p><code>runAsUserName</code> 字段接受的值的一些示例：<code>ContainerAdministrator</code>、<code>ContainerUser</code>、
<code>NT AUTHORITY\NETWORK SERVICE</code>、<code>NT AUTHORITY\LOCAL SERVICE</code>。</p>
<p>关于这些限制的更多信息，可以查看<a href="https://support.microsoft.com/en-us/help/909264/naming-conventions-in-active-directory-for-computers-domains-sites-and">这里</a>和<a href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.localaccounts/new-localuser?view=powershell-5.1">这里</a>。</p>
<h2 id="接下来">接下来</h2>
<!--
* [Guide for scheduling Windows containers in Kubernetes](/docs/setup/production-environment/windows/user-guide-windows-containers/)
* [Managing Workload Identity with Group Managed Service Accounts (GMSA)](/docs/setup/production-environment/windows/user-guide-windows-containers/#managing-workload-identity-with-group-managed-service-accounts)
* [Configure GMSA for Windows pods and containers](/docs/tasks/configure-pod-container/configure-gmsa/)
-->
<ul>
<li><a href="/zh/docs/setup/production-environment/windows/user-guide-windows-containers/">Kubernetes 中调度 Windows 容器的指南</a></li>
<li><a href="/zh/docs/setup/production-environment/windows/user-guide-windows-containers/#managing-workload-identity-with-group-managed-service-accounts">使用组托管服务帐户（GMSA）管理工作负载身份</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/configure-gmsa/">Windows 下 pod 和容器的 GMSA 配置</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8555af270ae7122cc0464bab3f5d1609">3.4 - 为容器和 Pods 分配 CPU 资源</h1>
    
	<!--
title: Assign CPU Resources to Containers and Pods
content_type: task
weight: 20
-->
<!-- overview -->
<!--
This page shows how to assign a CPU *request* and a CPU *limit* to
a container. Containers cannot use more CPU than the configured limit.
Provided the system has CPU time free, a container is guaranteed to be
allocated as much CPU as it requests.
-->
<p>本页面展示如何为容器设置 CPU <em>request（请求）</em> 和 CPU <em>limit（限制）</em>。
容器使用的 CPU 不能超过所配置的限制。
如果系统有空闲的 CPU 时间，则可以保证给容器分配其所请求数量的 CPU 资源。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Each node in your cluster must have at least 1 CPU.

A few of the steps on this page require you to run the
[metrics-server](https://github.com/kubernetes-sigs/metrics-server)
service in your cluster. If you have the metrics-server
running, you can skip those steps.

If you are running <a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>, run the
following command to enable metrics-server:
-->
<p>集群中的每个节点必须至少有 1 个 CPU 可用才能运行本任务中的示例。</p>
<p>本页的一些步骤要求你在集群中运行
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a>
服务。如果你的集群中已经有正在运行的 metrics-server 服务，可以跳过这些步骤。</p>
<p>如果你正在运行<a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>，请运行以下命令启用 metrics-server：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube addons <span style="color:#a2f">enable</span> metrics-server
</code></pre></div><!-- 
To see whether metrics-server (or another provider of the resource metrics
API, `metrics.k8s.io`) is running, type the following command:
-->
<p>查看 metrics-server（或者其他资源度量 API <code>metrics.k8s.io</code> 服务提供者）是否正在运行，
请键入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get apiservices
</code></pre></div><!-- 
If the resource metrics  API  is available, the output will include a
reference to `metrics.k8s.io`.
-->
<p>如果资源指标 API 可用，则会输出将包含一个对 <code>metrics.k8s.io</code> 的引用。</p>
<pre tabindex="0"><code>NAME
v1beta1.metrics.k8s.io
</code></pre><!-- steps -->
<!-- 
## Create a namespace

Create a <a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间（Namespace）'>名字空间（Namespace）</a> so that the resources you
create in this exercise are isolated from the rest of your cluster.
-->
<h2 id="创建一个名字空间">创建一个名字空间</h2>
<p>创建一个<a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>，以便将
本练习中创建的资源与集群的其余部分资源隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace cpu-example
</code></pre></div><!-- 
## Specify a CPU request and a CPU limit

To specify a CPU request for a container, include the `resources:requests` field
in the Container resource manifest. To specify a CPU limit, include `resources:limits`.

In this exercise, you create a Pod that has one container. The container has a request
of 0.5 CPU and a limit of 1 CPU. Here is the configuration file for the Pod:



 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/cpu-request-limit.yaml" download="pods/resource/cpu-request-limit.yaml"><code>pods/resource/cpu-request-limit.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-cpu-request-limit-yaml')" title="Copy pods/resource/cpu-request-limit.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-cpu-request-limit-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>cpu-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>vish/stress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -cpus<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>



The `args` section of the configuration file provides arguments for the container when it starts.
The `-cpus "2"` argument tells the Container to attempt to use 2 CPUs.

Create the Pod:
-->
<h2 id="指定-cpu-请求和-cpu-限制">指定 CPU 请求和 CPU 限制</h2>
<p>要为容器指定 CPU 请求，请在容器资源清单中包含 <code>resources: requests</code> 字段。
要指定 CPU 限制，请包含 <code>resources:limits</code>。</p>
<p>在本练习中，你将创建一个具有一个容器的 Pod。容器将会请求 0.5 个 CPU，而且最多限制使用 1 个 CPU。
这是 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/cpu-request-limit.yaml" download="pods/resource/cpu-request-limit.yaml"><code>pods/resource/cpu-request-limit.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-cpu-request-limit-yaml')" title="Copy pods/resource/cpu-request-limit.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-cpu-request-limit-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>cpu-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>vish/stress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;0.5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -cpus<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<p>配置文件的 <code>args</code> 部分提供了容器启动时的参数。
<code>-cpus &quot;2&quot;</code> 参数告诉容器尝试使用 2 个 CPU。</p>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit.yaml --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
Verify that the  Pod  is running:
-->
<p>验证所创建的 Pod 处于 Running 状态</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod cpu-demo --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
View detailed information about the Pod:
-->
<p>查看显示关于 Pod 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod cpu-demo --output<span style="color:#666">=</span>yaml --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
The output shows that the one container in the Pod has a CPU request of 500 milliCPU
and a CPU limit of 1 CPU.
-->
<p>输出显示 Pod 中的一个容器的 CPU 请求为 500 milli CPU，并且 CPU 限制为 1 个 CPU。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span></code></pre></div><!-- 
Use `kubectl top` to fetch the metrics for the pod:
-->
<p>使用 <code>kubectl top</code> 命令来获取该 Pod 的度量值数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl top pod cpu-demo --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
This example output shows that the Pod is using 974 milliCPU, which is
slightly less than the limit of 1 CPU specified in the Pod configuration.
-->
<p>此示例输出显示 Pod 使用的是 974 milliCPU，即略低于 Pod 配置中指定的 1 个 CPU 的限制。</p>
<pre tabindex="0"><code>NAME                        CPU(cores)   MEMORY(bytes)
cpu-demo                    974m         &lt;something&gt;
</code></pre><!-- 
Recall that by setting `-cpu "2"`, you configured the Container to attempt to use 2 CPUs, but the Container is only being allowed to use about 1 CPU. The container's CPU use is being throttled, because the container is attempting to use more CPU resources than its limit.
-->
<p>回想一下，通过设置 <code>-cpu &quot;2&quot;</code>，你将容器配置为尝试使用 2 个 CPU，
但是容器只被允许使用大约 1 个 CPU。
容器的 CPU 用量受到限制，因为该容器正尝试使用超出其限制的 CPU 资源。</p>
<!-- 
Another possible explanation for the CPU use being below 1.0 is that the Node might not have
enough CPU resources available. Recall that the prerequisites for this exercise require each of
your Nodes to have at least 1 CPU. If your Container runs on a Node that has only 1 CPU, the Container
cannot use more than 1 CPU regardless of the CPU limit specified for the Container.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> CPU 使用率低于 1.0 的另一种可能的解释是，节点可能没有足够的 CPU 资源可用。
回想一下，此练习的先决条件需要你的节点至少具有 1 个 CPU 可用。
如果你的容器在只有 1 个 CPU 的节点上运行，则容器无论为容器指定的 CPU 限制如何，
都不能使用超过 1 个 CPU。</div>
</blockquote>
<!-- 
## CPU units

The CPU resource is measured in *CPU* units. One CPU, in Kubernetes, is equivalent to:

* 1 AWS vCPU
* 1 GCP Core
* 1 Azure vCore
* 1 Hyperthread on a bare-metal Intel processor with Hyperthreading
-->
<h2 id="cpu-units">CPU 单位 </h2>
<p>CPU 资源以 <em>CPU</em> 单位度量。Kubernetes 中的一个 CPU 等同于：</p>
<ul>
<li>1 个 AWS vCPU</li>
<li>1 个 GCP核心</li>
<li>1 个 Azure vCore</li>
<li>裸机上具有超线程能力的英特尔处理器上的 1 个超线程</li>
</ul>
<!-- 
Fractional values are allowed. A Container that requests 0.5 CPU is guaranteed half as much
CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example
100m CPU, 100 milliCPU, and 0.1 CPU are all the same. Precision finer than 1m is not allowed.

CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same
amount of CPU on a single-core, dual-core, or 48-core machine.

Delete your Pod:
-->
<p>小数值是可以使用的。一个请求 0.5 CPU 的容器保证会获得请求 1 个 CPU 的容器的 CPU 的一半。
你可以使用后缀 <code>m</code> 表示毫。例如 <code>100m</code> CPU、100 milliCPU 和 0.1 CPU 都相同。
精度不能超过 1m。</p>
<p>CPU 请求只能使用绝对数量，而不是相对数量。0.1 在单核、双核或 48 核计算机上的 CPU 数量值是一样的。</p>
<p>删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod cpu-demo --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
## Specify a CPU request that is too big for your Nodes

CPU requests and limits are associated with Containers, but it is useful to think
of a Pod as having a CPU request and limit. The CPU request for a Pod is the sum
of the CPU requests for all the Containers in the Pod. Likewise, the CPU limit for
a Pod is the sum of the CPU limits for all the Containers in the Pod.

Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if
the Node has enough CPU resources available to satisfy the Pod CPU request.

In this exercise, you create a Pod that has a CPU request so big that it exceeds
the capacity of any Node in your cluster. Here is the configuration file for a Pod
that has one Container. The Container requests 100 CPU, which is likely to exceed the
capacity of any Node in your cluster.



 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/cpu-request-limit-2.yaml" download="pods/resource/cpu-request-limit-2.yaml"><code>pods/resource/cpu-request-limit-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-cpu-request-limit-2-yaml')" title="Copy pods/resource/cpu-request-limit-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-cpu-request-limit-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>cpu-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo-ctr-2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>vish/stress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -cpus<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>



Create the Pod:
-->
<h2 id="设置超过节点能力的-cpu-请求">设置超过节点能力的 CPU 请求</h2>
<p>CPU 请求和限制与都与容器相关，但是我们可以考虑一下 Pod 具有对应的 CPU 请求和限制这样的场景。
Pod 对 CPU 用量的请求等于 Pod 中所有容器的请求数量之和。
同样，Pod 的 CPU 资源限制等于 Pod 中所有容器 CPU 资源限制数之和。</p>
<p>Pod 调度是基于资源请求值来进行的。
仅在某节点具有足够的 CPU 资源来满足 Pod CPU 请求时，Pod 将会在对应节点上运行：</p>
<p>在本练习中，你将创建一个 Pod，该 Pod 的 CPU 请求对于集群中任何节点的容量而言都会过大。
下面是 Pod 的配置文件，其中有一个容器。容器请求 100 个 CPU，这可能会超出集群中任何节点的容量。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/cpu-request-limit-2.yaml" download="pods/resource/cpu-request-limit-2.yaml"><code>pods/resource/cpu-request-limit-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-cpu-request-limit-2-yaml')" title="Copy pods/resource/cpu-request-limit-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-cpu-request-limit-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>cpu-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu-demo-ctr-2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>vish/stress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -cpus<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit-2.yaml --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
View the  Pod  status:
-->
<p>查看该 Pod 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod cpu-demo-2 --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
The output shows that the Pod status is Pending. That is, the Pod has not been
scheduled to run on any Node, and it will remain in the Pending state indefinitely:
-->
<p>输出显示 Pod 状态为 Pending。也就是说，Pod 未被调度到任何节点上运行，
并且 Pod 将无限期地处于 Pending 状态：</p>
<pre tabindex="0"><code>NAME         READY     STATUS    RESTARTS   AGE
cpu-demo-2   0/1       Pending   0          7m
</code></pre><!-- 
View detailed information about the Pod, including events:
-->
<p>查看有关 Pod 的详细信息，包含事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod cpu-demo-2 --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
The output shows that the Container cannot be scheduled because of insufficient
CPU resources on the Nodes:
-->
<p>输出显示由于节点上的 CPU 资源不足，无法调度容器：</p>
<pre tabindex="0"><code>Events:
  Reason                        Message
  ------                        -------
  FailedScheduling      No nodes are available that match all of the following predicates:: Insufficient cpu (3).
</code></pre><!-- 
Delete your Pod:
-->
<p>删除你的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod cpu-demo-2 --namespace<span style="color:#666">=</span>cpu-example
</code></pre></div><!-- 
## If you do not specify a CPU limit

If you do not specify a CPU limit for a Container, then one of these situations applies:

* The Container has no upper bound on the CPU resources it can use. The Container
could use all of the CPU resources available on the Node where it is running.

* The Container is running in a namespace that has a default CPU limit, and the
Container is automatically assigned the default limit. Cluster administrators can use a
[LimitRange](/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core/)
to specify a default value for the CPU limit.
-->
<h2 id="如果不指定-cpu-限制">如果不指定 CPU 限制</h2>
<p>如果你没有为容器指定 CPU 限制，则会发生以下情况之一：</p>
<ul>
<li>
<p>容器在可以使用的 CPU 资源上没有上限。因而可以使用所在节点上所有的可用 CPU 资源。</p>
</li>
<li>
<p>容器在具有默认 CPU 限制的名字空间中运行，系统会自动为容器设置默认限制。
集群管理员可以使用
<a href="/docs/reference/generated/kubernetes-api/v1.22/#limitrange-v1-core/">LimitRange</a>
指定 CPU 限制的默认值。</p>
</li>
</ul>
<!--
## If you specify a CPU limit but do not specify a CPU request

If you specify a CPU limit for a Container but do not specify a CPU request, Kubernetes automatically
assigns a CPU request that matches the limit. Similarly, if a Container specifies its own memory limit,
but does not specify a memory request, Kubernetes automatically assigns a memory request that matches
the limit.
-->
<h2 id="如果你设置了-cpu-限制但未设置-cpu-请求">如果你设置了 CPU 限制但未设置 CPU 请求</h2>
<p>如果你为容器指定了 CPU 限制值但未为其设置 CPU 请求，Kubernetes 会自动为其
设置与 CPU 限制相同的 CPU 请求值。类似的，如果容器设置了内存限制值但未设置
内存请求值，Kubernetes 也会为其设置与内存限制值相同的内存请求。</p>
<!-- 
## Motivation for CPU requests and limits

By configuring the CPU requests and limits of the Containers that run in your
cluster, you can make efficient use of the CPU resources available on your cluster
Nodes. By keeping a Pod CPU request low, you give the Pod a good chance of being
scheduled. By having a CPU limit that is greater than the CPU request, you accomplish two things:

* The Pod can have bursts of activity where it makes use of CPU resources that happen to be available.
* The amount of CPU resources a Pod can use during a burst is limited to some reasonable amount.
-->
<h2 id="cpu-请求和限制的初衷">CPU 请求和限制的初衷</h2>
<p>通过配置你的集群中运行的容器的 CPU 请求和限制，你可以有效利用集群上可用的 CPU 资源。
通过将 Pod CPU 请求保持在较低水平，可以使 Pod 更有机会被调度。
通过使 CPU 限制大于 CPU 请求，你可以完成两件事：</p>
<ul>
<li>Pod 可能会有突发性的活动，它可以利用碰巧可用的 CPU 资源。</li>
<li>Pod 在突发负载期间可以使用的 CPU 资源数量仍被限制为合理的数量。</li>
</ul>
<!-- 
## Clean up

Delete your namespace:
-->
<h2 id="清理">清理</h2>
<p>删除名称空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace cpu-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!-- 
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)

* [Configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/)

-->
<h3 id="针对应用开发者">针对应用开发者</h3>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">将内存资源分配给容器和 Pod</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/quality-service-pod/">配置 Pod 服务质量</a></li>
</ul>
<!-- 
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)
* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)
* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)
* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)
* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)
* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)

-->
<h3 id="针对集群管理员">针对集群管理员</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">配置名称空间的默认内存请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">为名字空间配置默认 CPU 请求和限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster//manage-resources/memory-constraint-namespace/">为名字空间配置最小和最大内存限制</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为名字空间配置最小和最大 CPU 约束</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为名字空间配置内存和 CPU 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为名字空间配置 Pod 配额</a></li>
<li><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">配置 API 对象的配额</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-904cea8c8efd5c0d33adbfe579ec2dd2">3.5 - 配置 Pod 的服务质量</h1>
    
	<!--
title: Configure Quality of Service for Pods
content_type: task
weight: 30
-->
<!-- overview -->
<!--
This page shows how to configure Pods so that they will be assigned particular
Quality of Service (QoS) classes. Kubernetes uses QoS classes to make decisions about
scheduling and evicting Pods.
-->
<p>本页介绍怎样配置 Pod 让其获得特定的服务质量（QoS）类。Kubernetes 使用 QoS 类来决定 Pod 的调度和驱逐策略。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## QoS classes

When Kubernetes creates a Pod it assigns one of these QoS classes to the Pod:
-->
<h2 id="qos-classes">QoS 类 </h2>
<p>Kubernetes 创建 Pod 时就给它指定了下列一种 QoS 类：</p>
<ul>
<li>Guaranteed</li>
<li>Burstable</li>
<li>BestEffort</li>
</ul>
<!--
## Create a namespace

Create a namespace so that the resources you create in this exercise are
isolated from the rest of your cluster.
-->
<h2 id="创建命名空间">创建命名空间</h2>
<p>创建一个命名空间，以便将本练习所创建的资源与集群的其余资源相隔离。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create namespace qos-example
</code></pre></div><!--
## Create a Pod that gets assigned a QoS class of Guaranteed

For a Pod to be given a QoS class of Guaranteed:

* Every Container, including init containers, in the Pod must have a memory limit and a memory request, and they must be the same.
* Every Container, including init containers, in the Pod must have a CPU limit and a CPU request, and they must be the same.

Here is the configuration file for a Pod that has one Container. The Container has a memory limit and a
memory request, both equal to 200 MiB. The Container has a CPU limit and a CPU request, both equal to 700 milliCPU:
-->
<h2 id="创建一个-qos-类为-guaranteed-的-pod">创建一个 QoS 类为 Guaranteed 的 Pod</h2>
<p>对于 QoS 类为 Guaranteed 的 Pod：</p>
<ul>
<li>Pod 中的每个容器，包含初始化容器，必须指定内存请求和内存限制，并且两者要相等。</li>
<li>Pod 中的每个容器，包含初始化容器，必须指定 CPU 请求和 CPU 限制，并且两者要相等。</li>
</ul>
<p>下面是包含一个容器的 Pod 配置文件。
容器设置了内存请求和内存限制，值都是 200 MiB。
容器设置了 CPU 请求和 CPU 限制，值都是 700 milliCPU：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/qos/qos-pod.yaml" download="pods/qos/qos-pod.yaml"><code>pods/qos/qos-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-qos-qos-pod-yaml')" title="Copy pods/qos/qos-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-qos-qos-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>qos-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;700m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;700m&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/qos/qos-pod.yaml --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod qos-demo --namespace<span style="color:#666">=</span>qos-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that Kubernetes gave the Pod a QoS class of Guaranteed. The output also
verifies that the Pod Container has a memory request that matches its memory limit, and it has
a CPU request that matches its CPU limit.
-->
<p>结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Guaranteed。
结果也确认了 Pod 容器设置了与内存限制匹配的内存请求，设置了与 CPU 限制匹配的 CPU 请求。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>700m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>700m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">qosClass</span>:<span style="color:#bbb"> </span>Guaranteed<span style="color:#bbb">
</span></code></pre></div><!--
If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes
automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own
CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches
the limit.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果容器指定了自己的内存限制，但没有指定内存请求，Kubernetes 会自动为它指定与内存限制匹配的内存请求。
同样，如果容器指定了自己的 CPU 限制，但没有指定 CPU 请求，Kubernetes 会自动为它指定与 CPU 限制匹配的 CPU 请求。</div>
</blockquote>
<!--
Delete your Pod:
-->
<p>删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod qos-demo --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
## Create a Pod that gets assigned a QoS class of Burstable

A Pod is given a QoS class of Burstable if:

* The Pod does not meet the criteria for QoS class Guaranteed.
* At least one Container in the Pod has a memory or CPU request.

Here is the configuration file for a Pod that has one Container. The Container has a memory limit of 200 MiB
and a memory request of 100 MiB.
-->
<h2 id="创建一个-qos-类为-burstable-的-pod">创建一个 QoS 类为 Burstable 的 Pod</h2>
<p>如果满足下面条件，将会指定 Pod 的 QoS 类为 Burstable：</p>
<ul>
<li>Pod 不符合 Guaranteed QoS 类的标准。</li>
<li>Pod 中至少一个容器具有内存或 CPU 请求。</li>
</ul>
<p>下面是包含一个容器的 Pod 配置文件。
容器设置了内存限制 200 MiB 和内存请求 100 MiB。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/qos/qos-pod-2.yaml" download="pods/qos/qos-pod-2.yaml"><code>pods/qos/qos-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-qos-qos-pod-2-yaml')" title="Copy pods/qos/qos-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-qos-qos-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>qos-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-2.yaml --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod qos-demo-2 --namespace<span style="color:#666">=</span>qos-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that Kubernetes gave the Pod a QoS class of Burstable.
-->
<p>结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">qosClass</span>:<span style="color:#bbb"> </span>Burstable<span style="color:#bbb">
</span></code></pre></div><!--
Delete your Pod:
-->
<p>删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod qos-demo-2 --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
## Create a Pod that gets assigned a QoS class of BestEffort

For a Pod to be given a QoS class of BestEffort, the Containers in the Pod must not
have any memory or CPU limits or requests.

Here is the configuration file for a Pod that has one Container. The Container has no memory or CPU
limits or requests:
-->
<h2 id="创建一个-qos-类为-besteffort-的-pod">创建一个 QoS 类为 BestEffort 的 Pod</h2>
<p>对于 QoS 类为 BestEffort 的 Pod，Pod 中的容器必须没有设置内存和 CPU 限制或请求。</p>
<p>下面是包含一个容器的 Pod 配置文件。
容器没有设置内存和 CPU 限制或请求。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/qos/qos-pod-3.yaml" download="pods/qos/qos-pod-3.yaml"><code>pods/qos/qos-pod-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-qos-qos-pod-3-yaml')" title="Copy pods/qos/qos-pod-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-qos-qos-pod-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-3<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>qos-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-3-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-3.yaml --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod qos-demo-3 --namespace<span style="color:#666">=</span>qos-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that Kubernetes gave the Pod a QoS class of BestEffort.
-->
<p>结果表明 Kubernetes 为 Pod 配置的 QoS 类为 BestEffort。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">qosClass</span>:<span style="color:#bbb"> </span>BestEffort<span style="color:#bbb">
</span></code></pre></div><!--
Delete your Pod:
-->
<p>删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod qos-demo-3 --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
## Create a Pod that has two Containers

Here is the configuration file for a Pod that has two Containers. One container specifies a memory
request of 200 MiB. The other Container does not specify any requests or limits.
-->
<h2 id="创建包含两个容器的-pod">创建包含两个容器的 Pod</h2>
<p>下面是包含两个容器的 Pod 配置文件。
一个容器指定了内存请求 200 MiB。
另外一个容器没有指定任何请求和限制。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/qos/qos-pod-4.yaml" download="pods/qos/qos-pod-4.yaml"><code>pods/qos/qos-pod-4.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-qos-qos-pod-4-yaml')" title="Copy pods/qos/qos-pod-4.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-qos-qos-pod-4-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>qos-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-4-ctr-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-4-ctr-2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Notice that this Pod meets the criteria for QoS class Burstable. That is, it does not meet the
criteria for QoS class Guaranteed, and one of its Containers has a memory request.

Create the Pod:
-->
<p>注意此 Pod 满足 Burstable QoS 类的标准。
也就是说它不满足 Guaranteed QoS 类标准，因为它的一个容器设有内存请求。</p>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/qos/qos-pod-4.yaml --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
View detailed information about the Pod:
-->
<p>查看 Pod 详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod qos-demo-4 --namespace<span style="color:#666">=</span>qos-example --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
The output shows that Kubernetes gave the Pod a QoS class of Burstable:
-->
<p>结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-4-ctr-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>qos-demo-4-ctr-2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">qosClass</span>:<span style="color:#bbb"> </span>Burstable<span style="color:#bbb">
</span></code></pre></div><!--
Delete your Pod:
-->
<p>删除 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod qos-demo-4 --namespace<span style="color:#666">=</span>qos-example
</code></pre></div><!--
## Clean up

Delete your namespace:
-->
<h2 id="环境清理">环境清理</h2>
<p>删除命名空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete namespace qos-example
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For app developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)

* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
-->
<h3 id="应用开发者参考">应用开发者参考</h3>
<ul>
<li>
<p><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为 Pod 和容器分配内存资源</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为 Pod 和容器分配 CPU 资源</a></p>
</li>
</ul>
<!--
### For cluster administrators

* [Configure Default Memory Requests and Limits for a Namespace](/docs/tasks/administer-cluster/memory-default-namespace/)

* [Configure Default CPU Requests and Limits for a Namespace](/docs/tasks/administer-cluster/cpu-default-namespace/)

* [Configure Minimum and Maximum Memory Constraints for a Namespace](/docs/tasks/administer-cluster/memory-constraint-namespace/)

* [Configure Minimum and Maximum CPU Constraints for a Namespace](/docs/tasks/administer-cluster/cpu-constraint-namespace/)

* [Configure Memory and CPU Quotas for a Namespace](/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)

* [Configure a Pod Quota for a Namespace](/docs/tasks/administer-cluster/quota-pod-namespace/)

* [Configure Quotas for API Objects](/docs/tasks/administer-cluster/quota-api-object/)

* [Control Topology Management policies on a node](/docs/tasks/administer-cluster/topology-manager/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">为命名空间配置默认的内存请求和限制</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace">为命名空间配置默认的 CPU 请求和限制</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">为命名空间配置最小和最大内存限制</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">为命名空间配置最小和最大 CPU 限制</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">为命名空间配置内存和 CPU 配额</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">为命名空间配置 Pod 配额</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/quota-api-object/">为 API 对象配置配额</a></p>
</li>
<li>
<p><a href="/zh/docs/tasks/administer-cluster/topology-manager/">控制节点上的拓扑管理策略</a></p>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4219ac6ab56a3b88d20305083d57d03c">3.6 - 为容器分派扩展资源</h1>
    
	<!--
title: Assign Extended Resources to a Container
content_type: task
weight: 40
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [stable]</code>
</div>

<!--
This page shows how to assign extended resources to a Container.
-->
<p>本文介绍如何为容器指定扩展资源。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Before you do this exercise, do the exercise in
[Advertise Extended Resources for a Node](/docs/tasks/administer-cluster/extended-resource-node/).
That will configure one of your Nodes to advertise a dongle resource.
-->
<p>在你开始此练习前，请先练习
<a href="/zh/docs/tasks/administer-cluster/extended-resource-node/">为节点广播扩展资源</a>。
在那个练习中将配置你的一个节点来广播 dongle 资源。</p>
<!-- steps -->
<!--
## Assign an extended resource to a Pod

To request an extended resource, include the `resources:requests` field in your
Container manifest. Extended resources are fully qualified with any domain outside of
`*.kubernetes.io/`. Valid extended resource names have the form `example.com/foo` where
`example.com` is replaced with your organization's domain and `foo` is a
descriptive resource name.

Here is the configuration file for a Pod that has one Container:
-->
<h2 id="给-pod-分派扩展资源">给 Pod 分派扩展资源</h2>
<p>要请求扩展资源，需要在你的容器清单中包括 <code>resources:requests</code> 字段。
扩展资源可以使用任何完全限定名称，只是不能使用 <code>*.kubernetes.io/</code>。
有效的扩展资源名的格式为 <code>example.com/foo</code>，其中 <code>example.com</code> 应被替换为
你的组织的域名，而 <code>foo</code> 则是描述性的资源名称。</p>
<p>下面是包含一个容器的 Pod 配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/extended-resource-pod.yaml" download="pods/resource/extended-resource-pod.yaml"><code>pods/resource/extended-resource-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-extended-resource-pod-yaml')" title="Copy pods/resource/extended-resource-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-extended-resource-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>extended-resource-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>extended-resource-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Container requests 3 dongles.

Create a Pod:
-->
<p>在配置文件中，你可以看到容器请求了 3 个 dongles。</p>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod.yaml
</code></pre></div><!--
Verify that the Pod is running:
-->
<p>检查 Pod 是否运行正常：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod extended-resource-demo
</code></pre></div><!--
Describe the Pod:
-->
<p>描述 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod extended-resource-demo
</code></pre></div><!--
The output shows dongle requests:
-->
<p>输出结果显示 dongle 请求如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">Limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">Requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></code></pre></div><!--
## Attempt to create a second Pod

Here is the configuration file for a Pod that has one Container. The Container requests
two dongles.
-->
<h2 id="尝试创建第二个-pod">尝试创建第二个 Pod</h2>
<p>下面是包含一个容器的 Pod 配置文件，容器请求了 2 个 dongles。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/resource/extended-resource-pod-2.yaml" download="pods/resource/extended-resource-pod-2.yaml"><code>pods/resource/extended-resource-pod-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-resource-extended-resource-pod-2-yaml')" title="Copy pods/resource/extended-resource-pod-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-resource-extended-resource-pod-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>extended-resource-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>extended-resource-demo-2-ctr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">example.com/dongle</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Kubernetes will not be able to satisfy the request for two dongles, because the first Pod
used three of the four available dongles.

Attempt to create a Pod:
-->
<p>Kubernetes 将不能满足 2 个 dongles 的请求，因为第一个 Pod 已经使用了 4 个可用 dongles 中的 3 个。</p>
<p>尝试创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod-2.yaml
</code></pre></div><!--
Describe the Pod
-->
<p>描述 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod extended-resource-demo-2
</code></pre></div><!--
The output shows that the Pod cannot be scheduled, because there is no Node that has
2 dongles available:
-->
<p>输出结果表明 Pod 不能被调度，因为没有一个节点上存在两个可用的 dongles。</p>
<pre tabindex="0"><code>Conditions:
  Type    Status
  PodScheduled  False
...
Events:
  ...
  ... Warning   FailedScheduling  pod (extended-resource-demo-2) failed to fit in any node
fit failure summary on nodes : Insufficient example.com/dongle (1)
</code></pre><!--
View the Pod status:
-->
<p>查看 Pod 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod extended-resource-demo-2
</code></pre></div><!--
The output shows that the Pod was created, but not scheduled to run on a Node.
It has a status of Pending:
-->
<p>输出结果表明 Pod 虽然被创建了，但没有被调度到节点上正常运行。Pod 的状态为 Pending：</p>
<pre tabindex="0"><code>NAME                       READY     STATUS    RESTARTS   AGE
extended-resource-demo-2   0/1       Pending   0          6m
</code></pre><!--
## Clean up

Delete the Pods that you created for this exercise:
-->
<h2 id="清理">清理</h2>
<p>删除本练习中创建的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod extended-resource-demo
kubectl delete pod extended-resource-demo-2
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### For application developers

* [Assign Memory Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/)
* [Assign CPU Resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/)
-->
<h2 id="应用开发者参考">应用开发者参考</h2>
<ul>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-memory-resource/">为容器和 Pod 分配内存资源</a></li>
<li><a href="/zh/docs/tasks/configure-pod-container/assign-cpu-resource/">为容器和 Pod 分配 CPU 资源</a></li>
</ul>
<!--
### For cluster administrators

* [Advertise Extended Resources for a Node](/docs/tasks/administer-cluster/extended-resource-node/)
-->
<h3 id="集群管理员参考">集群管理员参考</h3>
<ul>
<li><a href="/zh/docs/tasks/administer-cluster/extended-resource-node/">为节点广播扩展资源</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-484833fb880d1e179cc2965d15f84da5">3.7 - 配置 Pod 以使用卷进行存储</h1>
    
	<!--
title: Configure a Pod to Use a Volume for Storage
content_type: task
weight: 50
-->
<!-- overview -->
<!--
This page shows how to configure a Pod to use a Volume for storage.

A Container's file system lives only as long as the Container does. So when a
Container terminates and restarts, filesystem changes are lost. For more
consistent storage that is independent of the Container, you can use a
[Volume](/docs/concepts/storage/volumes/). This is especially important for stateful
applications, such as key-value stores (such as Redis) and databases.
-->
<p>此页面展示了如何配置 Pod 以使用卷进行存储。</p>
<p>只要容器存在，容器的文件系统就会存在，因此当一个容器终止并重新启动，对该容器的文件系统改动将丢失。
对于独立于容器的持久化存储，你可以使用<a href="/zh/docs/concepts/storage/volumes/">卷</a>。
这对于有状态应用程序尤为重要，例如键值存储（如 Redis）和数据库。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Configure a volume for a Pod

In this exercise, you create a Pod that runs one Container. This Pod has a
Volume of type
[emptyDir](/docs/concepts/storage/volumes/#emptydir)
that lasts for the life of the Pod, even if the Container terminates and
restarts. Here is the configuration file for the Pod:
-->
<h2 id="configure-a-volume-for-a-pod">为 Pod 配置卷  </h2>
<p>在本练习中，你将创建一个运行 Pod，该 Pod 仅运行一个容器并拥有一个类型为
<a href="/zh/docs/concepts/storage/volumes/#emptydir">emptyDir</a> 的卷，
在整个 Pod 生命周期中一直存在，即使 Pod 中的容器被终止和重启。以下是 Pod 的配置：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/redis.yaml" download="pods/storage/redis.yaml"><code>pods/storage/redis.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-redis-yaml')" title="Copy pods/storage/redis.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-redis-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-storage<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/data/redis<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>redis-storage<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1.Create the Pod:
-->
<ol>
<li>
<p>创建 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/storage/redis.yaml
</code></pre></div></li>
</ol>
<!--
1.Verify that the Pod's Container is running, and then watch for changes to the Pod:
-->
<ol start="2">
<li>
<p>验证 Pod 中的容器是否正在运行，然后留意 Pod 的更改：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod redis --watch
</code></pre></div><p>输出如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME      READY     STATUS    RESTARTS   AGE
redis     1/1       Running   <span style="color:#666">0</span>          13s
</code></pre></div></li>
</ol>
<!--
1.In another terminal, get a shell to the running Container:
-->
<ol start="3">
<li>
<p>在另一个终端，用 shell 连接正在运行的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it redis -- /bin/bash
</code></pre></div></li>
</ol>
<!--
1.In your shell, go to `/data/redis`, and then create a file:
-->
<ol start="4">
<li>
<p>在你的 Shell中，切换到 <code>/data/redis</code> 目录下，然后创建一个文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@redis:/data# <span style="color:#a2f">cd</span> /data/redis/
root@redis:/data/redis# <span style="color:#a2f">echo</span> Hello &gt; test-file
</code></pre></div></li>
</ol>
<!--
1.In your shell, list the running processes:
-->
<ol start="5">
<li>
<p>在你的 Shell 中，列出正在运行的进程：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@redis:/data/redis# apt-get update
root@redis:/data/redis# apt-get install procps
root@redis:/data/redis# ps aux
</code></pre></div><p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
redis        <span style="color:#666">1</span>  0.1  0.1  <span style="color:#666">33308</span>  <span style="color:#666">3828</span> ?        Ssl  00:46   0:00 redis-server *:6379
root        <span style="color:#666">12</span>  0.0  0.0  <span style="color:#666">20228</span>  <span style="color:#666">3020</span> ?        Ss   00:47   0:00 /bin/bash
root        <span style="color:#666">15</span>  0.0  0.0  <span style="color:#666">17500</span>  <span style="color:#666">2072</span> ?        R+   00:48   0:00 ps aux
</code></pre></div></li>
</ol>
<!--
1.In your shell, kill the Redis process:
-->
<ol start="6">
<li>
<p>在你的 Shell 中，结束 Redis 进程：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@redis:/data/redis# <span style="color:#a2f">kill</span> &lt;pid&gt;
</code></pre></div><p>其中 <code>&lt;pid&gt;</code> 是 Redis 进程的 ID (PID)。</p>
</li>
</ol>
<!--
1. In your original terminal, watch for changes to the Redis Pod. Eventually,
you will see something like this:
-->
<ol start="7">
<li>
<p>在你原先终端中，留意 Redis Pod 的更改。最终你将会看到和下面类似的输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME      READY     STATUS     RESTARTS   AGE
redis     1/1       Running    <span style="color:#666">0</span>          13s
redis     0/1       Completed  <span style="color:#666">0</span>         6m
redis     1/1       Running    <span style="color:#666">1</span>         6m
</code></pre></div></li>
</ol>
<!--
At this point, the Container has terminated and restarted. This is because the
Redis Pod has a
[restartPolicy](/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core)
of `Always`.
-->
<p>此时，容器已经终止并重新启动。这是因为 Redis Pod 的
<a href="/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core">restartPolicy</a>
为 <code>Always</code>。</p>
<!--
1.Get a shell into the restarted Container:
-->
<ol>
<li>
<p>用 Shell 进入重新启动的容器中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it redis -- /bin/bash
</code></pre></div></li>
</ol>
<!--
1.In your shell, goto `/data/redis`, and verify that `test-file` is still there.
-->
<ol start="2">
<li>
<p>在你的 Shell 中，进入到 <code>/data/redis</code> 目录下，并确认 <code>test-file</code> 文件是否仍然存在。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@redis:/data/redis# <span style="color:#a2f">cd</span> /data/redis/
root@redis:/data/redis# ls
test-file
</code></pre></div></li>
</ol>
<!--
1.Delete the Pod that you created for this exercise:
-->
<ol start="3">
<li>
<p>删除为此练习所创建的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod redis
</code></pre></div></li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
* See [Volume](/docs/reference/generated/kubernetes-api/v1.22/#volume-v1-core).

* See [Pod](/docs/reference/generated/kubernetes-api/v1.22/#pod-v1-core).

* In addition to the local disk storage provided by `emptyDir`, Kubernetes
supports many different network-attached storage solutions, including PD on
GCE and EBS on EC2, which are preferred for critical data and will handle
details such as mounting and unmounting the devices on the nodes. See
[Volumes](/docs/concepts/storage/volumes/) for more details.
-->
<ul>
<li>参阅 <a href="/docs/reference/generated/kubernetes-api/v1.22/#volume-v1-core">Volume</a>。</li>
<li>参阅 <a href="/docs/reference/generated/kubernetes-api/v1.22/#pod-v1-core">Pod</a>。</li>
<li>除了 <code>emptyDir</code> 提供的本地磁盘存储外，Kubernetes 还支持许多不同的网络附加存储解决方案，
包括 GCE 上的 PD 和 EC2 上的 EBS，它们是关键数据的首选，并将处理节点上的一些细节，
例如安装和卸载设备。了解更多详情请参阅<a href="/zh/docs/concepts/storage/volumes/">卷</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-528d2422215cb9632b7b45e886b023b5">3.8 - 配置 Pod 以使用 PersistentVolume 作为存储</h1>
    
	<!--
title: Configure a Pod to Use a PersistentVolume for Storage
content_type: task
weight: 60
-->
<!-- overview -->
<!--
This page shows how to configure a Pod to use a
<a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PersistentVolumeClaim'>PersistentVolumeClaim</a>
for storage.
Here is a summary of the process:

1. You, as cluster administrator, create a PersistentVolume backed by physical
storage. You do not associate the volume with any Pod.

1. You, now taking the role of a developer / cluster user, create a
PersistentVolumeClaim that is automatically bound to a suitable
PersistentVolume.

1. You create a Pod that uses the above PersistentVolumeClaim for storage.
-->
<p>本文介绍如何配置 Pod 使用
<a class='glossary-tooltip' title='声明在持久卷中定义的存储资源，以便可以将其挂载为容器中的卷。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/storage/persistent-volumes/' target='_blank' aria-label='PersistentVolumeClaim'>PersistentVolumeClaim</a>
作为存储。
以下是该过程的总结：</p>
<ol>
<li>
<p>你作为集群管理员创建由物理存储支持的 PersistentVolume。你不会将卷与任何 Pod 关联。</p>
</li>
<li>
<p>你现在以开发人员或者集群用户的角色创建一个 PersistentVolumeClaim，
它将自动绑定到合适的 PersistentVolume。</p>
</li>
<li>
<p>你创建一个使用 PersistentVolumeClaim 作为存储的 Pod。</p>
</li>
</ol>
<h2 id="准备开始">准备开始</h2>
<!--
* You need to have a Kubernetes cluster that has only one Node, and the kubectl
command-line tool must be configured to communicate with your cluster. If you
do not already have a single-node cluster, you can create one by using
[Minikube](https://minikube.sigs.k8s.io/docs/).

* Familiarize yourself with the material in
[Persistent Volumes](/docs/concepts/storage/persistent-volumes/).
-->
<ul>
<li>你需要一个包含单个节点的 Kubernetes 集群，并且必须配置 kubectl 命令行工具以便与集群交互。
如果还没有单节点集群，可以使用
<a href="https://minikube.sigs.k8s.io/docs/">Minikube</a> 创建一个。
.</li>
<li>熟悉<a href="/zh/docs/concepts/storage/persistent-volumes/">持久卷</a>中的材料。</li>
</ul>
<!-- steps -->
<!--
## Create an index.html file on your Node

Open a shell to the Node in your cluster. How you open a shell depends on how
you set up your cluster. For example, if you are using Minikube, you can open a
shell to your Node by entering `minikube ssh`.

In your shell, create a `/mnt/data` directory:
-->
<h2 id="在你的节点上创建一个-index-html-文件">在你的节点上创建一个 index.html 文件</h2>
<p>打开集群中节点的一个 Shell。
如何打开 Shell 取决于集群的设置。
例如，如果你正在使用 Minikube，那么可以通过输入 <code>minikube ssh</code> 来打开节点的 Shell。</p>
<p>在 Shell 中，创建一个 <code>/mnt/data</code> 目录：</p>
<!--
# This assumes that your Node uses "sudo" to run commands
# as the superuser
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 这里假定你的节点使用 &#34;sudo&#34; 来以超级用户角色执行命令</span>
sudo mkdir /mnt/data
</code></pre></div><!--
In the `/mnt/data` directory, create an `index.html` file:
-->
<p>在 <code>/mnt/data</code> 目录中创建一个 index.html 文件：</p>
<!--
# This again assumes that your Node uses "sudo" to run commands
# as the superuser
-->
<pre tabindex="0"><code># 这里再次假定你的节点使用 &quot;sudo&quot; 来以超级用户角色执行命令
sudo sh -c &quot;echo 'Hello from Kubernetes storage' &gt; /mnt/data/index.html&quot;
</code></pre><!--
If your Node uses a tool for superuser access other than `sudo`, you can
usually make this work if you replace `sudo` with the name of the other tool.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果你的节点使用某工具而不是 <code>sudo</code> 来完成超级用户访问，你可以将上述命令
中的 <code>sudo</code> 替换为该工具的名称。</div>
</blockquote>
<!--
Test that the `index.html` file exists:
-->
<p>测试 <code>index.html</code> 文件确实存在：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat /mnt/data/index.html
</code></pre></div><!-- 
The output should be:
-->
<p>输出应该是：</p>
<pre tabindex="0"><code>Hello from Kubernetes storage
</code></pre><!--
You can now close the shell to your Node.
-->
<p>现在你可以关闭节点的 Shell 了。</p>
<!--
## Create a PersistentVolume

In this exercise, you create a *hostPath* PersistentVolume. Kubernetes supports
hostPath for development and testing on a single-node cluster. A hostPath
PersistentVolume uses a file or directory on the Node to emulate network-attached storage.
-->
<h2 id="创建-persistentvolume">创建 PersistentVolume</h2>
<p>在本练习中，你将创建一个 <em>hostPath</em> 类型的 PersistentVolume。
Kubernetes 支持用于在单节点集群上开发和测试的 hostPath 类型的 PersistentVolume。
hostPath 类型的 PersistentVolume 使用节点上的文件或目录来模拟网络附加存储。</p>
<!--
In a production cluster, you would not use hostPath. Instead a cluster administrator
would provision a network resource like a Google Compute Engine persistent disk,
an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also
use [StorageClasses](/docs/reference/generated/kubernetes-api/v1.22/#storageclass-v1-storage)
to set up
[dynamic provisioning](https://kubernetes.io/blog/2016/10/dynamic-provisioning-and-storage-in-kubernetes).

Here is the configuration file for the hostPath PersistentVolume:
-->
<p>在生产集群中，你不会使用 hostPath。
集群管理员会提供网络存储资源，比如 Google Compute Engine 持久盘卷、NFS 共享卷或 Amazon Elastic Block Store 卷。
集群管理员还可以使用 <a href="/docs/reference/generated/kubernetes-api/v1.22/#storageclass-v1-storage">StorageClasses</a> 来设置<a href="https://kubernetes.io/blog/2016/10/dynamic-provisioning-and-storage-in-kubernetes">动态提供存储</a>。</p>
<p>下面是 hostPath PersistentVolume 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/pv-volume.yaml" download="pods/storage/pv-volume.yaml"><code>pods/storage/pv-volume.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-pv-volume-yaml')" title="Copy pods/storage/pv-volume.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-pv-volume-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>local<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/mnt/data&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the PersistentVolume:
-->
<p>创建 PersistentVolume：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/storage/pv-volume.yaml
</code></pre></div><!--
View information about the PersistentVolume:
-->
<p>查看 PersistentVolume 的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pv task-pv-volume
</code></pre></div><!--
The output shows that the PersistentVolume has a `STATUS` of `Available`. This
means it has not yet been bound to a PersistentVolumeClaim.
-->
<p>输出结果显示该 PersistentVolume 的<code>状态（STATUS）</code> 为 <code>Available</code>。
这意味着它还没有被绑定给 PersistentVolumeClaim。</p>
<pre><code>NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE
task-pv-volume   10Gi       RWO           Retain          Available             manual                   4s
</code></pre>
<!--
## Create a PersistentVolumeClaim

The next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims
to request physical storage. In this exercise, you create a PersistentVolumeClaim
that requests a volume of at least three gibibytes that can provide read-write
access for at least one Node.

Here is the configuration file for the PersistentVolumeClaim:
-->
<h2 id="创建-persistentvolumeclaim">创建 PersistentVolumeClaim</h2>
<p>下一步是创建一个 PersistentVolumeClaim。
Pod 使用 PersistentVolumeClaim 来请求物理存储。
在本练习中，你将创建一个 PersistentVolumeClaim，它请求至少 3 GB 容量的卷，
该卷至少可以为一个节点提供读写访问。</p>
<p>下面是 PersistentVolumeClaim 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/pv-claim.yaml" download="pods/storage/pv-claim.yaml"><code>pods/storage/pv-claim.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-pv-claim-yaml')" title="Copy pods/storage/pv-claim.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-pv-claim-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>3Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the PersistentVolumeClaim:
-->
<p>创建 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/storage/pv-claim.yaml
</code></pre></div><!--
After you create the PersistentVolumeClaim, the Kubernetes control plane looks
for a PersistentVolume that satisfies the claim's requirements. If the control
plane finds a suitable PersistentVolume with the same StorageClass, it binds the
claim to the volume.

Look again at the PersistentVolume:
-->
<p>创建 PersistentVolumeClaim 之后，Kubernetes 控制平面将查找满足申领要求的 PersistentVolume。
如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume，
则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上。</p>
<p>再次查看 PersistentVolume 信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pv task-pv-volume
</code></pre></div><!--
Now the output shows a `STATUS` of `Bound`.
-->
<p>现在输出的 <code>STATUS</code> 为 <code>Bound</code>。</p>
<pre tabindex="0"><code>NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                   STORAGECLASS   REASON    AGE
task-pv-volume   10Gi       RWO           Retain          Bound     default/task-pv-claim   manual                   2m
</code></pre><!--
Look at the PersistentVolumeClaim:
-->
<p>查看 PersistentVolumeClaim：</p>
<pre tabindex="0"><code>kubectl get pvc task-pv-claim
</code></pre><!--
The output shows that the PersistentVolumeClaim is bound to your PersistentVolume,
`task-pv-volume`.
-->
<p>输出结果表明该 PersistentVolumeClaim 绑定了你的 PersistentVolume <code>task-pv-volume</code>。</p>
<pre tabindex="0"><code>NAME            STATUS    VOLUME           CAPACITY   ACCESSMODES   STORAGECLASS   AGE
task-pv-claim   Bound     task-pv-volume   10Gi       RWO           manual         30s
</code></pre><!--
## Create a Pod

The next step is to create a Pod that uses your PersistentVolumeClaim as a volume.

Here is the configuration file for the Pod:
-->
<h2 id="创建-pod">创建 Pod</h2>
<p>下一步是创建一个 Pod， 该 Pod 使用你的 PersistentVolumeClaim 作为存储卷。</p>
<p>下面是 Pod 的 配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/pv-pod.yaml" download="pods/storage/pv-pod.yaml"><code>pods/storage/pv-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-pv-pod-yaml')" title="Copy pods/storage/pv-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-pv-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>task-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;http-server&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/usr/share/nginx/html&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>task-pv-storage<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Notice that the Pod's configuration file specifies a PersistentVolumeClaim, but
it does not specify a PersistentVolume. From the Pod's point of view, the claim
is a volume.

Create the Pod:
-->
<p>注意 Pod 的配置文件指定了 PersistentVolumeClaim，但没有指定 PersistentVolume。
对 Pod 而言，PersistentVolumeClaim 就是一个存储卷。</p>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/storage/pv-pod.yaml
</code></pre></div><!--
Verify that the Container in the Pod is running;
-->
<p>检查 Pod 中的容器是否运行正常：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod task-pv-pod
</code></pre></div><!--
Get a shell to the Container running in your Pod:
-->
<p>打开一个 Shell 访问 Pod 中的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it task-pv-pod -- /bin/bash
</code></pre></div><!--
In your shell, verify that nginx is serving the `index.html` file from the
hostPath volume:
-->
<p>在 Shell 中，验证 nginx 是否正在从 hostPath 卷提供 <code>index.html</code> 文件：</p>
<!--
# Be sure to run these 3 commands inside the root shell that comes from
# running "kubectl exec" in the previous step
-->
<pre tabindex="0"><code># 一定要在上一步 &quot;kubectl exec&quot; 所返回的 Shell 中执行下面三个命令
root@task-pv-pod:/# apt-get update
root@task-pv-pod:/# apt-get install curl
root@task-pv-pod:/# curl localhost
</code></pre><!--
The output shows the text that you wrote to the `index.html` file on the
hostPath volume:
-->
<p>输出结果是你之前写到 hostPath 卷中的 <code>index.html</code> 文件中的内容：</p>
<pre tabindex="0"><code>Hello from Kubernetes storage
</code></pre><!--
If you see that message, you have successfully configured a Pod to
use storage from a PersistentVolumeClaim.
-->
<p>如果你看到此消息，则证明你已经成功地配置了 Pod 使用 PersistentVolumeClaim
的存储。</p>
<!--
## Clean up

Delete the Pod,  the PersistentVolumeClaim and the PersistentVolume:
-->
<h2 id="clean-up">清理   </h2>
<p>删除 Pod、PersistentVolumeClaim 和 PersistentVolume 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod task-pv-pod
kubectl delete pvc task-pv-claim
kubectl delete pv task-pv-volume
</code></pre></div><!--
If you don't already have a shell open to the Node in your cluster,
open a new shell the same way that you did earlier.

In the shell on your Node, remove the file and directory that you created:
-->
<p>如果你还没有连接到集群中节点的 Shell，可以按之前所做操作，打开一个新的 Shell。</p>
<p>在节点的 Shell 上，删除你所创建的目录和文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 这里假定你使用 &#34;sudo&#34; 来以超级用户的角色执行命令</span>
sudo rm /mnt/data/index.html
sudo rmdir /mnt/data
</code></pre></div><!--
You can now close the shell to your Node.
-->
<p>你现在可以关闭连接到节点的 Shell。</p>
<!-- discussion -->
<!--
## Access control

Storage configured with a group ID (GID) allows writing only by Pods using the same
GID. Mismatched or missing GIDs cause permission denied errors. To reduce the
need for coordination with users, an administrator can annotate a PersistentVolume
with a GID. Then the GID is automatically added to any Pod that uses the
PersistentVolume.

Use the `pv.beta.kubernetes.io/gid` annotation as follows:
-->
<h2 id="access-control">访问控制 </h2>
<p>使用组 ID（GID）配置的存储仅允许 Pod 使用相同的 GID 进行写入。
GID 不匹配或缺失将会导致无权访问错误。
为了减少与用户的协调，管理员可以对 PersistentVolume 添加 GID 注解。
这样 GID 就能自动添加到使用 PersistentVolume 的任何 Pod 中。</p>
<p>使用 <code>pv.beta.kubernetes.io/gid</code> 注解的方法如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pv1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pv.beta.kubernetes.io/gid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1234&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
When a Pod consumes a PersistentVolume that has a GID annotation, the annotated GID
is applied to all Containers in the Pod in the same way that GIDs specified in the
Pod's security context are. Every GID, whether it originates from a PersistentVolume
annotation or the Pod's specification, is applied to the first process run in
each Container.
-->
<p>当 Pod 使用带有 GID 注解的 PersistentVolume 时，注解的 GID 会被应用于 Pod 中的所有容器，
应用的方法与 Pod 的安全上下文中指定的 GID 相同。
每个 GID，无论是来自 PersistentVolume 注解还是来自 Pod 规约，都会被应用于每个容器中
运行的第一个进程。</p>
<!--
When a Pod consumes a PersistentVolume, the GIDs associated with the
PersistentVolume are not present on the Pod resource itself.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 当 Pod 使用 PersistentVolume 时，与 PersistentVolume 关联的 GID 不会在 Pod
资源本身的对象上出现。</div>
</blockquote>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [PersistentVolumes](/docs/concepts/storage/persistent-volumes/).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a></li>
<li>阅读<a href="https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md">持久存储设计文档</a></li>
</ul>
<!--
### Reference
-->
<h3 id="参考">参考</h3>
<ul>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#persistentvolume-v1-core">PersistentVolume</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#persistentvolumespec-v1-core">PersistentVolumeSpec</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#persistentvolumeclaim-v1-core">PersistentVolumeClaim</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#persistentvolumeclaimspec-v1-core">PersistentVolumeClaimSpec</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4621938ba53c04a77f51b5938a583439">3.9 - 配置 Pod 使用投射卷作存储</h1>
    
	<!--
reviewers:
- jpeeler
- pmorie
title: Configure a Pod to Use a Projected Volume for Storage
content_type: task
weight: 70
-->
<!-- overview -->
<!--
This page shows how to use a [`projected`](/docs/concepts/storage/volumes/#projected) volume to mount
several existing volume sources into the same directory. Currently, `secret`, `configMap`, `downwardAPI`,
and `serviceAccountToken` volumes can be projected.
-->
<p>本文介绍怎样通过<a href="/zh/docs/concepts/storage/volumes/#projected"><code>projected</code></a> 卷将现有的多个卷资源挂载到相同的目录。
当前，<code>secret</code>、<code>configMap</code>、<code>downwardAPI</code> 和 <code>serviceAccountToken</code> 卷可以被投射。</p>
<!--
`serviceAccountToken` is not a volume type.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>serviceAccountToken</code> 不是一种卷类型</div>
</blockquote>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Configure a projected volume for a pod

In this exercise, you create username and password Secrets from local files. You then create a Pod that runs one Container, using a [`projected`](/docs/concepts/storage/volumes/#projected) Volume to mount the Secrets into the same shared directory.

Here is the configuration file for the Pod:
-->
<h2 id="为-pod-配置-projected-卷">为 Pod 配置 projected 卷</h2>
<p>本练习中，您将从本地文件来创建包含有用户名和密码的 Secret。然后创建运行一个容器的 Pod，
该 Pod 使用<a href="/zh/docs/concepts/storage/volumes/#projected"><code>projected</code></a> 卷将 Secret 挂载到相同的路径下。</p>
<p>下面是 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/storage/projected.yaml" download="pods/storage/projected.yaml"><code>pods/storage/projected.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-storage-projected-yaml')" title="Copy pods/storage/projected.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-storage-projected-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-projected-volume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-projected-volume<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;86400&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/projected-volume&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>all-in-one<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">projected</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">sources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>user<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pass<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<ol>
<li>
<!--Create the Secrets:-->
<p>创建 Secret:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建包含用户名和密码的文件:</span>
<span style="color:#a2f">echo</span> -n <span style="color:#b44">&#34;admin&#34;</span> &gt; ./username.txt
<span style="color:#a2f">echo</span> -n <span style="color:#b44">&#34;1f2d1e2e67df&#34;</span> &gt; ./password.txt--&gt;

<span style="color:#080;font-style:italic"># 将上述文件引用到 Secret：</span>
kubectl create secret generic user --from-file<span style="color:#666">=</span>./username.txt
kubectl create secret generic pass --from-file<span style="color:#666">=</span>./password.txt
</code></pre></div></li>
<li>
<!--Create the Pod:-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/storage/projected.yaml
</code></pre></div></li>
<li>
<!--
Verify that the Pod's Container is running, and then watch for changes to
the Pod:
-->
<p>确认 Pod 中的容器运行正常，然后监视 Pod 的变化：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get --watch pod test-projected-volume
</code></pre></div><!--The output looks like this:-->
<p>输出结果和下面类似：</p>
<pre tabindex="0"><code>NAME                    READY     STATUS    RESTARTS   AGE
test-projected-volume   1/1       Running   0          14s
</code></pre></li>
<li>
<!--In another terminal, get a shell to the running Container:-->
<p>在另外一个终端中，打开容器的 shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it test-projected-volume -- /bin/sh
</code></pre></div></li>
<li>
<!--In your shell, verify that the `projected-volume` directory contains your projected sources:-->
<p>在 shell 中，确认 <code>projected-volume</code> 目录包含你的投射源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ls /projected-volume/
</code></pre></div></li>
</ol>
<!--
## Clean up
-->
<h2 id="清理">清理</h2>
<!--
Delete the Pod and the Secrets:
-->
<p>删除 Pod 和 Secret:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod test-projected-volume
kubectl delete secret user pass
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* Learn more about [`projected`](/docs/concepts/storage/volumes/#projected) volumes.
* Read the [all-in-one volume](https://github.com/kubernetes/community/blob/main/contributors/design-proposals/node/all-in-one-volume.md) design document.
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/storage/volumes/#projected"><code>projected</code></a> 卷。</li>
<li>阅读<a href="https://github.com/kubernetes/community/blob/main/contributors/design-proposals/node/all-in-one-volume.md">一体卷</a>设计文档。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-abd895c0803315e9717e6ff9ec4e3d30">3.10 - 为 Pod 或容器配置安全性上下文</h1>
    
	<!--
reviewers:
- erictune
- mikedanese
- thockin
title: Configure a Security Context for a Pod or Container
content_type: task
weight: 80
-->
<!-- overview -->
<!--
A security context defines privilege and access control settings for
a Pod or Container. Security context settings include, but are not limited to:

* Discretionary Access Control: Permission to access an object, like a file, is based on
  [user ID (UID) and group ID (GID)](https://wiki.archlinux.org/index.php/users_and_groups).
* [Security Enhanced Linux (SELinux)](https://en.wikipedia.org/wiki/Security-Enhanced_Linux): Objects are assigned security labels.
* Running as privileged or unprivileged.
* [Linux Capabilities](https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/): 
  Give a process some privileges, but not all the privileges of the root user.
-->
<p>安全上下文（Security Context）定义 Pod 或 Container 的特权与访问控制设置。
安全上下文包括但不限于：</p>
<ul>
<li>自主访问控制（Discretionary Access Control）：基于
<a href="https://wiki.archlinux.org/index.php/users_and_groups">用户 ID（UID）和组 ID（GID）</a>.
来判定对对象（例如文件）的访问权限。</li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%AE%89%E5%85%A8%E5%A2%9E%E5%BC%BA%E5%BC%8FLinux">安全性增强的 Linux（SELinux）</a>：
为对象赋予安全性标签。</li>
<li>以特权模式或者非特权模式运行。</li>
<li><a href="https://linux-audit.com/linux-capabilities-hardening-linux-binaries-by-removing-setuid/">Linux 权能</a>:
为进程赋予 root 用户的部分特权而非全部特权。</li>
</ul>
<!--
* [AppArmor](/docs/tutorials/clusters/apparmor/): Use program profiles to restrict the capabilities of individual programs.
* [Seccomp](https://en.wikipedia.org/wiki/Seccomp): Filter a process's system calls.
* AllowPrivilegeEscalation: Controls whether a process can gain more
  privileges than its parent process. This bool directly controls whether the
  [`no_new_privs`](https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt)
  flag gets set on the container process. AllowPrivilegeEscalation is true
  always when the container is: 1) run as Privileged OR 2) has `CAP_SYS_ADMIN`.
* readOnlyRootFilesystem: Mounts the container's root filesystem as read-only.
-->
<ul>
<li><a href="/zh/docs/tutorials/clusters/apparmor/">AppArmor</a>：使用程序框架来限制个别程序的权能。</li>
<li><a href="https://en.wikipedia.org/wiki/Seccomp">Seccomp</a>：过滤进程的系统调用。</li>
<li>AllowPrivilegeEscalation：控制进程是否可以获得超出其父进程的特权。
此布尔值直接控制是否为容器进程设置
<a href="https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt"><code>no_new_privs</code></a>标志。
当容器以特权模式运行或者具有 <code>CAP_SYS_ADMIN</code> 权能时，AllowPrivilegeEscalation 总是为 true。</li>
<li>readOnlyRootFilesystem：以只读方式加载容器的根文件系统。</li>
</ul>
<!--
The above bullets are not a complete set of security context settings - please see
[SecurityContext](/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core)
for a comprehensive list.

For more information about security mechanisms in Linux, see
[Overview of Linux Kernel Security Features](https://www.linux.com/learn/overview-linux-kernel-security-features)
-->
<p>以上条目不是安全上下文设置的完整列表 -- 请参阅
<a href="/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core">SecurityContext</a>
了解其完整列表。</p>
<p>关于在 Linux 系统中的安全机制的更多信息，可参阅
<a href="https://www.linux.com/learn/overview-linux-kernel-security-features">Linux 内核安全性能力概述</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Set the security context for a Pod

To specify security settings for a Pod, include the `securityContext` field
in the Pod specification. The `securityContext` field is a
[PodSecurityContext](/docs/reference/generated/kubernetes-api/v1.22/#podsecuritycontext-v1-core) object.
The security settings that you specify for a Pod apply to all Containers in the Pod.
Here is a configuration file for a Pod that has a `securityContext` and an `emptyDir` volume:
-->
<h2 id="set-the-security-context-for-a-pod">为 Pod 设置安全性上下文  </h2>
<p>要为 Pod 设置安全性设置，可在 Pod 规约中包含 <code>securityContext</code> 字段。<code>securityContext</code> 字段值是一个
<a href="/docs/reference/generated/kubernetes-api/v1.22/#podsecuritycontext-v1-core">PodSecurityContext</a>
对象。你为 Pod 所设置的安全性配置会应用到 Pod 中所有 Container 上。
下面是一个 Pod 的配置文件，该 Pod 定义了 <code>securityContext</code> 和一个 <code>emptyDir</code> 卷：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/security-context.yaml" download="pods/security/security-context.yaml"><code>pods/security/security-context.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-security-context-yaml')" title="Copy pods/security/security-context.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-security-context-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>security-context-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">runAsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">3000</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">2000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sec-ctx-vol<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sec-ctx-demo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;sleep 1h&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sec-ctx-vol<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/data/demo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">allowPrivilegeEscalation</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, the `runAsUser` field specifies that for any Containers in
the Pod, all processes run with user ID 1000. The `runAsGroup` field specifies the primary group ID of 3000 for
all processes within any containers of the Pod. If this field is omitted, the primary group ID of the containers
will be root(0). Any files created will also be owned by user 1000 and group 3000 when `runAsGroup` is specified.
Since `fsGroup` field is specified, all processes of the container are also part of the supplementary group ID 2000.
The owner for volume `/data/demo` and any files created in that volume will be Group ID 2000.

Create the Pod:
-->
<p>在配置文件中，<code>runAsUser</code> 字段指定 Pod 中的所有容器内的进程都使用用户 ID 1000
来运行。<code>runAsGroup</code> 字段指定所有容器中的进程都以主组 ID 3000 来运行。
如果忽略此字段，则容器的主组 ID 将是 root（0）。
当 <code>runAsGroup</code> 被设置时，所有创建的文件也会划归用户 1000 和组 3000。
由于 <code>fsGroup</code> 被设置，容器中所有进程也会是附组 ID 2000 的一部分。
卷 <code>/data/demo</code> 及在该卷中创建的任何文件的属主都会是组 ID 2000。</p>
<p>创建该 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/security-context.yaml
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>检查 Pod 的容器处于运行状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod security-context-demo
</code></pre></div><!--
Get a shell to the running Container:
-->
<p>开启一个 Shell 进入到运行中的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it security-context-demo -- sh
</code></pre></div><!--
In your shell, list the running processes:
-->
<p>在你的 Shell 中，列举运行中的进程：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ps
</code></pre></div><!--
The output shows that the processes are running as user 1000, which is the value of `runAsUser`:
-->
<p>输出显示进程以用户 1000 运行，即 <code>runAsUser</code> 所设置的值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">PID   USER     TIME  COMMAND
    <span style="color:#666">1</span> <span style="color:#666">1000</span>      0:00 sleep 1h
    <span style="color:#666">6</span> <span style="color:#666">1000</span>      0:00 sh
...
</code></pre></div><!--
In your shell, navigate to `/data`, and list the one directory:
-->
<p>在你的 Shell 中，进入 <code>/data</code> 目录列举其内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">cd</span> /data
ls -l
</code></pre></div><!--
The output shows that the `/data/demo` directory has group ID 2000, which is
the value of `fsGroup`.
-->
<p>输出显示 <code>/data/demo</code> 目录的组 ID 为 2000，即 <code>fsGroup</code> 的设置值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">drwxrwsrwx <span style="color:#666">2</span> root <span style="color:#666">2000</span> <span style="color:#666">4096</span> Jun  <span style="color:#666">6</span> 20:08 demo
</code></pre></div><!--
In your shell, navigate to `/data/demo`, and create a file:
-->
<p>在你的 Shell 中，进入到 <code>/data/demo</code> 目录下创建一个文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">cd</span> demo
<span style="color:#a2f">echo</span> hello &gt; testfile
</code></pre></div><!--
List the file in the `/data/demo` directory:
-->
<p>列举 <code>/data/demo</code> 目录下的文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ls -l
</code></pre></div><!--
The output shows that `testfile` has group ID 2000, which is the value of `fsGroup`.
-->
<p>输出显示 <code>testfile</code> 的组 ID 为 2000，也就是 <code>fsGroup</code> 所设置的值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">-rw-r--r-- <span style="color:#666">1</span> <span style="color:#666">1000</span> <span style="color:#666">2000</span> <span style="color:#666">6</span> Jun  <span style="color:#666">6</span> 20:08 testfile
</code></pre></div><!--
Run the following command:
-->
<p>运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">id
</code></pre></div><p>输出为：</p>
<pre tabindex="0"><code>uid=1000 gid=3000 groups=2000
</code></pre><!--
You will see that gid is 3000 which is same as `runAsGroup` field. If the `runAsGroup` was omitted the gid would
remain as 0(root) and the process will be able to interact with files that are owned by root(0) group and that have
the required group permissions for root(0) group.

Exit your shell:
-->
<p>你会看到 <code>gid</code> 值为 3000，也就是 <code>runAsGroup</code> 字段的值。
如果 <code>runAsGroup</code> 被忽略，则 <code>gid</code> 会取值 0（root），而进程就能够与 root
用户组所拥有以及要求 root 用户组访问权限的文件交互。</p>
<p>退出你的 Shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">exit</span>
</code></pre></div><!--
## Configure volume permission and ownership change policy for Pods
-->
<h2 id="为-pod-配置卷访问权限和属主变更策略">为 Pod 配置卷访问权限和属主变更策略</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code>
</div>

<!--
By default, Kubernetes recursively changes ownership and permissions for the contents of each
volume to match the `fsGroup` specified in a Pod's `securityContext` when that volume is
mounted.
For large volumes, checking and changing ownership and permissions can take a lot of time,
slowing Pod startup. You can use the `fsGroupChangePolicy` field inside a `securityContext`
to control the way that Kubernetes checks and manages ownership and permissions
for a volume.
-->
<p>默认情况下，Kubernetes 在挂载一个卷时，会递归地更改每个卷中的内容的属主和访问权限，使之与 Pod
的 <code>securityContext</code> 中指定的 <code>fsGroup</code> 匹配。
对于较大的数据卷，检查和变更属主与访问权限可能会花费很长时间，降低 Pod 启动速度。
你可以在 <code>securityContext</code> 中使用 <code>fsGroupChangePolicy</code> 字段来控制 Kubernetes
检查和管理卷属主和访问权限的方式。</p>
<!--
**fsGroupChangePolicy** -  `fsGroupChangePolicy` defines behavior for changing ownership and permission of the volume
before being exposed inside a Pod. This field only applies to volume types that support
`fsGroup` controlled ownership and permissions. This field has two possible values:

* _OnRootMismatch_: Only change permissions and ownership if permission and ownership of root directory does not match with expected permissions of the volume. This could help shorten the time it takes to change ownership and permission of a volume.
* _Always_: Always change permission and ownership of the volume when volume is mounted.

For example:
-->
<p><strong>fsGroupChangePolicy</strong> - <code>fsGroupChangePolicy</code> 定义在卷被暴露给 Pod 内部之前对其
内容的属主和访问许可进行变更的行为。此字段仅适用于那些支持使用 <code>fsGroup</code> 来
控制属主与访问权限的卷类型。此字段的取值可以是：</p>
<ul>
<li><code>OnRootMismatch</code>：只有根目录的属主与访问权限与卷所期望的权限不一致时，
才改变其中内容的属主和访问权限。这一设置有助于缩短更改卷的属主与访问
权限所需要的时间。</li>
<li><code>Always</code>：在挂载卷时总是更改卷中内容的属主和访问权限。</li>
</ul>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">runAsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">3000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsGroup</span>:<span style="color:#bbb"> </span><span style="color:#666">2000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">fsGroupChangePolicy</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;OnRootMismatch&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
This field has no effect on ephemeral volume types such as
[`secret`](/docs/concepts/storage/volumes/#secret),
[`configMap`](/docs/concepts/storage/volumes/#configmap),
and [`emptydir`](/docs/concepts/storage/volumes/#emptydir).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 此字段对于<a href="/zh/docs/concepts/storage/volumes/#secret"><code>secret</code></a>、
<a href="/zh/docs/concepts/storage/volumes/#configmap"><code>configMap</code></a>
和 <a href="/zh/docs/concepts/storage/volumes/#emptydir"><code>emptydir</code></a>
这类临时性存储无效。</div>
</blockquote>
<!--
## Set the security context for a Container

To specify security settings for a Container, include the `securityContext` field
in the Container manifest. The `securityContext` field is a
[SecurityContext](/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core) object.
Security settings that you specify for a Container apply only to
the individual Container, and they override settings made at the Pod level when
there is overlap. Container settings do not affect the Pod's Volumes.

Here is the configuration file for a Pod that has one Container. Both the Pod
and the Container have a `securityContext` field:
-->
<h2 id="set-the-security-context-for-a-container">为 Container 设置安全性上下文 </h2>
<p>若要为 Container 设置安全性配置，可以在 Container 清单中包含 <code>securityContext</code>
字段。<code>securityContext</code> 字段的取值是一个
<a href="/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core">SecurityContext</a>
对象。你为 Container 设置的安全性配置仅适用于该容器本身，并且所指定的设置
在与 Pod 层面设置的内容发生重叠时，会重载后者。Container 层面的设置不会影响
到 Pod 的卷。</p>
<p>下面是一个 Pod 的配置文件，其中包含一个 Container。Pod 和 Container 都有
<code>securityContext</code> 字段：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/security-context-2.yaml" download="pods/security/security-context-2.yaml"><code>pods/security/security-context-2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-security-context-2-yaml')" title="Copy pods/security/security-context-2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-security-context-2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>security-context-demo-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sec-ctx-demo-2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">runAsUser</span>:<span style="color:#bbb"> </span><span style="color:#666">2000</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">allowPrivilegeEscalation</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建该 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/security-context-2.yaml
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>验证 Pod 中的容器处于运行状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod security-context-demo-2
</code></pre></div><!--
Get a shell into the running Container:
-->
<p>启动一个 Shell 进入到运行中的容器内：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it security-context-demo-2 -- sh
</code></pre></div><!--
In your shell, list the running processes:
-->
<p>在你的 Shell 中，列举运行中的进程：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ps aux
</code></pre></div><!--
The output shows that the processes are running as user 2000. This is the value
of `runAsUser` specified for the Container. It overrides the value 1000 that is
specified for the Pod.
-->
<p>输出显示进程以用户 2000 账号运行。该值是在 Container 的 <code>runAsUser</code> 中设置的。
该设置值重载了 Pod 层面所设置的值 1000。</p>
<pre tabindex="0"><code>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
2000         1  0.0  0.0   4336   764 ?        Ss   20:36   0:00 /bin/sh -c node server.js
2000         8  0.1  0.5 772124 22604 ?        Sl   20:36   0:00 node server.js
...
</code></pre><!--
Exit your shell:
-->
<p>退出你的 Shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">exit</span>
</code></pre></div><!--
## Set capabilities for a Container

With [Linux capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html),
you can grant certain privileges to a process without granting all the privileges
of the root user. To add or remove Linux capabilities for a Container, include the
`capabilities` field in the `securityContext` section of the Container manifest.

First, see what happens when you don't include a `capabilities` field.
Here is configuration file that does not add or remove any Container capabilities:
-->
<h2 id="set-capabilities-for-a-container">为 Container 设置权能  </h2>
<p>使用 <a href="https://man7.org/linux/man-pages/man7/capabilities.7.html">Linux 权能</a>，你可以
赋予进程 root 用户所拥有的某些特权，但不必赋予其全部特权。
要为 Container 添加或移除 Linux 权能，可以在 Container 清单的 <code>securityContext</code> 节
包含 <code>capabilities</code> 字段。</p>
<p>首先，查看不包含 <code>capabilities</code> 字段时候会发生什么。
下面是一个配置文件，其中没有添加或移除容器的权能：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/security-context-3.yaml" download="pods/security/security-context-3.yaml"><code>pods/security/security-context-3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-security-context-3-yaml')" title="Copy pods/security/security-context-3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-security-context-3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>security-context-demo-3<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sec-ctx-3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建该 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/security-context-3.yaml
</code></pre></div><!--
Verify that the Pod's Container is running:
-->
<p>验证 Pod 的容器处于运行状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod security-context-demo-3
</code></pre></div><!--
Get a shell into the running Container:
-->
<p>启动一个 Shell 进入到运行中的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it security-context-demo-3 -- sh
</code></pre></div><!--
In your shell, list the running processes:
-->
<p>在你的 Shell 中，列举运行中的进程：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ps aux
</code></pre></div><!--
The output shows the process IDs (PIDs) for the Container:
-->
<p>输出显示容器中进程 ID（PIDs）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">USER  PID %CPU %MEM    VSZ   RSS TTY   STAT START   TIME COMMAND
root    <span style="color:#666">1</span>  0.0  0.0   <span style="color:#666">4336</span>   <span style="color:#666">796</span> ?     Ss   18:17   0:00 /bin/sh -c node server.js
root    <span style="color:#666">5</span>  0.1  0.5 <span style="color:#666">772124</span> <span style="color:#666">22700</span> ?     Sl   18:17   0:00 node server.js
</code></pre></div><!--
In your shell, view the status for process 1:
-->
<p>在你的 Shell 中，查看进程 1 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">cd</span> /proc/1
cat status
</code></pre></div><!--
The output shows the capabilities bitmap for the process:
-->
<p>输出显示进程的权能位图：</p>
<pre tabindex="0"><code>...
CapPrm:	00000000a80425fb
CapEff:	00000000a80425fb
...
</code></pre><!--
Make a note of the capabilities bitmap, and then exit your shell:
-->
<p>记下进程权能位图，之后退出你的 Shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">exit</span>
</code></pre></div><!--
Next, run a Container that is the same as the preceding container, except
that it has additional capabilities set.

Here is the configuration file for a Pod that runs one Container. The configuration
adds the `CAP_NET_ADMIN` and `CAP_SYS_TIME` capabilities:
-->
<p>接下来运行一个与前例中容器相同的容器，只是这个容器有一些额外的权能设置。</p>
<p>下面是一个 Pod 的配置，其中运行一个容器。配置为容器添加 <code>CAP_NET_ADMIN</code> 和
<code>CAP_SYS_TIME</code> 权能：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/security/security-context-4.yaml" download="pods/security/security-context-4.yaml"><code>pods/security/security-context-4.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-security-security-context-4-yaml')" title="Copy pods/security/security-context-4.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-security-security-context-4-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>security-context-demo-4<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>sec-ctx-4<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">capabilities</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">add</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;NET_ADMIN&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;SYS_TIME&#34;</span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/security/security-context-4.yaml
</code></pre></div><!--
Get a shell into the running Container:
-->
<p>启动一个 Shell，进入到运行中的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it security-context-demo-4 -- sh
</code></pre></div><!--
In your shell, view the capabilities for process 1:
-->
<p>在你的 Shell 中，查看进程 1 的权能：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">cd</span> /proc/1
cat status
</code></pre></div><!--
The output shows capabilities bitmap for the process:
-->
<p>输出显示的是进程的权能位图：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">...
CapPrm:	00000000aa0435fb
CapEff:	00000000aa0435fb
...
</code></pre></div><!--
Compare the capabilities of the two Containers:
-->
<p>比较两个容器的权能位图：</p>
<pre tabindex="0"><code>00000000a80425fb
00000000aa0435fb
</code></pre><!--
In the capability bitmap of the first container, bits 12 and 25 are clear. In the second container,
bits 12 and 25 are set. Bit 12 is `CAP_NET_ADMIN`, and bit 25 is `CAP_SYS_TIME`.
See [capability.h](https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h)
for definitions of the capability constants.
-->
<p>在第一个容器的权能位图中，位 12 和 25 是没有设置的。在第二个容器中，位 12
和 25 是设置了的。位 12 是 <code>CAP_NET_ADMIN</code> 而位 25 则是 <code>CAP_SYS_TIME</code>。
参见 <a href="https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h">capability.h</a>
了解权能常数的定义。</p>
<!--
Linux capability constants have the form `CAP_XXX`. But when you list capabilities in your Container manifest, you must omit the `CAP_` portion of the constant. For example, to add `CAP_SYS_TIME`, include `SYS_TIME` in your list of capabilities.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> Linux 权能常数定义的形式为 <code>CAP_XXX</code>。但是你在 Container 清单中列举权能时，
要将权能名称中的 <code>CAP_</code> 部分去掉。例如，要添加 <code>CAP_SYS_TIME</code>，可在权能
列表中添加 <code>SYS_TIME</code>。</div>
</blockquote>
<!--
## Set the Seccomp Profile for a Container

To set the Seccomp profile for a Container, include the `seccompProfile` field
in the `securityContext` section of your Pod or Container manifest. The
`seccompProfile` field is a
[SeccompProfile](/docs/reference/generated/kubernetes-api/v1.22/#seccompprofile-v1-core) object consisting of `type` and `localhostProfile`.
Valid options for `type` include `RuntimeDefault`, `Unconfined`, and
`Localhost`. `localhostProfile` must only be set set if `type: Localhost`. It
indicates the path of the pre-configured profile on the node, relative to the
kubelet's configured Seccomp profile location (configured with the `-root-dir`
flag).

Here is an example that sets the Seccomp profile to the node's container runtime
default profile:
-->
<h2 id="为容器设置-seccomp-样板">为容器设置 Seccomp 样板</h2>
<p>若要为容器设置 Seccomp 样板（Profile），可在你的 Pod 或 Container 清单的
<code>securityContext</code> 节中包含 <code>seccompProfile</code> 字段。该字段是一个
<a href="/docs/reference/generated/kubernetes-api/v1.22/#seccompprofile-v1-core">SeccompProfile</a>
对象，包含 <code>type</code> 和 <code>localhostProfile</code> 属性。
<code>type</code> 的合法选项包括 <code>RuntimeDefault</code>、<code>Unconfined</code> 和 <code>Localhost</code>。
<code>localhostProfile</code> 只能在 <code>type: Localhost</code> 配置下才需要设置。
该字段标明节点上预先配置的样板的路径，路径是相对于 kubelet 所配置的
Seccomp 样板路径（使用 <code>--root-dir</code> 配置）而言的。</p>
<p>下面是一个例子，设置容器使用节点上容器运行时的默认样板作为 Seccomp 样板：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">seccompProfile</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RuntimeDefault<span style="color:#bbb">
</span></code></pre></div><!--
Here is an example that sets the Seccomp profile to a pre-configured file at
`<kubelet-root-dir>/seccomp/my-profiles/profile-allow.json`:
-->
<p>下面是另一个例子，将 Seccomp 的样板设置为位于
<code>&lt;kubelet-根目录&gt;/seccomp/my-profiles/profile-allow.json</code>
的一个预先配置的文件。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">seccompProfile</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Localhost<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">localhostProfile</span>:<span style="color:#bbb"> </span>my-profiles/profile-allow.json<span style="color:#bbb">
</span></code></pre></div><!--
## Assign SELinux labels to a Container

To assign SELinux labels to a Container, include the `seLinuxOptions` field in
the `securityContext` section of your Pod or Container manifest. The
`seLinuxOptions` field is an
[SELinuxOptions](/docs/reference/generated/kubernetes-api/v1.22/#selinuxoptions-v1-core)
object. Here's an example that applies an SELinux level:
-->
<h2 id="为-container-赋予-selinux-标签">为 Container 赋予 SELinux 标签</h2>
<p>若要给 Container 设置 SELinux 标签，可以在 Pod 或 Container 清单的
<code>securityContext</code> 节包含 <code>seLinuxOptions</code> 字段。
<code>seLinuxOptions</code> 字段的取值是一个
<a href="/docs/reference/generated/kubernetes-api/v1.22/#selinuxoptions-v1-core">SELinuxOptions</a>
对象。下面是一个应用 SELinux 标签的例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">seLinuxOptions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;s0:c123,c456&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
To assign SELinux labels, the SELinux security module must be loaded on the host operating system.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 要指定 SELinux，需要在宿主操作系统中装载 SELinux 安全性模块。</div>
</blockquote>
<!--
## Discussion

The security context for a Pod applies to the Pod's Containers and also to
the Pod's Volumes when applicable. Specifically `fsGroup` and `seLinuxOptions` are
applied to Volumes as follows:
-->
<h2 id="discussion">讨论  </h2>
<p>Pod 的安全上下文适用于 Pod 中的容器，也适用于 Pod 所挂载的卷（如果有的话）。
尤其是，<code>fsGroup</code> 和 <code>seLinuxOptions</code> 按下面的方式应用到挂载卷上：</p>
<!--
* `fsGroup`: Volumes that support ownership management are modified to be owned
and writable by the GID specified in `fsGroup`. See the
[Ownership Management design document](https://git.k8s.io/community/contributors/design-proposals/storage/volume-ownership-management.md)
for more details.

* `seLinuxOptions`: Volumes that support SELinux labeling are relabeled to be accessible
by the label specified under `seLinuxOptions`. Usually you only
need to set the `level` section. This sets the
[Multi-Category Security (MCS)](https://selinuxproject.org/page/NB_MLS)
label given to all Containers in the Pod as well as the Volumes.
-->
<ul>
<li>
<p><code>fsGroup</code>：支持属主管理的卷会被修改，将其属主变更为 <code>fsGroup</code> 所指定的 GID，
并且对该 GID 可写。进一步的细节可参阅
<a href="https://git.k8s.io/community/contributors/design-proposals/storage/volume-ownership-management.md">属主变更设计文档</a>。</p>
</li>
<li>
<p><code>seLinuxOptions</code>：支持 SELinux 标签的卷会被重新打标签，以便可被 <code>seLinuxOptions</code>
下所设置的标签访问。通常你只需要设置 <code>level</code> 部分。
该部分设置的是赋予 Pod 中所有容器及卷的
<a href="https://selinuxproject.org/page/NB_MLS">多类别安全性（Multi-Category Security，MCS)</a>标签。</p>
<!--
After you specify an MCS label for a Pod, all Pods with the same label can
access the Volume. If you need inter-Pod protection, you must assign a unique
MCS label to each Pod.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 在为 Pod 设置 MCS 标签之后，所有带有相同标签的 Pod 可以访问该卷。
如果你需要跨 Pod 的保护，你必须为每个 Pod 赋予独特的 MCS 标签。</div>
</blockquote>

</li>
</ul>
<!--
## Clean up

Delete the Pod:
-->
<h2 id="清理">清理</h2>
<p>删除之前创建的所有 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod security-context-demo
kubectl delete pod security-context-demo-2
kubectl delete pod security-context-demo-3
kubectl delete pod security-context-demo-4
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* [PodSecurityContext](/docs/reference/generated/kubernetes-api/v1.22/#podsecuritycontext-v1-core)
* [SecurityContext](/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core)
* [Tuning Docker with the newest security enhancements](https://opensource.com/business/15/3/docker-security-tuning)
* [Security Contexts design document](https://git.k8s.io/community/contributors/design-proposals/auth/security_context.md)
* [Ownership Management design document](https://git.k8s.io/community/contributors/design-proposals/storage/volume-ownership-management.md)
* [Pod Security Policies](/docs/concepts/policy/pod-security-policy/)
* [AllowPrivilegeEscalation design
  document](https://git.k8s.io/community/contributors/design-proposals/auth/no-new-privs.md)
-->
<ul>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#podsecuritycontext-v1-core">PodSecurityContext</a> API 定义</li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core">SecurityContext</a> API 定义</li>
<li><a href="https://opensource.com/business/15/3/docker-security-tuning">使用最新的安全性增强来调优 Docker</a></li>
<li><a href="https://git.k8s.io/community/contributors/design-proposals/auth/security_context.md">安全性上下文的设计文档</a></li>
<li><a href="https://git.k8s.io/community/contributors/design-proposals/storage/volume-ownership-management.md">属主管理的设计文档</a></li>
<li><a href="/zh/docs/concepts/policy/pod-security-policy/">Pod 安全策略</a></li>
<li><a href="https://git.k8s.io/community/contributors/design-proposals/auth/no-new-privs.md">AllowPrivilegeEscalation 的设计文档</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2c0d882359718c4c69c67099bed2156c">3.11 - 为 Pod 配置服务账户</h1>
    
	<!--
reviewers:
- bprashanth
- liggitt
- thockin
title: Configure Service Accounts for Pods
content_type: task
weight: 90
-->
<!-- overview -->
<!--
A service account provides an identity for processes that run in a Pod.

This document is a user introduction to Service Accounts and describes how service accounts behave in a cluster set up
as recommended by the Kubernetes project. Your cluster administrator may have
customized the behavior in your cluster, in which case this documentation may
not apply.
-->
<p>服务账户为 Pod 中运行的进程提供了一个标识。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> 本文是服务账户的用户使用介绍，描述服务账号在集群中如何起作用。
你的集群管理员可能已经对你的集群做了定制，因此导致本文中所讲述的内容并不适用。</div>
</blockquote>
<!--
When you (a human) access the cluster (for example, using `kubectl`), you are
authenticated by the apiserver as a particular User Account (currently this is
usually `admin`, unless your cluster administrator has customized your
cluster).  Processes in containers inside pods can also contact the apiserver.
When they do, they are authenticated as a particular Service Account (for example,
`default`).
-->
<p>当你（自然人）访问集群时（例如，使用 <code>kubectl</code>），API 服务器将你的身份验证为
特定的用户帐户（当前这通常是 <code>admin</code>，除非你的集群管理员已经定制了你的集群配置）。
Pod 内的容器中的进程也可以与 api 服务器接触。
当它们进行身份验证时，它们被验证为特定的服务帐户（例如，<code>default</code>）。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Use the Default Service Account to access the API server.

When you create a pod, if you do not specify a service account, it is
automatically assigned the `default` service account in the same namespace.
If you get the raw json or yaml for a pod you have created (for example, `kubectl get pods/podname -o yaml`),
you can see the `spec.serviceAccountName` field has been
[automatically set](/docs/user-guide/working-with-resources/#resources-are-automatically-modified).
-->
<h2 id="使用默认的服务账户访问-api-服务器">使用默认的服务账户访问 API 服务器</h2>
<p>当你创建 Pod 时，如果没有指定服务账户，Pod 会被指定给命名空间中的 <code>default</code> 服务账户。
如果你查看 Pod 的原始 JSON 或 YAML（例如：<code>kubectl get pods/podname -o yaml</code>），
你可以看到 <code>spec.serviceAccountName</code> 字段已经被自动设置了。</p>
<!--
You can access the API from inside a pod using automatically mounted service account credentials,
as described in [Accessing the Cluster](/docs/user-guide/accessing-the-cluster/#accessing-the-api-from-a-pod).
The API permissions of the service account depend on the [authorization plugin and policy](/docs/reference/access-authn-authz/authorization/#authorization-modules) in use.

In version 1.6+, you can opt out of automounting API credentials for a service account by setting
`automountServiceAccountToken: false` on the service account:
-->
<p>你可以使用自动挂载给 Pod 的服务账户凭据访问 API，
<a href="/zh/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod">访问集群</a>
中有相关描述。
服务账户的 API 许可取决于你所使用的
<a href="/zh/docs/reference/access-authn-authz/authorization/#authorization-modules">鉴权插件和策略</a>。</p>
<p>在 1.6 以上版本中，你可以通过在服务账户上设置 <code>automountServiceAccountToken: false</code>
来实现不给服务账号自动挂载 API 凭据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>build-robot<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">automountServiceAccountToken</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
In version 1.6+, you can also opt out of automounting API credentials for a particular pod:
-->
<p>在 1.6 以上版本中，你也可以选择不给特定 Pod 自动挂载 API 凭据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>build-robot<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">automountServiceAccountToken</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
The pod spec takes precedence over the service account if both specify a `automountServiceAccountToken` value.
-->
<p>如果 Pod 和服务账户都指定了 <code>automountServiceAccountToken</code> 值，则 Pod 的 spec 优先于服务帐户。</p>
<!--
## Use Multiple Service Accounts.

Every namespace has a default service account resource called `default`.
You can list this and any other serviceAccount resources in the namespace with this command:
-->
<h2 id="use-multiple-service-accounts">使用多个服务账户  </h2>
<p>每个命名空间都有一个名为 <code>default</code> 的服务账户资源。
你可以用下面的命令查询这个服务账户以及命名空间中的其他 ServiceAccount 资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get serviceAccounts
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME      SECRETS    AGE
default   1          1d
</code></pre><!--
You can create additional ServiceAccount objects like this:
-->
<p>你可以像这样来创建额外的 ServiceAccount 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f - <span style="color:#b44">&lt;&lt;EOF
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: ServiceAccount
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: build-robot
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
The name of a ServiceAccount object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>ServiceAccount 对象的名字必须是一个有效的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.</p>
<!--
If you get a complete dump of the service account object, like this:
-->
<p>如果你查询服务帐户对象的完整信息，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get serviceaccounts/build-robot -o yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2015-06-16T00:12:59Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>build-robot<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;272500&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>721ab723-13bc-11e5-aec2-42010af0021e<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">secrets</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>build-robot-token-bvbk5<span style="color:#bbb">
</span></code></pre></div><!--
then you will see that a token has automatically been created and is referenced by the service account.

You may use authorization plugins to [set permissions on service accounts](/docs/reference/access-authn-authz/rbac/#service-account-permissions).

To use a non-default service account, set the `spec.serviceAccountName`
field of a pod to the name of the service account you wish to use.
-->
<p>那么你就能看到系统已经自动创建了一个令牌并且被服务账户所引用。</p>
<p>你可以使用授权插件来
<a href="/zh/docs/reference/access-authn-authz/rbac/#service-account-permissions">设置服务账户的访问许可</a>。</p>
<p>要使用非默认的服务账户，将 Pod 的 <code>spec.serviceAccountName</code> 字段设置为你想用的服务账户名称。</p>
<!--
The service account has to exist at the time the pod is created, or it will be rejected.

You cannot update the service account of an already created pod.

You can clean up the service account from this example like this:
-->
<p>Pod 被创建时服务账户必须存在，否则会被拒绝。</p>
<p>你不能更新已经创建好的 Pod 的服务账户。</p>
<p>你可以清除服务账户，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete serviceaccount/build-robot
</code></pre></div><!--
## Manually create a service account API token.

Suppose we have an existing service account named "build-robot" as mentioned above, and we create
a new secret manually.
-->
<h2 id="手动创建服务账户-api-令牌">手动创建服务账户 API 令牌</h2>
<p>假设我们有一个上面提到的名为 &quot;build-robot&quot; 的服务账户，然后我们手动创建一个新的 Secret。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f - <span style="color:#b44">&lt;&lt;EOF
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Secret
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: build-robot-secret
</span><span style="color:#b44">  annotations:
</span><span style="color:#b44">    kubernetes.io/service-account.name: build-robot
</span><span style="color:#b44">type: kubernetes.io/service-account-token
</span><span style="color:#b44">EOF</span>
secret/build-robot-secret created
</code></pre></div><!--
Now you can confirm that the newly built secret is populated with an API token for the "build-robot" service account.

Any tokens for non-existent service accounts will be cleaned up by the token controller.
-->
<p>现在，你可以确认新构建的 Secret 中填充了 &quot;build-robot&quot; 服务帐户的 API 令牌。</p>
<p>令牌控制器将清理不存在的服务帐户的所有令牌。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe secrets/build-robot-secret
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>Name:           build-robot-secret
Namespace:      default
Labels:         &lt;none&gt;
Annotations:    kubernetes.io/service-account.name=build-robot
                kubernetes.io/service-account.uid=da68f9c6-9d26-11e7-b84e-002dc52800da

Type:   kubernetes.io/service-account-token

Data
====
ca.crt:         1338 bytes
namespace:      7 bytes
token:          ...
</code></pre><!--
The content of `token` is elided here.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 这里省略了 <code>token</code> 的内容。</div>
</blockquote>
<!--
## Add ImagePullSecrets to a service account

### Create an imagePullSecret

- Create an imagePullSecret, as described in [Specifying ImagePullSecret on a Pod](/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod).
-->
<h2 id="add-imagepullsecrets-to-a-service-account">为服务账户添加 ImagePullSecrets </h2>
<h3 id="创建-imagepullsecret">创建 ImagePullSecret</h3>
<ul>
<li>
<p>创建一个 ImagePullSecret，如同<a href="/zh/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod">为 Pod 设置 ImagePullSecret</a>所述。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret docker-registry myregistrykey --docker-server<span style="color:#666">=</span>DUMMY_SERVER <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>          --docker-username<span style="color:#666">=</span>DUMMY_USERNAME --docker-password<span style="color:#666">=</span>DUMMY_DOCKER_PASSWORD <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>          --docker-email<span style="color:#666">=</span>DUMMY_DOCKER_EMAIL
</code></pre></div></li>
</ul>
<!--
- Verify it has been created.
-->
<ul>
<li>
<p>确认创建成功：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secrets myregistrykey
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME             TYPE                              DATA    AGE
myregistrykey    kubernetes.io/.dockerconfigjson   1       1d
</code></pre></li>
</ul>
<!--
### Add image pull secret to service account

Next, modify the default service account for the namespace to use this secret as an imagePullSecret.
-->
<h3 id="将镜像拉取-secret-添加到服务账号">将镜像拉取 Secret 添加到服务账号</h3>
<p>接着修改命名空间的 <code>default</code> 服务帐户，以将该 Secret 用作 imagePullSecret。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch serviceaccount default -p <span style="color:#b44">&#39;{&#34;imagePullSecrets&#34;: [{&#34;name&#34;: &#34;myregistrykey&#34;}]}&#39;</span>
</code></pre></div><!--
You can instead use `kubectl edit`, or manually edit the YAML manifests as shown below:
-->
<p>你也可以使用 <code>kubectl edit</code>，或者如下所示手动编辑 YAML 清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get serviceaccounts default -o yaml &gt; ./sa.yaml
</code></pre></div><p><code>sa.yaml</code> 文件的内容类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2015-08-07T22:02:39Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;243024&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>052fb0f4-3d50-11e5-b066-42010af0d7b6<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">secrets</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-token-uudge<span style="color:#bbb">
</span></code></pre></div><!--
Using your editor of choice (for example `vi`), open the `sa.yaml` file, delete line with key `resourceVersion`, add lines with `imagePullSecrets:` and save.

The output of the `sa.yaml` file is similar to this:
-->
<p>使用你常用的编辑器（例如 <code>vi</code>），打开 <code>sa.yaml</code> 文件，删除带有键名
<code>resourceVersion</code> 的行，添加带有 <code>imagePullSecrets:</code> 的行，最后保存文件。</p>
<p>所得到的 <code>sa.yaml</code> 文件类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2015-08-07T22:02:39Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>052fb0f4-3d50-11e5-b066-42010af0d7b6<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">secrets</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-token-uudge<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">imagePullSecrets</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myregistrykey<span style="color:#bbb">
</span></code></pre></div><!--
Finally replace the serviceaccount with the new updated `sa.yaml` file
-->
<p>最后，用新的更新的 <code>sa.yaml</code> 文件替换服务账号。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl replace serviceaccount default -f ./sa.yaml
</code></pre></div><!--
### Verify imagePullSecrets was added to pod spec

Now, when a new Pod is created in the current namespace and using the default ServiceAccount, the new Pod has its  `spec.imagePullSecrets` field set automatically:
-->
<h3 id="验证镜像拉取-secret-已经被添加到-pod-规约">验证镜像拉取 Secret 已经被添加到 Pod 规约</h3>
<p>现在，在当前命名空间中创建的每个使用默认服务账号的新 Pod，新 Pod 都会自动
设置其 <code>.spec.imagePullSecrets</code> 字段：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run nginx --image<span style="color:#666">=</span>nginx --restart<span style="color:#666">=</span>Never
kubectl get pod nginx -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.imagePullSecrets[0].name}{&#34;\n&#34;}&#39;</span>
</code></pre></div><!-- The output is: -->
<p>输出为：</p>
<pre tabindex="0"><code>myregistrykey
</code></pre><!--
## Service Account Token Volume Projection
-->
<h2 id="service-account-token-volume-projection">服务帐户令牌卷投射  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code>
</div>

<!--
To enable and use token request projection, you must specify each of the following
command line arguments to `kube-apiserver`:

* `--service-account-issuer`
* `--service-account-key-file`
* `--service-account-signing-key-file`
* `--api-audiences`

-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>为了启用令牌请求投射，你必须为 <code>kube-apiserver</code> 设置以下命令行参数：</p>
<ul>
<li><code>--service-account-issuer</code></li>
<li><code>--service-account-key-file</code></li>
<li><code>--service-account-signing-key-file</code></li>
<li><code>--api-audiences</code></li>
</ul>
</div>
</blockquote>
<!--
The kubelet can also project a service account token into a Pod. You can
specify desired properties of the token, such as the audience and the validity
duration. These properties are not configurable on the default service account
token. The service account token will also become invalid against the API when
the Pod or the ServiceAccount is deleted.
-->
<p>kubelet 还可以将服务帐户令牌投影到 Pod 中。
你可以指定令牌的所需属性，例如受众和有效持续时间。
这些属性在默认服务帐户令牌上无法配置。
当删除 Pod 或 ServiceAccount 时，服务帐户令牌也将对 API 无效。</p>
<!--
This behavior is configured on a PodSpec using a ProjectedVolume type called
[ServiceAccountToken](/docs/concepts/storage/volumes/#projected). To provide a
pod with a token with an audience of "vault" and a validity duration of two
hours, you would configure the following in your PodSpec:
-->
<p>使用名为 <a href="/zh/docs/concepts/storage/volumes/#projected">ServiceAccountToken</a> 的
ProjectedVolume 类型在 PodSpec 上配置此功能。
要向 Pod 提供具有 &quot;vault&quot; 用户以及两个小时有效期的令牌，可以在 PodSpec 中配置以下内容：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-projected-svc-token.yaml" download="pods/pod-projected-svc-token.yaml"><code>pods/pod-projected-svc-token.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-projected-svc-token-yaml')" title="Copy pods/pod-projected-svc-token.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-projected-svc-token-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/run/secrets/tokens<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>vault-token<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>build-robot<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>vault-token<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">projected</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">sources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">serviceAccountToken</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>vault-token<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">expirationSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">7200</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">audience</span>:<span style="color:#bbb"> </span>vault<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/pod-projected-svc-token.yaml
</code></pre></div><!--
The kubelet will request and store the token on behalf of the pod, make the
token available to the pod at a configurable file path, and refresh the token as it approaches expiration. 
The kubelet proactively rotates the token if it is older than 80% of its total TTL, or if the token is older than 24 hours.

The application is responsible for reloading the token when it rotates. Periodic reloading (e.g. once every 5 minutes) is sufficient for most use cases.
-->
<p><code>kubelet</code> 组件会替 Pod 请求令牌并将其保存起来，通过将令牌存储到一个可配置的
路径使之在 Pod 内可用，并在令牌快要到期的时候刷新它。
<code>kubelet</code> 会在令牌存在期达到其 TTL 的 80% 的时候或者令牌生命期超过 24 小时
的时候主动轮换它。</p>
<p>应用程序负责在令牌被轮换时重新加载其内容。对于大多数使用场景而言，周期性地
（例如，每隔 5 分钟）重新加载就足够了。</p>
<!--
## Service Account Issuer Discovery
-->
<h2 id="发现服务账号分发者">发现服务账号分发者</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>

<!--
The Service Account Issuer Discovery feature is enabled when the Service Account
Token Projection feature is enabled, as described
[above](#service-account-token-volume-projection).
-->
<p>当启用服务账号令牌投射时启用发现服务账号分发者（Service Account Issuer Discovery）这一功能特性，
如<a href="#service-account-token-volume-projection">上文所述</a>。</p>
<!--
The issuer URL must comply with the
[OIDC Discovery Spec](https://openid.net/specs/openid-connect-discovery-1_0.html). In
practice, this means it must use the `https` scheme, and should serve an OpenID
provider configuration at `{service-account-issuer}/.well-known/openid-configuration`.

If the URL does not comply, the `ServiceAccountIssuerDiscovery` endpoints will
not be registered, even if the feature is enabled.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>分发者的 URL 必须遵从
<a href="https://openid.net/specs/openid-connect-discovery-1_0.html">OIDC 发现规范</a>。
这意味着 URL 必须使用 <code>https</code> 模式，并且必须在
<code>{service-account-issuer}/.well-known/openid-configuration</code>
路径提供 OpenID 提供者（Provider）配置。</p>
<p>如果 URL 没有遵从这一规范，<code>ServiceAccountIssuerDiscovery</code> 末端就不会被注册，
即使该特性已经被启用。</p>
</div>
</blockquote>
<!--
The Service Account Issuer Discovery feature enables federation of Kubernetes
service account tokens issued by a cluster (the _identity provider_) with
external systems (_relying parties_).

When enabled, the Kubernetes API server provides an OpenID Provider
Configuration document at `/.well-known/openid-configuration` and the associated
JSON Web Key Set (JWKS) at `/openid/v1/jwks`. The OpenID Provider Configuration
is sometimes referred to as the _discovery document_.
-->
<p>发现服务账号分发者这一功能使得用户能够用联邦的方式结合使用 Kubernetes
集群（<em>Identity Provider</em>，标识提供者）与外部系统（<em>relying parties</em>，
依赖方）所分发的服务账号令牌。</p>
<p>当此功能被启用时，Kubernetes API 服务器会在 <code>/.well-known/openid-configuration</code>
提供一个 OpenID 提供者配置文档，并在 <code>/openid/v1/jwks</code> 处提供与之关联的
JSON Web Key Set（JWKS）。
这里的 OpenID 提供者配置有时候也被称作 <em>发现文档（Discovery Document）</em>。</p>
<!--
Clusters include a default RBAC ClusterRole called
`system:service-account-issuer-discovery`. No role bindings are provided
by default. Administrators may, for example, choose whether to bind the role to
`system:authenticated` or `system:unauthenticated` depending on their security
requirements and which external systems they intend to federate with.
-->
<p>集群包括一个默认的 RBAC ClusterRole，
名为 <code>system:service-account-issuer-discovery</code>。
默认情况下不提供角色绑定对象。
举例而言，管理员可以根据其安全性需要以及期望集成的外部系统选择是否将该角色绑定到
<code>system:authenticated</code> 或 <code>system:unauthenticated</code>。</p>
<!--
The responses served at `/.well-known/openid-configuration` and
`/openid/v1/jwks` are designed to be OIDC compatible, but not strictly OIDC
compliant. Those documents contain only the parameters necessary to perform
validation of Kubernetes service account tokens.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 对 <code>/.well-known/openid-configuration</code> 和 <code>/openid/v1/jwks</code> 路径请求的响应
被设计为与 OIDC 兼容，但不是完全与其一致。
返回的文档仅包含对 Kubernetes 服务账号令牌进行验证所必须的参数。</div>
</blockquote>
<!--
The JWKS response contains public keys that a relying party can use to validate
the Kubernetes service account tokens. Relying parties first query for the
OpenID Provider Configuration, and use the `jwks_uri` field in the response to
find the JWKS.
-->
<p>JWKS 响应包含依赖方可以用来验证 Kubernetes 服务账号令牌的公钥数据。
依赖方先会查询 OpenID 提供者配置，之后使用返回响应中的 <code>jwks_uri</code> 来查找
JWKS。</p>
<!--
In many cases, Kubernetes API servers are not available on the public internet,
but public endpoints that serve cached responses from the API server can be made
available by users or service providers. In these cases, it is possible to
override the `jwks_uri` in the OpenID Provider Configuration so that it points
to the public endpoint, rather than the API server's address, by passing the
`--service-account-jwks-uri` flag to the API server. Like the issuer URL, the
JWKS URI is required to use the `https` scheme.
-->
<p>在很多场合，Kubernetes API 服务器都不会暴露在公网上，不过对于缓存并向外提供 API
服务器响应数据的公开末端而言，用户或者服务提供商可以选择将其暴露在公网上。
在这种环境中，可能会重载 OpenID 提供者配置中的
<code>jwks_uri</code>，使之指向公网上可用的末端地址，而不是 API 服务器的地址。
这时需要向 API 服务器传递 <code>--service-account-jwks-uri</code> 参数。
与分发者 URL 类似，此 JWKS URI 也需要使用 <code>https</code> 模式。</p>
<h2 id="接下来">接下来</h2>
<!--
See also:

- [Cluster Admin Guide to Service Accounts](/docs/reference/access-authn-authz/service-accounts-admin/)
- [Service Account Signing Key Retrieval KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/20190730-oidc-discovery.md)
- [OIDC Discovery Spec](https://openid.net/specs/openid-connect-discovery-1_0.html)
-->
<p>另请参见：</p>
<ul>
<li><a href="/zh/docs/reference/access-authn-authz/service-accounts-admin/">服务账号的集群管理员指南</a></li>
<li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/20190730-oidc-discovery.md">服务账号签署密钥检索 KEP</a></li>
<li><a href="https://openid.net/specs/openid-connect-discovery-1_0.html">OIDC 发现规范</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d385b86a7cb496d3b1c3b2a47280ca70">3.12 - 从私有仓库拉取镜像</h1>
    
	<!--
title: Pull an Image from a Private Registry
content_type: task
weight: 100
-->
<!-- overview -->
<!--
This page shows how to create a Pod that uses a Secret to pull an image from a
private Docker registry or repository.
-->
<p>本文介绍如何使用 Secret 从私有的 Docker 镜像仓库或代码仓库拉取镜像来创建 Pod。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!--
* To do this exercise, you need a
[Docker ID](https://docs.docker.com/docker-id/) and password.
-->
<p>你需要 <a href="https://docs.docker.com/docker-id/">Docker ID</a> 和密码来进行本练习。</p>
<!-- steps -->
<!--
## Log in to Docker

On your laptop, you must authenticate with a registry in order to pull a private image:
-->
<h2 id="登录-docker-镜像仓库">登录 Docker 镜像仓库</h2>
<p>在个人电脑上，要想拉取私有镜像必须在镜像仓库上进行身份验证。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker login
</code></pre></div><!--
When prompted, enter your Docker username and password.

The login process creates or updates a `config.json` file that holds an authorization token.

View the `config.json` file:
-->
<p>当出现提示时，输入 Docker 用户名和密码。</p>
<p>登录过程会创建或更新保存有授权令牌的 <code>config.json</code> 文件。</p>
<p>查看 <code>config.json</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat ~/.docker/config.json
</code></pre></div><!--
The output contains a section similar to this:
-->
<p>输出结果包含类似于以下内容的部分：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;auths&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;https://index.docker.io/v1/&#34;</span>: {
            <span style="color:#008000;font-weight:bold">&#34;auth&#34;</span>: <span style="color:#b44">&#34;c3R...zE2&#34;</span>
        }
    }
}
</code></pre></div><!--
If you use a Docker credentials store, you won't see that `auth` entry but a `credsStore` entry with the name of the store as value.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果使用 Docker 凭证仓库，则不会看到 <code>auth</code> 条目，看到的将是以仓库名称作为值的 <code>credsStore</code> 条目。</div>
</blockquote>
<!--
## Create a Secret in the cluster that holds your authorization token

A Kubernetes cluster uses the Secret of `docker-registry` type to authenticate with a container registry to pull a private image.

Create this Secret, naming it `regcred`:
-->
<h2 id="在集群中创建保存授权令牌的-secret">在集群中创建保存授权令牌的 Secret</h2>
<p>Kubernetes 集群使用 <code>docker-registry</code> 类型的 Secret 来通过容器仓库的身份验证，进而提取私有映像。</p>
<p>创建 Secret，命名为 <code>regcred</code>：</p>
<!--
kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret docker-registry regcred <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-server<span style="color:#666">=</span>&lt;你的镜像仓库服务器&gt; <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-username<span style="color:#666">=</span>&lt;你的用户名&gt; <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-password<span style="color:#666">=</span>&lt;你的密码&gt; <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --docker-email<span style="color:#666">=</span>&lt;你的邮箱地址&gt;
</code></pre></div><!--
where:

* `<your-registry-server>` is your Private Docker Registry FQDN.
  Use `https://index.docker.io/v2/` for DockerHub.
* `<your-name>` is your Docker username.
* `<your-pword>` is your Docker password.
* `<your-email>` is your Docker email.

You have successfully set your Docker credentials in the cluster as a Secret called `regcred`.
-->
<p>在这里：</p>
<ul>
<li><code>&lt;your-registry-server&gt;</code> 是你的私有 Docker 仓库全限定域名（FQDN）。
DockerHub 使用 <code>https://index.docker.io/v2/</code>。</li>
<li><code>&lt;your-name&gt;</code> 是你的 Docker 用户名。</li>
<li><code>&lt;your-pword&gt;</code> 是你的 Docker 密码。</li>
<li><code>&lt;your-email&gt;</code> 是你的 Docker 邮箱。</li>
</ul>
<p>这样你就成功地将集群中的 Docker 凭据设置为名为 <code>regcred</code> 的 Secret。</p>
<!--
## Inspecting the Secret `regcred`

To understand the contents of the `regcred` Secret you created, start by viewing the Secret in YAML format:
-->
<h2 id="检查-secret-regcred">检查 Secret <code>regcred</code></h2>
<p>要了解你创建的 <code>regcred</code> Secret 的内容，可以用 YAML 格式进行查看：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secret regcred --output<span style="color:#666">=</span>yaml
</code></pre></div><!-- The output is similar to this: -->
<p>输出和下面类似：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">.dockerconfigjson</span>:<span style="color:#bbb"> </span>eyJodHRwczovL2luZGV4L ... J0QUl6RTIifX0=<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>regcred<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>kubernetes.io/dockerconfigjson<span style="color:#bbb">
</span></code></pre></div><!--
The value of the `.dockerconfigjson` field is a base64 representation of your Docker credentials.

To understand what is in the `.dockerconfigjson` field, convert the secret data to a
readable format:
-->
<p><code>.dockerconfigjson</code> 字段的值是 Docker 凭据的 base64 表示。</p>
<p>要了解 <code>dockerconfigjson</code> 字段中的内容，请将 Secret 数据转换为可读格式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secret regcred --output<span style="color:#666">=</span><span style="color:#b44">&#34;jsonpath={.data.\.dockerconfigjson}&#34;</span> | base64 --decode
</code></pre></div><!-- The output is similar to this: -->
<p>输出和下面类似：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{<span style="color:#008000;font-weight:bold">&#34;auths&#34;</span>:{<span style="color:#008000;font-weight:bold">&#34;yourprivateregistry.com&#34;</span>:{<span style="color:#008000;font-weight:bold">&#34;username&#34;</span>:<span style="color:#b44">&#34;janedoe&#34;</span>,<span style="color:#008000;font-weight:bold">&#34;password&#34;</span>:<span style="color:#b44">&#34;xxxxxxxxxxx&#34;</span>,<span style="color:#008000;font-weight:bold">&#34;email&#34;</span>:<span style="color:#b44">&#34;jdoe@example.com&#34;</span>,<span style="color:#008000;font-weight:bold">&#34;auth&#34;</span>:<span style="color:#b44">&#34;c3R...zE2&#34;</span>}}}
</code></pre></div><!--
To understand what is in the `auth` field, convert the base64-encoded data to a readable format:
-->
<p>要了解 <code>auth</code> 字段中的内容，请将 base64 编码过的数据转换为可读格式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> <span style="color:#b44">&#34;c3R...zE2&#34;</span> | base64 --decode
</code></pre></div><!--
The output, username and password concatenated with a `:`, is similar to this:
-->
<p>输出结果中，用户名和密码用 <code>:</code> 链接，类似下面这样：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">janedoe:xxxxxxxxxxx
</code></pre><!--
Notice that the Secret data contains the authorization token similar to your local `~/.docker/config.json` file.

You have successfully set your Docker credentials as a Secret called `regcred` in the cluster.
-->
<p>注意，Secret 数据包含与本地 <code>~/.docker/config.json</code> 文件类似的授权令牌。</p>
<p>这样你就已经成功地将 Docker 凭据设置为集群中的名为 <code>regcred</code> 的 Secret。</p>
<!--
## Create a Pod that uses your Secret

Here is a configuration file for a Pod that needs access to your Docker credentials in `regcred`:
-->
<h2 id="创建一个使用你的-secret-的-pod">创建一个使用你的 Secret 的 Pod</h2>
<p>下面是一个 Pod 配置文件，它需要访问 <code>regcred</code> 中的 Docker 凭据：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/private-reg-pod.yaml" download="pods/private-reg-pod.yaml"><code>pods/private-reg-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-private-reg-pod-yaml')" title="Copy pods/private-reg-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-private-reg-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>private-reg<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>private-reg-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>&lt;your-private-image&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">imagePullSecrets</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>regcred<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Download the above file: -->
<p>下载上述文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget -O my-private-reg-pod.yaml https://k8s.io/examples/pods/private-reg-pod.yaml
</code></pre></div><!--
In file `my-private-reg-pod.yaml`, replace `<your-private-image>` with the path to an image in a private registry such as:
-->
<p>在<code>my-private-reg-pod.yaml</code> 文件中，使用私有仓库的镜像路径替换 <code>&lt;your-private-image&gt;</code>，例如：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">janedoe/jdoe-private:v1
</code></pre><!--
To pull the image from the private registry, Kubernetes needs credentials.
The `imagePullSecrets` field in the configuration file specifies that
Kubernetes should get the credentials from a Secret named `regcred`.

Create a Pod that uses your Secret, and verify that the Pod is running:
-->
<p>要从私有仓库拉取镜像，Kubernetes 需要凭证。
配置文件中的 <code>imagePullSecrets</code> 字段表明 Kubernetes 应该通过名为 <code>regcred</code> 的 Secret 获取凭证。</p>
<p>创建使用了你的 Secret 的 Pod，并检查它是否正常运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-private-reg-pod.yaml
kubectl get pod private-reg
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* Learn more about [Secrets](/docs/concepts/configuration/secret/).
* Learn more about [using a private registry](/docs/concepts/containers/images/#using-a-private-registry).
* See [kubectl create secret docker-registry](/docs/reference/generated/kubectl/kubectl-commands/#-em-secret-docker-registry-em-).
* See [Secret](/docs/reference/generated/kubernetes-api/v1.22/#secret-v1-core).
* See the `imagePullSecrets` field of [PodSpec](/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/configuration/secret/">Secret</a></li>
<li>进一步了解 <a href="/zh/docs/concepts/containers/images/#using-a-private-registry">使用私有仓库</a></li>
<li>参考 <a href="/docs/reference/generated/kubectl/kubectl-commands/#-em-secret-docker-registry-em-">kubectl create secret docker-registry</a></li>
<li>参考 <a href="/docs/reference/generated/kubernetes-api/v1.22/#secret-v1-core">Secret</a></li>
<li>参考 <a href="/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core">PodSpec</a> 中的 <code>imagePullSecrets</code> 字段</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eb54daf87df373096b5e830680194dfc">3.13 - 配置存活、就绪和启动探测器</h1>
    
	<!-- overview -->
<!--
This page shows how to configure liveness, readiness and startup probes for Containers.

The [kubelet](/docs/reference/command-line-tools-reference/kubelet/) uses liveness probes to know when to
restart a container. For example, liveness probes could catch a deadlock,
where an application is running, but unable to make progress. Restarting a
container in such a state can help to make the application more available
despite bugs.
-->
<p>这篇文章介绍如何给容器配置存活、就绪和启动探测器。</p>
<p><a href="/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
使用存活探测器来知道什么时候要重启容器。
例如，存活探测器可以捕捉到死锁（应用程序在运行，但是无法继续执行后面的步骤）。
这样的情况下重启容器有助于让应用程序在有问题的情况下更可用。</p>
<!--
The kubelet uses readiness probes to know when a container is ready to start
accepting traffic. A Pod is considered ready when all of its containers are ready.
One use of this signal is to control which Pods are used as backends for Services.
When a Pod is not ready, it is removed from Service load balancers.

The kubelet uses startup probes to know when a container application has started.
If such a probe is configured, it disables liveness and readiness checks until
it succeeds, making sure those probes don't interfere with the application startup.
This can be used to adopt liveness checks on slow starting containers, avoiding them
getting killed by the kubelet before they are up and running.
-->
<p>kubelet 使用就绪探测器可以知道容器什么时候准备好了并可以开始接受请求流量， 当一个 Pod
内的所有容器都准备好了，才能把这个 Pod 看作就绪了。
这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端。
在 Pod 还没有准备好的时候，会从 Service 的负载均衡器中被剔除的。</p>
<p>kubelet 使用启动探测器可以知道应用程序容器什么时候启动了。
如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查，
确保这些存活、就绪探测器不会影响应用程序的启动。
这可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!--
## Define a liveness command

Many applications running for long periods of time eventually transition to
broken states, and cannot recover except by being restarted. Kubernetes provides
liveness probes to detect and remedy such situations.

In this exercise, you create a Pod that runs a container based on the
`k8s.gcr.io/busybox` image. Here is the configuration file for the Pod:
-->
<h2 id="define-a-liveness-command">定义存活命令</h2>
<p>许多长时间运行的应用程序最终会过渡到断开的状态，除非重新启动，否则无法恢复。
Kubernetes 提供了存活探测器来发现并补救这种情况。</p>
<p>在这篇练习中，你会创建一个 Pod，其中运行一个基于 <code>k8s.gcr.io/busybox</code> 镜像的容器。
下面是这个 Pod 的配置文件。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/probe/exec-liveness.yaml" download="pods/probe/exec-liveness.yaml"><code>pods/probe/exec-liveness.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-probe-exec-liveness-yaml')" title="Copy pods/probe/exec-liveness.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-probe-exec-liveness-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">test</span>:<span style="color:#bbb"> </span>liveness<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>liveness-exec<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>liveness<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- cat<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- /tmp/healthy<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a single `Container`.
The `periodSeconds` field specifies that the kubelet should perform a liveness
probe every 5 seconds. The `initialDelaySeconds` field tells the kubelet that it
should wait 5 seconds before performing the first probe. To perform a probe, the
kubelet executes the command `cat /tmp/healthy` in the target container. If the
command succeeds, it returns 0, and the kubelet considers the container to be alive and
healthy. If the command returns a non-zero value, the kubelet kills the container
and restarts it.

When the container starts, it executes this command:
-->
<p>在这个配置文件中，可以看到 Pod 中只有一个容器。
<code>periodSeconds</code> 字段指定了 kubelet 应该每 5 秒执行一次存活探测。
<code>initialDelaySeconds</code> 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。
kubelet 在容器内执行命令 <code>cat /tmp/healthy</code> 来进行探测。
如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。
如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。</p>
<p>当容器启动时，执行如下的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">/bin/sh -c <span style="color:#b44">&#34;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600&#34;</span>
</code></pre></div><!--
For the first 30 seconds of the container's life, there is a `/tmp/healthy` file.
So during the first 30 seconds, the command `cat /tmp/healthy` returns a success
code. After 30 seconds, `cat /tmp/healthy` returns a failure code.

Create the Pod:
-->
<p>这个容器生命的前 30 秒， <code>/tmp/healthy</code> 文件是存在的。
所以在这最开始的 30 秒内，执行命令 <code>cat /tmp/healthy</code> 会返回成功代码。
30 秒之后，执行命令 <code>cat /tmp/healthy</code> 就会返回失败代码。</p>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yaml
</code></pre></div><!--
Within 30 seconds, view the Pod events:
-->
<p>在 30 秒内，查看 Pod 的事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod liveness-exec
</code></pre></div><!--
The output indicates that no liveness probes have failed yet:
-->
<p>输出结果表明还没有存活探测器失败：</p>
<pre tabindex="0"><code>FirstSeen    LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
24s       24s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image &quot;k8s.gcr.io/busybox&quot;
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image &quot;k8s.gcr.io/busybox&quot;
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
</code></pre><!--
After 35 seconds, view the Pod events again:
-->
<p>35 秒之后，再来看 Pod 的事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod liveness-exec
</code></pre></div><!--
At the bottom of the output, there are messages indicating that the liveness
probes have failed, and the containers have been killed and recreated.
-->
<p>在输出结果的最下面，有信息显示存活探测器失败了，这个容器被杀死并且被重建了。</p>
<pre tabindex="0"><code>FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
37s       37s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image &quot;k8s.gcr.io/busybox&quot;
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image &quot;k8s.gcr.io/busybox&quot;
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; Security:[seccomp=unconfined]
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
2s        2s      1   {kubelet worker0}   spec.containers{liveness}   Warning     Unhealthy   Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
</code></pre><!--
Wait another 30 seconds, and verify that the Container has been restarted:
-->
<p>再等另外 30 秒，检查看这个容器被重启了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod liveness-exec
</code></pre></div><!--
The output shows that `RESTARTS` has been incremented:
-->
<p>输出结果显示 <code>RESTARTS</code> 的值增加了 1。</p>
<pre tabindex="0"><code>NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m
</code></pre><!--
## Define a liveness HTTP request

Another kind of liveness probe uses an HTTP GET request. Here is the configuration
file for a Pod that runs a container based on the `k8s.gcr.io/liveness`
image.
-->
<h2 id="define-a-liveness-HTTP-request">定义一个存活态 HTTP 请求接口</h2>
<p>另外一种类型的存活探测方式是使用 HTTP GET 请求。
下面是一个 Pod 的配置文件，其中运行一个基于 <code>k8s.gcr.io/liveness</code> 镜像的容器。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/probe/http-liveness.yaml" download="pods/probe/http-liveness.yaml"><code>pods/probe/http-liveness.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-probe-http-liveness-yaml')" title="Copy pods/probe/http-liveness.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-probe-http-liveness-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">test</span>:<span style="color:#bbb"> </span>liveness<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>liveness-http<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>liveness<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/liveness<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- /server<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">httpHeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>Custom-Header<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>Awesome<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a single container.
The `periodSeconds` field specifies that the kubelet should perform a liveness
probe every 3 seconds. The `initialDelaySeconds` field tells the kubelet that it
should wait 3 seconds before performing the first probe. To perform a probe, the
kubelet sends an HTTP GET request to the server that is running in the container
and listening on port 8080. If the handler for the server's `/healthz` path
returns a success code, the kubelet considers the container to be alive and
healthy. If the handler returns a failure code, the kubelet kills the container
and restarts it.
-->
<p>在这个配置文件中，可以看到 Pod 也只有一个容器。
<code>periodSeconds</code> 字段指定了 kubelet 每隔 3 秒执行一次存活探测。
<code>initialDelaySeconds</code> 字段告诉 kubelet 在执行第一次探测前应该等待 3 秒。
kubelet 会向容器内运行的服务（服务会监听 8080 端口）发送一个 HTTP GET 请求来执行探测。
如果服务器上 <code>/healthz</code>  路径下的处理程序返回成功代码，则 kubelet 认为容器是健康存活的。
如果处理程序返回失败代码，则 kubelet 会杀死这个容器并且重新启动它。</p>
<!--
Any code greater than or equal to 200 and less than 400 indicates success. Any
other code indicates failure.

You can see the source code for the server in
[server.go](https://github.com/kubernetes/kubernetes/blob/main/test/images/agnhost/liveness/server.go).

For the first 10 seconds that the container is alive, the `/healthz` handler
returns a status of 200. After that, the handler returns a status of 500.
-->
<p>任何大于或等于 200 并且小于 400 的返回代码标示成功，其它返回代码都标示失败。</p>
<p>可以在这里看服务的源码 <a href="https://github.com/kubernetes/kubernetes/blob/main/test/images/agnhost/liveness/server.go">server.go</a>。</p>
<p>容器存活的最开始 10 秒中，<code>/healthz</code> 处理程序返回一个 200 的状态码。之后处理程序返回 500 的状态码。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">http.<span style="color:#00a000">HandleFunc</span>(<span style="color:#b44">&#34;/healthz&#34;</span>, <span style="color:#a2f;font-weight:bold">func</span>(w http.ResponseWriter, r <span style="color:#666">*</span>http.Request) {
    duration <span style="color:#666">:=</span> time.<span style="color:#00a000">Now</span>().<span style="color:#00a000">Sub</span>(started)
    <span style="color:#a2f;font-weight:bold">if</span> duration.<span style="color:#00a000">Seconds</span>() &gt; <span style="color:#666">10</span> {
        w.<span style="color:#00a000">WriteHeader</span>(<span style="color:#666">500</span>)
        w.<span style="color:#00a000">Write</span>([]<span style="color:#a2f">byte</span>(fmt.<span style="color:#00a000">Sprintf</span>(<span style="color:#b44">&#34;error: %v&#34;</span>, duration.<span style="color:#00a000">Seconds</span>())))
    } <span style="color:#a2f;font-weight:bold">else</span> {
        w.<span style="color:#00a000">WriteHeader</span>(<span style="color:#666">200</span>)
        w.<span style="color:#00a000">Write</span>([]<span style="color:#a2f">byte</span>(<span style="color:#b44">&#34;ok&#34;</span>))
    }
})
</code></pre></div><!--
The kubelet starts performing health checks 3 seconds after the container starts.
So the first couple of health checks will succeed. But after 10 seconds, the health
checks will fail, and the kubelet will kill and restart the container.

To try the HTTP liveness check, create a Pod:
-->
<p>kubelet 在容器启动之后 3 秒开始执行健康检测。所以前几次健康检查都是成功的。
但是 10 秒之后，健康检查会失败，并且 kubelet 会杀死容器再重新启动容器。</p>
<p>创建一个 Pod 来测试 HTTP 的存活检测：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/probe/http-liveness.yaml
</code></pre></div><!--
After 10 seconds, view Pod events to verify that liveness probes have failed and
the container has been restarted:
-->
<p>10 秒之后，通过看 Pod 事件来检测存活探测器已经失败了并且容器被重新启动了。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod liveness-http
</code></pre></div><!--
In releases prior to v1.13 (including v1.13), if the environment variable
`http_proxy` (or `HTTP_PROXY`) is set on the node where a Pod is running,
the HTTP liveness probe uses that proxy.
In releases after v1.13, local HTTP proxy environment variable settings do not
affect the HTTP liveness probe.
-->
<p>在 1.13（包括 1.13版本）之前的版本中，如果在 Pod 运行的节点上设置了环境变量
<code>http_proxy</code>（或者 <code>HTTP_PROXY</code>），HTTP 的存活探测会使用这个代理。
在 1.13 之后的版本中，设置本地的 HTTP 代理环境变量不会影响 HTTP 的存活探测。</p>
<!--
## Define a TCP liveness probe

A third type of liveness probe uses a TCP socket. With this configuration, the
kubelet will attempt to open a socket to your container on the specified port.
If it can establish a connection, the container is considered healthy, if it
can't it is considered a failure.
-->
<h2 id="define-a-TCP-liveness-probe">定义 TCP 的存活探测</h2>
<p>第三种类型的存活探测是使用 TCP 套接字。
通过配置，kubelet 会尝试在指定端口和容器建立套接字链接。
如果能建立连接，这个容器就被看作是健康的，如果不能则这个容器就被看作是有问题的。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/probe/tcp-liveness-readiness.yaml" download="pods/probe/tcp-liveness-readiness.yaml"><code>pods/probe/tcp-liveness-readiness.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-probe-tcp-liveness-readiness-yaml')" title="Copy pods/probe/tcp-liveness-readiness.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-probe-tcp-liveness-readiness-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>goproxy<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>goproxy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>goproxy<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/goproxy:0.1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tcpSocket</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tcpSocket</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">20</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
As you can see, configuration for a TCP check is quite similar to an HTTP check.
This example uses both readiness and liveness probes. The kubelet will send the
first readiness probe 5 seconds after the container starts. This will attempt to
connect to the `goproxy` container on port 8080. If the probe succeeds, the Pod
will be marked as ready. The kubelet will continue to run this check every 10
seconds.

In addition to the readiness probe, this configuration includes a liveness probe.
The kubelet will run the first liveness probe 15 seconds after the container
starts. Similar to the readiness probe, this will attempt to connect to the
`goproxy` container on port 8080. If the liveness probe fails, the container
will be restarted.

To try the TCP liveness check, create a Pod:
-->
<p>如你所见，TCP 检测的配置和 HTTP 检测非常相似。
下面这个例子同时使用就绪和存活探测器。kubelet 会在容器启动 5 秒后发送第一个就绪探测。
这会尝试连接 <code>goproxy</code> 容器的 8080 端口。
如果探测成功，这个 Pod 会被标记为就绪状态，kubelet 将继续每隔 10 秒运行一次检测。</p>
<p>除了就绪探测，这个配置包括了一个存活探测。
kubelet 会在容器启动 15 秒后进行第一次存活探测。
与就绪探测类似，会尝试连接 <code>goproxy</code> 容器的 8080 端口。
如果存活探测失败，这个容器会被重新启动。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/probe/tcp-liveness-readiness.yaml
</code></pre></div><!--
After 15 seconds, view Pod events to verify that liveness probes:
-->
<p>15 秒之后，通过看 Pod 事件来检测存活探测器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod goproxy
</code></pre></div><!--
## Use a named port

You can use a named
[ContainerPort](/docs/reference/generated/kubernetes-api/v1.22/#containerport-v1-core)
for HTTP or TCP liveness checks:
-->
<h2 id="use-a-named-port">使用命名端口</h2>
<p>对于 HTTP 或者 TCP 存活检测可以使用命名的
<a href="/docs/reference/generated/kubernetes-api/v1.22/#containerport-v1-core">ContainerPort</a>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>liveness-port<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span>liveness-port<span style="color:#bbb">
</span></code></pre></div><!--
## Protect slow starting containers with startup probes {#define-startup-probes}

Sometimes, you have to deal with legacy applications that might require
an additional startup time on their first initialization.
In such cases, it can be tricky to set up liveness probe parameters without
compromising the fast response to deadlocks that motivated such a probe.
The trick is to set up a startup probe with the same command, HTTP or TCP
check, with a `failureThreshold * periodSeconds` long enough to cover the
worse case startup time.

So, the previous example would become:
-->
<h2 id="define-startup-probes">使用启动探测器保护慢启动容器</h2>
<p>有时候，会有一些现有的应用程序在启动时需要较多的初始化时间。
要不影响对引起探测死锁的快速响应，这种情况下，设置存活探测参数是要技巧的。
技巧就是使用一个命令来设置启动探测，针对HTTP 或者 TCP 检测，可以通过设置
<code>failureThreshold * periodSeconds</code> 参数来保证有足够长的时间应对糟糕情况下的启动时间。</p>
<p>所以，前面的例子就变成了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>liveness-port<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span>liveness-port<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">failureThreshold</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">startupProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span>liveness-port<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">failureThreshold</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span></code></pre></div><!--
Thanks to the startup probe, the application will have a maximum of 5 minutes
(30 * 10 = 300s) to finish its startup.
Once the startup probe has succeeded once, the liveness probe takes over to
provide a fast response to container deadlocks.
If the startup probe never succeeds, the container is killed after 300s and
subject to the pod's `restartPolicy`.
-->
<p>幸亏有启动探测，应用程序将会有最多 5 分钟(30 * 10 = 300s) 的时间来完成它的启动。
一旦启动探测成功一次，存活探测任务就会接管对容器的探测，对容器死锁可以快速响应。
如果启动探测一直没有成功，容器会在 300 秒后被杀死，并且根据 <code>restartPolicy</code> 来设置 Pod 状态。</p>
<!--
## Define readiness probes

Sometimes, applications are temporarily unable to serve traffic.
For example, an application might need to load large data or configuration
files during startup, or depend on external services after startup.
In such cases, you don't want to kill the application,
but you don't want to send it requests either. Kubernetes provides
readiness probes to detect and mitigate these situations. A pod with containers
reporting that they are not ready does not receive traffic through Kubernetes
Services.
-->
<h2 id="define-readiness-probes">定义就绪探测器</h2>
<p>有时候，应用程序会暂时性的不能提供通信服务。
例如，应用程序在启动时可能需要加载很大的数据或配置文件，或是启动后要依赖等待外部服务。
在这种情况下，既不想杀死应用程序，也不想给它发送请求。
Kubernetes 提供了就绪探测器来发现并缓解这些情况。
容器所在 Pod 上报还未就绪的信息，并且不接受通过 Kubernetes Service 的流量。</p>
<!--
Readiness probes runs on the container during its whole lifecycle.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 就绪探测器在容器的整个生命周期中保持运行状态。</div>
</blockquote>
<!--
Liveness probes *do not* wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 活跃性探测器 <em>不等待</em> 就绪性探测器成功。
如果要在执行活跃性探测器之前等待，应该使用 initialDelaySeconds 或 startupProbe。</div>
</blockquote>

<!--
Readiness probes are configured similarly to liveness probes. The only difference
is that you use the `readinessProbe` field instead of the `livenessProbe` field.
-->
<p>就绪探测器的配置和存活探测器的配置相似。
唯一区别就是要使用 <code>readinessProbe</code> 字段，而不是 <code>livenessProbe</code> 字段。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- cat<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- /tmp/healthy<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div><!--
Configuration for HTTP and TCP readiness probes also remains identical to
liveness probes.

Readiness and liveness probes can be used in parallel for the same container.
Using both can ensure that traffic does not reach a container that is not ready
for it, and that containers are restarted when they fail.
-->
<p>HTTP 和 TCP 的就绪探测器配置也和存活探测器的配置一样的。</p>
<p>就绪和存活探测可以在同一个容器上并行使用。
两者都用可以确保流量不会发给还没有准备好的容器，并且容器会在它们失败的时候被重新启动。</p>
<!--
## Configure Probes
-->
<h2 id="configure-probes">配置探测器</h2>
<!--
Eventually, some of this section could be moved to a concept topic.
-->

<!--
[Probes](/docs/reference/generated/kubernetes-api/v1.22/#probe-v1-core) have a number of fields that
you can use to more precisely control the behavior of liveness and readiness
checks:
-->
<p><a href="/docs/reference/generated/kubernetes-api/v1.22/#probe-v1-core">Probe</a>
有很多配置字段，可以使用这些字段精确的控制存活和就绪检测的行为：</p>
<!--
* `initialDelaySeconds`: Number of seconds after the container has started
before liveness or readiness probes are initiated. Defaults to 0 seconds. Minimum value is 0.
* `periodSeconds`: How often (in seconds) to perform the probe. Default to 10
seconds. Minimum value is 1.
* `timeoutSeconds`: Number of seconds after which the probe times out. Defaults
to 1 second. Minimum value is 1.
* `successThreshold`: Minimum consecutive successes for the probe to be
considered successful after having failed. Defaults to 1. Must be 1 for liveness
and startup Probes. Minimum value is 1.
* `failureThreshold`: When a probe fails, Kubernetes will
try `failureThreshold` times before giving up. Giving up in case of liveness probe means restarting the container. In case of readiness probe the Pod will be marked Unready.
Defaults to 3. Minimum value is 1.
-->
<ul>
<li><code>initialDelaySeconds</code>：容器启动后要等待多少秒后存活和就绪探测器才被初始化，默认是 0 秒，最小值是 0。</li>
<li><code>periodSeconds</code>：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。</li>
<li><code>timeoutSeconds</code>：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。</li>
<li><code>successThreshold</code>：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。
存活和启动探测的这个值必须是 1。最小值是 1。</li>
<li><code>failureThreshold</code>：当探测失败时，Kubernetes 的重试次数。
存活探测情况下的放弃就意味着重新启动容器。
就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。</li>
</ul>
<p><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
Before Kubernetes 1.20, the field `timeoutSeconds` was not respected for exec probes:
probes continued running indefinitely, even past their configured deadline,
until a result was returned.
-->
<p>在 Kubernetes 1.20 版本之前，exec 探针会忽略 <code>timeoutSeconds</code>：探针会无限期地
持续运行，甚至可能超过所配置的限期，直到返回结果为止。</p>
<!--
This defect was corrected in Kubernetes v1.20. You may have been relying on the previous behavior,
even without realizing it, as the default timeout is 1 second.
As a cluster administrator, you can disable the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `ExecProbeTimeout` (set it to `false`)
on each kubelet to restore the  behavior from older versions, then remove that override
once all the exec probes in the cluster have a `timeoutSeconds` value set.  
If you have pods that are impacted from the default 1 second timeout,
you should update their probe timeout so that you're ready for the
eventual removal of that feature gate.
-->
<p>这一缺陷在 Kubernetes v1.20 版本中得到修复。你可能一直依赖于之前错误的探测行为，
甚至你都没有觉察到这一问题的存在，因为默认的超时值是 1 秒钟。
作为集群管理员，你可以在所有的 kubelet 上禁用 <code>ExecProbeTimeout</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
（将其设置为 <code>false</code>），从而恢复之前版本中的运行行为，之后当集群中所有的
exec 探针都设置了 <code>timeoutSeconds</code> 参数后，移除此标志重载。
如果你有 Pods 受到此默认 1 秒钟超时值的影响，你应该更新 Pod 对应的探针的
超时值，这样才能为最终去除该特性门控做好准备。</p>
<!--
With the fix of the defect, for exec probes, on Kubernetes `1.20+` with the `dockershim` container runtime,
the process inside the container may keep running even after probe returned failure because of the timeout.
-->
<p>当此缺陷被修复之后，在使用 <code>dockershim</code> 容器运行时的 Kubernetes <code>1.20+</code>
版本中，对于 exec 探针而言，容器中的进程可能会因为超时值的设置保持持续运行，
即使探针返回了失败状态。</p>
</div>
</blockquote>
<blockquote class="caution callout">
  <div><strong>注意：</strong> <!--
Incorrect implementation of readiness probes may result in an ever growing number
of processes in the container, and resource starvation if this is left unchecked.
-->
<p>如果就绪态探针的实现不正确，可能会导致容器中进程的数量不断上升。
如果不对其采取措施，很可能导致资源枯竭的状况。</div>
</blockquote>
</p>
<!--
### HTTP probes

[HTTP probes](/docs/reference/generated/kubernetes-api/v1.22/#httpgetaction-v1-core)
have additional fields that can be set on `httpGet`:

* `host`: Host name to connect to, defaults to the pod IP. You probably want to
set "Host" in httpHeaders instead.
* `scheme`: Scheme to use for connecting to the host (HTTP or HTTPS). Defaults to HTTP.
* `path`: Path to access on the HTTP server. Defaults to /.
* `httpHeaders`: Custom headers to set in the request. HTTP allows repeated headers.
* `port`: Name or number of the port to access on the container. Number must be
in the range 1 to 65535.
-->
<h3 id="http-probes">HTTP 探测 </h3>
<p><a href="/docs/reference/generated/kubernetes-api/v1.22/#httpgetaction-v1-core">HTTP Probes</a>
可以在 <code>httpGet</code> 上配置额外的字段：</p>
<ul>
<li><code>host</code>：连接使用的主机名，默认是 Pod 的 IP。也可以在 HTTP 头中设置 “Host” 来代替。</li>
<li><code>scheme</code> ：用于设置连接主机的方式（HTTP 还是 HTTPS）。默认是 HTTP。</li>
<li><code>path</code>：访问 HTTP 服务的路径。默认值为 &quot;/&quot;。</li>
<li><code>httpHeaders</code>：请求中自定义的 HTTP 头。HTTP 头字段允许重复。</li>
<li><code>port</code>：访问容器的端口号或者端口名。如果数字必须在 1 ～ 65535 之间。</li>
</ul>
<!--
For an HTTP probe, the kubelet sends an HTTP request to the specified path and
port to perform the check. The kubelet sends the probe to the pod's IP address,
unless the address is overridden by the optional `host` field in `httpGet`. If
`scheme` field is set to `HTTPS`, the kubelet sends an HTTPS request skipping the
certificate verification. In most scenarios, you do not want to set the `host` field.
Here's one scenario where you would set it. Suppose the container listens on 127.0.0.1
and the Pod's `hostNetwork` field is true. Then `host`, under `httpGet`, should be set
to 127.0.0.1. If your pod relies on virtual hosts, which is probably the more common
case, you should not use `host`, but rather set the `Host` header in `httpHeaders`.
-->
<p>对于 HTTP 探测，kubelet 发送一个 HTTP 请求到指定的路径和端口来执行检测。
除非 <code>httpGet</code> 中的 <code>host</code> 字段设置了，否则 kubelet 默认是给 Pod 的 IP 地址发送探测。
如果 <code>scheme</code> 字段设置为了 <code>HTTPS</code>，kubelet 会跳过证书验证发送 HTTPS 请求。
大多数情况下，不需要设置<code>host</code> 字段。
这里有个需要设置 <code>host</code> 字段的场景，假设容器监听 127.0.0.1，并且 Pod 的 <code>hostNetwork</code>
字段设置为了 <code>true</code>。那么 <code>httpGet</code> 中的 <code>host</code> 字段应该设置为 127.0.0.1。
可能更常见的情况是如果 Pod 依赖虚拟主机，你不应该设置 <code>host</code> 字段，而是应该在
<code>httpHeaders</code> 中设置 <code>Host</code>。</p>
<!--
For an HTTP probe, the kubelet sends two request headers in addition to the mandatory `Host` header:
`User-Agent`, and `Accept`. The default values for these headers are `kube-probe/1.22`
(where `1.22` is the version of the kubelet ), and `*/*` respectively.

You can override the default headers by defining `.httpHeaders` for the probe; for example
-->
<p>针对 HTTP 探针，kubelet 除了必需的 <code>Host</code> 头部之外还发送两个请求头部字段：
<code>User-Agent</code> 和 <code>Accept</code>。这些头部的默认值分别是 <code>kube-probe/{{ skew latestVersion &gt;}}</code>
（其中 <code>1.22</code> 是 kubelet 的版本号）和 <code>*/*</code>。</p>
<p>你可以通过为探测设置 <code>.httpHeaders</code> 来重载默认的头部字段值；例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">httpHeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>Accept<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>application/json<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">startupProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">httpHeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>User-Agent<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>MyUserAgent<span style="color:#bbb">
</span></code></pre></div><!--
You can also remove these two headers by defining them with an empty value.
-->
<p>你也可以通过将这些头部字段定义为空值，从请求中去掉这些头部字段。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">httpHeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>Accept<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">startupProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">httpHeaders</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>User-Agent<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### TCP probes

For a TCP probe, the kubelet makes the probe connection at the node, not in the pod, which
means that you can not use a service name in the `host` parameter since the kubelet is unable
to resolve it.
-->
<h3 id="tcp-probes">TCP 探测 </h3>
<p>对于一次 TCP 探测，kubelet 在节点上（不是在 Pod 里面）建立探测连接，
这意味着你不能在 <code>host</code> 参数上配置服务名称，因为 kubelet 不能解析服务名称。</p>
<!--
### Probe-level `terminationGracePeriodSeconds`
-->
<h3 id="探测器级别-terminationgraceperiodseconds">探测器级别 <code>terminationGracePeriodSeconds</code></h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code>
</div>

<!--
Prior to release 1.21, the pod-level `terminationGracePeriodSeconds` was used
for terminating a container that failed its liveness or startup probe. This
coupling was unintended and may have resulted in failed containers taking an
unusually long time to restart when a pod-level `terminationGracePeriodSeconds`
was set.
-->
<p>在 1.21 版之前，pod 级别的 <code>terminationGracePeriodSeconds</code> 被用来终止
未能成功处理活跃性探测或启动探测的容器。
这种耦合是意料之外的，可能会导致在设置了 pod 级别的 <code>terminationGracePeriodSeconds</code> 后，
需要很长的时间来重新启动失败的容器。</p>
<!--
In 1.21, when the feature flag `ProbeTerminationGracePeriod` is enabled, users
can specify a probe-level `terminationGracePeriodSeconds` as part of the probe
specification. When the feature flag is enabled, and both a pod- and
probe-level `terminationGracePeriodSeconds` are set, the kubelet will use the
probe-level value.

For example,
-->
<p>在1.21中，启用特性标志 <code>ProbeTerminationGracePeriod</code> 后，
用户可以指定一个探测器级别的 <code>terminationGracePeriodSeconds</code> 作为探测器规格的一部分。
当该特性标志被启用时，若同时设置了 Pod 级别和探测器级别的 <code>terminationGracePeriodSeconds</code>，
kubelet 将使用探测器级的值。</p>
<p>例如，</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">3600</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># pod-level</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>liveness-port<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span>liveness-port<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">failureThreshold</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Override pod-level terminationGracePeriodSeconds #</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></code></pre></div><!--
Probe-level `terminationGracePeriodSeconds` cannot be set for readiness probes.
It will be rejected by the API server.
-->
<p>探测器级别的 <code>terminationGracePeriodSeconds</code> 不能用于设置就绪态探针。
它将被 API 服务器拒绝。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about
[Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#container-probes">容器探针</a>。</li>
</ul>
<!--
You can also read the API references for:

* [Pod](/docs/reference/generated/kubernetes-api/v1.22/#pod-v1-core)
* [Container](/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core)
* [Probe](/docs/reference/generated/kubernetes-api/v1.22/#probe-v1-core)
-->
<p>你也可以阅读以下的 API 参考资料：</p>
<ul>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#pod-v1-core">Pod</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core">Container</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#probe-v1-core">Probe</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bbc17480da6d051c696489654c64064a">3.14 - 将 Pod 分配给节点</h1>
    
	<!--
title: Assign Pods to Nodes
content_type: task
weight: 120
-->
<!-- overview -->
<!--
This page shows how to assign a Kubernetes Pod to a particular node in a
Kubernetes cluster.
-->
<p>此页面显示如何将 Kubernetes Pod 分配给 Kubernetes 集群中的特定节点。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Add a label to a node

1. List the nodes in your cluster:
-->
<h2 id="给节点添加标签">给节点添加标签</h2>
<ol>
<li>
<p>列出集群中的节点</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似如下：</p>
<pre tabindex="0"><code>NAME      STATUS    AGE     VERSION
worker0   Ready     1d      v1.6.0+fff5156
worker1   Ready     1d      v1.6.0+fff5156
worker2   Ready     1d      v1.6.0+fff5156
</code></pre></li>
</ol>
<!--
1. Chose one of your nodes, and add a label to it:
-->
<ol start="2">
<li>
<p>选择其中一个节点，为它添加标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label nodes &lt;your-node-name&gt; <span style="color:#b8860b">disktype</span><span style="color:#666">=</span>ssd
</code></pre></div><!--
where `<your-node-name>` is the name of your chosen node.
-->
<p><code>&lt;your-node-name&gt;</code> 是你选择的节点的名称。</p>
</li>
</ol>
<!--
1. Verify that your chosen node has a `disktype=ssd` label:
-->
<ol start="3">
<li>
<p>验证你选择的节点是否有 <code>disktype=ssd</code> 标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes --show-labels
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似如下：</p>
<pre tabindex="0"><code>NAME      STATUS    AGE     VERSION            LABELS
worker0   Ready     1d      v1.6.0+fff5156     ...,disktype=ssd,kubernetes.io/hostname=worker0
worker1   Ready     1d      v1.6.0+fff5156     ...,kubernetes.io/hostname=worker1
worker2   Ready     1d      v1.6.0+fff5156     ...,kubernetes.io/hostname=worker2
</code></pre><!--
In the preceding output, you can see that the `worker0` node has a
`disktype=ssd` label.
-->
<p>在前面的输出中，你可以看到 <code>worker0</code> 节点有 <code>disktype=ssd</code> 标签。</p>
</li>
</ol>
<!--
## Create a pod that gets scheduled to your chosen node

This pod configuration file describes a pod that has a node selector,
`disktype: ssd`. This means that the pod will get scheduled on a node that has
a `disktype=ssd` label.
-->
<h2 id="创建一个调度到你选择的节点的-pod">创建一个调度到你选择的节点的 pod</h2>
<p>此 Pod 配置文件描述了一个拥有节点选择器 <code>disktype: ssd</code> 的 Pod。这表明该 Pod 将被调度到
有 <code>disktype=ssd</code> 标签的节点。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-nginx.yaml" download="pods/pod-nginx.yaml"><code>pods/pod-nginx.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-nginx-yaml')" title="Copy pods/pod-nginx.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-nginx-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb"> </span>test<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">disktype</span>:<span style="color:#bbb"> </span>ssd<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Use the configuration file to create a pod that will get scheduled on your
   chosen node:
-->
<ol>
<li>
<p>使用该配置文件去创建一个 pod，该 pod 将被调度到你选择的节点上：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/pod-nginx.yaml
</code></pre></div></li>
</ol>
<!--
1. Verify that the pod is running on your chosen node:
-->
<ol start="2">
<li>
<p>验证 pod 是不是运行在你选择的节点上：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --output<span style="color:#666">=</span>wide
</code></pre></div><!-- The output is similar to this: -->
<p>输出类似如下：</p>
<pre tabindex="0"><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre></li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
Learn more about
[labels and selectors](/docs/concepts/overview/working-with-objects/labels/).
-->
<p>进一步了解<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签和选择器</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fc3f4777ae8ea685d2b54e175277ac01">3.15 - 用节点亲和性把 Pods 分配到节点</h1>
    
	<!--
title: Assign Pods to Nodes using Node Affinity
min-kubernetes-server-version: v1.10
content_type: task
weight: 120
-->
<!-- overview -->
<!--
This page shows how to assign a Kubernetes Pod to a particular node using Node Affinity in a
Kubernetes cluster.
-->
<p>本页展示在 Kubernetes 集群中，如何使用节点亲和性把 Kubernetes Pod 分配到特定节点。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.10.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Add a label to a node

1. List the nodes in your cluster, along with their labels:
-->
<h2 id="给节点添加标签">给节点添加标签</h2>
<ol>
<li>
<p>列出集群中的节点及其标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes --show-labels
</code></pre></div> <!--
 The output is similar to this:
 -->
<p>输出类似于此：</p>
<pre tabindex="0"><code>NAME      STATUS    ROLES    AGE     VERSION        LABELS
worker0   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=worker0
worker1   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=worker1
worker2   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=worker2
</code></pre> <!--
 1. Chose one of your nodes, and add a label to it:
 -->
</li>
<li>
<p>选择一个节点，给它添加一个标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl label nodes &lt;your-node-name&gt; <span style="color:#b8860b">disktype</span><span style="color:#666">=</span>ssd
</code></pre></div> <!--
     where `<your-node-name>` is the name of your chosen node.

 1. Verify that your chosen node has a `disktype=ssd` label:
 -->
<p>其中 <code>&lt;your-node-name&gt;</code> 是你所选节点的名称。</p>
</li>
<li>
<p>验证你所选节点具有 <code>disktype=ssd</code> 标签：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes --show-labels
</code></pre></div> <!--
 The output is similar to this:
 -->
<p>输出类似于此：</p>
<pre tabindex="0"><code>NAME      STATUS    ROLES    AGE     VERSION        LABELS
worker0   Ready     &lt;none&gt;   1d      v1.13.0        ...,disktype=ssd,kubernetes.io/hostname=worker0
worker1   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=worker1
worker2   Ready     &lt;none&gt;   1d      v1.13.0        ...,kubernetes.io/hostname=worker2
</code></pre> <!--
     In the preceding output, you can see that the `worker0` node has a
     `disktype=ssd` label.
 -->
<p>在前面的输出中，可以看到 <code>worker0</code> 节点有一个 <code>disktype=ssd</code> 标签。</p>
</li>
</ol>
<!--
## Schedule a Pod using required node affinity

This manifest describes a Pod that has a `requiredDuringSchedulingIgnoredDuringExecution` node affinity,`disktype: ssd`. 
This means that the pod will get scheduled only on a node that has a `disktype=ssd` label. 
-->
<h2 id="schedule-a-Pod-using-required-node-affinity">依据强制的节点亲和性调度 Pod </h2>
<p>下面清单描述了一个 Pod，它有一个节点亲和性配置 <code>requiredDuringSchedulingIgnoredDuringExecution</code>，<code>disktype=ssd</code>。
这意味着 pod 只会被调度到具有 <code>disktype=ssd</code> 标签的节点上。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-nginx-required-affinity.yaml" download="pods/pod-nginx-required-affinity.yaml"><code>pods/pod-nginx-required-affinity.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-nginx-required-affinity-yaml')" title="Copy pods/pod-nginx-required-affinity.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-nginx-required-affinity-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requiredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>disktype<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- ssd            <span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Apply the manifest to create a Pod that is scheduled onto your
   chosen node:
-->
<ol>
<li>
<p>执行（Apply）此清单来创建一个调度到所选节点上的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/pod-nginx-required-affinity.yaml
</code></pre></div> <!--
 1. Verify that the pod is running on your chosen node:
 -->
</li>
<li>
<p>验证 pod 已经在所选节点上运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --output<span style="color:#666">=</span>wide
</code></pre></div> <!--
 The output is similar to this:
 -->
<p>输出类似于此：</p>
<pre tabindex="0"><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre></li>
</ol>
<!--    
## Schedule a Pod using preferred node affinity

This manifest describes a Pod that has a `preferredDuringSchedulingIgnoredDuringExecution` node affinity,`disktype: ssd`. 
This means that the pod will prefer a node that has a `disktype=ssd` label. 
-->
<h2 id="schedule-a-Pod-using-preferred-node-affinity">使用首选的节点亲和性调度 Pod</h2>
<p>本清单描述了一个Pod，它有一个节点亲和性设置 <code>preferredDuringSchedulingIgnoredDuringExecution</code>，<code>disktype: ssd</code>。
这意味着 pod 将首选具有 <code>disktype=ssd</code> 标签的节点。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-nginx-preferred-affinity.yaml" download="pods/pod-nginx-preferred-affinity.yaml"><code>pods/pod-nginx-preferred-affinity.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-nginx-preferred-affinity-yaml')" title="Copy pods/pod-nginx-preferred-affinity.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-nginx-preferred-affinity-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">affinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">preferredDuringSchedulingIgnoredDuringExecution</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">weight</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">preference</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>disktype<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- ssd          <span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Apply the manifest to create a Pod that is scheduled onto your
   chosen node:
-->
<ol>
<li>
<p>执行此清单创建一个会调度到所选节点上的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/pod-nginx-preferred-affinity.yaml
</code></pre></div> <!--
 1. Verify that the pod is running on your chosen node:
 -->
</li>
<li>
<p>验证 pod 是否在所选节点上运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --output<span style="color:#666">=</span>wide
</code></pre></div> <!--
 The output is similar to this:
 -->
<p>输出类似于此：</p>
<pre tabindex="0"><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre></li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
Learn more about
[Node Affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity).
-->
<p>进一步了解
<a href="/zh/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">节点亲和性</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1e7baac1825631a5af5d2aebcf059249">3.16 - 配置 Pod 初始化</h1>
    
	<!--
title: Configure Pod Initialization
content_type: task
weight: 130
-->
<!-- overview -->
<!--
This page shows how to use an Init Container to initialize a Pod before an
application Container runs.
-->
<p>本文介绍在应用容器运行前，怎样利用 Init 容器初始化 Pod。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a Pod that has an Init Container

In this exercise you create a Pod that has one application Container and one
Init Container. The init container runs to completion before the application
container starts.

Here is the configuration file for the Pod:
-->
<h2 id="creating-a-pod-that-has-an-init-container">创建一个包含 Init 容器的 Pod </h2>
<p>本例中你将创建一个包含一个应用容器和一个 Init 容器的 Pod。Init 容器在应用容器启动前运行完成。</p>
<p>下面是 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/init-containers.yaml" download="pods/init-containers.yaml"><code>pods/init-containers.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-init-containers-yaml')" title="Copy pods/init-containers.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-init-containers-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>init-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>workdir<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># These containers are run during pod initialization</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">initContainers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>install<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- wget<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;-O&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;/work-dir/index.html&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- http://info.cern.ch<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>workdir<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/work-dir&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dnsPolicy</span>:<span style="color:#bbb"> </span>Default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>workdir<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a Volume that the init
container and the application container share.

The init container mounts the
shared Volume at `/work-dir`, and the application container mounts the shared
Volume at `/usr/share/nginx/html`. The init container runs the following command
and then terminates:
-->
<p>配置文件中，你可以看到应用容器和 Init 容器共享了一个卷。</p>
<p>Init 容器将共享卷挂载到了 <code>/work-dir</code> 目录，应用容器将共享卷挂载到了 <code>/usr/share/nginx/html</code> 目录。
Init 容器执行完下面的命令就终止：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget -O /work-dir/index.html http://info.cern.ch
</code></pre></div><!--
Notice that the init container writes the `index.html` file in the root directory
of the nginx server.

Create the Pod:
-->
<p>请注意 Init 容器在 nginx 服务器的根目录写入 <code>index.html</code>。</p>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/init-containers.yaml
</code></pre></div><!--
Verify that the nginx container is running:
-->
<p>检查 nginx 容器运行正常：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod init-demo
</code></pre></div><!--
The output shows that the nginx container is running:
-->
<p>结果表明 nginx 容器运行正常：</p>
<pre tabindex="0"><code>NAME        READY     STATUS    RESTARTS   AGE
init-demo   1/1       Running   0          1m
</code></pre><!--
Get a shell into the nginx container running in the init-demo Pod:
-->
<p>通过 shell 进入 init-demo Pod 中的 nginx 容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it init-demo -- /bin/bash
</code></pre></div><!--
In your shell, send a GET request to the nginx server:
-->
<p>在 shell 中，发送个 GET 请求到 nginx 服务器：</p>
<pre tabindex="0"><code>root@nginx:~# apt-get update
root@nginx:~# apt-get install curl
root@nginx:~# curl localhost
</code></pre><!--
The output shows that nginx is serving the web page that was written by the init container:
-->
<p>结果表明 nginx 正在为 Init 容器编写的 web 页面服务：</p>
<pre tabindex="0"><code>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;header&gt;
&lt;title&gt;http://info.cern.ch&lt;/title&gt;
&lt;/header&gt;

&lt;h1&gt;http://info.cern.ch - home of the first website&lt;/h1&gt;
  ...
&lt;li&gt;&lt;a href=&quot;http://info.cern.ch/hypertext/WWW/TheProject.html&quot;&gt;Browse the first website&lt;/a&gt;&lt;/li&gt;
  ...
</code></pre><h2 id="接下来">接下来</h2>
<!--
* Learn more about
[communicating between Containers running in the same Pod](/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/).
* Learn more about [Init Containers](/docs/concepts/workloads/pods/init-containers/).
* Learn more about [Volumes](/docs/concepts/storage/volumes/).
* Learn more about [Debugging Init Containers](/docs/tasks/debug-application-cluster/debug-init-containers/)
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/">同一 Pod 中的容器间的通信</a>。</li>
<li>进一步了解 <a href="/zh/docs/concepts/workloads/pods/init-containers/">Init 容器</a>。</li>
<li>进一步了解<a href="/zh/docs/concepts/storage/volumes/">卷</a>。</li>
<li>进一步了解 <a href="/zh/docs/tasks/debug-application-cluster/debug-init-containers/">Init 容器排错</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-efbc43486296f0439d1a89c12d944d94">3.17 - 为容器的生命周期事件设置处理函数</h1>
    
	<!--
title: Attach Handlers to Container Lifecycle Events
content_type: task
weight: 140
-->
<!-- overview -->
<!--
This page shows how to attach handlers to Container lifecycle events. Kubernetes supports
the postStart and preStop events. Kubernetes sends the postStart event immediately
after a Container is started, and it sends the preStop event immediately before the
Container is terminated.A Container may specify one handler per event.
-->
<p>这个页面将演示如何为容器的生命周期事件挂接处理函数。Kubernetes 支持 postStart 和 preStop 事件。
当一个容器启动后，Kubernetes 将立即发送 postStart 事件；在容器被终结之前，
Kubernetes 将发送一个 preStop 事件。容器可以为每个事件指定一个处理程序。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Define postStart and preStop handlers

In this exercise, you create a Pod that has one Container. The Container has handlers
for the postStart and preStop events.
-->
<h2 id="定义-poststart-和-prestop-处理函数">定义 postStart 和 preStop 处理函数</h2>
<p>在本练习中，你将创建一个包含一个容器的 Pod，该容器为 postStart 和 preStop 事件提供对应的处理函数。</p>
<!--
Here is the configuration file for the Pod:
-->
<p>下面是对应 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/lifecycle-events.yaml" download="pods/lifecycle-events.yaml"><code>pods/lifecycle-events.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-lifecycle-events-yaml')" title="Copy pods/lifecycle-events.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-lifecycle-events-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lifecycle-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>lifecycle-demo-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">lifecycle</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">postStart</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;echo Hello from the postStart handler &gt; /usr/share/message&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">preStop</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#b44">&#34;nginx -s quit; while killall -0 nginx; do sleep 1; done&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the postStart command writes a `message`
file to the Container's `/usr/share` directory. The preStop command shuts down
nginx gracefully. This is helpful if the Container is being terminated because of a failure.
-->
<p>在上述配置文件中，你可以看到 postStart 命令在容器的 <code>/usr/share</code> 目录下写入文件 <code>message</code>。
命令 preStop 负责优雅地终止 nginx 服务。当因为失效而导致容器终止时，这一处理方式很有用。</p>
<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/lifecycle-events.yaml
</code></pre></div><!--
Verify that the Container in the Pod is running:
-->
<p>验证 Pod 中的容器已经运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod lifecycle-demo
</code></pre></div><!--
Get a shell into the Container running in your Pod:
-->
<p>使用 shell 连接到你的 Pod 里的容器：</p>
<pre tabindex="0"><code>kubectl exec -it lifecycle-demo -- /bin/bash
</code></pre><!--
In your shell, verify that the `postStart` handler created the `message` file:
-->
<p>在 shell 中，验证 <code>postStart</code> 处理函数创建了 <code>message</code> 文件：</p>
<pre tabindex="0"><code>root@lifecycle-demo:/# cat /usr/share/message
</code></pre><!--
The output shows the text written by the postStart handler:
-->
<p>命令行输出的是 <code>postStart</code> 处理函数所写入的文本</p>
<pre tabindex="0"><code>Hello from the postStart handler
</code></pre><!-- discussion -->
<!--
## Discussion

Kubernetes sends the postStart event immediately after the Container is created.
There is no guarantee, however, that the postStart handler is called before
the Container's entrypoint is called. The postStart handler runs asynchronously
relative to the Container's code, but Kubernetes' management of the container
blocks until the postStart handler completes. The Container's status is not
set to RUNNING until the postStart handler completes.
-->
<h2 id="讨论">讨论</h2>
<p>Kubernetes 在容器创建后立即发送 postStart 事件。
然而，postStart 处理函数的调用不保证早于容器的入口点（entrypoint）
的执行。postStart 处理函数与容器的代码是异步执行的，但 Kubernetes
的容器管理逻辑会一直阻塞等待 postStart 处理函数执行完毕。
只有 postStart 处理函数执行完毕，容器的状态才会变成
RUNNING。</p>
<!--
Kubernetes sends the preStop event immediately before the Container is terminated.
Kubernetes' management of the Container blocks until the preStop handler completes,
unless the Pod's grace period expires. For more details, see
[Termination of Pods](/docs/user-guide/pods/#termination-of-pods).
-->
<p>Kubernetes 在容器结束前立即发送 preStop 事件。除非 Pod 宽限期限超时，Kubernetes 的容器管理逻辑
会一直阻塞等待 preStop 处理函数执行完毕。更多的相关细节，可以参阅
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">Pods 的结束</a>。</p>
<!--
Kubernetes only sends the preStop event when a Pod is *terminated*.
This means that the preStop hook is not invoked when the Pod is *completed*.
This limitation is tracked in [issue #55087](https://github.com/kubernetes/kubernetes/issues/55807).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> Kubernetes 只有在 Pod <em>结束（Terminated）</em> 的时候才会发送 preStop 事件，
这意味着在 Pod <em>完成（Completed）</em> 时
preStop 的事件处理逻辑不会被触发。这个限制在
<a href="https://github.com/kubernetes/kubernetes/issues/55807">issue #55087</a> 中被追踪。</div>
</blockquote>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [Container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).
* Learn more about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/containers/container-lifecycle-hooks/">容器生命周期回调</a>。</li>
<li>进一步了解<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Pod 的生命周期</a>。</li>
</ul>
<!--
### Reference

* [Lifecycle](/docs/reference/generated/kubernetes-api/v1.22/#lifecycle-v1-core)
* [Container](/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core)
* See `terminationGracePeriodSeconds` in [PodSpec](/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core)
-->
<h3 id="参考">参考</h3>
<ul>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#lifecycle-v1-core">Lifecycle</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core">Container</a></li>
<li>参阅 <a href="/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core">PodSpec</a> 中关于<code>terminationGracePeriodSeconds</code> 的部分</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ed34e761c3dbd00fa79577fa78e30020">3.18 - 配置 Pod 使用 ConfigMap</h1>
    
	<!--
title: Configure a Pod to Use a ConfigMap
content_type: task
weight: 150
card:
  name: tasks
  weight: 50
-->
<!-- overview -->
<!--
ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable. This page provides a series of usage examples demonstrating how to create ConfigMaps and configure Pods using data stored in ConfigMaps. 
-->
<p>ConfigMap 允许你将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。
本页提供了一系列使用示例，这些示例演示了如何创建 ConfigMap 以及配置 Pod
使用存储在 ConfigMap 中的数据。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a ConfigMap

You can use either `kubectl create configmap` or a ConfigMap generator in `kustomization.yaml` to create a ConfigMap. Note that `kubectl` starts to support `kustomization.yaml` since 1.14.
-->
<h2 id="创建-configmap">创建 ConfigMap</h2>
<p>你可以使用 <code>kubectl create configmap</code> 或者在 <code>kustomization.yaml</code> 中的 ConfigMap 生成器
来创建 ConfigMap。注意，<code>kubectl</code> 从 1.14 版本开始支持 <code>kustomization.yaml</code>。</p>
<!--
### Create a ConfigMap Using kubectl create configmap

Use the `kubectl create configmap` command to create configmaps from [directories](#create-configmaps-from-directories), [files](#create-configmaps-from-files), or [literal values](#create-configmaps-from-literal-values):
-->
<h3 id="使用-kubectl-create-configmap-创建-configmap">使用 kubectl create configmap 创建 ConfigMap</h3>
<p>你可以使用 <code>kubectl create configmap</code> 命令基于
<a href="#create-configmaps-from-directories">目录</a>、<a href="#create-configmaps-from-files">文件</a>
或者<a href="#create-configmaps-from-literal-values">字面值</a>来创建 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap &lt;map-name&gt; &lt;data-source&gt;
</code></pre></div><!--
where \<map-name> is the name you want to assign to the ConfigMap and \<data-source> is the directory, file, or literal value to draw the data from.
The name of a ConfigMap object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
-->
<p>其中，&lt;map-name&gt; 是要设置的 ConfigMap 名称，&lt;data-source&gt; 是要从中提取数据的目录、
文件或者字面值。
ConfigMap 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.</p>
<!--
When you are creating a ConfigMap based on a file, the key in the \<data-source> defaults to the basename of the file, and the value defaults to the file content.
-->
<p>在你基于文件来创建 ConfigMap 时，&lt;data-source&gt; 中的键名默认取自
文件的基本名，而对应的值则默认为文件的内容。</p>
<!--
You can use [`kubectl describe`](/docs/reference/generated/kubectl/kubectl-commands/#describe) or
[`kubectl get`](/docs/reference/generated/kubectl/kubectl-commands/#get) to retrieve information
about a ConfigMap.
-->
<p>你可以使用<a href="/docs/reference/generated/kubectl/kubectl-commands/#describe"><code>kubectl describe</code></a> 或者
<a href="/docs/reference/generated/kubectl/kubectl-commands/#get"><code>kubectl get</code></a> 获取有关 ConfigMap 的信息。</p>
<!--
#### Create ConfigMaps from directories

You can use `kubectl create configmap` to create a ConfigMap from multiple
files in the same directory.
When you are creating a ConfigMap based on a directory, kubectl identifies
files whose basename is a valid key in the directory and packages each of
those files into the new ConfigMap. Any directory entries except regular files
are ignored (e.g. subdirectories, symlinks, devices, pipes, etc).

For example:
-->
<h4 id="create-configmaps-from-directories">基于目录创建 ConfigMap    </h4>
<p>你可以使用 <code>kubectl create configmap</code> 基于同一目录中的多个文件创建 ConfigMap。
当你基于目录来创建 ConfigMap 时，kubectl 识别目录下基本名可以作为合法键名的
文件，并将这些文件打包到新的 ConfigMap 中。普通文件之外的所有目录项都会被
忽略（例如，子目录、符号链接、设备、管道等等）。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建本地目录</span>
mkdir -p configure-pod-container/configmap/

<span style="color:#080;font-style:italic"># 将实例文件下载到 `configure-pod-container/configmap/` 目录</span>
wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties
wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties

<span style="color:#080;font-style:italic"># 创建 configmap</span>
kubectl create configmap game-config --from-file<span style="color:#666">=</span>configure-pod-container/configmap/
</code></pre></div><!--
The above command packages each file, in this case, `game.properties` and
`ui.properties` in the `configure-pod-container/configmap/` directory into the
game-config ConfigMap. You can display details of the ConfigMap using the
following command:
-->
<p>以上命令将 <code>configure-pod-container/configmap</code> 目录下的所有文件，也就是
<code>game.properties</code> 和 <code>ui.properties</code> 打包到 game-config ConfigMap
中。你可以使用下面的命令显示 ConfigMap 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe configmaps game-config
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似以下内容：</p>
<pre tabindex="0"><code>Name:         game-config
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
ui.properties:
----
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice
</code></pre><!--
The `game.properties` and `ui.properties` files in the `configure-pod-container/configmap/` directory are represented in the `data` section of the ConfigMap.
-->
<p><code>configure-pod-container/configmap/</code> 目录中的 <code>game.properties</code> 和 <code>ui.properties</code>
文件出现在 ConfigMap 的 <code>data</code> 部分。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmaps game-config -o yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似以下内容:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2016-02-18T18:52:05Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>game-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;516&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/namespaces/default/configmaps/game-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>b4952dc3-d670-11e5-8cd0-68f728db1985<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">game.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    enemies=aliens
</span><span style="color:#b44;font-style:italic">    lives=3
</span><span style="color:#b44;font-style:italic">    enemies.cheat=true
</span><span style="color:#b44;font-style:italic">    enemies.cheat.level=noGoodRotten
</span><span style="color:#b44;font-style:italic">    secret.code.passphrase=UUDDLRLRBABAS
</span><span style="color:#b44;font-style:italic">    secret.code.allowed=true
</span><span style="color:#b44;font-style:italic">    secret.code.lives=30</span><span style="color:#bbb">    
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ui.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    color.good=purple
</span><span style="color:#b44;font-style:italic">    color.bad=yellow
</span><span style="color:#b44;font-style:italic">    allow.textmode=true
</span><span style="color:#b44;font-style:italic">    how.nice.to.look=fairlyNice</span><span style="color:#bbb">    
</span></code></pre></div><!--
#### Create ConfigMaps from files

You can use `kubectl create configmap` to create a ConfigMap from an individual file, or from multiple files.

For example,
-->
<h4 id="create-configmaps-from-files">基于文件创建 ConfigMap  </h4>
<p>你可以使用 <code>kubectl create configmap</code> 基于单个文件或多个文件创建 ConfigMap。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap game-config-2 --from-file<span style="color:#666">=</span>configure-pod-container/configmap/game.properties
</code></pre></div><!--
would produce the following ConfigMap:
-->
<p>将产生以下 ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe configmaps game-config-2
</code></pre></div><!--
where the output is similar to this:
-->
<p>输出类似以下内容:</p>
<pre tabindex="0"><code>Name:         game-config-2
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
</code></pre><!--
You can pass in the `--from-file` argument multiple times to create a ConfigMap from multiple data sources.
-->
<p>你可以多次使用 <code>--from-file</code> 参数，从多个数据源创建 ConfigMap。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap game-config-2 --from-file<span style="color:#666">=</span>configure-pod-container/configmap/game.properties --from-file<span style="color:#666">=</span>configure-pod-container/configmap/ui.properties
</code></pre></div><!--
Describe the above `game-config-2` configmap created
-->
<p>描述上面创建的 <code>game-config-2</code> configmap</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe configmaps game-config-2
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似以下内容:</p>
<pre tabindex="0"><code>Name:         game-config-2
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
ui.properties:
----
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice
</code></pre><!--
When `kubectl` creates a ConfigMap from inputs that are not ASCII or UTF-8, the tool puts these into the `binaryData` field of the ConfigMap, and not in `data`. Both text and binary data sources can be combined in one ConfigMap.
If you want to view the `binaryData` keys (and their values) in a ConfigMap, you can run `kubectl get configmap -o jsonpath='{.binaryData}' <name>`.

Use the option `--from-env-file` to create a ConfigMap from an env-file, for example:
-->
<p>当 <code>kubectl</code> 基于非 ASCII 或 UTF-8 的输入创建 ConfigMap 时，
该工具将这些输入放入 ConfigMap 的 <code>binaryData</code> 字段，而不是 <code>data</code> 中。
同一个 ConfigMap 中可同时包含文本数据和二进制数据源。
如果你想查看 ConfigMap 中的 <code>binaryData</code> 键（及其值），
你可以运行 <code>kubectl get configmap -o jsonpath='{.binaryData}' &lt;name&gt;</code>。</p>
<p>使用 <code>--from-env-file</code> 选项从环境文件创建 ConfigMap，例如：</p>
<!--
```shell
# Env-files contain a list of environment variables.
# These syntax rules apply:
#   Each line in an env file has to be in VAR=VAL format.
#   Lines beginning with # (i.e. comments) are ignored.
#   Blank lines are ignored.
#   There is no special handling of quotation marks (i.e. they will be part of the ConfigMap value)).

# Download the sample files into `configure-pod-container/configmap/` directory
wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties

# The env-file `game-env-file.properties` looks like below
cat configure-pod-container/configmap/game-env-file.properties
enemies=aliens
lives=3
allowed="true"
-->
<p>Env 文件包含环境变量列表。
其中适用以下语法规则:</p>
<ul>
<li>Env 文件中的每一行必须为 VAR=VAL 格式。</li>
<li>以＃开头的行（即注释）将被忽略。</li>
<li>空行将被忽略。</li>
<li>引号不会被特殊处理（即它们将成为 ConfigMap 值的一部分）。</li>
</ul>
<p>将示例文件下载到 <code>configure-pod-container/configmap/</code> 目录</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties
</code></pre></div><p>env 文件 <code>game-env-file.properties</code> 如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat configure-pod-container/configmap/game-env-file.properties
</code></pre></div><pre tabindex="0"><code>enemies=aliens
lives=3
allowed=&quot;true&quot;
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap game-config-env-file <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>       --from-env-file<span style="color:#666">=</span>configure-pod-container/configmap/game-env-file.properties
</code></pre></div><!--
would produce the following ConfigMap:
-->
<p>将产生以下 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmap game-config-env-file -o yaml
</code></pre></div><!--
where the output is similar to this:
-->
<p>输出类似以下内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2017-12-27T18:36:28Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>game-config-env-file<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;809965&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/namespaces/default/configmaps/game-config-env-file<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>d9d1ca5b-eb34-11e7-887b-42010a8002b8<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">allowed</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;&#34;true&#34;&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">enemies</span>:<span style="color:#bbb"> </span>aliens<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">lives</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;3&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
When passing `--from-env-file` multiple times to create a ConfigMap from multiple data sources, only the last env-file is used:
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 当多次使用 <code>--from-env-file</code> 来从多个数据源创建 ConfigMap 时，仅仅最后一个 env 文件有效。</div>
</blockquote>

<!--
The behavior of passing `--from-env-file` multiple times is demonstrated by:
-->
<p>下面是一个多次使用 <code>--from-env-file</code> 参数的示例：</p>
<!--
```shell
# Download the sample files into `configure-pod-container/configmap/` directory
wget https://k8s.io/examples/configmap/ui-env-file.properties -O configure-pod-container/configmap/ui-env-file.properties

# Create the configmap
kubectl create configmap config-multi-env-files \
        --from-env-file=configure-pod-container/configmap/game-env-file.properties \
        --from-env-file=configure-pod-container/configmap/ui-env-file.properties
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 将样本文件下载到 `configure-pod-container/configmap/` 目录</span>
wget https://k8s.io/examples/configmap/ui-env-file.properties -O configure-pod-container/configmap/ui-env-file.properties

<span style="color:#080;font-style:italic"># 创建 configmap</span>
kubectl create configmap config-multi-env-files <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>        --from-env-file<span style="color:#666">=</span>configure-pod-container/configmap/game-env-file.properties <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>        --from-env-file<span style="color:#666">=</span>configure-pod-container/configmap/ui-env-file.properties
</code></pre></div><!--
would produce the following ConfigMap:
-->
<p>将产生以下 ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmap config-multi-env-files -o yaml
</code></pre></div><!--
where the output is similar to this:
-->
<p>输出类似以下内容:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2017-12-27T18:38:34Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-multi-env-files<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;810136&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/namespaces/default/configmaps/config-multi-env-files<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>252c4572-eb35-11e7-887b-42010a8002b8<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">color</span>:<span style="color:#bbb"> </span>purple<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">how</span>:<span style="color:#bbb"> </span>fairlyNice<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">textmode</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
#### Define the key to use when creating a ConfigMap from a file

You can define a key other than the file name to use in the `data` section of your ConfigMap when using the `--from-file` argument:
-->
<h4 id="定义从文件创建-configmap-时要使用的键">定义从文件创建 ConfigMap 时要使用的键</h4>
<p>在使用 <code>--from-file</code> 参数时，你可以定义在 ConfigMap 的 <code>data</code> 部分出现键名，
而不是按默认行为使用文件名：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap game-config-3 --from-file<span style="color:#666">=</span>&lt;my-key-name&gt;<span style="color:#666">=</span>&lt;path-to-file&gt;
</code></pre></div><!--
where <my-key-name> is the key you want to use in the ConfigMap and `<path-to-file>` is the location of the data source file you want the key to represent.
-->
<p><code>&lt;my-key-name&gt;</code> 是你要在 ConfigMap 中使用的键名，<code>&lt;path-to-file&gt;</code> 是你想要键表示数据源文件的位置。</p>
<!-- For example: -->
<p>例如:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap game-config-3 --from-file<span style="color:#666">=</span>game-special-key<span style="color:#666">=</span>configure-pod-container/configmap/game.properties
</code></pre></div><!--
would produce the following ConfigMap:
-->
<p>将产生以下 ConfigMap:</p>
<pre tabindex="0"><code>kubectl get configmaps game-config-3 -o yaml
</code></pre><!-- where the output is similar to this: -->
<p>输出类似以下内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2016-02-18T18:54:22Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>game-config-3<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;530&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/namespaces/default/configmaps/game-config-3<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>05f8da22-d671-11e5-8cd0-68f728db1985<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">game-special-key</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    enemies=aliens
</span><span style="color:#b44;font-style:italic">    lives=3
</span><span style="color:#b44;font-style:italic">    enemies.cheat=true
</span><span style="color:#b44;font-style:italic">    enemies.cheat.level=noGoodRotten
</span><span style="color:#b44;font-style:italic">    secret.code.passphrase=UUDDLRLRBABAS
</span><span style="color:#b44;font-style:italic">    secret.code.allowed=true
</span><span style="color:#b44;font-style:italic">    secret.code.lives=30</span><span style="color:#bbb">    
</span></code></pre></div><!--
#### Create ConfigMaps from literal values

You can use `kubectl create configmap` with the `--from-literal` argument to define a literal value from the command line:
-->
<h4 id="create-configmaps-from-literal-values">根据字面值创建 ConfigMap        </h4>
<p>你可以将 <code>kubectl create configmap</code> 与 <code>--from-literal</code> 参数一起使用，从命令行定义文字值:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap special-config --from-literal<span style="color:#666">=</span>special.how<span style="color:#666">=</span>very --from-literal<span style="color:#666">=</span>special.type<span style="color:#666">=</span>charm
</code></pre></div><!--
You can pass in multiple key-value pairs. Each pair provided on the command line is represented as a separate entry in the `data` section of the ConfigMap.
-->
<p>你可以传入多个键值对。命令行中提供的每对键值在 ConfigMap 的 <code>data</code> 部分中均表示为单独的条目。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmaps special-config -o yaml
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似以下内容:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2016-02-18T19:14:38Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;651&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/namespaces/default/configmaps/special-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>dadce046-d673-11e5-8cd0-68f728db1985<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">special.how</span>:<span style="color:#bbb"> </span>very<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">special.type</span>:<span style="color:#bbb"> </span>charm<span style="color:#bbb">
</span></code></pre></div><!--
### Create a ConfigMap from generator


`kubectl` supports `kustomization.yaml` since 1.14.
You can also create a ConfigMap from generators and then apply it to create the object on
the Apiserver. The generators
should be specified in a `kustomization.yaml` inside a directory.
-->
<h3 id="基于生成器创建-configmap">基于生成器创建 ConfigMap</h3>
<p>自 1.14 开始，<code>kubectl</code> 开始支持 <code>kustomization.yaml</code>。
你还可以基于生成器创建 ConfigMap，然后将其应用于 API 服务器上创建对象。
生成器应在目录内的 <code>kustomization.yaml</code> 中指定。</p>
<!--
#### Generate ConfigMaps from files

For example, to generate a ConfigMap from files `configure-pod-container/configmap/kubectl/game.properties`
-->
<h4 id="基于文件生成-configmap">基于文件生成 ConfigMap</h4>
<p>例如，要从 <code>configure-pod-container/configmap/kubectl/game.properties</code> 文件生成一个 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建包含 ConfigMapGenerator 的 kustomization.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">configMapGenerator:
</span><span style="color:#b44">- name: game-config-4
</span><span style="color:#b44">  files:
</span><span style="color:#b44">  - configure-pod-container/configmap/kubectl/game.properties
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Apply the kustomization directory to create the ConfigMap object.
-->
<p>使用 kustomization 目录创建 ConfigMap 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k .
</code></pre></div><pre tabindex="0"><code>configmap/game-config-4-m9dm2f92bt created
</code></pre><!--
You can check that the ConfigMap was created like this:
-->
<p>你可以检查 ConfigMap 是这样创建的:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get configmap
</code></pre></div><pre tabindex="0"><code>NAME                       DATA   AGE
game-config-4-m9dm2f92bt   1      37s


kubectl describe configmaps/game-config-4-m9dm2f92bt
Name:         game-config-4-m9dm2f92bt
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:{&quot;game.properties&quot;:&quot;enemies=aliens\nlives=3\nenemies.cheat=true\nenemies.cheat.level=noGoodRotten\nsecret.code.p...

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
Events:  &lt;none&gt;
</code></pre><!--
Note that the generated ConfigMap name has a suffix appended by hashing the contents. This ensures that a
new ConfigMap is generated each time the content is modified.
-->
<p>请注意，生成的 ConfigMap 名称具有通过对内容进行散列而附加的后缀，
这样可以确保每次修改内容时都会生成新的 ConfigMap。</p>
<!--
#### Define the key to use when generating a ConfigMap from a file

You can define a key other than the file name to use in the ConfigMap generator.
For example, to generate a ConfigMap from files `configure-pod-container/configmap/kubectl/game.properties`
with the key `game-special-key`
-->
<h4 id="定义从文件生成-configmap-时要使用的键">定义从文件生成 ConfigMap 时要使用的键</h4>
<p>在 ConfigMap 生成器，你可以定义一个非文件名的键名。
例如，从 <code>configure-pod-container/configmap/game.properties</code> 文件生成 ConfigMap，
但使用 <code>game-special-key</code> 作为键名：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建包含 ConfigMapGenerator 的 kustomization.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">configMapGenerator:
</span><span style="color:#b44">- name: game-config-5
</span><span style="color:#b44">  files:
</span><span style="color:#b44">  - game-special-key=configure-pod-container/configmap/kubectl/game.properties
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Apply the kustomization directory to create the ConfigMap object.
-->
<p>使用 Kustomization 目录创建 ConfigMap 对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k .
</code></pre></div><pre tabindex="0"><code>configmap/game-config-5-m67dt67794 created
</code></pre><!--
#### Generate ConfigMaps from Literals

To generate a ConfigMap from literals `special.type=charm` and `special.how=very`,
you can specify the ConfigMap generator in `kusotmization.yaml` as
-->
<h4 id="从字面值生成-configmap">从字面值生成 ConfigMap</h4>
<p>要基于字符串 <code>special.type=charm</code> 和 <code>special.how=very</code> 生成 ConfigMap，
可以在 <code>kusotmization.yaml</code> 中配置 ConfigMap 生成器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建带有 ConfigMapGenerator 的 kustomization.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">configMapGenerator:
</span><span style="color:#b44">- name: special-config-2
</span><span style="color:#b44">  literals:
</span><span style="color:#b44">  - special.how=very
</span><span style="color:#b44">  - special.type=charm
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Apply the kustomization directory to create the ConfigMap object.
-->
<p>应用 Kustomization 目录创建 ConfigMap 对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k .
</code></pre></div><pre tabindex="0"><code>configmap/special-config-2-c92b5mmcf2 created
</code></pre><!--
## Define container environment variables using ConfigMap data

### Define a container environment variable with data from a single ConfigMap
-->
<h2 id="使用-configmap-数据定义容器环境变量">使用 ConfigMap 数据定义容器环境变量</h2>
<h3 id="使用单个-configmap-中的数据定义容器环境变量">使用单个 ConfigMap 中的数据定义容器环境变量</h3>
<!--
1.  Define an environment variable as a key-value pair in a ConfigMap:
-->
<ol>
<li>
<p>在 ConfigMap 中将环境变量定义为键值对:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap special-config --from-literal<span style="color:#666">=</span>special.how<span style="color:#666">=</span>very
</code></pre></div></li>
</ol>
<!--
2.  Assign the `special.how` value defined in the ConfigMap to the `SPECIAL_LEVEL_KEY` environment variable in the Pod specification.
-->
<ol start="2">
<li>
<p>将 ConfigMap 中定义的 <code>special.how</code> 值分配给 Pod 规范中的 <code>SPECIAL_LEVEL_KEY</code> 环境变量。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-single-configmap-env-variable.yaml" download="pods/pod-single-configmap-env-variable.yaml"><code>pods/pod-single-configmap-env-variable.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-single-configmap-env-variable-yaml')" title="Copy pods/pod-single-configmap-env-variable.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-single-configmap-env-variable-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;env&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># Define the environment variable</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SPECIAL_LEVEL_KEY<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">configMapKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#080;font-style:italic"># The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#080;font-style:italic"># Specify the key associated with the value</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>special.how<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Create the Pod: -->
<p>创建 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/pods/pod-single-configmap-env-variable.yaml
</code></pre></div><!-- Now, the Pod's output includes environment variable `SPECIAL_LEVEL_KEY=very`. -->
<p>现在，Pod 的输出包含环境变量 <code>SPECIAL_LEVEL_KEY=very</code>。</p>
</li>
</ol>
<!--
### Define container environment variables with data from multiple ConfigMaps
-->
<h3 id="使用来自多个-configmap-的数据定义容器环境变量">使用来自多个 ConfigMap 的数据定义容器环境变量</h3>
<!-- * As with the previous example, create the ConfigMaps first. -->
<ul>
<li>
<p>与前面的示例一样，首先创建 ConfigMap。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/configmap/configmaps.yaml" download="configmap/configmaps.yaml"><code>configmap/configmaps.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('configmap-configmaps-yaml')" title="Copy configmap/configmaps.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="configmap-configmaps-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">special.how</span>:<span style="color:#bbb"> </span>very<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>env-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">log_level</span>:<span style="color:#bbb"> </span>INFO<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Create the ConfigMap: -->
<p>创建 ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/configmap/configmaps.yaml
</code></pre></div></li>
</ul>
<!--
* Define the environment variables in the Pod specification.
-->
<ul>
<li>
<p>在 Pod 规范中定义环境变量。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-multiple-configmap-env-variable.yaml" download="pods/pod-multiple-configmap-env-variable.yaml"><code>pods/pod-multiple-configmap-env-variable.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-multiple-configmap-env-variable-yaml')" title="Copy pods/pod-multiple-configmap-env-variable.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-multiple-configmap-env-variable-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;env&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SPECIAL_LEVEL_KEY<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">configMapKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>special.how<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>LOG_LEVEL<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">configMapKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>env-config<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>log_level<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Create the Pod: -->
<p>创建 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/pods/pod-multiple-configmap-env-variable.yaml
</code></pre></div><!-- Now, the Pod's output includes environment variables `SPECIAL_LEVEL_KEY=very` and `LOG_LEVEL=INFO`. -->
<p>现在，Pod 的输出包含环境变量 <code>SPECIAL_LEVEL_KEY=very</code> 和 <code>LOG_LEVEL=INFO</code>。</p>
</li>
</ul>
<!--
## Configure all key-value pairs in a ConfigMap as container environment variables
-->
<h2 id="将-configmap-中的所有键值对配置为容器环境变量">将 ConfigMap 中的所有键值对配置为容器环境变量</h2>
<!--
This functionality is available in Kubernetes v1.6 and later.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> Kubernetes v1.6 和更高版本支持此功能。</div>
</blockquote>
<!--
* Create a ConfigMap containing multiple key-value pairs.
-->
<ul>
<li>
<p>创建一个包含多个键值对的 ConfigMap。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/configmap/configmap-multikeys.yaml" download="configmap/configmap-multikeys.yaml"><code>configmap/configmap-multikeys.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('configmap-configmap-multikeys-yaml')" title="Copy configmap/configmap-multikeys.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="configmap-configmap-multikeys-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">SPECIAL_LEVEL</span>:<span style="color:#bbb"> </span>very<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">SPECIAL_TYPE</span>:<span style="color:#bbb"> </span>charm<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Create the ConfigMap: -->
<p>创建 ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml
</code></pre></div></li>
</ul>
<!--
* Use `envFrom` to define all of the ConfigMap's data as container environment variables. The key from the ConfigMap becomes the environment variable name in the Pod.
-->
<ul>
<li>
<p>使用 <code>envFrom</code> 将所有 ConfigMap 的数据定义为容器环境变量，ConfigMap 中的键成为 Pod 中的环境变量名称。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-configmap-envFrom.yaml" download="pods/pod-configmap-envFrom.yaml"><code>pods/pod-configmap-envFrom.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-configmap-envfrom-yaml')" title="Copy pods/pod-configmap-envFrom.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-configmap-envfrom-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;env&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">envFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">configMapRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Create the Pod: -->
<p>创建 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-envFrom.yaml
</code></pre></div><!-- Now, the Pod's output includes environment variables `SPECIAL_LEVEL=very` and `SPECIAL_TYPE=charm`.  -->
<p>现在，Pod 的输出包含环境变量 <code>SPECIAL_LEVEL=very</code> 和 <code>SPECIAL_TYPE=charm</code>。</p>
</li>
</ul>
<!--
## Use ConfigMap-defined environment variables in Pod commands
-->
<h2 id="在-pod-命令中使用-configmap-定义的环境变量">在 Pod 命令中使用 ConfigMap 定义的环境变量</h2>
<!--
You can use ConfigMap-defined environment variables in the `command` and `args` of a container using the `$(VAR_NAME)` Kubernetes substitution syntax.
-->
<p>你可以使用 <code>$(VAR_NAME)</code> Kubernetes 替换语法在容器的 <code>command</code> 和 <code>args</code> 部分中使用 ConfigMap 定义的环境变量。</p>
<!--
For example, the following Pod specification
-->
<p>例如，以下 Pod 规范</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-configmap-env-var-valueFrom.yaml" download="pods/pod-configmap-env-var-valueFrom.yaml"><code>pods/pod-configmap-env-var-valueFrom.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-configmap-env-var-valuefrom-yaml')" title="Copy pods/pod-configmap-env-var-valueFrom.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-configmap-env-var-valuefrom-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SPECIAL_LEVEL_KEY<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">configMapKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>SPECIAL_LEVEL<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SPECIAL_TYPE_KEY<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">configMapKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>SPECIAL_TYPE<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
created by running
-->
<p>通过运行下面命令创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-env-var-valueFrom.yaml
</code></pre></div><!--
produces the following output in the `test-container` container:
-->
<p>在 <code>test-container</code> 容器中产生以下输出:</p>
<pre tabindex="0"><code>very charm
</code></pre><!--
## Add ConfigMap data to a Volume

As explained in [Create ConfigMaps from files](#create-configmaps-from-files), when you create a ConfigMap using ``--from-file``, the filename becomes a key stored in the `data` section of the ConfigMap. The file contents become the key's value.
-->
<h2 id="将-configmap-数据添加到一个卷中">将 ConfigMap 数据添加到一个卷中</h2>
<p>如基于文件创建 <a href="#create-configmaps-from-files">ConfigMap</a> 中所述，当你使用
<code>--from-file</code> 创建 ConfigMap 时，文件名成为存储在 ConfigMap 的 <code>data</code> 部分中的键，
文件内容成为键对应的值。</p>
<!--
The examples in this section refer to a ConfigMap named special-config, shown below.
-->
<p>本节中的示例引用了一个名为 special-config 的 ConfigMap，如下所示：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/configmap/configmap-multikeys.yaml" download="configmap/configmap-multikeys.yaml"><code>configmap/configmap-multikeys.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('configmap-configmap-multikeys-yaml')" title="Copy configmap/configmap-multikeys.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="configmap-configmap-multikeys-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">SPECIAL_LEVEL</span>:<span style="color:#bbb"> </span>very<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">SPECIAL_TYPE</span>:<span style="color:#bbb"> </span>charm<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the ConfigMap:
-->
<p>创建 ConfigMap:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml
</code></pre></div><!--
### Populate a Volume with data stored in a ConfigMap

Add the ConfigMap name under the `volumes` section of the Pod specification.
This adds the ConfigMap data to the directory specified as `volumeMounts.mountPath` (in this case, `/etc/config`).
The `command` section references the `special.level` item stored in the ConfigMap.
-->
<h3 id="使用存储在-configmap-中的数据填充数据卷">使用存储在 ConfigMap 中的数据填充数据卷</h3>
<p>在 Pod 规约的 <code>volumes</code> 部分下添加 ConfigMap 名称。
这会将 ConfigMap 数据添加到指定为 <code>volumeMounts.mountPath</code> 的目录（在本例中为 <code>/etc/config</code>）。
<code>command</code> 部分引用存储在 ConfigMap 中的 <code>special.level</code>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-configmap-volume.yaml" download="pods/pod-configmap-volume.yaml"><code>pods/pod-configmap-volume.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-configmap-volume-yaml')" title="Copy pods/pod-configmap-volume.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-configmap-volume-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;ls /etc/config/&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># Provide the name of the ConfigMap containing the files you want</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># to add to the container</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Create the Pod: -->
<p>创建 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume.yaml
</code></pre></div><!--
When the pod runs, the command `ls /etc/config/` produces the output below:
-->
<p>Pod 运行时，命令 <code>ls /etc/config/</code> 产生下面的输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">SPECIAL_LEVEL
SPECIAL_TYPE
</code></pre></div><!--
If there are some files in the `/etc/config/` directory, they will be deleted.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 如果在 <code>/etc/config/</code> 目录中有一些文件，它们将被删除。</div>
</blockquote>

<!--
Text data is exposed as files using the UTF-8 character encoding. To use some other character encoding, use binaryData.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 文本数据会使用 UTF-8 字符编码的形式展现为文件。如果使用其他字符编码，
可以使用 <code>binaryData</code>。</div>
</blockquote>
<!--
### Add ConfigMap data to a specific path in the Volume

Use the `path` field to specify the desired file path for specific ConfigMap items.
In this case, the `SPECIAL_LEVEL` item will be mounted in the `config-volume` volume at `/etc/config/keys`.
-->
<h3 id="将-configmap-数据添加到数据卷中的特定路径">将 ConfigMap 数据添加到数据卷中的特定路径</h3>
<p>使用 <code>path</code> 字段为特定的 ConfigMap 项目指定预期的文件路径。
在这里，ConfigMap中，键值 <code>SPECIAL_LEVEL</code> 的内容将挂载在 <code>config-volume</code> 数据卷中 <code>/etc/config/keys</code> 文件下。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/pod-configmap-volume-specific-key.yaml" download="pods/pod-configmap-volume-specific-key.yaml"><code>pods/pod-configmap-volume-specific-key.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-pod-configmap-volume-specific-key-yaml')" title="Copy pods/pod-configmap-volume-specific-key.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-pod-configmap-volume-specific-key-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;/bin/sh&#34;</span>,<span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#b44">&#34;cat /etc/config/keys&#34;</span><span style="color:#bbb"> </span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>special-config<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>SPECIAL_LEVEL<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>keys<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume-specific-key.yaml
</code></pre></div><!--
When the pod runs, the command `cat /etc/config/keys` produces the output below:
-->
<p>当 pod 运行时，命令 <code>cat /etc/config/keys</code> 产生以下输出：</p>
<pre tabindex="0"><code>very
</code></pre><!--
Like before, all previous files in the `/etc/config/` directory will be deleted.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 如前，<code>/etc/config/</code> 目录中所有先前的文件都将被删除。</div>
</blockquote>

<!--
### Project keys to specific paths and file permissions

You can project keys to specific paths and specific permissions on a per-file
basis. The [Secrets](/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod) user guide explains the syntax.
-->
<h3 id="映射键以指定路径和文件权限">映射键以指定路径和文件权限</h3>
<p>你可以通过指定键名到特定目录的投射关系，也可以逐个文件地设定访问权限。
<a href="/zh/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod">Secret 用户指南</a>
中对这一语法提供了解释。</p>
<!--
### Mounted ConfigMaps are updated automatically
-->
<h3 id="挂载的-configmap-将自动更新">挂载的 ConfigMap 将自动更新</h3>
<!--
When a ConfigMap already being consumed in a volume is updated, projected keys
are eventually updated as well. Kubelet is checking whether the mounted
ConfigMap is fresh on every periodic sync. However, it is using its local
ttl-based cache for getting the current value of the ConfigMap. As a result,
the total delay from the moment when the ConfigMap is updated to the moment
when new keys are projected to the pod can be as long as kubelet sync period
(1 minute by default) + ttl of ConfigMaps cache (1 minute by default) in
kubelet. You can trigger an immediate refresh by updating one of the pod's
annotations.
-->
<p>更新已经在数据卷中使用的 ConfigMap 时，已映射的键最终也会被更新。
<code>kubelet</code> 在每次定期同步时都会检查已挂载的 ConfigMap 是否是最新的。
但是，它使用其本地的基于 TTL 的缓存来获取 ConfigMap 的当前值。
因此，从更新 ConfigMap 到将新键映射到 Pod 的总延迟可能与
kubelet 同步周期 + ConfigMap 在 kubelet 中缓存的 TTL 一样长。</p>
<!--
A container using a ConfigMap as a [subPath](/docs/concepts/storage/volumes/#using-subpath) volume will not receive ConfigMap updates.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 使用 ConfigMap 作为 <a href="/zh/docs/concepts/storage/volumes/#using-subpath">subPath</a>
的数据卷将不会收到 ConfigMap 更新。</div>
</blockquote>
<!-- discussion -->
<!--
## Understanding ConfigMaps and Pods

The ConfigMap API resource stores configuration data as key-value pairs. The data can be consumed in pods or provide the configurations for system components such as controllers. ConfigMap is similar to [Secrets](/docs/concepts/configuration/secret/), but provides a means of working with strings that don't contain sensitive information. Users and system components alike can store configuration data in ConfigMap.
-->
<h2 id="了解-configmap-和-pod">了解 ConfigMap 和 Pod</h2>
<p>ConfigMap API 资源将配置数据存储为键值对。
数据可以在 Pod 中使用，也可以提供系统组件（如控制器）的配置。
ConfigMap 与 <a href="/zh/docs/concepts/configuration/secret/">Secret</a> 类似，
但是提供了一种使用不包含敏感信息的字符串的方法。
用户和系统组件都可以在 ConfigMap 中存储配置数据。</p>
<!--
ConfigMaps should reference properties files, not replace them. Think of the ConfigMap as representing something similar to the Linux `/etc` directory and its contents. For example, if you create a [Kubernetes Volume](/docs/concepts/storage/volumes/) from a ConfigMap, each data item in the ConfigMap is represented by an individual file in the volume.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> ConfigMap 应该引用属性文件，而不是替换它们。可以将 ConfigMap 理解为类似于 Linux
<code>/etc</code> 目录及其内容的东西。例如，如果你从 ConfigMap 创建
<a href="/zh/docs/concepts/storage/volumes/">Kubernetes 卷</a>，则 ConfigMap
中的每个数据项都由该数据卷中的单个文件表示。</div>
</blockquote>
<!--
The ConfigMap's `data` field contains the configuration data. As shown in the example below, this can be simple -- like individual properties defined using `--from-literal` -- or complex -- like configuration files or JSON blobs defined using `--from-file`.
-->
<p>ConfigMap 的 <code>data</code> 字段包含配置数据。如下例所示，它可以简单
（如用 <code>--from-literal</code> 的单个属性定义）或复杂
（如用 <code>--from-file</code> 的配置文件或 JSON blob定义）。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2016-02-18T19:14:38Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-config<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 使用 --from-literal 定义的简单属性</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">example.property.1</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">example.property.2</span>:<span style="color:#bbb"> </span>world<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 使用 --from-file 定义复杂属性的例子</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">example.property.file</span>:<span style="color:#bbb"> </span>|-<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    property.1=value-1
</span><span style="color:#b44;font-style:italic">    property.2=value-2
</span><span style="color:#b44;font-style:italic">    property.3=value-3</span><span style="color:#bbb">    
</span></code></pre></div><!--
### Restrictions

- You must create a ConfigMap before referencing it in a Pod specification (unless you mark the ConfigMap as "optional"). If you reference a ConfigMap that doesn't exist, the Pod won't start. Likewise, references to keys that don't exist in the ConfigMap will prevent the pod from starting.
-->
<h3 id="限制">限制</h3>
<ul>
<li>在 Pod 规范中引用之前，必须先创建一个 ConfigMap（除非将 ConfigMap 标记为&quot;可选&quot;）。
如果引用的 ConfigMap 不存在，则 Pod 将不会启动。同样，引用 ConfigMap 中不存在的键也会阻止 Pod 启动。</li>
</ul>
<!--
- If you use `envFrom` to define environment variables from ConfigMaps, keys that are considered invalid will be skipped. The pod will be allowed to start, but the invalid names will be recorded in the event log (`InvalidVariableNames`). The log message lists each skipped key. For example:
-->
<ul>
<li>
<p>如果你使用 <code>envFrom</code> 基于 ConfigMap 定义环境变量，那么无效的键将被忽略。
可以启动 Pod，但无效名称将记录在事件日志中（<code>InvalidVariableNames</code>）。
日志消息列出了每个跳过的键。例如:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events
</code></pre></div><!-- The output is similar to this: -->
<p>输出与此类似:</p>
<pre tabindex="0"><code>LASTSEEN FIRSTSEEN COUNT NAME          KIND  SUBOBJECT  TYPE      REASON                            SOURCE                MESSAGE
0s       0s        1     dapi-test-pod Pod              Warning   InvalidEnvironmentVariableNames   {kubelet, 127.0.0.1}  Keys [1badkey, 2alsobad] from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names.
</code></pre></li>
</ul>
<!--
- ConfigMaps reside in a specific <a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间（Namespace）'>名字空间（Namespace）</a>. A ConfigMap can only be referenced by pods residing in the same namespace.
-->
<ul>
<li>ConfigMap 位于特定的<a class='glossary-tooltip' title='名字空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/namespaces/' target='_blank' aria-label='名字空间'>名字空间</a>
中。每个 ConfigMap 只能被同一名字空间中的 Pod 引用.</li>
</ul>
<!--
- You can't use ConfigMaps for <a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='static pods'>static pods</a>, because the Kubelet does not support this.
-->
<ul>
<li>你不能将 ConfigMap 用于 <a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='静态 Pod'>静态 Pod</a>，
因为 Kubernetes 不支持这种用法。</li>
</ul>
<h2 id="接下来">接下来</h2>
<!--
* Follow a real world example of [Configuring Redis using a ConfigMap](/docs/tutorials/configuration/configure-redis-using-configmap/).
-->
<ul>
<li>浏览<a href="/zh/docs/tutorials/configuration/configure-redis-using-configmap/">使用 ConfigMap 配置 Redis</a>
真实实例</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3d7b9cb24a647c36ba63f7a02ec49010">3.19 - 在 Pod 中的容器之间共享进程命名空间</h1>
    
	<!--
---
title: Share Process Namespace between Containers in a Pod
min-kubernetes-server-version: v1.10
reviewers:
- verb
- yujuhong
- dchen1107
content_type: task
weight: 160
---
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code>
</div>

<!--
This page shows how to configure process namespace sharing for a pod. When
process namespace sharing is enabled, processes in a container are visible
to all other containers in that pod.
-->
<p>此页面展示如何为 pod 配置进程命名空间共享。
当启用进程命名空间共享时，容器中的进程对该 pod 中的所有其他容器都是可见的。</p>
<!--
You can use this feature to configure cooperating containers, such as a log
handler sidecar container, or to troubleshoot container images that don't
include debugging utilities like a shell.
-->
<p>您可以使用此功能来配置协作容器，比如日志处理 sidecar 容器，或者对那些不包含诸如 shell 等调试实用工具的镜像进行故障排查。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.10.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Configure a Pod
-->
<h2 id="配置-pod">配置 Pod</h2>
<!--
Process Namespace Sharing is enabled using the `ShareProcessNamespace` field of
`v1.PodSpec`. For example:
-->
<p>进程命名空间共享使用 <code>v1.PodSpec</code> 中的 <code>ShareProcessNamespace</code> 字段启用。例如：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/share-process-namespace.yaml" download="pods/share-process-namespace.yaml"><code>pods/share-process-namespace.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-share-process-namespace-yaml')" title="Copy pods/share-process-namespace.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-share-process-namespace-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">shareProcessNamespace</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>shell<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">capabilities</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">add</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- SYS_PTRACE<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">stdin</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tty</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Create the pod `nginx` on your cluster:

    ```shell
    kubectl apply -f https://k8s.io/examples/pods/share-process-namespace.yaml
    ```

1. Attach to the `shell` container and run `ps`:

    ```shell
    kubectl attach -it nginx -c shell
    ```

    If you don't see a command prompt, try pressing enter.

    ```
    / # ps ax
    PID   USER     TIME  COMMAND
        1 root      0:00 /pause
        8 root      0:00 nginx: master process nginx -g daemon off;
       14 101       0:00 nginx: worker process
       15 root      0:00 sh
       21 root      0:00 ps ax
    ```
-->
<ol>
<li>
<p>在集群中创建 <code>nginx</code> pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/share-process-namespace.yaml
</code></pre></div></li>
<li>
<p>获取容器 <code>shell</code>，执行 <code>ps</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl attach -it nginx -c shell
</code></pre></div><p>如果没有看到命令提示符，请按 enter 回车键。</p>
<pre tabindex="0"><code>/ # ps ax
PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   14 101       0:00 nginx: worker process
   15 root      0:00 sh
   21 root      0:00 ps ax
</code></pre></li>
</ol>
<!--
You can signal processes in other containers. For example, send `SIGHUP` to
nginx to restart the worker process. This requires the `SYS_PTRACE` capability.
-->
<p>您可以在其他容器中对进程发出信号。例如，发送 <code>SIGHUP</code> 到 nginx 以重启工作进程。这需要 <code>SYS_PTRACE</code> 功能。</p>
<pre tabindex="0"><code>/ # kill -HUP 8
/ # ps ax
PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   15 root      0:00 sh
   22 101       0:00 nginx: worker process
   23 root      0:00 ps ax
</code></pre><!--
It's even possible to access another container image using the
`/proc/$pid/root` link.
-->
<p>甚至可以使用 <code>/proc/$pid/root</code> 链接访问另一个容器镜像。</p>
<pre tabindex="0"><code>/ # head /proc/8/root/etc/nginx/nginx.conf

user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
</code></pre><!-- discussion -->
<!--
## Understanding Process Namespace Sharing
-->
<h2 id="理解进程命名空间共享">理解进程命名空间共享</h2>
<!--
Pods share many resources so it makes sense they would also share a process
namespace. Some container images may expect to be isolated from other
containers, though, so it's important to understand these differences:
-->
<p>Pod 共享许多资源，因此它们共享进程命名空间是很有意义的。
不过，有些容器镜像可能希望与其他容器隔离，因此了解这些差异很重要:</p>
<!--
1. **The container process no longer has PID 1.** Some container images refuse
   to start without PID 1 (for example, containers using `systemd`) or run
   commands like `kill -HUP 1` to signal the container process. In pods with a
   shared process namespace, `kill -HUP 1` will signal the pod sandbox.
   (`/pause` in the above example.)

1. **Processes are visible to other containers in the pod.** This includes all
   information visible in `/proc`, such as passwords that were passed as arguments
   or environment variables. These are protected only by regular Unix permissions.

1. **Container filesystems are visible to other containers in the pod through the
   `/proc/$pid/root` link.** This makes debugging easier, but it also means
   that filesystem secrets are protected only by filesystem permissions.
-->
<ol>
<li>
<p><strong>容器进程不再具有 PID 1。</strong> 在没有 PID 1 的情况下，一些容器镜像拒绝启动（例如，使用 <code>systemd</code> 的容器)，或者拒绝执行 <code>kill -HUP 1</code> 之类的命令来通知容器进程。在具有共享进程命名空间的 pod 中，<code>kill -HUP 1</code> 将通知 pod 沙箱（在上面的例子中是 <code>/pause</code>）。</p>
</li>
<li>
<p><strong>进程对 pod 中的其他容器可见。</strong> 这包括 <code>/proc</code> 中可见的所有信息，例如作为参数或环境变量传递的密码。这些仅受常规 Unix 权限的保护。</p>
</li>
<li>
<p><strong>容器文件系统通过 <code>/proc/$pid/root</code> 链接对 pod 中的其他容器可见。</strong> 这使调试更加容易，但也意味着文件系统安全性只受文件系统权限的保护。</p>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-42a59b878d4c58e5c6f4bb87483dda93">3.20 - 创建静态 Pod</h1>
    
	<!-- overview -->
<!--
*Static Pods* are managed directly by the kubelet daemon on a specific node,
without the <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API server'>API server</a>
observing them.
Unlike Pods that are managed by the control plane (for example, a
<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>);
instead, the kubelet watches each static Pod (and restarts it if it crashes).
-->
<p><em>静态 Pod</em> 在指定的节点上由 kubelet 守护进程直接管理，不需要
<a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='API 服务器'>API 服务器</a> 监管。
与由控制面管理的 Pod（例如，<a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a>）
不同；kubelet 监视每个静态 Pod（在它崩溃之后重新启动）。</p>
<!--
Static Pods are always bound to one <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a> on a specific node.

The kubelet automatically tries to create a <a class='glossary-tooltip' title='API 服务器中的一个对象，用于跟踪 kubelet 上的静态 pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-mirror-pod' target='_blank' aria-label='mirror Pod'>mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there.
The Pod names will suffixed with the node hostname with a leading hyphen

<blockquote class="note callout">
  <div><strong>说明：</strong> If you are running clustered Kubernetes and are using static
Pods to run a Pod on every node, you should probably be using a
<a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>
instead.</div>
</blockquote>
-->
<p>静态 Pod 永远都会绑定到一个指定节点上的 <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='Kubelet'>Kubelet</a>。</p>
<p>kubelet 会尝试通过 Kubernetes API 服务器为每个静态 Pod 自动创建一个
<a class='glossary-tooltip' title='API 服务器中的一个对象，用于跟踪 kubelet 上的静态 pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-mirror-pod' target='_blank' aria-label='镜像 Pod'>镜像 Pod</a>。
这意味着节点上运行的静态 Pod 对 API 服务来说是可见的，但是不能通过 API 服务器来控制。
Pod 名称将把以连字符开头的节点主机名作为后缀。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果你在运行一个 Kubernetes 集群，并且在每个节点上都运行一个静态 Pod，
就可能需要考虑使用 <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a> 替代这种方式。</div>
</blockquote>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
This page assumes you're using <a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a> to run Pods,
and that your nodes are running the Fedora operating system.
Instructions for other distributions or Kubernetes installations may vary.
-->
<p>本文假定你在使用 <a class='glossary-tooltip' title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/kubectl/docker-cli-to-kubectl/' target='_blank' aria-label='Docker'>Docker</a> 来运行 Pod，
并且你的节点是运行着 Fedora 操作系统。
其它发行版或者 Kubernetes 部署版本上操作方式可能不一样。</p>
<!-- steps -->
<!--
## Create a static pod {#static-pod-creation}

You can configure a static Pod with either a [file system hosted configuration file](/docs/tasks/configure-pod-container/static-pod/#configuration-files) or a [web hosted configuration file](/docs/tasks/configure-pod-container/static-pod/#pods-created-via-http).
-->
<h2 id="static-pod-creation">创建静态 Pod</h2>
<p>可以通过<a href="/zh/docs/tasks/configure-pod-container/static-pod/#configuration-files">文件系统上的配置文件</a>
或者 <a href="/zh/docs/tasks/configure-pod-container/static-pod/#pods-created-via-http">web 网络上的配置文件</a>
来配置静态 Pod。</p>
<!--
### Filesystem-hosted static Pod manifest {#configuration-files}

Manifests are standard Pod definitions in JSON or YAML format in a specific directory. Use the `staticPodPath: <the directory>` field in the
[kubelet configuration file](/docs/reference/config-api/kubelet-config.v1beta1/),
which periodically scans the directory and creates/deletes static Pods as YAML/JSON files appear/disappear there.
Note that the kubelet will ignore files starting with dots when scanning the specified directory.

For example, this is how to start a simple web server as a static Pod:
-->
<h3 id="configuration-files">文件系统上的静态 Pod 声明文件</h3>
<p>声明文件是标准的 Pod 定义文件，以 JSON 或者 YAML 格式存储在指定目录。路径设置在
<a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/">Kubelet 配置文件</a>
的 <code>staticPodPath: &lt;目录&gt;</code> 字段，kubelet 会定期的扫描这个文件夹下的 YAML/JSON
文件来创建/删除静态 Pod。
注意 kubelet 扫描目录的时候会忽略以点开头的文件。</p>
<p>例如：下面是如何以静态 Pod 的方式启动一个简单 web 服务：</p>
<!--
1. Choose a node where you want to run the static Pod. In this example, it's `my-node1`.
-->
<ol>
<li>
<p>选择一个要运行静态 Pod 的节点。在这个例子中选择 <code>my-node1</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ssh my-node1
</code></pre></div></li>
</ol>
<!--
2. Choose a directory, say `/etc/kubelet.d` and place a web server Pod definition there, e.g. `/etc/kubelet.d/static-web.yaml`:

   ```shell
    # Run this command on the node where kubelet is running
    mkdir /etc/kubelet.d/
    cat <<EOF >/etc/kubelet.d/static-web.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: static-web
      labels:
        role: myrole
    spec:
      containers:
        - name: web
          image: nginx
          ports:
            - name: web
              containerPort: 80
              protocol: TCP
    EOF
-->
<ol start="2">
<li>
<p>选择一个目录，比如在 <code>/etc/kubelet.d</code> 目录来保存 web 服务 Pod 的定义文件，
<code>/etc/kubelet.d/static-web.yaml</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在 kubelet 运行的节点上执行以下命令</span>
mkdir /etc/kubelet.d/
cat <span style="color:#b44">&lt;&lt;EOF &gt;/etc/kubelet.d/static-web.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Pod
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: static-web
</span><span style="color:#b44">  labels:
</span><span style="color:#b44">    role: myrole
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  containers:
</span><span style="color:#b44">    - name: web
</span><span style="color:#b44">      image: nginx
</span><span style="color:#b44">      ports:
</span><span style="color:#b44">        - name: web
</span><span style="color:#b44">          containerPort: 80
</span><span style="color:#b44">          protocol: TCP
</span><span style="color:#b44">EOF</span>
</code></pre></div></li>
</ol>
<!--
3. Configure your kubelet on the node to use this directory by running it with `--pod-manifest-path=/etc/kubelet.d/` argument. On Fedora edit `/etc/kubernetes/kubelet` to include this line:

   ```
   KUBELET_ARGS="--cluster-dns=10.254.0.10 --cluster-domain=kube.local --pod-manifest-path=/etc/kubelet.d/"
   ```
   or add the `staticPodPath: <the directory>` field in the
   [kubelet configuration file](/docs/reference/config-api/kubelet-config.v1beta1/).
-->
<ol start="3">
<li>
<p>配置这个节点上的 kubelet，使用这个参数执行 <code>--pod-manifest-path=/etc/kubelet.d/</code>。
在 Fedora 上编辑 <code>/etc/kubernetes/kubelet</code> 以包含下行：</p>
<pre tabindex="0"><code>KUBELET_ARGS=&quot;--cluster-dns=10.254.0.10 --cluster-domain=kube.local --pod-manifest-path=/etc/kubelet.d/&quot;
</code></pre><p>或者在 <a href="/zh/docs/reference/config-api/kubelet-config.v1beta1/">Kubelet 配置文件</a>
中添加 <code>staticPodPath: &lt;目录&gt;</code>字段。</p>
</li>
</ol>
<!--
4. Restart the kubelet. On Fedora, you would run:

   ```shell
   # Run this command on the node where the kubelet is running
   systemctl restart kubelet
   ```
-->
<ol start="4">
<li>
<p>重启 kubelet。Fedora 上使用下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在 kubelet 运行的节点上执行以下命令</span>
systemctl restart kubelet
</code></pre></div></li>
</ol>
<!--
### Web-hosted static pod manifest {#pods-created-via-http}

Kubelet periodically downloads a file specified by `--manifest-url=<URL>` argument
and interprets it as a JSON/YAML file that contains Pod definitions.
Similar to how [filesystem-hosted manifests](#configuration-files) work, the kubelet
refetches the manifest on a schedule. If there are changes to the list of static
Pods, the kubelet applies them.

To use this approach:
-->
<h3 id="pods-created-via-http">Web 网上的静态 Pod 声明文件</h3>
<p>Kubelet 根据 <code>--manifest-url=&lt;URL&gt;</code> 参数的配置定期的下载指定文件，并且转换成
JSON/YAML 格式的 Pod 定义文件。
与<a href="#configuration-files">文件系统上的清单文件</a>使用方式类似，kubelet 调度获取清单文件。
如果静态 Pod 的清单文件有改变，kubelet 会应用这些改变。</p>
<p>按照下面的方式来：</p>
<!--
1. Create a YAML file and store it on a web server so that you can pass the URL of that file to the kubelet.
-->
<ol>
<li>
<p>创建一个 YAML 文件，并保存在 web 服务上，为 kubelet 生成一个 URL。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>static-web<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">role</span>:<span style="color:#bbb"> </span>myrole<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2. Configure the kubelet on your selected node to use this web manifest by running it with `--manifest-url=<manifest-url>`. On Fedora, edit `/etc/kubernetes/kubelet` to include this line:
-->
<ol start="2">
<li>
<p>通过在选择的节点上使用 <code>--manifest-url=&lt;manifest-url&gt;</code> 配置运行 kubelet。
在 Fedora 添加下面这行到 <code>/etc/kubernetes/kubelet</code> ：</p>
<pre tabindex="0"><code>KUBELET_ARGS=&quot;--cluster-dns=10.254.0.10 --cluster-domain=kube.local --manifest-url=&lt;manifest-url&gt;&quot;
</code></pre></li>
</ol>
<!--
3. Restart the kubelet. On Fedora, you would run:

    ```shell
    # Run this command on the node where the kubelet is running
    systemctl restart kubelet
    ```
-->
<ol start="3">
<li>
<p>重启 kubelet。在 Fedora 上运行如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在 kubelet 运行的节点上执行以下命令</span>
systemctl restart kubelet
</code></pre></div></li>
</ol>
<!--
## Observe static pod behavior {#behavior-of-static-pods}

When the kubelet starts, it automatically starts all defined static Pods. As you have
defined a static Pod and restarted the kubelet, the new static Pod should
already be running.

You can view running containers (including static Pods) by running (on the node):
```shell
# Run this command on the node where kubelet is running
docker ps
```

The output might be something like:
-->
<h2 id="behavior-of-static-pods">观察静态 pod 的行为</h2>
<p>当 kubelet 启动时，会自动启动所有定义的静态 Pod。
当定义了一个静态 Pod 并重新启动 kubelet 时，新的静态 Pod 就应该已经在运行了。</p>
<p>可以在节点上运行下面的命令来查看正在运行的容器（包括静态 Pod）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在 kubelet 运行的节点上执行以下命令</span>
docker ps
</code></pre></div><!--
The output might be something like:
-->
<p>输出可能会像这样：</p>
<pre tabindex="0"><code>CONTAINER ID IMAGE         COMMAND  CREATED        STATUS         PORTS     NAMES
f6d05272b57e nginx:latest  &quot;nginx&quot;  8 minutes ago  Up 8 minutes             k8s_web.6f802af4_static-web-fk-node1_default_67e24ed9466ba55986d120c867395f3c_378e5f3c
</code></pre><!--
You can see the mirror Pod on the API server:
-->
<p>可以在 API 服务上看到镜像 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><pre tabindex="0"><code>NAME                       READY     STATUS    RESTARTS   AGE
static-web-my-node1        1/1       Running   0          2m
</code></pre><!--
Make sure the kubelet has permission to create the mirror Pod in the API server. If not, the creation request is rejected by the API server. See
[PodSecurityPolicy](/docs/concepts/policy/pod-security-policy/).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 要确保 kubelet 在 API 服务上有创建镜像 Pod 的权限。如果没有，创建请求会被 API 服务拒绝。
可以看<a href="/zh/docs/concepts/policy/pod-security-policy/">Pod安全策略</a>。</div>
</blockquote>
<!--
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='Labels'>Labels</a> from the static Pod are
propagated into the mirror Pod. You can use those labels as normal via
<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='selectors'>selectors</a>, etc.
-->
<p>静态 Pod 上的<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a> 被传到镜像 Pod。
你可以通过 <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='选择算符'>选择算符</a> 使用这些标签。</p>
<!--
If you try to use `kubectl` to delete the mirror Pod from the API server,
the kubelet _doesn't_ remove the static Pod:
-->
<p>如果你用 <code>kubectl</code> 从 API 服务上删除镜像 Pod，kubelet <em>不会</em> 移除静态 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod static-web-my-node1
</code></pre></div><pre tabindex="0"><code>pod &quot;static-web-my-node1&quot; deleted
</code></pre><!--
You can see that the Pod is still running:
-->
<p>可以看到 Pod 还在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><pre tabindex="0"><code>NAME                       READY     STATUS    RESTARTS   AGE
static-web-my-node1        1/1       Running   0          12s
</code></pre><!--
Back on your node where the kubelet is running, you can try to stop the Docker
container manually.
You'll see that, after a time, the kubelet will notice and will restart the Pod
automatically:

```shell
# Run these commands on the node where the kubelet is running
docker stop f6d05272b57e # replace with the ID of your container
sleep 20
docker ps
```
-->
<p>回到 kubelet 运行的节点上，可以手工停止 Docker 容器。
可以看到过了一段时间后 kubelet 会发现容器停止了并且会自动重启 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在 kubelet 运行的节点上执行以下命令</span>
<span style="color:#080;font-style:italic"># 把 ID 换为你的容器的 ID</span>
docker stop f6d05272b57e
sleep <span style="color:#666">20</span>
docker ps
</code></pre></div><pre tabindex="0"><code>CONTAINER ID        IMAGE         COMMAND                CREATED       ...
5b920cbaf8b1        nginx:latest  &quot;nginx -g 'daemon of   2 seconds ago ...
</code></pre><!--
## Dynamic addition and removal of static pods

The running kubelet periodically scans the configured directory (`/etc/kubelet.d` in our example) for changes and adds/removes Pods as files appear/disappear in this directory.

```shell
# This assumes you are using filesystem-hosted static Pod configuration
# Run these commands on the node where the kubelet is running
#
mv /etc/kubelet.d/static-web.yaml /tmp
sleep 20
docker ps
# You see that no nginx container is running
mv /tmp/static-web.yaml  /etc/kubelet.d/
sleep 20
docker ps
```
-->
<h2 id="动态增加和删除静态-pod">动态增加和删除静态 pod</h2>
<p>运行中的 kubelet 会定期扫描配置的目录(比如例子中的 <code>/etc/kubelet.d</code> 目录)中的变化，
并且根据文件中出现/消失的 Pod 来添加/删除 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 前提是你在用主机文件系统上的静态 Pod 配置文件</span>
<span style="color:#080;font-style:italic"># 在 kubelet 运行的节点上执行以下命令</span>
mv /etc/kubelet.d/static-web.yaml /tmp
sleep <span style="color:#666">20</span>
docker ps
<span style="color:#080;font-style:italic"># 可以看到没有 nginx 容器在运行</span>
mv /tmp/static-web.yaml  /etc/kubelet.d/
sleep <span style="color:#666">20</span>
docker ps
</code></pre></div><pre tabindex="0"><code>CONTAINER ID        IMAGE         COMMAND                CREATED           ...
e7a62e3427f1        nginx:latest  &quot;nginx -g 'daemon of   27 seconds ago
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1bb997c61a85de753d9994e7a312a291">3.21 - 将 Docker Compose 文件转换为 Kubernetes 资源</h1>
    
	<!--
title: Translate a Docker Compose File to Kubernetes Resources
content_type: task
weight: 170
-->
<!-- overview -->
<!--
What's Kompose? It's a conversion tool for all things compose (namely Docker Compose) to container orchestrators (Kubernetes or OpenShift).
-->
<p>Kompose 是什么？它是个转换工具，可将 compose（即 Docker Compose）所组装的所有内容
转换成容器编排器（Kubernetes 或 OpenShift）可识别的形式。</p>
<!--
More information can be found on the Kompose website at [http://kompose.io](http://kompose.io).
-->
<p>更多信息请参考 Kompose 官网 <a href="http://kompose.io">http://kompose.io</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Install Kompose

We have multiple ways to install Kompose. Our preferred method is downloading the binary from the latest GitHub release.
-->
<h2 id="安装-kompose">安装 Kompose</h2>
<p>我们有很多种方式安装 Kompose。首选方式是从最新的 GitHub 发布页面下载二进制文件。</p>
<ul class="nav nav-tabs" id="install-ways" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#install-ways-0" role="tab" aria-controls="install-ways-0" aria-selected="true">GitHub 下载</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#install-ways-1" role="tab" aria-controls="install-ways-1">基于源代码构建</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#install-ways-2" role="tab" aria-controls="install-ways-2">CentOS 包</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#install-ways-3" role="tab" aria-controls="install-ways-3">Fedora package</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#install-ways-4" role="tab" aria-controls="install-ways-4">Homebrew (macOS)</a></li></ul>
<div class="tab-content" id="install-ways"><div id="install-ways-0" class="tab-pane show active" role="tabpanel" aria-labelledby="install-ways-0">

<p><!--
Kompose is released via GitHub on a three-week cycle, you can see all current releases on the [GitHub release page](https://github.com/kubernetes/kompose/releases).
-->
<p>Kompose 通过 GitHub 发布，发布周期为三星期。
你可以在 <a href="https://github.com/kubernetes/kompose/releases">GitHub 发布页面</a>
上看到所有当前版本。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># Linux</span>
curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-linux-amd64 -o kompose

<span style="color:#080;font-style:italic"># macOS</span>
curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-darwin-amd64 -o kompose

<span style="color:#080;font-style:italic"># Windows</span>
curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-windows-amd64.exe -o kompose.exe

chmod +x kompose
sudo mv ./kompose /usr/local/bin/kompose
</code></pre></div><!--
Alternatively, you can download the [tarball](https://github.com/kubernetes/kompose/releases).
-->
<p>或者，你可以下载 <a href="https://github.com/kubernetes/kompose/releases">tar 包</a>。</p>
</div>
  <div id="install-ways-1" class="tab-pane" role="tabpanel" aria-labelledby="install-ways-1">

<p><!--
Installing using `go get` pulls from the master branch with the latest development changes.
-->
<p>用 <code>go get</code> 命令从主分支拉取最新的开发变更的方法安装 Kompose。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">go get -u github.com/kubernetes/kompose
</code></pre></div></div>
  <div id="install-ways-2" class="tab-pane" role="tabpanel" aria-labelledby="install-ways-2">

<p><!--
Kompose is in [EPEL](https://fedoraproject.org/wiki/EPEL) CentOS repository.
If you don't have [EPEL](https://fedoraproject.org/wiki/EPEL) repository already installed and enabled you can do it by running  `sudo yum install epel-release`
-->
<p>Kompose 位于 <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a> CentOS 代码仓库。
如果你还没有安装启用 <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a> 代码仓库，
请运行命令 <code>sudo yum install epel-release</code>。</p>
<!--
If you have [EPEL](https://fedoraproject.org/wiki/EPEL) enabled in your system, you can install Kompose like any other package.
-->
<p>如果你的系统中已经启用了 <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a>，
你就可以像安装其他软件包一样安装 Kompose。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo yum -y install kompose
</code></pre></div></div>
  <div id="install-ways-3" class="tab-pane" role="tabpanel" aria-labelledby="install-ways-3">

<p><!--
Kompose is in Fedora 24, 25 and 26 repositories. You can install it like any other package.
-->
<p>Kompose 位于 Fedora 24、25 和 26 的代码仓库。你可以像安装其他软件包一样安装 Kompose。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo dnf -y install kompose
</code></pre></div></div>
  <div id="install-ways-4" class="tab-pane" role="tabpanel" aria-labelledby="install-ways-4">

<p><!--
On macOS you can install latest release via [Homebrew](https://brew.sh):
-->
<p>在 macOS 上你可以通过 <a href="https://brew.sh">Homebrew</a> 安装 Kompose 的最新版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">brew install kompose
</code></pre></div></div></div>

<!--
## Use Kompose
-->
<h2 id="使用-kompose">使用 Kompose</h2>
<!--
In a few steps, we'll take you from Docker Compose to Kubernetes. All
you need is an existing `docker-compose.yml` file.
-->
<p>再需几步，我们就把你从 Docker Compose 带到 Kubernetes。
你只需要一个现有的 <code>docker-compose.yml</code> 文件。</p>
<ol>
<li>
<!--Go to the directory containing your `docker-compose.yml` file. If you don't
have one, test using this one.-->
<p>进入 <code>docker-compose.yml</code> 文件所在的目录。如果没有，请使用下面这个进行测试。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">services</span>:<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">redis-master</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/redis:e2e<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;6379&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">redis-slave</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google_samples/gb-redisslave:v3<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;6379&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">environment</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- GET_HOSTS_FROM=dns<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">frontend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/gb-frontend:v4<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;80:80&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">environment</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- GET_HOSTS_FROM=dns<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kompose.service.type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
2. To convert the `docker-compose.yml` file to files that you can use with
   `kubectl`, run `kompose convert` and then `kubectl create -f <output file>`.
-->
<ol start="2">
<li>
<p>要将 <code>docker-compose.yml</code> 转换为 <code>kubectl</code> 可用的文件，请运行 <code>kompose convert</code>
命令进行转换，然后运行 <code>kubectl create -f &lt;output file&gt;</code> 进行创建。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose convert                           
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">INFO Kubernetes file &quot;frontend-service.yaml&quot; created
   INFO Kubernetes file &quot;frontend-service.yaml&quot; created
INFO Kubernetes file &quot;frontend-service.yaml&quot; created
INFO Kubernetes file &quot;redis-master-service.yaml&quot; created
   INFO Kubernetes file &quot;redis-master-service.yaml&quot; created
INFO Kubernetes file &quot;redis-master-service.yaml&quot; created
INFO Kubernetes file &quot;redis-slave-service.yaml&quot; created
   INFO Kubernetes file &quot;redis-slave-service.yaml&quot; created
INFO Kubernetes file &quot;redis-slave-service.yaml&quot; created
INFO Kubernetes file &quot;frontend-deployment.yaml&quot; created
   INFO Kubernetes file &quot;frontend-deployment.yaml&quot; created
INFO Kubernetes file &quot;frontend-deployment.yaml&quot; created
INFO Kubernetes file &quot;redis-master-deployment.yaml&quot; created
   INFO Kubernetes file &quot;redis-master-deployment.yaml&quot; created
INFO Kubernetes file &quot;redis-master-deployment.yaml&quot; created
INFO Kubernetes file &quot;redis-slave-deployment.yaml&quot; created
   INFO Kubernetes file &quot;redis-slave-deployment.yaml&quot; created
INFO Kubernetes file &quot;redis-slave-deployment.yaml&quot; created
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f frontend-service.yaml,redis-master-service.yaml,redis-slave-service.yaml,frontend-deployment.yaml,
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">service/frontend created
service/redis-master created
service/redis-slave created
deployment.apps/frontend created
deployment.apps/redis-master created
deployment.apps/redis-slave created
</code></pre><!--
Your deployments are running in Kubernetes.
-->
<p>你部署的应用在 Kubernetes 中运行起来了。</p>
</li>
</ol>
<!--
3. Access your application.
-->
<ol start="3">
<li>
<p>访问你的应用</p>
<!--
If you're already using `minikube` for your development process:
-->
<p>如果你在开发过程中使用 <code>minikube</code>，请执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube service frontend
</code></pre></div><!--
Otherwise, let's look up what IP your service is using!
-->
<p>否则，我们要查看一下你的服务使用了什么 IP！</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe svc frontend
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Name:                   frontend
Namespace:              default
Labels:                 service=frontend
Selector:               service=frontend
Type:                   LoadBalancer
IP:                     10.0.0.183
LoadBalancer Ingress:   192.0.2.89
Port:                   80      80/TCP
NodePort:               80      31144/TCP
Endpoints:              172.17.0.4:80
Session Affinity:       None
No events.
</code></pre><!--
If you're using a cloud provider, your IP will be listed next to `LoadBalancer Ingress`.
-->
<p>如果你使用的是云提供商，你的 IP 将在 <code>LoadBalancer Ingress</code> 字段给出。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl http://192.0.2.89
</code></pre></div></li>
</ol>
<!-- discussion -->
<!--
## User Guide
-->
<h2 id="user-guide">用户指南 </h2>
<!--
- CLI
  - [`kompose convert`](#kompose-convert)
  - [`kompose up`](#kompose-up)
  - [`kompose down`](#kompose-down)
- Documentation
  - [Build and Push Docker Images](#build-and-push-docker-images)
  - [Alternative Conversions](#alternative-conversions)
  - [Labels](#labels)
  - [Restart](#restart)
  - [Docker Compose Versions](#docker-compose-versions)
-->
<ul>
<li>
<p>CLI</p>
<ul>
<li><a href="#kompose-convert"><code>kompose convert</code></a></li>
<li><a href="#kompose-up"><code>kompose up</code></a></li>
<li><a href="#kompose-down"><code>kompose down</code></a></li>
</ul>
</li>
<li>
<p>文档</p>
<ul>
<li><a href="#build-and-push-docker-images">构建和推送 Docker 镜像</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E8%BD%AC%E6%8D%A2%E6%96%B9%E5%BC%8F">其他转换方式</a></li>
<li><a href="#labels">标签</a></li>
<li><a href="#restart">重启</a></li>
<li><a href="#docker-compose-versions">Docker Compose 版本</a></li>
</ul>
</li>
</ul>
<!--
Kompose has support for two providers: OpenShift and Kubernetes.
You can choose a targeted provider using global option `--provider`. If no provider is specified, Kubernetes is set by default.
-->
<p>Kompose 支持两种驱动：OpenShift 和 Kubernetes。
你可以通过全局选项 <code>--provider</code> 选择驱动。如果没有指定，
会将 Kubernetes 作为默认驱动。</p>
<h2 id="kompose-convert"><code>kompose convert</code></h2>
<!--
Kompose supports conversion of V1, V2, and V3 Docker Compose files into Kubernetes and OpenShift objects.
-->
<p>Kompose 支持将 V1、V2 和 V3 版本的 Docker Compose 文件转换为 Kubernetes 和 OpenShift 资源对象。</p>
<!--
### Kubernetes `kompose convert` example
-->
<h3 id="kubernetes-kompose-convert-示例">Kubernetes <code>kompose convert</code> 示例</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose --file docker-voting.yml convert
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">WARN Unsupported key networks - ignoring
WARN Unsupported key build - ignoring
INFO Kubernetes file &quot;worker-svc.yaml&quot; created
INFO Kubernetes file &quot;db-svc.yaml&quot; created
INFO Kubernetes file &quot;redis-svc.yaml&quot; created
INFO Kubernetes file &quot;result-svc.yaml&quot; created
INFO Kubernetes file &quot;vote-svc.yaml&quot; created
INFO Kubernetes file &quot;redis-deployment.yaml&quot; created
INFO Kubernetes file &quot;result-deployment.yaml&quot; created
INFO Kubernetes file &quot;vote-deployment.yaml&quot; created
INFO Kubernetes file &quot;worker-deployment.yaml&quot; created
INFO Kubernetes file &quot;db-deployment.yaml&quot; created
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ls
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">db-deployment.yaml  docker-compose.yml         docker-gitlab.yml  redis-deployment.yaml  result-deployment.yaml  vote-deployment.yaml  worker-deployment.yaml
db-svc.yaml         docker-voting.yml          redis-svc.yaml     result-svc.yaml        vote-svc.yaml           worker-svc.yaml
</code></pre><!--
You can also provide multiple docker-compose files at the same time:
-->
<p>你也可以同时提供多个 docker-compose 文件进行转换：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose -f docker-compose.yml -f docker-guestbook.yml convert
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">INFO Kubernetes file &quot;frontend-service.yaml&quot; created         
INFO Kubernetes file &quot;mlbparks-service.yaml&quot; created         
INFO Kubernetes file &quot;mongodb-service.yaml&quot; created          
INFO Kubernetes file &quot;redis-master-service.yaml&quot; created     
INFO Kubernetes file &quot;redis-slave-service.yaml&quot; created      
INFO Kubernetes file &quot;frontend-deployment.yaml&quot; created      
INFO Kubernetes file &quot;mlbparks-deployment.yaml&quot; created      
INFO Kubernetes file &quot;mongodb-deployment.yaml&quot; created       
INFO Kubernetes file &quot;mongodb-claim0-persistentvolumeclaim.yaml&quot; created
INFO Kubernetes file &quot;redis-master-deployment.yaml&quot; created  
INFO Kubernetes file &quot;redis-slave-deployment.yaml&quot; created   
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ls
</code></pre></div><pre tabindex="0"><code>mlbparks-deployment.yaml  mongodb-service.yaml                       redis-slave-service.jsonmlbparks-service.yaml  
frontend-deployment.yaml  mongodb-claim0-persistentvolumeclaim.yaml  redis-master-service.yaml
frontend-service.yaml     mongodb-deployment.yaml                    redis-slave-deployment.yaml
redis-master-deployment.yaml
</code></pre><!--
When multiple docker-compose files are provided the configuration is merged. Any configuration that is common will be over ridden by subsequent file.
-->
<p>当提供多个 docker-compose 文件时，配置将会合并。任何通用的配置都将被后续文件覆盖。</p>
<!--
### OpenShift `kompose convert` example
-->
<h3 id="openshift-kompose-convert-示例">OpenShift <code>kompose convert</code> 示例</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose --provider openshift --file docker-voting.yml convert
</code></pre></div><pre tabindex="0"><code>WARN [worker] Service cannot be created because of missing port.
INFO OpenShift file &quot;vote-service.yaml&quot; created             
INFO OpenShift file &quot;db-service.yaml&quot; created               
INFO OpenShift file &quot;redis-service.yaml&quot; created            
INFO OpenShift file &quot;result-service.yaml&quot; created           
INFO OpenShift file &quot;vote-deploymentconfig.yaml&quot; created    
INFO OpenShift file &quot;vote-imagestream.yaml&quot; created         
INFO OpenShift file &quot;worker-deploymentconfig.yaml&quot; created  
INFO OpenShift file &quot;worker-imagestream.yaml&quot; created       
INFO OpenShift file &quot;db-deploymentconfig.yaml&quot; created      
INFO OpenShift file &quot;db-imagestream.yaml&quot; created           
INFO OpenShift file &quot;redis-deploymentconfig.yaml&quot; created   
INFO OpenShift file &quot;redis-imagestream.yaml&quot; created        
INFO OpenShift file &quot;result-deploymentconfig.yaml&quot; created  
INFO OpenShift file &quot;result-imagestream.yaml&quot; created  
</code></pre><!--
It also supports creating buildconfig for build directive in a service. By default, it uses the remote repo for the current git branch as the source repo, and the current branch as the source branch for the build. You can specify a different source repo and branch using ``--build-repo`` and ``--build-branch`` options respectively.
-->
<p>kompose 还支持为服务中的构建指令创建 buildconfig。
默认情况下，它使用当前 git 分支的 remote 仓库作为源仓库，使用当前分支作为构建的源分支。
你可以分别使用 <code>--build-repo</code> 和 <code>--build-branch</code> 选项指定不同的源仓库和分支。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose --provider openshift --file buildconfig/docker-compose.yml convert
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">WARN [foo] Service cannot be created because of missing port.
INFO OpenShift Buildconfig using git@github.com:rtnpro/kompose.git::master as source.
INFO OpenShift file &quot;foo-deploymentconfig.yaml&quot; created     
INFO OpenShift file &quot;foo-imagestream.yaml&quot; created          
INFO OpenShift file &quot;foo-buildconfig.yaml&quot; created
</code></pre><!--
If you are manually pushing the Openshift artifacts using ``oc create -f``, you need to ensure that you push the imagestream artifact before the buildconfig artifact, to workaround this Openshift issue: https://github.com/openshift/origin/issues/4518 .
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果使用 <code>oc create -f</code> 手动推送 Openshift 工件，则需要确保在构建配置工件之前推送
imagestream 工件，以解决 Openshift 的这个问题：https://github.com/openshift/origin/issues/4518 。</div>
</blockquote>
<h2 id="kompose-up"><code>kompose up</code></h2>
<!--
Kompose supports a straightforward way to deploy your "composed" application to Kubernetes or OpenShift via `kompose up`.
-->
<p>Kompose 支持通过 <code>kompose up</code> 直接将你的&quot;复合的（composed）&quot; 应用程序
部署到 Kubernetes 或 OpenShift。</p>
<!--
### Kubernetes `kompose up` example
-->
<h3 id="kubernetes-kompose-up-示例">Kubernetes <code>kompose up</code> 示例</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose --file ./examples/docker-guestbook.yml up
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">We are going to create Kubernetes deployments and services for your Dockerized application.
If you need different kind of resources, use the 'kompose convert' and 'kubectl create -f' commands instead.

INFO Successfully created service: redis-master   
INFO Successfully created service: redis-slave    
INFO Successfully created service: frontend       
INFO Successfully created deployment: redis-master
INFO Successfully created deployment: redis-slave
INFO Successfully created deployment: frontend    

Your application has been deployed to Kubernetes. You can run 'kubectl get deployment,svc,pods' for details.
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment,svc,pods
</code></pre></div><pre tabindex="0"><code>NAME                                              DESIRED       CURRENT       UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/frontend                    1             1             1            1           4m
deployment.extensions/redis-master                1             1             1            1           4m
deployment.extensions/redis-slave                 1             1             1            1           4m

NAME                         TYPE               CLUSTER-IP    EXTERNAL-IP   PORT(S)      AGE
service/frontend             ClusterIP          10.0.174.12   &lt;none&gt;        80/TCP       4m
service/kubernetes           ClusterIP          10.0.0.1      &lt;none&gt;        443/TCP      13d
service/redis-master         ClusterIP          10.0.202.43   &lt;none&gt;        6379/TCP     4m
service/redis-slave          ClusterIP          10.0.1.85     &lt;none&gt;        6379/TCP     4m

NAME                                READY         STATUS        RESTARTS     AGE
pod/frontend-2768218532-cs5t5       1/1           Running       0            4m
pod/redis-master-1432129712-63jn8   1/1           Running       0            4m
pod/redis-slave-2504961300-nve7b    1/1           Running       0            4m
</code></pre><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
- You must have a running Kubernetes cluster with a pre-configured kubectl context.
- Only deployments and services are generated and deployed to Kubernetes. If you need different kind of resources, use the `kompose convert` and `kubectl create -f` commands instead.
-->
<ul>
<li>你必须有一个运行正常的 Kubernetes 集群，该集群具有预先配置的 kubectl 上下文。</li>
<li>此操作仅生成 Deployment 和 Service 对象并将其部署到 Kubernetes。
如果需要部署其他不同类型的资源，请使用 <code>kompose convert</code> 和 <code>kubectl create -f</code> 命令。</li>
</ul>
</div>
</blockquote>
<!--
### OpenShift `kompose up` example
-->
<h3 id="openshift-kompose-up-示例">OpenShift <code>kompose up</code> 示例</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose --file ./examples/docker-guestbook.yml --provider openshift up
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">We are going to create OpenShift DeploymentConfigs and Services for your Dockerized application.
If you need different kind of resources, use the 'kompose convert' and 'oc create -f' commands instead.

INFO Successfully created service: redis-slave    
INFO Successfully created service: frontend       
INFO Successfully created service: redis-master   
INFO Successfully created deployment: redis-slave
INFO Successfully created ImageStream: redis-slave
INFO Successfully created deployment: frontend    
INFO Successfully created ImageStream: frontend   
INFO Successfully created deployment: redis-master
INFO Successfully created ImageStream: redis-master

Your application has been deployed to OpenShift. You can run 'oc get dc,svc,is' for details.
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">oc get dc,svc,is
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME               REVISION                              DESIRED       CURRENT    TRIGGERED BY
dc/frontend        0                                     1             0          config,image(frontend:v4)
dc/redis-master    0                                     1             0          config,image(redis-master:e2e)
dc/redis-slave     0                                     1             0          config,image(redis-slave:v1)
NAME               CLUSTER-IP                            EXTERNAL-IP   PORT(S)    AGE
svc/frontend       172.30.46.64                          &lt;none&gt;        80/TCP     8s
svc/redis-master   172.30.144.56                         &lt;none&gt;        6379/TCP   8s
svc/redis-slave    172.30.75.245                         &lt;none&gt;        6379/TCP   8s
NAME               DOCKER REPO                           TAGS          UPDATED
is/frontend        172.30.12.200:5000/fff/frontend                     
is/redis-master    172.30.12.200:5000/fff/redis-master                 
is/redis-slave     172.30.12.200:5000/fff/redis-slave    v1  
</code></pre><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
You must have a running OpenShift cluster with a pre-configured `oc` context (`oc login`)
-->
<p>你必须有一个运行正常的 OpenShift 集群，该集群具有预先配置的 <code>oc</code> 上下文 (<code>oc login</code>)。</div>
</blockquote>
<h2 id="kompose-down"><code>kompose down</code></h2>
<!--
Once you have deployed "composed" application to Kubernetes, `$ kompose down` will help you to take the application out by deleting its deployments and services. If you need to remove other resources, use the 'kubectl' command.
-->
<p>你一旦将&quot;复合(composed)&quot; 应用部署到 Kubernetes，<code>kompose down</code>
命令将能帮你通过删除 Deployment 和 Service 对象来删除应用。
如果需要删除其他资源，请使用 'kubectl' 命令。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose --file docker-guestbook.yml down
</code></pre></div><pre tabindex="0"><code>INFO Successfully deleted service: redis-master   
INFO Successfully deleted deployment: redis-master
INFO Successfully deleted service: redis-slave    
INFO Successfully deleted deployment: redis-slave
INFO Successfully deleted service: frontend       
INFO Successfully deleted deployment: frontend
</code></pre><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
You must have a running Kubernetes cluster with a pre-configured kubectl context.
-->
<ul>
<li>你必须有一个运行正常的 Kubernetes 集群，该集群具有预先配置的 kubectl 上下文。</li>
</ul>
</div>
</blockquote>
<!--
## Build and Push Docker Images

Kompose supports both building and pushing Docker images. When using the `build` key within your Docker Compose file, your image will:

  - Automatically be built with Docker using the `image` key specified within your file
  - Be pushed to the correct Docker repository using local credentials (located at `.docker/config`)

Using an [example Docker Compose file](https://raw.githubusercontent.com/kubernetes/kompose/master/examples/buildconfig/docker-compose.yml):
-->
<h2 id="build-and-push-docker-images">构建和推送 Docker 镜像  </h2>
<p>Kompose 支持构建和推送 Docker 镜像。如果 Docker Compose 文件中使用了 <code>build</code>
关键字，你的镜像将会：</p>
<ul>
<li>使用文档中指定的 <code>image</code> 键自动构建 Docker 镜像</li>
<li>使用本地凭据推送到正确的 Docker 仓库</li>
</ul>
<p>使用 <a href="https://raw.githubusercontent.com/kubernetes/kompose/master/examples/buildconfig/docker-compose.yml">Docker Compose 文件示例</a></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">services</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">build</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;./build&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>docker.io/foo/bar<span style="color:#bbb">
</span></code></pre></div><!--
Using `kompose up` with a `build` key:
-->
<p>使用带有 <code>build</code> 键的 <code>kompose up</code> 命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose up
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">INFO Build key detected. Attempting to build and push image 'docker.io/foo/bar'
INFO Building image 'docker.io/foo/bar' from directory 'build'
INFO Image 'docker.io/foo/bar' from directory 'build' built successfully
INFO Pushing image 'foo/bar:latest' to registry 'docker.io'
INFO Attempting authentication credentials 'https://index.docker.io/v1/
INFO Successfully pushed image 'foo/bar:latest' to registry 'docker.io'
INFO We are going to create Kubernetes Deployments, Services and PersistentVolumeClaims for your Dockerized application. If you need different kind of resources, use the 'kompose convert' and 'kubectl create -f' commands instead.

INFO Deploying application in &quot;default&quot; namespace
INFO Successfully created Service: foo            
INFO Successfully created Deployment: foo         

Your application has been deployed to Kubernetes. You can run 'kubectl get deployment,svc,pods,pvc' for details.
</code></pre><!--
In order to disable the functionality, or choose to use BuildConfig generation (with OpenShift) `--build (local|build-config|none)` can be passed.
-->
<p>要想禁用该功能，或者使用 BuildConfig 中的版本（在 OpenShift 中），
可以通过传递 <code>--build (local|build-config|none)</code> 参数来实现。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 禁止构造和推送 Docker 镜像</span>
kompose up --build none

<span style="color:#080;font-style:italic"># 为 OpenShift 生成 Build Config 工件</span>
kompose up --provider openshift --build build-config
</code></pre></div><!--
## Alternative Conversions

The default `kompose` transformation will generate Kubernetes [Deployments](/docs/concepts/workloads/controllers/deployment/) and [Services](/docs/concepts/services-networking/service/), in yaml format. You have alternative option to generate json with `-j`. Also, you can alternatively generate [Replication Controllers](/docs/concepts/workloads/controllers/replicationcontroller/) objects, [Daemon Sets](/docs/concepts/workloads/controllers/daemonset/), or [Helm](https://github.com/helm/helm) charts.
-->
<h2 id="alternative-conversions">其他转换方式   </h2>
<p>默认的 <code>kompose</code> 转换会生成 yaml 格式的 Kubernetes
<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a> 和
<a href="/zh/docs/concepts/services-networking/service/">Service</a> 对象。
你可以选择通过 <code>-j</code> 参数生成 json 格式的对象。
你也可以替换生成 <a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">Replication Controllers</a> 对象、
<a href="/zh/docs/concepts/workloads/controllers/daemonset/">Daemon Sets</a> 或
<a href="https://github.com/helm/helm">Helm</a> charts。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose convert -j
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">INFO Kubernetes file &quot;redis-svc.json&quot; created
INFO Kubernetes file &quot;web-svc.json&quot; created
INFO Kubernetes file &quot;redis-deployment.json&quot; created
INFO Kubernetes file &quot;web-deployment.json&quot; created
</code></pre><!--
The `*-deployment.json` files contain the Deployment objects.
-->
<p><code>*-deployment.json</code> 文件中包含 Deployment 对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose convert --replication-controller
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">INFO Kubernetes file &quot;redis-svc.yaml&quot; created
INFO Kubernetes file &quot;web-svc.yaml&quot; created
INFO Kubernetes file &quot;redis-replicationcontroller.yaml&quot; created
INFO Kubernetes file &quot;web-replicationcontroller.yaml&quot; created
</code></pre><!--
The `*-replicationcontroller.yaml` files contain the Replication Controller objects. If you want to specify replicas (default is 1), use `--replicas` flag: `$ kompose convert --replication-controller --replicas 3`
-->
<p><code>*-replicationcontroller.yaml</code> 文件包含 Replication Controller 对象。
如果你想指定副本数（默认为 1），可以使用 <code>--replicas</code> 参数：
<code>kompose convert --replication-controller --replicas 3</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose convert --daemon-set
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">INFO Kubernetes file &quot;redis-svc.yaml&quot; created
INFO Kubernetes file &quot;web-svc.yaml&quot; created
INFO Kubernetes file &quot;redis-daemonset.yaml&quot; created
INFO Kubernetes file &quot;web-daemonset.yaml&quot; created
</code></pre><!--
The `*-daemonset.yaml` files contain the DaemonSet objects
If you want to generate a Chart to be used with [Helm](https://github.com/kubernetes/helm) simply do:
-->
<p><code>*-daemonset.yaml</code> 文件包含 DaemonSet 对象。</p>
<p>如果你想生成 <a href="https://github.com/kubernetes/helm">Helm</a> 可用的 Chart，
只需简单的执行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kompose convert -c
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">INFO Kubernetes file &quot;web-svc.yaml&quot; created
INFO Kubernetes file &quot;redis-svc.yaml&quot; created
INFO Kubernetes file &quot;web-deployment.yaml&quot; created
INFO Kubernetes file &quot;redis-deployment.yaml&quot; created
chart created in &quot;./docker-compose/&quot;
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">tree docker-compose/
</code></pre></div><pre tabindex="0"><code>docker-compose
├── Chart.yaml
├── README.md
└── templates
    ├── redis-deployment.yaml
    ├── redis-svc.yaml
    ├── web-deployment.yaml
    └── web-svc.yaml
</code></pre><!--
The chart structure is aimed at providing a skeleton for building your Helm charts.
-->
<p>这个 Chart 结构旨在为构建 Helm Chart 提供框架。</p>
<!--
## Labels

`kompose` supports Kompose-specific labels within the `docker-compose.yml` file in order to explicitly define a service's behavior upon conversion.
- `kompose.service.type` defines the type of service to be created.

For example:
-->
<h2 id="labels">标签  </h2>
<p><code>kompose</code> 支持 <code>docker-compose.yml</code> 文件中用于 Kompose 的标签，以便
在转换时明确定义 Service 的行为。</p>
<ul>
<li>
<p><code>kompose.service.type</code> 定义要创建的 Service 类型。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">services</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nginx</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">dockerfile</span>:<span style="color:#bbb"> </span>foobar<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">build</span>:<span style="color:#bbb"> </span>./foobar<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cap_add</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- ALL<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">container_name</span>:<span style="color:#bbb"> </span>foobar<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kompose.service.type</span>:<span style="color:#bbb"> </span>nodeport<span style="color:#bbb">
</span></code></pre></div></li>
</ul>
<!--
- `kompose.service.expose` defines if the service needs to be made accessible from outside the cluster or not. If the value is set to "true", the provider sets the endpoint automatically, and for any other value, the value is set as the hostname. If multiple ports are defined in a service, the first one is chosen to be the exposed.
  - For the Kubernetes provider, an ingress resource is created and it is assumed that an ingress controller has already been configured.
  - For the OpenShift provider, a route is created.
For example:
-->
<ul>
<li>
<p><code>kompose.service.expose</code> 定义是否允许从集群外部访问 Service。
如果该值被设置为 &quot;true&quot;，提供程序将自动设置端点，
对于任何其他值，该值将被设置为主机名。
如果在 Service 中定义了多个端口，则选择第一个端口作为公开端口。</p>
<ul>
<li>如果使用 Kubernetes 驱动，会有一个 Ingress 资源被创建，并且假定
已经配置了相应的 Ingress 控制器。</li>
<li>如果使用 OpenShift 驱动, 则会有一个 route 被创建。</li>
</ul>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">services</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">web</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>tuna/docker-counter23<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span>- <span style="color:#b44">&#34;5000:5000&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">links</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span>- redis<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kompose.service.expose</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;counter.example.com&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">redis</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis:3.0<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span>- <span style="color:#b44">&#34;6379&#34;</span><span style="color:#bbb">
</span></code></pre></div></li>
</ul>
<!--
The currently supported options are:
| Key                  | Value                               |
|----------------------|-------------------------------------|
| kompose.service.type | nodeport / clusterip / loadbalancer |
| kompose.service.expose| true / hostname |
-->
<p>当前支持的选项有:</p>
<table>
<thead>
<tr>
<th>键</th>
<th>值</th>
</tr>
</thead>
<tbody>
<tr>
<td>kompose.service.type</td>
<td>nodeport / clusterip / loadbalancer</td>
</tr>
<tr>
<td>kompose.service.expose</td>
<td>true / hostname</td>
</tr>
</tbody>
</table>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
The `kompose.service.type` label should be defined with `ports` only, otherwise `kompose` will fail.
-->
<p><code>kompose.service.type</code> 标签应该只用 <code>ports</code> 来定义，否则 <code>kompose</code> 会失败。</div>
</blockquote>
<!--
## Restart

If you want to create normal pods without controllers you can use `restart` construct of docker-compose to define that. Follow table below to see what happens on the `restart` value.
-->
<h2 id="restart">重启  </h2>
<p>如果你想创建没有控制器的普通 Pod，可以使用 docker-compose 的 <code>restart</code>
结构来指定这一行为。请参考下表了解 <code>restart</code> 的不同参数。</p>
<!--
| `docker-compose` `restart` | object created    | Pod `restartPolicy` |
|----------------------------|-------------------|---------------------|
| `""`                       | controller object | `Always`            |
| `always`                   | controller object | `Always`            |
| `on-failure`               | Pod               | `OnFailure`         |
| `no`                       | Pod               | `Never`             |
-->
<table>
<thead>
<tr>
<th><code>docker-compose</code> <code>restart</code></th>
<th>创建的对象</th>
<th>Pod <code>restartPolicy</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&quot;&quot;</code></td>
<td>控制器对象</td>
<td><code>Always</code></td>
</tr>
<tr>
<td><code>always</code></td>
<td>控制器对象</td>
<td><code>Always</code></td>
</tr>
<tr>
<td><code>on-failure</code></td>
<td>Pod</td>
<td><code>OnFailure</code></td>
</tr>
<tr>
<td><code>no</code></td>
<td>Pod</td>
<td><code>Never</code></td>
</tr>
</tbody>
</table>
<!--
The controller object could be `deployment` or `replicationcontroller`, etc.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 控制器对象可以是 <code>deployment</code> 或 <code>replicationcontroller</code> 等。</div>
</blockquote>
<!--
For example, the `pival` service will become pod down here. This container calculated value of `pi`.
-->
<p>例如，<code>pival</code> Service 将在这里变成 Pod。这个容器计算 <code>pi</code> 的取值。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;2&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">services</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">pival</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>perl<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;perl&#34;</span>,<span style="color:#bbb">  </span><span style="color:#b44">&#34;-Mbignum=bpi&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-wle&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;print bpi(2000)&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">restart</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;on-failure&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
### Warning about Deployment Configurations

If the Docker Compose file has a volume specified for a service, the Deployment (Kubernetes) or DeploymentConfig (OpenShift) strategy is changed to "Recreate" instead of "RollingUpdate" (default). This is done to avoid multiple instances of a service from accessing a volume at the same time.
-->
<h3 id="关于-deployment-config-的提醒">关于 Deployment Config 的提醒</h3>
<p>如果 Docker Compose 文件中为服务声明了卷，Deployment (Kubernetes) 或
DeploymentConfig (OpenShift) 策略会从 &quot;RollingUpdate&quot; (默认) 变为 &quot;Recreate&quot;。
这样做的目的是为了避免服务的多个实例同时访问卷。</p>
<!--
If the Docker Compose file has service name with `_` in it (eg.`web_service`), then it will be replaced by `-` and the service name will be renamed accordingly (eg.`web-service`). Kompose does this because "Kubernetes" doesn't allow `_` in object name.
Please note that changing service name might break some `docker-compose` files.
-->
<p>如果 Docker Compose 文件中的服务名包含 <code>_</code>（例如 <code>web_service</code>），
那么将会被替换为 <code>-</code>，服务也相应的会重命名（例如 <code>web-service</code>）。
Kompose 这样做的原因是 &quot;Kubernetes&quot; 不允许对象名称中包含 <code>_</code>。</p>
<p>请注意，更改服务名称可能会破坏一些 <code>docker-compose</code> 文件。</p>
<!--
## Docker Compose Versions

Kompose supports Docker Compose versions: 1, 2 and 3. We have limited support on versions 2.1 and 3.2 due to their experimental nature.

A full list on compatibility between all three versions is listed in our [conversion document](https://github.com/kubernetes/kompose/blob/master/docs/conversion.md) including a list of all incompatible Docker Compose keys.
-->
<h2 id="docker-compose-versions">Docker Compose 版本  </h2>
<p>Kompose 支持的 Docker Compose 版本包括：1、2 和 3。
对 2.1 和 3.2 版本的支持还有限，因为它们还在实验阶段。</p>
<p>所有三个版本的兼容性列表请查看我们的
<a href="https://github.com/kubernetes/kompose/blob/master/docs/conversion.md">转换文档</a>，
文档中列出了所有不兼容的 Docker Compose 关键字。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-aa0731e8aa8e2f6cc9e3c1a5e9895863">4 - 管理 Kubernetes 对象</h1>
    <div class="lead">用声明式和命令式范型与 Kubernetes API 交互。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-df206392be6f4d19bd8da41cee7170fa">4.1 - 使用配置文件对 Kubernetes 对象进行声明式管理</h1>
    
	<!--
title: Declarative Management of Kubernetes Objects Using Configuration Files
content_type: task
weight: 10
-->
<!-- overview -->
<!--
Kubernetes objects can be created, updated, and deleted by storing multiple
object configuration files in a directory and using `kubectl apply` to
recursively create and update those objects as needed. This method
retains writes made to live objects without merging the changes
back into the object configuration files. `kubectl diff` also gives you a
preview of what changes `apply` will make.
-->
<p>你可以通过在一个目录中存储多个对象配置文件、并使用 <code>kubectl apply</code>
来递归地创建和更新对象来创建、更新和删除 Kubernetes 对象。
这种方法会保留对现有对象已作出的修改，而不会将这些更改写回到对象配置文件中。
<code>kubectl diff</code> 也会给你呈现 <code>apply</code> 将作出的变更的预览。</p>
<h2 id="准备开始">准备开始</h2>
<!--
Install [`kubectl`](/docs/tasks/tools/).
-->
<p>安装 <a href="/zh/docs/tasks/tools/"><code>kubectl</code></a>。</p>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Trade-offs

The `kubectl` tool supports three kinds of object management:

* Imperative commands
* Imperative object configuration
* Declarative object configuration

See [Kubernetes Object Management](/docs/concepts/overview/working-with-objects/object-management/)
for a discussion of the advantages and disadvantage of each kind of object management.
-->
<h2 id="trade-offs">权衡取舍  </h2>
<p><code>kubectl</code> 工具能够支持三种对象管理方式：</p>
<ul>
<li>指令式命令</li>
<li>指令式对象配置</li>
<li>声明式对象配置</li>
</ul>
<p>关于每种对象管理的优缺点的讨论，可参见
<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">Kubernetes 对象管理</a>。</p>
<!--
## Overview

Declarative object configuration requires a firm understanding of
the Kubernetes object definitions and configuration. Read and complete
the following documents if you have not already:

* [Managing Kubernetes Objects Using Imperative Commands](/docs/tasks/manage-kubernetes-objects/imperative-command/)
* [Imperative Management of Kubernetes Objects Using Configuration Files](/docs/tasks/manage-kubernetes-objects/imperative-config/)
-->
<h2 id="overview">概览 </h2>
<p>声明式对象管理需要用户对 Kubernetes 对象定义和配置有比较深刻的理解。
如果你还没有这方面的知识储备，请先阅读下面的文档：</p>
<ul>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-command/">使用指令式命令管理 Kubernetes 对象</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-config/">使用配置文件对 Kubernetes 对象进行指令式管理</a></li>
</ul>
<!--
Following are definitions for terms used in this document:

- *object configuration file / configuration file*: A file that defines the
  configuration for a Kubernetes object. This topic shows how to pass configuration
  files to `kubectl apply`. Configuration files are typically stored in source control, such as Git.
- *live object configuration / live configuration*: The live configuration
  values of an object, as observed by the Kubernetes cluster. These are kept in the Kubernetes
  cluster storage, typically etcd.
- *declarative configuration writer /  declarative writer*: A person or software component
  that makes updates to a live object. The live writers referred to in this topic make changes
  to object configuration files and run `kubectl apply` to write the changes.
-->
<p>以下是本文档中使用的术语的定义：</p>
<ul>
<li><em>对象配置文件/配置文件</em>：一个定义 Kubernetes 对象的配置的文件。
本主题展示如何将配置文件传递给 <code>kubectl apply</code>。
配置文件通常存储于类似 Git 这种源码控制系统中。</li>
<li><em>现时对象配置/现时配置</em>：由 Kubernetes 集群所观测到的对象的现时配置值。
这些配置保存在 Kubernetes 集群存储（通常是 etcd）中。</li>
<li><em>声明式配置写者/声明式写者</em>：负责更新现时对象的人或者软件组件。
本主题中的声明式写者负责改变对象配置文件并执行 <code>kubectl apply</code> 命令
以写入变更。</li>
</ul>
<!--
## How to create objects

Use `kubectl apply` to create all objects, except those that already exist,
defined by configuration files in a specified directory:

```shell
kubectl apply -f <directory>/
```

This sets the `kubectl.kubernetes.io/last-applied-configuration: '{...}'`
annotation on each object. The annotation contains the contents of the object
configuration file that was used to create the object.
-->
<h2 id="how-to-create-objects">如何创建对象</h2>
<p>使用 <code>kubectl apply</code> 来创建指定目录中配置文件所定义的所有对象，除非对应对象已经存在：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f &lt;目录&gt;/
</code></pre></div><p>此操作会在每个对象上设置 <code>kubectl.kubernetes.io/last-applied-configuration: '{...}'</code>
注解。注解值中包含了用来创建对象的配置文件的内容。</p>
<!--
Add the `-R` flag to recursively process directories.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 添加 <code>-R</code> 标志可以递归地处理目录。</div>
</blockquote>
<!--
Here's an example of an object configuration file:
-->
<p>下面是一个对象配置文件示例：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/simple_deployment.yaml" download="application/simple_deployment.yaml"><code>application/simple_deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-simple-deployment-yaml')" title="Copy application/simple_deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-simple-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Run `kubectl diff` to print the object that will be created:
-->
<p>执行 <code>kubectl diff</code> 可以打印出将被创建的对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl diff -f https://k8s.io/examples/application/simple_deployment.yaml
</code></pre></div><!--
`diff` uses [server-side dry-run](/docs/reference/using-api/api-concepts/#dry-run),
which needs to be enabled on `kube-apiserver`.

Since `diff` performs a server-side apply request in dry-run mode,
it requires granting `PATCH`, `CREATE`, and `UPDATE` permissions.
See [Dry-Run Authorization](/docs/reference/using-api/api-concepts#dry-run-authorization)
for details.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p><code>diff</code> 使用<a href="/zh/docs/reference/using-api/api-concepts/#dry-run">服务器端试运行（Server-side Dry-run）</a>
功能特性；而该功能特性需要在 <code>kube-apiserver</code> 上启用。</p>
<p>由于 <code>diff</code> 操作会使用试运行模式执行服务器端 apply 请求，因此需要为
用户配置 <code>PATCH</code>、<code>CREATE</code> 和 <code>UPDATE</code> 操作权限。
参阅<a href="/zh/docs/reference/using-api/api-concepts#dry-run-authorization">试运行授权</a>
了解详情。</p>
</div>
</blockquote>
<!--
Create the object using `kubectl apply`:
-->
<p>使用 <code>kubectl apply</code> 来创建对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml
</code></pre></div><!--
Print the live configuration using `kubectl get`:
-->
<p>使用 <code>kubectl get</code> 打印其现时配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml
</code></pre></div><!--
The output shows that the `kubectl.kubernetes.io/last-applied-configuration` annotation
was written to the live configuration, and it matches the configuration file:
-->
<p>输出显示注解 <code>kubectl.kubernetes.io/last-applied-configuration</code> 被写入到
现时配置中，并且其内容与配置文件相同：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># This is the json representation of simple_deployment.yaml</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># It was written by kubectl apply when the object was created</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;Deployment&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;nginx-deployment&#34;,&#34;namespace&#34;:&#34;default&#34;},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;minReadySeconds&#34;:5,&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;app&#34;:nginx}},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;app&#34;:&#34;nginx&#34;}},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;containers&#34;:[{&#34;image&#34;:&#34;nginx:1.14.2&#34;,&#34;name&#34;:&#34;nginx&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;ports&#34;:[{&#34;containerPort&#34;:80}]}]}}}}</span><span style="color:#bbb">      
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
## How to update objects

You can also use `kubectl apply` to update all objects defined in a directory, even
if those objects already exist. This approach accomplishes the following:

1. Sets fields that appear in the configuration file in the live configuration.
2. Clears fields removed from the configuration file in the live configuration.

```shell
kubectl diff -f <directory>/
kubectl apply -f <directory>/
```
-->
<h2 id="how-to-update-objects">如何更新对象  </h2>
<p>你也可以使用 <code>kubectl apply</code> 来更新某个目录中定义的所有对象，即使那些对象已经存在。
这一操作会隐含以下行为：</p>
<ol>
<li>在现时配置中设置配置文件中出现的字段；</li>
<li>在现时配置中清除配置文件中已删除的字段。</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl diff -f &lt;目录&gt;/
kubectl apply -f &lt;目录&gt;/
</code></pre></div><!--
Add the `-R` flag to recursively process directories.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 使用 <code>-R</code> 标志递归处理目录。</div>
</blockquote>
<!--
Here's an example configuration file:
-->
<p>下面是一个配置文件示例：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/simple_deployment.yaml" download="application/simple_deployment.yaml"><code>application/simple_deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-simple-deployment-yaml')" title="Copy application/simple_deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-simple-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the object using `kubectl apply`:
-->
<p>使用 <code>kubectl apply</code> 来创建对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml
</code></pre></div><!--
For purposes of illustration, the preceding command refers to a single
configuration file instead of a directory.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 出于演示的目的，上面的命令引用的是单个文件而不是整个目录。</div>
</blockquote>
<!--
Print the live configuration using `kubectl get`:
-->
<p>使用 <code>kubectl get</code> 打印现时配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml
</code></pre></div><!--
The output shows that the `kubectl.kubernetes.io/last-applied-configuration` annotation
was written to the live configuration, and it matches the configuration file:
-->
<p>输出显示，注解 <code>kubectl.kubernetes.io/last-applied-configuration</code> 被写入到
现时配置中，并且其取值与配置文件内容相同。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此为 simple_deployment.yaml 的 JSON 表示</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 在对象创建时由 kubectl apply 命令写入</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;Deployment&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;nginx-deployment&#34;,&#34;namespace&#34;:&#34;default&#34;},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;minReadySeconds&#34;:5,&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;app&#34;:nginx}},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;app&#34;:&#34;nginx&#34;}},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;containers&#34;:[{&#34;image&#34;:&#34;nginx:1.14.2&#34;,&#34;name&#34;:&#34;nginx&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;ports&#34;:[{&#34;containerPort&#34;:80}]}]}}}}</span><span style="color:#bbb">      
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
Directly update the `replicas` field in the live configuration by using `kubectl scale`.
This does not use `kubectl apply`:
-->
<p>通过 <code>kubeclt scale</code> 命令直接更新现时配置中的 <code>replicas</code> 字段。
这一命令没有使用 <code>kubectl apply</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment/nginx-deployment --replicas<span style="color:#666">=</span><span style="color:#666">2</span>
</code></pre></div><!--
Print the live configuration using `kubectl get`:
-->
<p>使用 <code>kubectl get</code> 来打印现时配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment nginx-deployment -o yaml
</code></pre></div><!--
The output shows that the `replicas` field has been set to 2, and the `last-applied-configuration`
annotation does not contain a `replicas` field:
-->
<p>输出显示，<code>replicas</code> 字段已经被设置为 2，而 <code>last-applied-configuration</code> 注解中
并不包含 <code>replicas</code> 字段。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 注意注解中并不包含 replicas</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 这是因为更新并不是通过 kubectl apply 来执行的</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;Deployment&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;nginx-deployment&#34;,&#34;namespace&#34;:&#34;default&#34;},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;minReadySeconds&#34;:5,&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;app&#34;:nginx}},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;app&#34;:&#34;nginx&#34;}},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;containers&#34;:[{&#34;image&#34;:&#34;nginx:1.14.2&#34;,&#34;name&#34;:&#34;nginx&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;ports&#34;:[{&#34;containerPort&#34;:80}]}]}}}}</span><span style="color:#bbb">      
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># written by scale</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
Update the `simple_deployment.yaml` configuration file to change the image from
`nginx:1.14.2` to `nginx:1.16.1`, and delete the `minReadySeconds` field:
-->
<p>现在更新 <code>simple_deployment.yaml</code> 配置文件，将镜像文件从
<code>nginx:1.14.2</code> 更改为 <code>nginx:1.16.1</code>，同时删除<code>minReadySeconds</code> 字段：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/update_deployment.yaml" download="application/update_deployment.yaml"><code>application/update_deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-update-deployment-yaml')" title="Copy application/update_deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-update-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16.1<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># update the image</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Apply the changes made to the configuration file:
-->
<p>应用对配置文件所作更改：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl diff -f https://k8s.io/examples/application/update_deployment.yaml
kubectl apply -f https://k8s.io/examples/application/update_deployment.yaml
</code></pre></div><!--
Print the live configuration using `kubectl get`:
-->
<p>使用 <code>kubectl get</code> 打印现时配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f https://k8s.io/examples/application/update_deployment.yaml -o yaml
</code></pre></div><!--
The output shows the following changes to the live configuration:

* The `replicas` field retains the value of 2 set by `kubectl scale`.
  This is possible because it is omitted from the configuration file.
* The `image` field has been updated to `nginx:1.16.1` from `nginx:1.14.2`.
* The `last-applied-configuration` annotation has been updated with the new image.
* The `minReadySeconds` field has been cleared.
* The `last-applied-configuration` annotation no longer contains the `minReadySeconds` field.
-->
<p>输出显示现时配置中发生了以下更改：</p>
<ul>
<li>字段 <code>replicas</code> 保留了 <code>kubectl scale</code> 命令所设置的值：2；
之所以该字段被保留是因为配置文件中并没有设置 <code>replicas</code>。</li>
<li>字段 <code>image</code> 的内容已经从 <code>nginx:1.14.2</code> 更改为 <code>nginx:1.16.1</code>。</li>
<li>注解 <code>last-applied-configuration</code> 内容被更改为新的镜像名称。</li>
<li>字段 <code>minReadySeconds</code> 被移除。</li>
<li>注解 <code>last-applied-configuration</code> 中不再包含 <code>minReadySeconds</code> 字段。</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 注解中包含更新后的镜像 nginx 1.16.1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 但是其中并不包含更改后的 replicas 值 2</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;Deployment&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;nginx-deployment&#34;,&#34;namespace&#34;:&#34;default&#34;},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;app&#34;:nginx}},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;app&#34;:&#34;nginx&#34;}},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;containers&#34;:[{&#34;image&#34;:&#34;nginx:1.16.1&#34;,&#34;name&#34;:&#34;nginx&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;ports&#34;:[{&#34;containerPort&#34;:80}]}]}}}}</span><span style="color:#bbb">      
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 由 `kubectl scale` 设置，被 `kubectl apply` 命令忽略</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># minReadySeconds 被 `kubectl apply` 清除</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16.1<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 由 `kubectl apply` 设置</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
Mixing `kubectl apply` with the imperative object configuration commands
`create` and `replace` is not supported. This is because `create`
and `replace` do not retain the `kubectl.kubernetes.io/last-applied-configuration`
that `kubectl apply` uses to compute updates.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 将 <code>kubectl apply</code> 与指令式对象配置命令 <code>kubectl create</code> 或 <code>kubectl replace</code>
混合使用是不受支持的。这是因为 <code>create</code> 和 <code>replace</code> 命令都不会保留
<code>kubectl apply</code> 用来计算更新内容所使用的
<code>kubectl.kubernetes.io/last-applied-configuration</code> 注解值。</div>
</blockquote>

<!--
## How to delete objects

There are two approaches to delete objects managed by `kubectl apply`.

### Recommended: `kubectl delete -f <filename>`

Manually deleting objects using the imperative command is the recommended
approach, as it is more explicit about what is being deleted, and less likely
to result in the user deleting something unintentionally:

```shell
kubectl delete -f <filename>
```
-->
<h2 id="how-to-delete-objects">如何删除对象 </h2>
<p>有两种方法来删除 <code>kubectl apply</code> 管理的对象。</p>
<h3 id="建议操作-kubectl-delete-f-文件名">建议操作：<code>kubectl delete -f &lt;文件名&gt;</code></h3>
<p>使用指令式命令来手动删除对象是建议的方法，因为这种方法更为明确地给出了
要删除的内容是什么，且不容易造成用户不小心删除了其他对象的情况。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -f &lt;文件名&gt;
</code></pre></div><!--
### Alternative: `kubectl apply -f <directory/> -prune -l your=label`

Only use this if you know what you are doing.
-->
<h3 id="替代方式-kubectl-apply-f-目录名称-prune-l-your-label">替代方式：<code>kubectl apply -f &lt;目录名称/&gt; --prune -l your=label</code></h3>
<p>只有在充分理解此命令背后含义的情况下才建议这样操作。</p>
<!--
`kubectl apply -prune` is in alpha, and backwards incompatible
changes might be introduced in subsequent releases.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> <code>kubectl apply --prune</code> 命令本身仍处于 Alpha 状态，在后续发布版本中可能会
引入一些向后不兼容的变化。</div>
</blockquote>

<blockquote class="warning callout">
  <div><strong>警告：</strong> 在使用此命令时必须小心，这样才不会无意中删除不想删除的对象。</div>
</blockquote>

<!--
As an alternative to `kubectl delete`, you can use `kubectl apply` to identify objects to be deleted after their
configuration files have been removed from the directory. Apply with `-prune`
queries the API server for all objects matching a set of labels, and attempts
to match the returned live object configurations against the object
configuration files. If an object matches the query, and it does not have a
configuration file in the directory, and it has a `last-applied-configuration` annotation,
it is deleted.
-->
<p>作为 <code>kubectl delete</code> 操作的替代方式，你可以在目录中对象配置文件被删除之后，
使用 <code>kubectl apply</code> 来辩识要删除的对象。
带 <code>--prune</code> 标志的 <code>apply</code> 命令会首先查询 API 服务器，获得与某组标签相匹配
的对象列表，之后将返回的现时对象配置与目录中的对象配置文件相比较。
如果某对象在查询中被匹配到，但在目录中没有文件与其相对应，并且其中还包含
<code>last-applied-configuration</code> 注解，则该对象会被删除。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f &lt;directory/&gt; --prune -l &lt;labels&gt;
</code></pre></div><!--
Apply with prune should only be run against the root directory
containing the object configuration files. Running against sub-directories
can cause objects to be unintentionally deleted if they are returned
by the label selector query specified with `-l <labels>` and
do not appear in the subdirectory.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 带剪裁（prune）行为的 <code>apply</code> 操作应在包含对象配置文件的目录的根目录运行。
如果在其子目录中运行，可能导致对象被不小心删除。
因为某些对象可能与 <code>-l &lt;标签&gt;</code> 的标签选择算符匹配，但其配置文件不在当前
子目录下。</div>
</blockquote>

<!--
## How to view an object

You can use `kubectl get` with `-o yaml` to view the configuration of a live object:

```shell
kubectl get -f <filename|url> -o yaml
```
-->
<h2 id="how-to-view-an-object">如何查看对象 </h2>
<p>你可以使用 <code>kubectl get</code> 并指定 <code>-o yaml</code> 选项来查看现时对象的配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f &lt;文件名 | URL&gt; -o yaml
</code></pre></div><!--
## How apply calculates differences and merges changes

A *patch* is an update operation that is scoped to specific fields of an object
instead of the entire object. This enables updating only a specific set of fields
on an object without reading the object first.
-->
<h2 id="apply-操作是如何计算配置差异并合并变更的">apply 操作是如何计算配置差异并合并变更的？</h2>
<blockquote class="caution callout">
  <div><strong>注意：</strong> <em>patch</em> 是一种更新操作，其作用域为对象的一些特定字段而不是整个对象。
这使得你可以更新对象的特定字段集合而不必先要读回对象。</div>
</blockquote>

<!--
When `kubectl apply` updates the live configuration for an object,
it does so by sending a patch request to the API server. The
patch defines updates scoped to specific fields of the live object
configuration. The `kubectl apply` command calculates this patch request
using the configuration file, the live configuration, and the
`last-applied-configuration` annotation stored in the live configuration.
-->
<p><code>kubectl apply</code> 更新对象的现时配置，它是通过向 API 服务器发送一个 patch 请求
来执行更新动作的。
所提交的补丁中定义了对现时对象配置中特定字段的更新。
<code>kubectl apply</code> 命令会使用当前的配置文件、现时配置以及现时配置中保存的
<code>last-applied-configuration</code> 注解内容来计算补丁更新内容。</p>
<!--
### Merge patch calculation

The `kubectl apply` command writes the contents of the configuration file to the
`kubectl.kubernetes.io/last-applied-configuration` annotation. This
is used to identify fields that have been removed from the configuration
file and need to be cleared from the live configuration. Here are the steps used
to calculate which fields should be deleted or set:
-->
<h3 id="merge-patch-calculation">合并补丁计算 </h3>
<p><code>kubectl apply</code> 命令将配置文件的内容写入到
<code>kubectl.kubernetes.io/last-applied-configuration</code> 注解中。
这些内容用来识别配置文件中已经移除的、因而也需要从现时配置中删除的字段。
用来计算要删除或设置哪些字段的步骤如下：</p>
<!--
1. Calculate the fields to delete. These are the fields present in `last-applied-configuration` and missing from the configuration file.
2. Calculate the fields to add or set. These are the fields present in the configuration file whose values don't match the live configuration.

Here's an example. Suppose this is the configuration file for a Deployment object:
-->
<ol>
<li>计算要删除的字段，即在 <code>last-applied-configuration</code> 中存在但在
配置文件中不再存在的字段。</li>
<li>计算要添加或设置的字段，即在配置文件中存在但其取值与现时配置不同的字段。</li>
</ol>
<p>下面是一个例子。假定此文件是某 Deployment 对象的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/update_deployment.yaml" download="application/update_deployment.yaml"><code>application/update_deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-update-deployment-yaml')" title="Copy application/update_deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-update-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16.1<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># update the image</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Also, suppose this is the live configuration for the same Deployment object:
-->
<p>同时假定同一 Deployment 对象的现时配置如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;Deployment&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;nginx-deployment&#34;,&#34;namespace&#34;:&#34;default&#34;},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;minReadySeconds&#34;:5,&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;app&#34;:nginx}},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;app&#34;:&#34;nginx&#34;}},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;containers&#34;:[{&#34;image&#34;:&#34;nginx:1.14.2&#34;,&#34;name&#34;:&#34;nginx&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;ports&#34;:[{&#34;containerPort&#34;:80}]}]}}}}</span><span style="color:#bbb">      
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
Here are the merge calculations that would be performed by `kubectl apply`:

1. Calculate the fields to delete by reading values from
   `last-applied-configuration` and comparing them to values in the
   configuration file.
   Clear fields explicitly set to null in the local object configuration file
   regardless of whether they appear in the `last-applied-configuration`.
   In this example, `minReadySeconds` appears in the
   `last-applied-configuration` annotation, but does not appear in the configuration file.
    **Action:** Clear `minReadySeconds` from the live configuration.
2. Calculate the fields to set by reading values from the configuration
   file and comparing them to values in the live configuration. In this example,
   the value of `image` in the configuration file does not match
    the value in the live configuration. **Action:** Set the value of `image` in the live configuration.
3. Set the `last-applied-configuration` annotation to match the value
   of the configuration file.
4. Merge the results from 1, 2, 3 into a single patch request to the API server.

Here is the live configuration that is the result of the merge:
-->
<p>下面是 <code>kubectl apply</code> 将执行的合并计算：</p>
<ol>
<li>通过读取 <code>last-applied-configuration</code> 并将其与配置文件中的值相比较，
计算要删除的字段。
对于本地对象配置文件中显式设置为空的字段，清除其在现时配置中的设置，
无论这些字段是否出现在 <code>last-applied-configuration</code> 中。
在此例中，<code>minReadySeconds</code> 出现在 <code>last-applied-configuration</code> 注解中，但
并不存在于配置文件中。
<strong>动作：</strong> 从现时配置中删除 <code>minReadySeconds</code> 字段。</li>
<li>通过读取配置文件中的值并将其与现时配置相比较，计算要设置的字段。
在这个例子中，配置文件中的 <code>image</code> 值与现时配置中的 <code>image</code> 不匹配。
<strong>动作</strong>：设置现时配置中的 <code>image</code> 值。</li>
<li>设置 <code>last-applied-configuration</code> 注解的内容，使之与配置文件匹配。</li>
<li>将第 1、2、3 步骤得出的结果合并，构成向 API 服务器发送的补丁请求内容。</li>
</ol>
<p>下面是此合并操作之后形成的现时配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 注解中包含更新后的 image，nginx 1.11.9,</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 但不包含更新后的 replicas</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;Deployment&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;nginx-deployment&#34;,&#34;namespace&#34;:&#34;default&#34;},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;app&#34;:nginx}},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;app&#34;:&#34;nginx&#34;}},
</span><span style="color:#b44;font-style:italic">      &#34;spec&#34;:{&#34;containers&#34;:[{&#34;image&#34;:&#34;nginx:1.16.1&#34;,&#34;name&#34;:&#34;nginx&#34;,
</span><span style="color:#b44;font-style:italic">      &#34;ports&#34;:[{&#34;containerPort&#34;:80}]}]}}}}</span><span style="color:#bbb">      
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># minReadySeconds  此字段被清除</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
### How different types of fields are merged

How a particular field in a configuration file is merged with
the live configuration depends on the
type of the field. There are several types of fields:
-->
<h3 id="不同类型字段的合并方式">不同类型字段的合并方式</h3>
<p>配置文件中的特定字段与现时配置合并时，合并方式取决于字段类型。
字段类型有几种：</p>
<!--
- *primitive*: A field of type string, integer, or boolean.
  For example, `image` and `replicas` are primitive fields. **Action:** Replace.

- *map*, also called *object*: A field of type map or a complex type that contains subfields. For example, `labels`,
  `annotations`,`spec` and `metadata` are all maps. **Action:** Merge elements or subfields.

- *list*: A field containing a list of items that can be either primitive types or maps.
  For example, `containers`, `ports`, and `args` are lists. **Action:** Varies.
-->
<ul>
<li>
<p><em>基本类型</em>：字段类型为 <code>string</code>、<code>integer</code> 或 <code>boolean</code> 之一。
例如：<code>image</code> 和 <code>replicas</code> 字段都是基本类型字段。</p>
<p><strong>动作：</strong> 替换。</p>
</li>
<li>
<p><em>map</em>：也称作 <em>object</em>。类型为 <code>map</code> 或包含子域的复杂结构。例如，<code>labels</code>、
<code>annotations</code>、<code>spec</code> 和 <code>metadata</code> 都是 map。</p>
<p><strong>动作：</strong> 合并元素或子字段。</p>
</li>
<li>
<p><em>list</em>：包含元素列表的字段，其中每个元素可以是基本类型或 map。
例如，<code>containers</code>、<code>ports</code> 和 <code>args</code> 都是 list。</p>
<p><strong>动作：</strong> 不一定。</p>
</li>
</ul>
<!--
When `kubectl apply` updates a map or list field, it typically does
not replace the entire field, but instead updates the individual subelements.
For instance, when merging the `spec` on a Deployment, the entire `spec` is
not replaced. Instead the subfields of `spec`, such as `replicas`, are compared
and merged.
-->
<p>当 <code>kubectl apply</code> 更新某个 map 或 list 字段时，它通常不会替换整个字段，而是会
更新其中的各个子元素。例如，当合并 Deployment 的 <code>spec</code> 时，<code>kubectl</code> 并不会
将其整个替换掉。相反，实际操作会是对 <code>replicas</code> 这类 <code>spec</code>
的子字段来执行比较和更新。</p>
<!--
### Merging changes to primitive fields

Primitive fields are replaced or cleared.
-->
<h3 id="合并对基本类型字段的更新">合并对基本类型字段的更新</h3>
<p>基本类型字段会被替换或清除。</p>
<!--
`-` is used for "not applicable" because the value is not used.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>-</code> 表示的是“不适用”，因为指定数值未被使用。</div>
</blockquote>
<table>
<thead>
<tr>
<th>字段在对象配置文件中</th>
<th>字段在现时对象配置中</th>
<th>字段在 <code>last-applied-configuration</code> 中</th>
<th>动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>是</td>
<td>是</td>
<td>-</td>
<td>将配置文件中值设置到现时配置上。</td>
</tr>
<tr>
<td>是</td>
<td>否</td>
<td>-</td>
<td>将配置文件中值设置到现时配置上。</td>
</tr>
<tr>
<td>否</td>
<td>-</td>
<td>是</td>
<td>从现时配置中移除。</td>
</tr>
<tr>
<td>否</td>
<td>-</td>
<td>否</td>
<td>什么也不做。保持现时值。</td>
</tr>
</tbody>
</table>
<!--
### Merging changes to map fields

Fields that represent maps are merged by comparing each of the subfields or elements of the map:
-->
<h3 id="合并对-map-字段的变更">合并对 map 字段的变更</h3>
<p>用来表示映射的字段在合并时会逐个子字段或元素地比较：</p>
<!--
`-` is used for "not applicable" because the value is not used.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>-</code> 表示的是“不适用”，因为指定数值未被使用。</div>
</blockquote>
<table>
<thead>
<tr>
<th>键存在于对象配置文件中</th>
<th>键存在于现时对象配置中</th>
<th>键存在于 <code>last-applied-configuration</code> 中</th>
<th>动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>是</td>
<td>是</td>
<td>-</td>
<td>比较子域取值。</td>
</tr>
<tr>
<td>是</td>
<td>否</td>
<td>-</td>
<td>将现时配置设置为本地配置值。</td>
</tr>
<tr>
<td>否</td>
<td>-</td>
<td>是</td>
<td>从现时配置中删除键。</td>
</tr>
<tr>
<td>否</td>
<td>-</td>
<td>否</td>
<td>什么也不做，保留现时值。</td>
</tr>
</tbody>
</table>
<!--
### Merging changes for fields of type list

Merging changes to a list uses one of three strategies:

* Replace the list if all its elements are primitives.
* Merge individual elements in a list of complex elements.
* Merge a list of primitive elements.

The choice of strategy is made on a per-field basis.
-->
<h3 id="合并-list-类型字段的变更">合并 list 类型字段的变更</h3>
<p>对 list 类型字段的变更合并会使用以下三种策略之一：</p>
<ul>
<li>如果 list 所有元素都是基本类型则替换整个 list。</li>
<li>如果 list 中元素是复合结构则逐个元素执行合并操作。</li>
<li>合并基本类型元素构成的 list。</li>
</ul>
<p>策略的选择是基于各个字段做出的。</p>
<!--
#### Replace the list if all its elements are primitives

Treat the list the same as a primitive field. Replace or delete the
entire list. This preserves ordering.
-->
<h4 id="如果-list-中元素都是基本类型则替换整个-list">如果 list 中元素都是基本类型则替换整个 list</h4>
<p>将整个 list 视为一个基本类型字段。或者整个替换或者整个删除。
此操作会保持 list 中元素顺序不变</p>
<!--
**Example:** Use `kubectl apply` to update the `args` field of a Container in a Pod. This sets
the value of `args` in the live configuration to the value in the configuration file.
Any `args` elements that had previously been added to the live configuration are lost.
The order of the `args` elements defined in the configuration file is
retained in the live configuration.
-->
<p><strong>示例：</strong> 使用 <code>kubectl apply</code> 来更新 Pod 中 Container 的 <code>args</code> 字段。此操作会
将现时配置中的 <code>args</code> 值设为配置文件中的值。
所有之前添加到现时配置中的 <code>args</code> 元素都会丢失。
配置文件中的 <code>args</code> 元素的顺序在被添加到现时配置中时保持不变。</p>
<!--
```yaml
# last-applied-configuration value
    args: ["a", "b"]

# configuration file value
    args: ["a", "c"]

# live configuration
    args: ["a", "b", "d"]

# result after merge
    args: ["a", "c"]
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># last-applied-configuration 值</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;a&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;b&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 配置文件值</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;a&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;c&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 现时配置</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;a&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;b&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;d&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 合并结果</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;a&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;c&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
**Explanation:** The merge used the configuration file value as the new list value.
-->
<p><strong>解释：</strong> 合并操作将配置文件中的值当做新的 list 值。</p>
<!--
#### Merge individual elements of a list of complex elements:

Treat the list as a map, and treat a specific field of each element as a key.
Add, delete, or update individual elements. This does not preserve ordering.
-->
<h4 id="如果-list-中元素为复合类型则逐个执行合并">如果 list 中元素为复合类型则逐个执行合并</h4>
<p>此操作将 list 视为 map，并将每个元素中的特定字段当做其主键。
逐个元素地执行添加、删除或更新操作。结果顺序无法得到保证。</p>
<!--
This merge strategy uses a special tag on each field called a `patchMergeKey`. The
`patchMergeKey` is defined for each field in the Kubernetes source code:
[types.go](https://github.com/kubernetes/api/blob/d04500c8c3dda9c980b668c57abc2ca61efcf5c4/core/v1/types.go#L2747)
When merging a list of maps, the field specified as the `patchMergeKey` for a given element
is used like a map key for that element.

**Example:** Use `kubectl apply` to update the `containers` field of a PodSpec.
This merges the list as though it was a map where each element is keyed
by `name`.
-->
<p>此合并策略会使用每个字段上的一个名为 <code>patchMergeKey</code> 的特殊标签。
Kubernetes 源代码中为每个字段定义了 <code>patchMergeKey</code>：
<a href="https://github.com/kubernetes/api/blob/d04500c8c3dda9c980b668c57abc2ca61efcf5c4/core/v1/types.go#L2747">types.go</a>
当合并由 map 组成的 list 时，给定元素中被设置为 <code>patchMergeKey</code> 的字段会被
当做该元素的 map 键值来使用。</p>
<p><strong>例如：</strong> 使用 <code>kubectl apply</code> 来更新 Pod 规约中的 <code>containers</code> 字段。
此操作会将 <code>containers</code> 列表视作一个映射来执行合并，每个元素的主键为 <code>name</code>。</p>
<!--
```yaml
# last-applied-configuration value
    containers:
    - name: nginx
      image: nginx:1.16
    - name: nginx-helper-a # key: nginx-helper-a; will be deleted in result
      image: helper:1.3
    - name: nginx-helper-b # key: nginx-helper-b; will be retained
      image: helper:1.3

# configuration file value
    containers:
    - name: nginx
      image: nginx:1.16
    - name: nginx-helper-b
      image: helper:1.3
    - name: nginx-helper-c # key: nginx-helper-c; will be added in result
      image: helper:1.3

# live configuration
    containers:
    - name: nginx
      image: nginx:1.16
    - name: nginx-helper-a
      image: helper:1.3
    - name: nginx-helper-b
      image: helper:1.3
      args: ["run"] # Field will be retained
    - name: nginx-helper-d # key: nginx-helper-d; will be retained
      image: helper:1.3

# result after merge
    containers:
    - name: nginx
      image: nginx:1.16
      # Element nginx-helper-a was deleted
    - name: nginx-helper-b
      image: helper:1.3
      args: ["run"] # Field was retained
    - name: nginx-helper-c # Element was added
      image: helper:1.3
    - name: nginx-helper-d # Element was ignored
      image: helper:1.3
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># last-applied-configuration 值</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-a<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 键 nginx-helper-a 会被删除</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-b<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 键 nginx-helper-b 会被保留</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 配置文件值</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-b<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-c<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 键 nginx-helper-c 会被添加</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 现时配置</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-a<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-b<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;run&#34;</span>]<span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 字段会被保留</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-d<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 键 nginx-helper-d 会被保留</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 合并结果</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 元素 nginx-helper-a 被删除</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-b<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;run&#34;</span>]<span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># 字段被保留</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-c<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 新增元素</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-helper-d<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 此元素被忽略（保留）</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>helper:1.3<span style="color:#bbb">
</span></code></pre></div><!--
**Explanation:**

- The container named "nginx-helper-a" was deleted because no container
  named "nginx-helper-a" appeared in the configuration file.
- The container named "nginx-helper-b" retained the changes to `args`
  in the live configuration. `kubectl apply` was able to identify
  that "nginx-helper-b" in the live configuration was the same
  "nginx-helper-b" as in the configuration file, even though their fields
  had different values (no `args` in the configuration file). This is
  because the `patchMergeKey` field value (name) was identical in both.
- The container named "nginx-helper-c" was added because no container
  with that name appeared in the live configuration, but one with
  that name appeared in the configuration file.
- The container named "nginx-helper-d" was retained because
  no element with that name appeared in the last-applied-configuration.
-->
<p><strong>解释：</strong></p>
<ul>
<li>名为 &quot;nginx-helper-a&quot; 的容器被删除，因为配置文件中不存在同名的容器。</li>
<li>名为 &quot;nginx-helper-b&quot; 的容器的现时配置中的 <code>args</code> 被保留。
<code>kubectl apply</code> 能够辩识出现时配置中的容器 &quot;nginx-helper-b&quot; 与配置文件
中的容器 &quot;nginx-helper-b&quot; 相同，即使它们的字段值有些不同（配置文件中未给定
<code>args</code> 值）。这是因为 <code>patchMergeKey</code> 字段（name）的值在两个版本中都一样。</li>
<li>名为 &quot;nginx-helper-c&quot; 的容器是新增的，因为在配置文件中的这个容器尚不存在
于现时配置中。</li>
<li>名为 &quot;nginx-helper-d&quot; 的容器被保留下来，因为在 last-applied-configuration
中没有与之同名的元素。</li>
</ul>
<!--
#### Merge a list of primitive elements

As of Kubernetes 1.5, merging lists of primitive elements is not supported.
-->
<h4 id="合并基本类型元素-list">合并基本类型元素 list</h4>
<p>在 Kubernetes 1.5 中，尚不支持对由基本类型元素构成的 list 进行合并。</p>
<!--
Which of the above strategies is chosen for a given field is controlled by
the `patchStrategy` tag in [types.go](https://github.com/kubernetes/api/blob/d04500c8c3dda9c980b668c57abc2ca61efcf5c4/core/v1/types.go#L2748)
If no `patchStrategy` is specified for a field of type list, then
the list is replaced.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 选择上述哪种策略是由源码中给定字段的 <code>patchStrategy</code> 标记来控制的：
<a href="https://github.com/kubernetes/api/blob/d04500c8c3dda9c980b668c57abc2ca61efcf5c4/core/v1/types.go#L2748">types.go</a>
如果 list 类型字段未设置 <code>patchStrategy</code>，则整个 list 会被替换掉。</div>
</blockquote>

<!--
## Default field values

The API server sets certain fields to default values in the live configuration if they are
not specified when the object is created.

Here's a configuration file for a Deployment. The file does not specify `strategy`:
-->
<h2 id="default-field-values">默认字段值 </h2>
<p>API 服务器会在对象创建时其中某些字段未设置的情况下在现时配置中为其设置默认值。</p>
<p>下面是一个 Deployment 的配置文件。文件未设置 <code>strategy</code>：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/simple_deployment.yaml" download="application/simple_deployment.yaml"><code>application/simple_deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-simple-deployment-yaml')" title="Copy application/simple_deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-simple-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the object using `kubectl apply`:
-->
<p>使用 <code>kubectl apply</code> 创建对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml
</code></pre></div><!--
Print the live configuration using `kubectl get`:
-->
<p>使用 <code>kubectl get</code> 打印现时配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml
</code></pre></div><!--
The output shows that the API server set several fields to default values in the live
configuration. These fields were not specified in the configuration file.
-->
<p>输出显示 API 在现时配置中为某些字段设置了默认值。
这些字段在配置文件中并未设置。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReadySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">           </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rollingUpdate</span>:<span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># API 服务器基于 strategy.type 所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxSurge</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent   <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP      <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">         </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">terminationMessagePath</span>:<span style="color:#bbb"> </span>/dev/termination-log   <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">dnsPolicy</span>:<span style="color:#bbb"> </span>ClusterFirst      <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Always        <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">           </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># API 服务器所设默认值</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># ...</span><span style="color:#bbb">
</span></code></pre></div><!--
In a patch request, defaulted fields are not re-defaulted unless they are explicitly cleared
as part of a patch request. This can cause unexpected behavior for
fields that are defaulted based
on the values of other fields. When the other fields are later changed,
the values defaulted from them will not be updated unless they are
explicitly cleared.
-->
<p>在补丁请求中，已经设置了默认值的字段不会被重新设回其默认值，除非
在补丁请求中显式地要求清除。对于默认值取决于其他字段的某些字段而言，
这可能会引发一些意想不到的行为。当所依赖的其他字段后来发生改变时，
基于它们所设置的默认值只能在显式执行清除操作时才会被更新。</p>
<!--
For this reason, it is recommended that certain fields defaulted
by the server are explicitly defined in the configuration file, even
if the desired values match the server defaults. This makes it
easier to recognize conflicting values that will not be re-defaulted
by the server.

**Example:**
-->
<p>为此，建议在配置文件中为服务器设置默认值的字段显式提供定义，即使所
给的定义与服务器端默认值设定相同。这样可以使得辩识无法被服务器重新
基于默认值来设置的冲突字段变得容易。</p>
<p><strong>示例：</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># last-applied-configuration</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 配置文件</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate  <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 更新的值</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 现时配置</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RollingUpdate   <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 默认设置的值</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rollingUpdate</span>:<span style="color:#bbb">         </span><span style="color:#080;font-style:italic"># 基于 type 设置的默认值</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxSurge </span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 合并后的结果 - 出错！</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate    <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># 更新的值：与 rollingUpdate 不兼容</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rollingUpdate</span>:<span style="color:#bbb">     </span><span style="color:#080;font-style:italic"># 默认设置的值：与 &#34;type: Recreate&#34; 冲突</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxSurge </span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div><!--
**Explanation:**

1. The user creates a Deployment without defining `strategy.type`.
2. The server defaults `strategy.type` to `RollingUpdate` and defaults the
   `strategy.rollingUpdate` values.
3. The user changes `strategy.type` to `Recreate`. The `strategy.rollingUpdate`
   values remain at their defaulted values, though the server expects them to be cleared.
   If the `strategy.rollingUpdate` values had been defined initially in the configuration file,
   it would have been more clear that they needed to be deleted.
4. Apply fails because `strategy.rollingUpdate` is not cleared. The `strategy.rollingupdate`
   field cannot be defined with a `strategy.type` of `Recreate`.
-->
<p><strong>解释：</strong></p>
<ol>
<li>用户创建 Deployment，未设置 <code>strategy.type</code>。</li>
<li>服务器为 <code>strategy.type</code> 设置默认值 <code>RollingUpdate</code>，并为 <code>strategy.rollingUpdate</code>
设置默认值。</li>
<li>用户改变 <code>strategy.type</code> 为 <code>Recreate</code>。字段 <code>strategy.rollingUpdate</code> 仍会取其
默认设置值，尽管服务器期望该字段被清除。
如果 <code>strategy.rollingUpdate</code> 值最初于配置文件中定义，则它们需要被清除
这一点就更明确一些。</li>
<li><code>apply</code> 操作失败，因为 <code>strategy.rollingUpdate</code> 未被清除。
<code>strategy.rollingupdate</code> 在 <code>strategy.type</code> 为 <code>Recreate</code> 不可被设定。</li>
</ol>
<!--
Recommendation: These fields should be explicitly defined in the object configuration file:

- Selectors and PodTemplate labels on workloads, such as Deployment, StatefulSet, Job, DaemonSet,
  ReplicaSet, and ReplicationController
- Deployment rollout strategy
-->
<p>建议：以下字段应该在对象配置文件中显式定义：</p>
<ul>
<li>如 Deployment、StatefulSet、Job、DaemonSet、ReplicaSet 和 ReplicationController
这类负载的选择算符和 <code>PodTemplate</code> 标签</li>
<li>Deployment 的上线策略</li>
</ul>
<!--
### How to clear server-defaulted fields or fields set by other writers

Fields that do not appear in the configuration file can be cleared by
setting their values to `null` and then applying the configuration file.
For fields defaulted by the server, this triggers re-defaulting
the values.
-->
<h3 id="如何清除服务器端按默认值设置的字段或者被其他写者设置的字段">如何清除服务器端按默认值设置的字段或者被其他写者设置的字段</h3>
<p>没有出现在配置文件中的字段可以通过将其值设置为 <code>null</code> 并应用配置文件来清除。
对于由服务器按默认值设置的字段，清除操作会触发重新为字段设置新的默认值。</p>
<!--
## How to change ownership of a field between the configuration file and direct imperative writers

These are the only methods you should use to change an individual object field:

- Use `kubectl apply`.
- Write directly to the live configuration without modifying the configuration file:
for example, use `kubectl scale`.
-->
<h2 id="如何将字段的属主在配置文件和直接指令式写者之间切换">如何将字段的属主在配置文件和直接指令式写者之间切换</h2>
<p>更改某个对象字段时，应该采用下面的方法：</p>
<ul>
<li>使用 <code>kubectl apply</code>.</li>
<li>直接写入到现时配置，但不更改配置文件本身，例如使用 <code>kubectl scale</code>。</li>
</ul>
<!--
### Changing the owner from a direct imperative writer to a configuration file

Add the field to the configuration file. For the field, discontinue direct updates to
the live configuration that do not go through `kubectl apply`.
-->
<h3 id="将属主从直接指令式写者更改为配置文件">将属主从直接指令式写者更改为配置文件</h3>
<p>将字段添加到配置文件。针对该字段，不再直接执行对现时配置的修改。
修改均通过 <code>kubectl apply</code> 来执行。</p>
<!--
### Changing the owner from a configuration file to a direct imperative writer

As of Kubernetes 1.5, changing ownership of a field from a configuration file to
an imperative writer requires manual steps:

- Remove the field from the configuration file.
- Remove the field from the `kubectl.kubernetes.io/last-applied-configuration` annotation on the live object.
-->
<h3 id="将属主从配置文件改为直接指令式写者">将属主从配置文件改为直接指令式写者</h3>
<p>在 Kubernetes 1.5 中，将字段的属主从配置文件切换到某指令式写者需要手动
执行以下步骤：</p>
<ul>
<li>从配置文件中删除该字段；</li>
<li>将字段从现时对象的 <code>kubectl.kubernetes.io/last-applied-configuration</code> 注解
中删除。</li>
</ul>
<!--
## Changing management methods

Kubernetes objects should be managed using only one method at a time.
Switching from one method to another is possible, but is a manual process.
-->
<h2 id="changing-management-methods">更改管理方法 </h2>
<p>Kubernetes 对象在同一时刻应该只用一种方法来管理。
从一种方法切换到另一种方法是可能的，但这一切换是一个手动过程。</p>
<!--
It is OK to use imperative deletion with declarative management.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 在声明式管理方法中使用指令式命令来删除对象是可以的。</div>
</blockquote>
<!--
### Migrating from imperative command management to declarative object configuration

Migrating from imperative command management to declarative object
configuration involves several manual steps:
-->
<h3 id="从指令式命令管理切换到声明式对象配置">从指令式命令管理切换到声明式对象配置</h3>
<p>从指令式命令管理切换到声明式对象配置管理的切换包含以下几个手动步骤：</p>
<!--
1. Export the live object to a local configuration file:

     ```shell
     kubectl get <kind>/<name> -o yaml > <kind>_<name>.yaml
     ```

1. Manually remove the `status` field from the configuration file.

    <blockquote class="note callout">
  <div><strong>说明：</strong> This step is optional, as <code>kubectl apply</code> does not update the status field
even if it is present in the configuration file.</div>
</blockquote>

1. Set the `kubectl.kubernetes.io/last-applied-configuration` annotation on the object:

    ```shell
    kubectl replace -save-config -f <kind>_<name>.yaml
    ```

1. Change processes to use `kubectl apply` for managing the object exclusively.
-->
<ol>
<li>
<p>将现时对象导出到本地配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get &lt;kind&gt;/&lt;name&gt; -o yaml &gt; &lt;kind&gt;_&lt;name&gt;.yaml
</code></pre></div></li>
<li>
<p>手动移除配置文件中的 <code>status</code> 字段。</p>
<!--
This step is optional, as `kubectl apply` does not update the status field
even if it is present in the configuration file.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 这一步骤是可选的，因为 <code>kubectl apply</code> 并不会更新 status 字段，即便
配置文件中包含 status 字段。</div>
</blockquote>
</li>
<li>
<p>设置对象上的 <code>kubectl.kubernetes.io/last-applied-configuration</code> 注解：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl replace --save-config -f &lt;kind&gt;_&lt;name&gt;.yaml
</code></pre></div></li>
<li>
<p>更改过程，使用 <code>kubectl apply</code> 专门管理对象。</p>
</li>
</ol>
<!--
### Migrating from imperative object configuration to declarative object configuration

1. Set the `kubectl.kubernetes.io/last-applied-configuration` annotation on the object:

    ```shell
    kubectl replace -save-config -f <kind>_<name>.yaml
    ```

1. Change processes to use `kubectl apply` for managing the object exclusively.
-->
<h3 id="从指令式对象配置切换到声明式对象配置">从指令式对象配置切换到声明式对象配置</h3>
<ol>
<li>
<p>在对象上设置 <code>kubectl.kubernetes.io/last-applied-configuration</code> 注解：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl replace -save-config -f &lt;kind&gt;_&lt;name&gt;.yaml
</code></pre></div></li>
<li>
<p>自此排他性地使用 <code>kubectl apply</code> 来管理对象。</p>
</li>
</ol>
<!--
## Defining controller selectors and PodTemplate labels
-->
<h2 id="定义控制器选择算符和-podtemplate-标签">定义控制器选择算符和 PodTemplate 标签</h2>
<!--
Updating selectors on controllers is strongly discouraged.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 强烈不建议更改控制器上的选择算符。</div>
</blockquote>

<!--
The recommended approach is to define a single, immutable PodTemplate label
used only by the controller selector with no other semantic meaning.

**Example:**
-->
<p>建议的方法是定义一个不可变更的 PodTemplate 标签，仅用于控制器选择算符且
不包含其他语义性的含义。</p>
<p><strong>示例：</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-selector</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;apps/v1/deployment/nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-selector</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;apps/v1/deployment/nginx&#34;</span><span style="color:#bbb">
</span></code></pre></div><h2 id="接下来">接下来</h2>
<!--
* [Managing Kubernetes Objects Using Imperative Commands](/docs/tasks/manage-kubernetes-objects/imperative-command/)
* [Imperative Management of Kubernetes Objects Using Configuration Files](/docs/tasks/manage-kubernetes-objects/imperative-config/)
* [Kubectl Command Reference](/docs/reference/generated/kubectl/kubectl-commands/)
* [Kubernetes API Reference](/docs/reference/generated/kubernetes-api/v1.22/)
-->
<ul>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-command/">使用指令式命令管理 Kubernetes 对象</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-config/">使用配置文件对 Kubernetes 对象执行指令式管理</a></li>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl 命令参考</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/">Kubernetes API 参考</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-11aa6950fcb203094823c8e2cbdd517f">4.2 - 使用 Kustomize 对 Kubernetes 对象进行声明式管理</h1>
    
	<!--
title: Declarative Management of Kubernetes Objects Using Kustomize
content_type: task
weight: 20
-->
<!-- overview -->
<!--
[Kustomize](https://github.com/kubernetes-sigs/kustomize) is a standalone tool
to customize Kubernetes objects
through a [kustomization file](https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#kustomization).
-->
<p><a href="https://github.com/kubernetes-sigs/kustomize">Kustomize</a> 是一个独立的工具，用来通过
<a href="https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#kustomization">kustomization 文件</a>
定制 Kubernetes 对象。</p>
<!--
Since 1.14, Kubectl also
supports the management of Kubernetes objects using a kustomization file.
To view Resources found in a directory containing a kustomization file, run the following command:
-->
<p>从 1.14 版本开始，<code>kubectl</code> 也开始支持使用 kustomization 文件来管理 Kubernetes 对象。
要查看包含 kustomization 文件的目录中的资源，执行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl kustomize &lt;kustomization_directory&gt;
</code></pre></div><!--
To apply those Resources, run `kubectl apply` with `- -kustomize` or `-k` flag:
-->
<p>要应用这些资源，使用参数 <code>--kustomize</code> 或 <code>-k</code> 标志来执行 <code>kubectl apply</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k &lt;kustomization_directory&gt;
</code></pre></div><h2 id="准备开始">准备开始</h2>
<!--
Install [`kubectl`](/docs/tasks/tools/).
-->
<p>安装 <a href="/zh/docs/tasks/tools/"><code>kubectl</code></a>.</p>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Overview of Kustomize

Kustomize is a tool for customizing Kubernetes configurations. It has the following features to manage application configuration files:

* generating resources from other sources
* setting cross-cutting fields for resources
* composing and customizing collections of resources
-->
<h2 id="overview-of-kustomize">Kustomize 概述   </h2>
<p>Kustomize 是一个用来定制 Kubernetes 配置的工具。它提供以下功能特性来管理
应用配置文件：</p>
<ul>
<li>从其他来源生成资源</li>
<li>为资源设置贯穿性（Cross-Cutting）字段</li>
<li>组织和定制资源集合</li>
</ul>
<!--
### Generating Resources

ConfigMaps and Secrets hold configuration or sensitive data that are used by other Kubernetes objects, such as Pods. The source of truth of ConfigMaps or Secrets are usually external to a cluster, such as a `.properties` file or an SSH keyfile.
Kustomize has `secretGenerator` and `configMapGenerator`, which generate Secret and ConfigMap from files or literals.
-->
<h3 id="generating-resources">生成资源  </h3>
<p>ConfigMap 和 Secret 包含其他 Kubernetes 对象（如 Pod）所需要的配置或敏感数据。
ConfigMap 或 Secret 中数据的来源往往是集群外部，例如某个 <code>.properties</code>
文件或者 SSH 密钥文件。
Kustomize 提供 <code>secretGenerator</code> 和 <code>configMapGenerator</code>，可以基于文件或字面
值来生成 Secret 和 ConfigMap。</p>
<!--
#### configMapGenerator

To generate a ConfigMap from a file, add an entry to the `files` list in `configMapGenerator`. Here is an example of generating a ConfigMap with a data item from a `.properties` file:
-->
<h4 id="configmapgenerator">configMapGenerator</h4>
<p>要基于文件来生成 ConfigMap，可以在 <code>configMapGenerator</code> 的 <code>files</code>
列表中添加表项。
下面是一个根据 <code>.properties</code> 文件中的数据条目来生成 ConfigMap 的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 生成一个  application.properties 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;application.properties
</span><span style="color:#b44">FOO=Bar
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">configMapGenerator:
</span><span style="color:#b44">- name: example-configmap-1
</span><span style="color:#b44">  files:
</span><span style="color:#b44">  - application.properties
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
The generated ConfigMap can be examined with the following command:
-->
<p>所生成的 ConfigMap 可以使用下面的命令来检查：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl kustomize ./
</code></pre></div><!--
The generated ConfigMap is:
-->
<p>所生成的 ConfigMap 为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">application.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>FOO=Bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-1-8mbdf7882g<span style="color:#bbb">
</span></code></pre></div><!--
To generate a ConfigMap from an env file, add an entry to the `envs` list in `configMapGenerator`. Here is an example of generating a ConfigMap with a data item from a `.env` file:
-->
<p>要从 env 文件生成 ConfigMap，请在 <code>configMapGenerator</code> 中的 <code>envs</code> 列表中添加一个条目。
下面是一个用来自 <code>.env</code> 文件的数据生成 ConfigMap 的例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个 .env 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;.env
</span><span style="color:#b44">FOO=Bar
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">configMapGenerator:
</span><span style="color:#b44">- name: example-configmap-1
</span><span style="color:#b44">  envs:
</span><span style="color:#b44">  - .env
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
The generated ConfigMap can be examined with the following command:
-->
<p>可以使用以下命令检查生成的 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl kustomize ./
</code></pre></div><!--
The generated ConfigMap is:
-->
<p>生成的 ConfigMap 为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>FOO=Bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-1-8mbdf7882g<span style="color:#bbb">
</span></code></pre></div><!--
Each variable in the `.env` file becomes a separate key in the ConfigMap that you generate. This is different from the previous example which embeds a file named `.properties` (and all its entries) as the value for a single key.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>.env</code> 文件中的每个变量在生成的 ConfigMap 中成为一个单独的键。
这与之前的示例不同，前一个示例将一个名为 <code>.properties</code> 的文件（及其所有条目）嵌入到同一个键的值中。</div>
</blockquote>
<!--
ConfigMaps can also be generated from literal key-value pairs. To generate a ConfigMap from a literal key-value pair, add an entry to the `literals` list in configMapGenerator. Here is an example of generating a ConfigMap with a data item from a key-value pair:
-->
<p>ConfigMap 也可基于字面的键值偶对来生成。要基于键值偶对来生成 ConfigMap，
在 <code>configMapGenerator</code> 的 <code>literals</code> 列表中添加表项。下面是一个例子，展示
如何使用键值偶对中的数据条目来生成 ConfigMap 对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">configMapGenerator:
</span><span style="color:#b44">- name: example-configmap-2
</span><span style="color:#b44">  literals:
</span><span style="color:#b44">  - FOO=Bar
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
The generated ConfigMap can be checked by the following command:
-->
<p>可以用下面的命令检查所生成的 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl kustomize ./
</code></pre></div><!--
The generated ConfigMap is:
-->
<p>所生成的 ConfigMap 为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">FOO</span>:<span style="color:#bbb"> </span>Bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-2-g2hdhfc6tk<span style="color:#bbb">
</span></code></pre></div><!--
To use a generated ConfigMap in a Deployment, reference it by the name of the configMapGenerator. Kustomize will automatically replace this name with the generated name.

This is an example deployment that uses a generated ConfigMap:
-->
<p>要在 Deployment 中使用生成的 ConfigMap，使用 configMapGenerator 的名称对其进行引用。
Kustomize 将自动使用生成的名称替换该名称。</p>
<p>这是使用生成的 ConfigMap 的 deployment 示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 创建一个 application.properties 文件</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>cat &lt;&lt;EOF &gt;application.properties<span style="color:#bbb">
</span><span style="color:#bbb"></span>FOO=Bar<span style="color:#bbb">
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>cat &lt;&lt;EOF &gt;deployment.yaml<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/config<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-1<span style="color:#bbb">
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>cat &lt;&lt;EOF &gt;./kustomization.yaml<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- deployment.yaml<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">configMapGenerator</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">files</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- application.properties<span style="color:#bbb">
</span><span style="color:#bbb"></span>EOF<span style="color:#bbb">
</span></code></pre></div><!--
Generate the ConfigMap and Deployment:
-->
<p>生成 ConfigMap 和 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl kustomize ./
</code></pre></div><!--
The generated Deployment will refer to the generated ConfigMap by name:
-->
<p>生成的 Deployment 将通过名称引用生成的 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">application.properties</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    </span><span style="color:#bbb">    </span>FOO=Bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-1-g4hk9g2ff8<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-app<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/config<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-1-g4hk9g2ff8<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb">
</span></code></pre></div><h4 id="secretgenerator">secretGenerator</h4>
<!--
You can generate Secrets from files or literal key-value pairs. To generate a Secret from a file, add an entry to the `files` list in `secretGenerator`. Here is an example of generating a Secret with a data item from a file:
-->
<p>你可以基于文件或者键值偶对来生成 Secret。要使用文件内容来生成 Secret，
在 <code>secretGenerator</code> 下面的 <code>files</code> 列表中添加表项。
下面是一个根据文件中数据来生成 Secret 对象的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个 password.txt 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./password.txt
</span><span style="color:#b44">username=admin
</span><span style="color:#b44">password=secret
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">secretGenerator:
</span><span style="color:#b44">- name: example-secret-1
</span><span style="color:#b44">  files:
</span><span style="color:#b44">  - password.txt
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
The generated Secret is as follows:
-->
<p>所生成的 Secret 如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">password.txt</span>:<span style="color:#bbb"> </span>dXNlcm5hbWU9YWRtaW4KcGFzc3dvcmQ9c2VjcmV0Cg==<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-secret-1-t2kt65hgtb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span></code></pre></div><!--
To generate a Secret from a literal key-value pair, add an entry to `literals` list in `secretGenerator`. Here is an example of generating a Secret with a data item from a key-value pair:
-->
<p>要基于键值偶对字面值生成 Secret，先要在 <code>secretGenerator</code> 的 <code>literals</code>
列表中添加表项。下面是基于键值偶对中数据条目来生成 Secret 的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">secretGenerator:
</span><span style="color:#b44">- name: example-secret-2
</span><span style="color:#b44">  literals:
</span><span style="color:#b44">  - username=admin
</span><span style="color:#b44">  - password=secret
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
The generated Secret is as follows:
-->
<p>所生成的 Secret 如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>c2VjcmV0<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>YWRtaW4=<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-secret-2-t52t6g96d8<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span></code></pre></div><!--
Like ConfigMaps, generated Secrets can be used in Deployments by refering to the name of the secretGenerator:
-->
<p>与 ConfigMaps 一样，生成的 Secrets 可以通过引用 secretGenerator 的名称在部署中使用：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个 password.txt 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./password.txt
</span><span style="color:#b44">username=admin
</span><span style="color:#b44">password=secret
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-app
</span><span style="color:#b44">  labels:
</span><span style="color:#b44">    app: my-app
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      app: my-app
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        app: my-app
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: app
</span><span style="color:#b44">        image: my-app
</span><span style="color:#b44">        volumeMounts:
</span><span style="color:#b44">        - name: password
</span><span style="color:#b44">          mountPath: /secrets
</span><span style="color:#b44">      volumes:
</span><span style="color:#b44">      - name: password
</span><span style="color:#b44">        secret:
</span><span style="color:#b44">          secretName: example-secret-1
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">secretGenerator:
</span><span style="color:#b44">- name: example-secret-1
</span><span style="color:#b44">  files:
</span><span style="color:#b44">  - password.txt
</span><span style="color:#b44">EOF</span>
</code></pre></div><h4 id="generatoroptions">generatorOptions</h4>
<!--
The generated ConfigMaps and Secrets have a content hash suffix appended. This ensures that a new ConfigMap or Secret is generated when the contents are changed. To disable the behavior of appending a suffix, one can use `generatorOptions`. Besides that, it is also possible to specify cross-cutting options for generated ConfigMaps and Secrets.
-->
<p>所生成的 ConfigMap 和 Secret 都会包含内容哈希值后缀。
这是为了确保内容发生变化时，所生成的是新的 ConfigMap 或 Secret。
要禁止自动添加后缀的行为，用户可以使用 <code>generatorOptions</code>。
除此以外，为生成的 ConfigMap 和 Secret 指定贯穿性选项也是可以的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">configMapGenerator:
</span><span style="color:#b44">- name: example-configmap-3
</span><span style="color:#b44">  literals:
</span><span style="color:#b44">  - FOO=Bar
</span><span style="color:#b44">generatorOptions:
</span><span style="color:#b44">  disableNameSuffixHash: true
</span><span style="color:#b44">  labels:
</span><span style="color:#b44">    type: generated
</span><span style="color:#b44">  annotations:
</span><span style="color:#b44">    note: generated
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Run`kubectl kustomize ./` to view the generated ConfigMap:
-->
<p>运行 <code>kubectl kustomize ./</code> 来查看所生成的 ConfigMap：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">FOO</span>:<span style="color:#bbb"> </span>Bar<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">note</span>:<span style="color:#bbb"> </span>generated<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>generated<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-configmap-3<span style="color:#bbb">
</span></code></pre></div><!--
### Setting cross-cutting fields

It is quite common to set cross-cutting fields for all Kubernetes resources in a project.
Some use cases for setting cross-cutting fields:

* setting the same namespace for all Resources
* adding the same name prefix or suffix
* adding the same set of labels
* adding the same set of annotations

Here is an example:
-->
<h3 id="setting-cross-cutting-fields">设置贯穿性字段 </h3>
<p>在项目中为所有 Kubernetes 对象设置贯穿性字段是一种常见操作。
贯穿性字段的一些使用场景如下：</p>
<ul>
<li>为所有资源设置相同的名字空间</li>
<li>为所有对象添加相同的前缀或后缀</li>
<li>为对象添加相同的标签集合</li>
<li>为对象添加相同的注解集合</li>
</ul>
<p>下面是一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个 deployment.yaml</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: nginx-deployment
</span><span style="color:#b44">  labels:
</span><span style="color:#b44">    app: nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      app: nginx
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        app: nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">namespace: my-namespace
</span><span style="color:#b44">namePrefix: dev-
</span><span style="color:#b44">nameSuffix: &#34;-001&#34;
</span><span style="color:#b44">commonLabels:
</span><span style="color:#b44">  app: bingo
</span><span style="color:#b44">commonAnnotations:
</span><span style="color:#b44">  oncallPager: 800-555-1212
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Run `kubectl kustomize ./` to view those fields are all set in the Deployment Resource:
-->
<p>执行 <code>kubectl kustomize ./</code> 查看这些字段都被设置到 Deployment 资源上：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">oncallPager</span>:<span style="color:#bbb"> </span><span style="color:#666">800-555-1212</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>bingo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-nginx-deployment-001<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>my-namespace<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>bingo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">oncallPager</span>:<span style="color:#bbb"> </span><span style="color:#666">800-555-1212</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>bingo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div><!--
### Composing and Customizing Resources

It is common to compose a set of Resources in a project and manage them inside
the same file or directory.
Kustomize offers composing Resources from different files and applying patches or other customization to them.
-->
<h3 id="composing-and-customizing-resources">组织和定制资源   </h3>
<p>一种常见的做法是在项目中构造资源集合并将其放到同一个文件或目录中管理。
Kustomize 提供基于不同文件来组织资源并向其应用补丁或者其他定制的能力。</p>
<!--
#### Composing

Kustomize supports composition of different resources. The `resources` field, in the `kustomization.yaml` file, defines the list of resources to include in a configuration. Set the path to a resource's configuration file in the `resources` list.
Here is an example of an NGINX application comprised of a Deployment and a Service:
-->
<h4 id="composing">组织   </h4>
<p>Kustomize 支持组合不同的资源。<code>kustomization.yaml</code> 文件的 <code>resources</code> 字段
定义配置中要包含的资源列表。你可以将 <code>resources</code> 列表中的路径设置为资源配置文件
的路径。下面是由 Deployment 和 Service 构成的 NGINX 应用的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建 deployment.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      run: my-nginx
</span><span style="color:#b44">  replicas: 2
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        run: my-nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">        ports:
</span><span style="color:#b44">        - containerPort: 80
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建 service.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; service.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Service
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">  labels:
</span><span style="color:#b44">    run: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  ports:
</span><span style="color:#b44">  - port: 80
</span><span style="color:#b44">    protocol: TCP
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    run: my-nginx
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建 kustomization.yaml 来组织以上两个资源</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">- service.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
The Resources from `kubectl kustomize ./` contain both the Deployment and the Service objects.
-->
<p><code>kubectl kustomize ./</code> 所得到的资源中既包含 Deployment 也包含 Service 对象。</p>
<!--
#### Customizing

Patches can be used to apply different customizations to Resources. Kustomize supports different patching
mechanisms through `patchesStrategicMerge` and `patchesJson6902`. `patchesStrategicMerge` is a list of file paths. Each file should be resolved to a [strategic merge patch](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md). The names inside the patches must match Resource names that are already loaded. Small patches that do one thing are recommended. For example, create one patch for increasing the deployment replica number and another patch for setting the memory limit.
-->
<h4 id="customizing">定制  </h4>
<p>补丁文件（Patches）可以用来对资源执行不同的定制。
Kustomize 通过 <code>patchesStrategicMerge</code> 和 <code>patchesJson6902</code> 支持不同的打补丁
机制。<code>patchesStrategicMerge</code> 的内容是一个文件路径的列表，其中每个文件都应可解析为
<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md">策略性合并补丁（Strategic Merge Patch）</a>。
补丁文件中的名称必须与已经加载的资源的名称匹配。
建议构造规模较小的、仅做一件事情的补丁。
例如，构造一个补丁来增加 Deployment
的副本个数；构造另外一个补丁来设置内存限制。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建 deployment.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      run: my-nginx
</span><span style="color:#b44">  replicas: 2
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        run: my-nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">        ports:
</span><span style="color:#b44">        - containerPort: 80
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 生成一个补丁 increase_replicas.yaml</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; increase_replicas.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  replicas: 3
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 生成另一个补丁 set_memory.yaml</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; set_memory.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        resources:
</span><span style="color:#b44">          limits:
</span><span style="color:#b44">            memory: 512Mi
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">patchesStrategicMerge:
</span><span style="color:#b44">- increase_replicas.yaml
</span><span style="color:#b44">- set_memory.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Run `kubectl kustomize ./` to view the Deployment:
-->
<p>执行 <code>kubectl kustomize ./</code> 来查看 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>512Mi<span style="color:#bbb">
</span></code></pre></div><!--
Not all Resources or fields support strategic merge patches. To support modifying arbitrary fields in arbitrary Resources,
Kustomize offers applying [JSON patch](https://tools.ietf.org/html/rfc6902) through `patchesJson6902`.
To find the correct Resource for a Json patch, the group, version, kind and name of that Resource need to be
specified in `kustomization.yaml`. For example, increasing the replica number of a Deployment object can also be done
through `patchesJson6902`.
-->
<p>并非所有资源或者字段都支持策略性合并补丁。为了支持对任何资源的任何字段进行修改，
Kustomize 提供通过 <code>patchesJson6902</code> 来应用 <a href="https://tools.ietf.org/html/rfc6902">JSON 补丁</a>
的能力。为了给 JSON 补丁找到正确的资源，需要在 <code>kustomization.yaml</code> 文件中指定资源的
组（group）、版本（version）、类别（kind）和名称（name）。
例如，为某 Deployment 对象增加副本个数的操作也可以通过 <code>patchesJson6902</code>
来完成：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个 deployment.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      run: my-nginx
</span><span style="color:#b44">  replicas: 2
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        run: my-nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">        ports:
</span><span style="color:#b44">        - containerPort: 80
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建一个 JSON 补丁文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; patch.yaml
</span><span style="color:#b44">- op: replace
</span><span style="color:#b44">  path: /spec/replicas
</span><span style="color:#b44">  value: 3
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建一个 kustomization.yaml</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">
</span><span style="color:#b44">patchesJson6902:
</span><span style="color:#b44">- target:
</span><span style="color:#b44">    group: apps
</span><span style="color:#b44">    version: v1
</span><span style="color:#b44">    kind: Deployment
</span><span style="color:#b44">    name: my-nginx
</span><span style="color:#b44">  path: patch.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Run `kubectl kustomize ./` to see the `replicas` field is updated:
-->
<p>执行 <code>kubectl kustomize ./</code> 以查看 <code>replicas</code> 字段被更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div><!--
In addition to patches, Kustomize also offers customizing container images or injecting field values from other objects into containers
without creating patches. For example, you can change the image used inside containers by specifying the new image in `images` field in `kustomization.yaml`.
-->
<p>除了补丁之外，Kustomize 还提供定制容器镜像或者将其他对象的字段值注入到容器
中的能力，并且不需要创建补丁。
例如，你可以通过在 <code>kustomization.yaml</code> 文件的 <code>images</code> 字段设置新的镜像来
更改容器中使用的镜像。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF &gt; deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      run: my-nginx
</span><span style="color:#b44">  replicas: 2
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        run: my-nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">        ports:
</span><span style="color:#b44">        - containerPort: 80
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">images:
</span><span style="color:#b44">- name: nginx
</span><span style="color:#b44">  newName: my.image.registry/nginx
</span><span style="color:#b44">  newTag: 1.4.0
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Run `kubectl kustomize ./` to see that the image being used is updated:
-->
<p>执行 <code>kubectl kustomize ./</code> 以查看所使用的镜像已被更新：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my.image.registry/nginx:1.4.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div><!--
Sometimes, the application running in a Pod may need to use configuration values from other objects. For example,
a Pod from a Deployment object need to read the corresponding Service name from Env or as a command argument.
Since the Service name may change as `namePrefix` or `nameSuffix` is added in the `kustomization.yaml` file. It is
not recommended to hard code the Service name in the command argument. For this usage, Kustomize can inject the Service name into containers through `vars`.
-->
<p>有些时候，Pod 中运行的应用可能需要使用来自其他对象的配置值。
例如，某 Deployment 对象的 Pod 需要从环境变量或命令行参数中读取读取
Service 的名称。
由于在 <code>kustomization.yaml</code> 文件中添加 <code>namePrefix</code> 或 <code>nameSuffix</code> 时
Service 名称可能发生变化，建议不要在命令参数中硬编码 Service 名称。
对于这种使用场景，Kustomize 可以通过 <code>vars</code> 将 Service 名称注入到容器中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个 deployment.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      run: my-nginx
</span><span style="color:#b44">  replicas: 2
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        run: my-nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">        command: [&#34;start&#34;, &#34;--host&#34;, &#34;$(MY_SERVICE_NAME)&#34;]
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建一个 service.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; service.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Service
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">  labels:
</span><span style="color:#b44">    run: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  ports:
</span><span style="color:#b44">  - port: 80
</span><span style="color:#b44">    protocol: TCP
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    run: my-nginx
</span><span style="color:#b44">EOF</span>

cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">namePrefix: dev-
</span><span style="color:#b44">nameSuffix: &#34;-001&#34;
</span><span style="color:#b44">
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">- service.yaml
</span><span style="color:#b44">
</span><span style="color:#b44">vars:
</span><span style="color:#b44">- name: MY_SERVICE_NAME
</span><span style="color:#b44">  objref:
</span><span style="color:#b44">    kind: Service
</span><span style="color:#b44">    name: my-nginx
</span><span style="color:#b44">    apiVersion: v1
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Run `kubectl kustomize ./` to see that the Service name injected into containers is `dev-my-nginx-001`:
-->
<p>执行 <code>kubectl kustomize ./</code> 以查看注入到容器中的 Service 名称是 <code>dev-my-nginx-001</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-my-nginx-001<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- start<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --host<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- dev-my-nginx-001<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-nginx<span style="color:#bbb">
</span></code></pre></div><!--
## Bases and Overlays

Kustomize has the concepts of **bases** and **overlays**. A **base** is a directory with a `kustomization.yaml`, which contains a
set of resources and associated customization. A base could be either a local directory or a directory from a remote repo,
as long as a `kustomization.yaml` is present inside. An **overlay** is a directory with a `kustomization.yaml` that refers to other
kustomization directories as its `bases`. A **base** has no knowledge of an overlay and can be used in multiple overlays.
An overlay may have multiple bases and it composes all resources
from bases and may also have customization on top of them.

Here is an example of a base:
-->
<h2 id="基准-bases-与覆盖-overlays">基准（Bases）与覆盖（Overlays）</h2>
<p>Kustomize 中有 <strong>基准（bases）</strong> 和 <strong>覆盖（overlays）</strong> 的概念区分。
<strong>基准</strong> 是包含 <code>kustomization.yaml</code> 文件的一个目录，其中包含一组资源及其相关的定制。
基准可以是本地目录或者来自远程仓库的目录，只要其中存在 <code>kustomization.yaml</code> 文件即可。
<strong>覆盖</strong> 也是一个目录，其中包含将其他 kustomization 目录当做 <code>bases</code> 来引用的
<code>kustomization.yaml</code> 文件。
<strong>基准</strong>不了解覆盖的存在，且可被多个覆盖所使用。
覆盖则可以有多个基准，且可针对所有基准中的资源执行组织操作，还可以在其上执行定制。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个包含基准的目录 </span>
mkdir base
<span style="color:#080;font-style:italic"># 创建 base/deployment.yaml</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; base/deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      run: my-nginx
</span><span style="color:#b44">  replicas: 2
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        run: my-nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建 base/service.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; base/service.yaml
</span><span style="color:#b44">apiVersion: v1
</span><span style="color:#b44">kind: Service
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">  labels:
</span><span style="color:#b44">    run: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  ports:
</span><span style="color:#b44">  - port: 80
</span><span style="color:#b44">    protocol: TCP
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    run: my-nginx
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建 base/kustomization.yaml</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; base/kustomization.yaml
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">- service.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
This base can be used in multiple overlays. You can add different `namePrefix` or other cross-cutting fields
in different overlays. Here are two overlays using the same base.
-->
<p>此基准可在多个覆盖中使用。你可以在不同的覆盖中添加不同的 <code>namePrefix</code> 或
其他贯穿性字段。下面是两个使用同一基准的覆盖：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">mkdir dev
cat <span style="color:#b44">&lt;&lt;EOF &gt; dev/kustomization.yaml
</span><span style="color:#b44">bases:
</span><span style="color:#b44">- ../base
</span><span style="color:#b44">namePrefix: dev-
</span><span style="color:#b44">EOF</span>

mkdir prod
cat <span style="color:#b44">&lt;&lt;EOF &gt; prod/kustomization.yaml
</span><span style="color:#b44">bases:
</span><span style="color:#b44">- ../base
</span><span style="color:#b44">namePrefix: prod-
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
## How to apply/view/delete objects using Kustomize

Use `--kustomize` or `-k` in `kubectl` commands to recognize Resources managed by `kustomization.yaml`.
Note that `-k` should point to a kustomization directory, such as
-->
<h2 id="如何使用-kustomize-来应用-查看和删除对象">如何使用 Kustomize 来应用、查看和删除对象</h2>
<p>在 <code>kubectl</code> 命令中使用 <code>--kustomize</code> 或 <code>-k</code> 参数来识别被 <code>kustomization.yaml</code> 所管理的资源。
注意 <code>-k</code> 要指向一个 kustomization 目录。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k &lt;kustomization 目录&gt;/
</code></pre></div><!--
Given the following `kustomization.yaml`,
-->
<p>假定使用下面的 <code>kustomization.yaml</code>，</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建 deployment.yaml 文件</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt; deployment.yaml
</span><span style="color:#b44">apiVersion: apps/v1
</span><span style="color:#b44">kind: Deployment
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-nginx
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  selector:
</span><span style="color:#b44">    matchLabels:
</span><span style="color:#b44">      run: my-nginx
</span><span style="color:#b44">  replicas: 2
</span><span style="color:#b44">  template:
</span><span style="color:#b44">    metadata:
</span><span style="color:#b44">      labels:
</span><span style="color:#b44">        run: my-nginx
</span><span style="color:#b44">    spec:
</span><span style="color:#b44">      containers:
</span><span style="color:#b44">      - name: my-nginx
</span><span style="color:#b44">        image: nginx
</span><span style="color:#b44">        ports:
</span><span style="color:#b44">        - containerPort: 80
</span><span style="color:#b44">EOF</span>

<span style="color:#080;font-style:italic"># 创建 kustomization.yaml</span>
cat <span style="color:#b44">&lt;&lt;EOF &gt;./kustomization.yaml
</span><span style="color:#b44">namePrefix: dev-
</span><span style="color:#b44">commonLabels:
</span><span style="color:#b44">  app: my-nginx
</span><span style="color:#b44">resources:
</span><span style="color:#b44">- deployment.yaml
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Run the following command to apply the Deployment object `dev-my-nginx`:
-->
<p>执行下面的命令来应用 Deployment 对象 <code>dev-my-nginx</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k ./
</code></pre></div><pre tabindex="0"><code>deployment.apps/dev-my-nginx created
</code></pre><!--
Run one of the following commands to view the Deployment object `dev-my-nginx`:
-->
<p>运行下面的命令之一来查看 Deployment 对象 <code>dev-my-nginx</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get -k ./
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe -k ./
</code></pre></div><!--
Run the following command to compare the Deployment object `dev-my-nginx` against the state that the cluster would be in if the manifest was applied:
-->
<p>执行下面的命令来比较 Deployment 对象 <code>dev-my-nginx</code> 与清单被应用之后
集群将处于的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl diff -k ./
</code></pre></div><!--
Run the following command to delete the Deployment object `dev-my-nginx`:
-->
<p>执行下面的命令删除 Deployment 对象 <code>dev-my-nginx</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -k ./
</code></pre></div><pre tabindex="0"><code>deployment.apps &quot;dev-my-nginx&quot; deleted
</code></pre><!--
## Kustomize Feature List
-->
<h2 id="kustomize-功能特性列表">Kustomize 功能特性列表</h2>
<!--
| Field                 | Type                                                                                                         | Explanation                                                                        |
|-----------------------|--------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| namespace             | string                                                                                                       | add namespace to all resources                                                     |
| namePrefix            | string                                                                                                       | value of this field is prepended to the names of all resources                     |
| nameSuffix            | string                                                                                                       | value of this field is appended to the names of all resources                      |
| commonLabels          | map[string]string                                                                                            | labels to add to all resources and selectors                                       |
| commonAnnotations     | map[string]string                                                                                            | annotations to add to all resources                                                |
| resources             | []string                                                                                                     | each entry in this list must resolve to an existing resource configuration file    |
| configMapGenerator    | [][ConfigMapArgs](https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/configmapargs.go#L7)    | Each entry in this list generates a ConfigMap                                      |
| secretGenerator       | [][SecretArgs](https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/secretargs.go#L7)          | Each entry in this list generates a Secret                                         |
| generatorOptions      | [GeneratorOptions](https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/generatoroptions.go#L7) | Modify behaviors of all ConfigMap and Secret generator                             |
| bases                 | []string                                                                                                     | Each entry in this list should resolve to a directory containing a kustomization.yaml file |
| patchesStrategicMerge | []string                                                                                                     | Each entry in this list should resolve a strategic merge patch of a Kubernetes object |
| patchesJson6902       | [][Patch](https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/patch.go#L10)                   | Each entry in this list should resolve to a Kubernetes object and a Json Patch     |
| vars                  | [][Var](https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/var.go#L19)                       | Each entry is to capture text from one resource's field                            |
| images                | [][Image](https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/image.go#L8)                    | Each entry is to modify the name, tags and/or digest for one image without creating patches |
| configurations        | []string                                                                                                     | Each entry in this list should resolve to a file containing [Kustomize transformer configurations](https://github.com/kubernetes-sigs/kustomize/tree/master/examples/transformerconfigs) |
| crds                  | []string                                                                                                     | Each entry in this list should resolve to an OpenAPI definition file for Kubernetes types |
-->
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>namespace</td>
<td>string</td>
<td>为所有资源添加名字空间</td>
</tr>
<tr>
<td>namePrefix</td>
<td>string</td>
<td>此字段的值将被添加到所有资源名称前面</td>
</tr>
<tr>
<td>nameSuffix</td>
<td>string</td>
<td>此字段的值将被添加到所有资源名称后面</td>
</tr>
<tr>
<td>commonLabels</td>
<td>map[string]string</td>
<td>要添加到所有资源和选择算符的标签</td>
</tr>
<tr>
<td>commonAnnotations</td>
<td>map[string]string</td>
<td>要添加到所有资源的注解</td>
</tr>
<tr>
<td>resources</td>
<td>[]string</td>
<td>列表中的每个条目都必须能够解析为现有的资源配置文件</td>
</tr>
<tr>
<td>configMapGenerator</td>
<td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/configmapargs.go#L7">ConfigMapArgs</a></td>
<td>列表中的每个条目都会生成一个 ConfigMap</td>
</tr>
<tr>
<td>secretGenerator</td>
<td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/secretargs.go#L7">SecretArgs</a></td>
<td>列表中的每个条目都会生成一个 Secret</td>
</tr>
<tr>
<td>generatorOptions</td>
<td><a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/generatoroptions.go#L7">GeneratorOptions</a></td>
<td>更改所有 ConfigMap 和 Secret 生成器的行为</td>
</tr>
<tr>
<td>bases</td>
<td>[]string</td>
<td>列表中每个条目都应能解析为一个包含 kustomization.yaml 文件的目录</td>
</tr>
<tr>
<td>patchesStrategicMerge</td>
<td>[]string</td>
<td>列表中每个条目都能解析为某 Kubernetes 对象的策略性合并补丁</td>
</tr>
<tr>
<td>patchesJson6902</td>
<td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/patch.go#L10">Patch</a></td>
<td>列表中每个条目都能解析为一个 Kubernetes 对象和一个 JSON 补丁</td>
</tr>
<tr>
<td>vars</td>
<td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/var.go#L19">Var</a></td>
<td>每个条目用来从某资源的字段来析取文字</td>
</tr>
<tr>
<td>images</td>
<td>[]<a href="https://github.com/kubernetes-sigs/kustomize/blob/master/api/types/image.go#L8">Image</a></td>
<td>每个条目都用来更改镜像的名称、标记与/或摘要，不必生成补丁</td>
</tr>
<tr>
<td>configurations</td>
<td>[]string</td>
<td>列表中每个条目都应能解析为一个包含 <a href="https://github.com/kubernetes-sigs/kustomize/tree/master/examples/transformerconfigs">Kustomize 转换器配置</a> 的文件</td>
</tr>
<tr>
<td>crds</td>
<td>[]string</td>
<td>列表中每个条目都赢能够解析为 Kubernetes 类别的 OpenAPI 定义文件</td>
</tr>
</tbody>
</table>
<h2 id="接下来">接下来</h2>
<!--
* [Kustomize](https://github.com/kubernetes-sigs/kustomize)
* [Kubectl Book](https://kubectl.docs.kubernetes.io)
* [Kubectl Command Reference](/docs/reference/generated/kubectl/kubectl-commands/)
* [Kubernetes API Reference](/docs/reference/generated/kubernetes-api/v1.22/)
-->
<ul>
<li><a href="https://github.com/kubernetes-sigs/kustomize">Kustomize</a></li>
<li><a href="https://kubectl.docs.kubernetes.io">Kubectl Book</a></li>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl 命令参考</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/">Kubernetes API 参考</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-80c83fe9b80d0fef2681c8d59c0aa197">4.3 - 使用指令式命令管理 Kubernetes 对象</h1>
    
	<!--
title: Managing Kubernetes Objects Using Imperative Commands
content_type: task
weight: 30
-->
<!-- overview -->
<!--
Kubernetes objects can quickly be created, updated, and deleted directly using
imperative commands built into the `kubectl` command-line tool. This document
explains how those commands are organized and how to use them to manage live objects.
-->
<p>使用构建在 <code>kubectl</code> 命令行工具中的指令式命令可以直接快速创建、更新和删除
Kubernetes 对象。本文档解释这些命令的组织方式以及如何使用它们来管理现时对象。</p>
<h2 id="准备开始">准备开始</h2>
<!--
Install [`kubectl`](/docs/tasks/tools/).
-->
<p>安装<a href="/zh/docs/tasks/tools/"><code>kubectl</code></a>。</p>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Trade-offs

The `kubectl` tool supports three kinds of object management:

* Imperative commands
* Imperative object configuration
* Declarative object configuration

See [Kubernetes Object Management](/docs/concepts/overview/working-with-objects/object-management/)
for a discussion of the advantages and disadvantage of each kind of object management.
-->
<h2 id="trade-offs">权衡取舍  </h2>
<p><code>kubectl</code> 工具能够支持三种对象管理方式：</p>
<ul>
<li>指令式命令</li>
<li>指令式对象配置</li>
<li>声明式对象配置</li>
</ul>
<p>关于每种对象管理的优缺点的讨论，可参见
<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">Kubernetes 对象管理</a>。</p>
<!--
## How to create objects

The `kubectl` tool supports verb-driven commands for creating some of the most common
object types. The commands are named to be recognizable to users unfamiliar with
the Kubernetes object types.

- `run`: Create a new Pod to run a Container.
- `expose`: Create a new Service object to load balance traffic across Pods.
- `autoscale`: Create a new Autoscaler object to automatically horizontally scale a controller, such as a Deployment.
-->
<h2 id="how-to-create-objects">如何创建对象 </h2>
<p><code>kubectl</code> 工具支持动词驱动的命令，用来创建一些最常见的对象类别。
命令的名称设计使得不熟悉 Kubernetes 对象类型的用户也能做出判断。</p>
<ul>
<li><code>run</code>：创建一个新的 Pod 来运行一个容器。</li>
<li><code>expose</code>：创建一个新的 Service 对象为若干 Pod 提供流量负载均衡。</li>
<li><code>autoscale</code>：创建一个新的 Autoscaler 对象来自动对某控制器（如 Deployment）
执行水平扩缩。</li>
</ul>
<!--
The `kubectl` tool also supports creation commands driven by object type.
These commands support more object types and are more explicit about
their intent, but require users to know the type of objects they intend
to create.

- `create <objecttype> [<subtype>] <instancename>`
-->
<p><code>kubectl</code> 命令也支持一些对象类型驱动的创建命令。
这些命令可以支持更多的对象类别，并且在其动机上体现得更为明显，不过要求
用户了解它们所要创建的对象的类别。</p>
<ul>
<li><code>create &lt;对象类别&gt; [&lt;子类别&gt;] &lt;实例名称&gt;</code></li>
</ul>
<!--
Some objects types have subtypes that you can specify in the `create` command.
For example, the Service object has several subtypes including ClusterIP,
LoadBalancer, and NodePort. Here's an example that creates a Service with
subtype NodePort:

```shell
kubectl create service nodeport <myservicename>
```
-->
<p>某些对象类别拥有自己的子类别，可以在 <code>create</code> 命令中设置。
例如，Service 对象有 ClusterIP、LoadBalancer 和 NodePort 三种子类别。
下面是一个创建 NodePort 子类别的 Service 的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create service nodeport &lt;服务名称&gt;
</code></pre></div><!--
In the preceding example, the `create service nodeport` command is called
a subcommand of the `create service` command.

You can use the `-h` flag to find the arguments and flags supported by
a subcommand:
-->
<p>在前述示例中，<code>create service nodeport</code> 命令也称作 <code>create service</code>
命令的子命令。
可以使用 <code>-h</code> 标志找到一个子命令所支持的参数和标志。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create service nodeport -h
</code></pre></div><!--
## How to update objects

The `kubectl` command supports verb-driven commands for some common update operations.
These commands are named to enable users unfamiliar with Kubernetes
objects to perform updates without knowing the specific fields
that must be set:

- `scale`: Horizontally scale a controller to add or remove Pods by updating the replica count of the controller.
- `annotate`: Add or remove an annotation from an object.
- `label`: Add or remove a label from an object.
-->
<h2 id="how-to-update-objects">如何更新对象 </h2>
<p><code>kubectl</code> 命令也支持一些动词驱动的命令，用来执行一些常见的更新操作。
这些命令的设计是为了让一些不了解 Kubernetes 对象的用户也能执行更新操作，
但不需要了解哪些字段必须设置：</p>
<ul>
<li><code>scale</code>：对某控制器进行水平扩缩以便通过更新控制器的副本个数来添加或删除 Pod。</li>
<li><code>annotate</code>：为对象添加或删除注解。</li>
<li><code>label</code>：为对象添加或删除标签。</li>
</ul>
<!--
The `kubectl` command also supports update commands driven by an aspect of the object.
Setting this aspect may set different fields for different object types:

- `set` `<field>`: Set an aspect of an object.
-->
<p><code>kubectl</code> 命令也支持由对象的某一方面来驱动的更新命令。
设置对象的这一方面可能对不同类别的对象意味着不同的字段：</p>
<ul>
<li><code>set &lt;字段&gt;</code>：设置对象的某一方面。</li>
</ul>
<!--
In Kubernetes version 1.5, not every verb-driven command has an associated aspect-driven command.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 在 Kubernetes 1.5 版本中，并非所有动词驱动的命令都有对应的方面驱动的命令。</div>
</blockquote>
<!--
The `kubectl` tool supports these additional ways to update a live object directly,
however they require a better understanding of the Kubernetes object schema.

- `edit`: Directly edit the raw configuration of a live object by opening its configuration in an editor.
- `patch`: Directly modify specific fields of a live object by using a patch string.
For more details on patch strings, see the patch section in
[API Conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#patch-operations).
-->
<p><code>kubectl</code> 工具支持以下额外的方式用来直接更新现时对象，不过这些操作要求
用户对 Kubernetes 对象的模式定义有很好的了解：</p>
<ul>
<li><code>edit</code>：通过在编辑器中打开现时对象的配置，直接编辑其原始配置。</li>
<li><code>patch</code>：通过使用补丁字符串（Patch String）直接更改某现时对象的的特定字段。
关于补丁字符串的更详细信息，参见
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#patch-operations">API 约定</a>
的 patch 节。</li>
</ul>
<!--
## How to delete objects

You can use the `delete` command to delete an object from a cluster:

- `delete <type>/<name>`
-->
<h2 id="how-to-delete-objects">如何删除对象 </h2>
<p>你可以使用 <code>delete</code> 命令从集群中删除一个对象：</p>
<ul>
<li><code>delete &lt;类别&gt;/&lt;名称&gt;</code></li>
</ul>
<!--
You can use `kubectl delete` for both imperative commands and imperative object
configuration. The difference is in the arguments passed to the command. To use
`kubectl delete` as an imperative command, pass the object to be deleted as
an argument. Here's an example that passes a Deployment object named nginx:
-->
<p>你可以使用 <code>kubectl delete</code> 来执行指令式命令或者指令式对象配置。不同之处在于
传递给命令的参数。要将 <code>kubectl delete</code> 作为指令式命令使用，将要删除的对象作为
参数传递给它。下面是一个删除名为 <code>nginx</code> 的 Deployment 对象的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment/nginx
</code></pre></div><!--
## How to view an object


-->
<h2 id="how-to-view-an-object">如何查看对象 </h2>
<p>用来打印对象信息的命令有好几个：</p>
<ul>
<li><code>get</code>：打印匹配到的对象的基本信息。使用 <code>get -h</code> 可以查看选项列表。</li>
<li><code>describe</code>：打印匹配到的对象的详细信息的汇集版本。</li>
<li><code>logs</code>：打印 Pod 中运行的容器的 stdout 和 stderr 输出。</li>
</ul>
<!--
## Using `set` commands to modify objects before creation

There are some object fields that don't have a flag you can use
in a `create` command. In some of those cases, you can use a combination of
`set` and `create` to specify a value for the field before object
creation. This is done by piping the output of the `create` command to the
`set` command, and then back to the `create` command. Here's an example:
-->
<h2 id="使用-set-命令在创建对象之前修改对象">使用 <code>set</code> 命令在创建对象之前修改对象</h2>
<p>有些对象字段在 <code>create</code> 命令中没有对应的标志。在这些场景中，
你可以使用 <code>set</code> 和 <code>create</code> 命令的组合来在对象创建之前设置字段值。
这是通过将 <code>create</code> 命令的输出用管道方式传递给 <code>set</code> 命令来实现的，
最后执行 <code>create</code> 命令来创建对象。下面是一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl create service clusterip my-svc --clusterip<span style="color:#666">=</span><span style="color:#b44">&#34;None&#34;</span> -o yaml --dry-run<span style="color:#666">=</span>client | kubectl <span style="color:#a2f">set</span> selector --local -f - <span style="color:#b44">&#39;environment=qa&#39;</span> -o yaml | kubectl create -f -
</code></pre></div><!--
1. The `kubectl create service -o yaml --dry-run=client` command creates the configuration for the Service, but prints it to stdout as YAML instead of sending it to the Kubernetes API server.
1. The `kubectl set selector --local -f - -o yaml` command reads the configuration from stdin, and writes the updated configuration to stdout as YAML.
1. The `kubectl create -f -` command creates the object using the configuration provided via stdin.
-->
<ol>
<li>命令 <code>kubectl create service -o yaml --dry-run=client</code> 创建 Service 的配置，但
将其以 YAML 格式在标准输出上打印而不是发送给 API 服务器。</li>
<li>命令 <code>kubectl set selector --local -f - -o yaml</code> 从标准输入读入配置，并将更新后的
配置以 YAML 格式输出到标准输出。</li>
<li>命令 <code>kubectl create -f -</code> 使用标准输入上获得的配置创建对象。</li>
</ol>
<!--
## Using `--edit` to modify objects before creation

You can use `kubectl create --edit` to make arbitrary changes to an object
before it is created. Here's an example:
-->
<h2 id="在创建之前使用-edit-更改对象">在创建之前使用 <code>--edit</code> 更改对象</h2>
<p>你可以用 <code>kubectl create --edit</code> 来在对象被创建之前执行任意的变更。
下面是一个例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">kubectl create service clusterip my-svc --clusterip<span style="color:#666">=</span><span style="color:#b44">&#34;None&#34;</span> -o yaml --dry-run<span style="color:#666">=</span>client &gt; /tmp/srv.yaml
kubectl create --edit -f /tmp/srv.yaml
</code></pre></div><!--
1. The `kubectl create service` command creates the configuration for the Service and saves it to `/tmp/srv.yaml`.
1. The `kubectl create --edit` command opens the configuration file for editing before it creates the object.
-->
<ol>
<li>命令 <code>kubectl create service</code> 创建 Service 的配置并将其保存到
<code>/tmp/srv.yaml</code> 文件。</li>
<li>命令 <code>kubectl create --edit</code> 在创建 Service 对象打开其配置文件进行编辑。</li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
* [Managing Kubernetes Objects Using Object Configuration (Imperative)](/docs/tasks/manage-kubernetes-objects/imperative-config/)
* [Managing Kubernetes Objects Using Object Configuration (Declarative)](/docs/tasks/manage-kubernetes-objects/declarative-config/)
* [Kubectl Command Reference](/docs/reference/generated/kubectl/kubectl-commands/)
* [Kubernetes API Reference](/docs/reference/generated/kubernetes-api/v1.22/)
-->
<ul>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-config/">使用指令式对象配置管理 Kubernetes 对象</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/declarative-config/">使用声明式对象配置管理 Kubernetes 对象</a></li>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl 命令参考</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/">Kubernetes API 参考</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b18886277c410fc6f32ce068e2160537">4.4 - 使用配置文件对 Kubernetes 对象进行命令式管理</h1>
    
	<!--
title: Imperative Management of Kubernetes Objects Using Configuration Files
content_type: task
weight: 40
-->
<!-- overview -->
<!--
Kubernetes objects can be created, updated, and deleted by using the `kubectl`
command-line tool along with an object configuration file written in YAML or JSON.
This document explains how to define and manage objects using configuration files.
-->
<p>可以使用 <code>kubectl</code> 命令行工具以及用 YAML 或 JSON 编写的对象配置文件来创建、更新和删除 Kubernetes 对象。
本文档说明了如何使用配置文件定义和管理对象。</p>
<h2 id="准备开始">准备开始</h2>
<!--
Install [`kubectl`](/docs/tasks/tools/).
-->
<p>安装 <a href="/zh/docs/tasks/tools/"><code>kubectl</code></a> 。</p>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Trade-offs

The `kubectl` tool supports three kinds of object management:
-->
<h2 id="权衡">权衡</h2>
<p><code>kubectl</code> 工具支持三种对象管理：</p>
<!--
* Imperative commands
* Imperative object configuration
* Declarative object configuration
-->
<ul>
<li>命令式命令</li>
<li>命令式对象配置</li>
<li>声明式对象配置</li>
</ul>
<!--
See [Kubernetes Object Management](/docs/concepts/overview/working-with-objects/object-management/)
for a discussion of the advantages and disadvantage of each kind of object management.
-->
<p>参看 <a href="/zh/docs/concepts/overview/working-with-objects/object-management/">Kubernetes 对象管理</a>
中关于每种对象管理的优缺点的讨论。</p>
<!--
## How to create objects

You can use `kubectl create -f` to create an object from a configuration file.
Refer to the [kubernetes API reference](/docs/reference/generated/kubernetes-api/v1.22/)
for details.
-->
<h2 id="如何创建对象">如何创建对象</h2>
<p>你可以使用 <code>kubectl create -f</code> 从配置文件创建一个对象。
请参考 <a href="/docs/reference/generated/kubernetes-api/v1.22/">kubernetes API 参考</a> 有关详细信息。</p>
<ul>
<li><code>kubectl create -f &lt;filename|url&gt;</code></li>
</ul>
<!--
## How to update objects

Updating objects with the `replace` command drops all
parts of the spec not specified in the configuration file.  This
should not be used with objects whose specs are partially managed
by the cluster, such as Services of type `LoadBalancer`, where
the `externalIPs` field is managed independently from the configuration
file.  Independently managed fields must be copied to the configuration
file to prevent `replace` from dropping them.
-->
<h2 id="如何更新对象">如何更新对象</h2>
<blockquote class="warning callout">
  <div><strong>警告：</strong> 使用 <code>replace</code> 命令更新对象会删除所有未在配置文件中指定的规范的某些部分。
不应将其规范由集群部分管理的对象使用，比如类型为 <code>LoadBalancer</code> 的服务，
其中 <code>externalIPs</code> 字段独立于配置文件进行管理。
必须将独立管理的字段复制到配置文件中，以防止 <code>replace</code> 删除它们。</div>
</blockquote>

<!--
You can use `kubectl replace -f` to update a live object according to a
configuration file.
-->
<p>你可以使用 <code>kubectl replace -f</code> 根据配置文件更新活动对象。</p>
<ul>
<li><code>kubectl replace -f &lt;filename|url&gt;</code></li>
</ul>
<!--
## How to delete objects

You can use `kubectl delete -f` to delete an object that is described in a
configuration file.
-->
<h2 id="如何删除对象">如何删除对象</h2>
<p>你可以使用 <code>kubectl delete -f</code> 删除配置文件中描述的对象。</p>
<ul>
<li><code>kubectl delete -f &lt;filename|url&gt;</code></li>
</ul>
<!-- note
If configuration file has specified the `generateName` field in the `metadata`
section instead of the `name` field, you cannot delete the object using
`kubectl delete -f <filename|url>`.
You will have to use other flags for deleting the object. For example:

```shell
kubectl delete <type> <name>
kubectl delete <type> -l <label>
```
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>如果配置文件在 <code>metadata</code> 节中设置了 <code>generateName</code> 字段而非 <code>name</code> 字段，
你无法使用 <code>kubectl delete -f &lt;filename|url&gt;</code> 来删除该对象。
你必须使用其他标志才能删除对象。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete &lt;type&gt; &lt;name&gt;
kubectl delete &lt;type&gt; -l &lt;label&gt;
</code></pre></div></div>
</blockquote>
<!--
## How to view an object

You can use `kubectl get -f` to view information about an object that is
described in a configuration file.
-->
<h2 id="如何查看对象">如何查看对象</h2>
<p>你可以使用 <code>kubectl get -f</code> 查看有关配置文件中描述的对象的信息。</p>
<ul>
<li><code>kubectl get -f &lt;filename|url&gt; -o yaml</code></li>
</ul>
<!--
The `-o yaml` flag specifies that the full object configuration is printed.
Use `kubectl get -h` to see a list of options.
-->
<p><code>-o yaml</code> 标志指定打印完整的对象配置。
使用 <code>kubectl get -h</code> 查看选项列表。</p>
<!--
## Limitations

The `create`, `replace`, and `delete` commands work well when each object's
configuration is fully defined and recorded in its configuration
file. However when a live object is updated, and the updates are not merged
into its configuration file, the updates will be lost the next time a `replace`
is executed. This can happen if a controller, such as
a HorizontalPodAutoscaler, makes updates directly to a live object. Here's
an example:
-->
<h2 id="局限性">局限性</h2>
<p>当完全定义每个对象的配置并将其记录在其配置文件中时，<code>create</code>、 <code>replace</code> 和<code>delete</code> 命令会很好的工作。
但是，当更新一个活动对象，并且更新没有合并到其配置文件中时，下一次执行 <code>replace</code> 时，更新将丢失。
如果控制器,例如 HorizontalPodAutoscaler ,直接对活动对象进行更新，则会发生这种情况。
这有一个例子：</p>
<!--
1. You create an object from a configuration file.
1. Another source updates the object by changing some field.
1. You replace the object from the configuration file. Changes made by
the other source in step 2 are lost.
-->
<ol>
<li>从配置文件创建一个对象。</li>
<li>另一个源通过更改某些字段来更新对象。</li>
<li>从配置文件中替换对象。在步骤2中所做的其他源的更改将丢失。</li>
</ol>
<!--
If you need to support multiple writers to the same object, you can use
`kubectl apply` to manage the object.
-->
<p>如果需要支持同一对象的多个编写器，则可以使用 <code>kubectl apply</code> 来管理该对象。</p>
<!--
## Creating and editing an object from a URL without saving the configuration

Suppose you have the URL of an object configuration file. You can use
`kubectl create --edit` to make changes to the configuration before the
object is created. This is particularly useful for tutorials and tasks
that point to a configuration file that could be modified by the reader.
-->
<h2 id="从-url-创建和编辑对象而不保存配置">从 URL 创建和编辑对象而不保存配置</h2>
<p>假设你具有对象配置文件的 URL。
你可以在创建对象之前使用 <code>kubectl create --edit</code> 对配置进行更改。
这对于指向可以由读者修改的配置文件的教程和任务特别有用。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f &lt;url&gt; --edit
</code></pre></div><!--
## Migrating from imperative commands to imperative object configuration

Migrating from imperative commands to imperative object configuration involves
several manual steps.
-->
<h2 id="从命令式命令迁移到命令式对象配置">从命令式命令迁移到命令式对象配置</h2>
<p>从命令式命令迁移到命令式对象配置涉及几个手动步骤。</p>
<!--
1. Export the live object to a local object configuration file:
-->
<ol>
<li>
<p>将活动对象导出到本地对象配置文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get &lt;kind&gt;/&lt;name&gt; -o yaml &gt; &lt;kind&gt;_&lt;name&gt;.yaml
</code></pre></div></li>
</ol>
<!--
1. Manually remove the status field from the object configuration file.
-->
<ol start="2">
<li>从对象配置文件中手动删除状态字段。</li>
</ol>
<!--
1. For subsequent object management, use `replace` exclusively.
-->
<ol start="3">
<li>
<p>对于后续的对象管理，只能使用 <code>replace</code> 。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl replace -f &lt;kind&gt;_&lt;name&gt;.yaml
</code></pre></div></li>
</ol>
<!--
## Defining controller selectors and PodTemplate labels
-->
<h2 id="定义控制器选择器和-podtemplate-标签">定义控制器选择器和 PodTemplate 标签</h2>
<!--
Updating selectors on controllers is strongly discouraged.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 不建议在控制器上更新选择器。</div>
</blockquote>

<!--
The recommended approach is to define a single, immutable PodTemplate label
used only by the controller selector with no other semantic meaning.
-->
<p>推荐的方法是定义单个不变的 PodTemplate 标签，该标签仅由控制器选择器使用，而没有其他语义。</p>
<!-- Example label: -->
<p>标签示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-selector</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;apps/v1/deployment/nginx&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">controller-selector</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;apps/v1/deployment/nginx&#34;</span><span style="color:#bbb">
</span></code></pre></div><h2 id="接下来">接下来</h2>
<!--
* [Managing Kubernetes Objects Using Imperative Commands](/docs/tasks/manage-kubernetes-objects/imperative-command/)
* [Managing Kubernetes Objects Using Object Configuration (Declarative)](/docs/tasks/manage-kubernetes-objects/declarative-config/)
* [Kubectl Command Reference](/docs/reference/generated/kubectl/kubectl/)
* [Kubernetes API Reference](/docs/reference/generated/kubernetes-api/v1.22/)
-->
<ul>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-command/">使用命令式命令管理 Kubernetes 对象</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/declarative-config/">使用对象配置管理 Kubernetes 对象 (声明式)</a></li>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands/">Kubectl 命令参考</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/">Kubernetes API 参考</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d4d4414dc91b63cfe0f65ca4f0c2fe31">4.5 - 使用 kubectl patch 更新 API 对象</h1>
    <div class="lead">使用 kubectl patch 更新 Kubernetes API 对象。做一个策略性的合并 patch 或 JSON 合并 patch。</div>
	<!--
title: Update API Objects in Place Using kubectl patch
description: Use kubectl patch to update Kubernetes API objects in place. Do a strategic merge patch or a JSON merge patch.
content_type: task
weight: 40
-->
<!-- overview -->
<!--
This task shows how to use `kubectl patch` to update an API object in place. The exercises
in this task demonstrate a strategic merge patch and a JSON merge patch.
-->
<p>这个任务展示如何使用 <code>kubectl patch</code> 就地更新 API 对象。
这个任务中的练习演示了一个策略性合并 patch 和一个 JSON 合并 patch。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Use a strategic merge patch to update a Deployment

Here's the configuration file for a Deployment that has two replicas. Each replica
is a Pod that has one container:
-->
<h2 id="使用策略合并-patch-更新-deployment">使用策略合并 patch 更新 Deployment</h2>
<p>下面是具有两个副本的 Deployment 的配置文件。每个副本是一个 Pod，有一个容器：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/deployment-patch.yaml" download="application/deployment-patch.yaml"><code>application/deployment-patch.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-deployment-patch-yaml')" title="Copy application/deployment-patch.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-deployment-patch-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>patch-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>patch-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>dedicated<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>test-team<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Deployment:
-->
<p>创建 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/application/deployment-patch.yaml
</code></pre></div><!--
View the Pods associated with your Deployment:
-->
<p>查看与 Deployment 相关的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output shows that the Deployment has two Pods. The `1/1` indicates that
each Pod has one container:
-->
<p>输出显示 Deployment 有两个 Pod。<code>1/1</code> 表示每个 Pod 有一个容器:</p>
<pre tabindex="0"><code>NAME                        READY     STATUS    RESTARTS   AGE
patch-demo-28633765-670qr   1/1       Running   0          23s
patch-demo-28633765-j5qs3   1/1       Running   0          23s
</code></pre><!--
Make a note of the names of the running Pods. Later, you will see that these Pods
get terminated and replaced by new ones.
-->
<p>把运行的 Pod 的名字记下来。稍后，你将看到这些 Pod 被终止并被新的 Pod 替换。</p>
<!--
At this point, each Pod has one Container that runs the nginx image. Now suppose
you want each Pod to have two containers: one that runs nginx and one that runs redis.
-->
<p>此时，每个 Pod 都有一个运行 nginx 镜像的容器。现在假设你希望每个 Pod 有两个容器：一个运行 nginx，另一个运行 redis。</p>
<!--
Create a file named `patch-file-containers.yaml` that has this content:
-->
<p>创建一个名为 <code>patch-file-containers.yaml</code> 的文件。内容如下:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>patch-demo-ctr-2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span></code></pre></div><!--
Patch your Deployment:
-->
<p>修补你的 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch deployment patch-demo --patch <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat patch-file-containers.yaml<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
</code></pre></div><!--
View the patched Deployment:
-->
<p>查看修补后的 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment patch-demo --output yaml
</code></pre></div><!--
The output shows that the PodSpec in the Deployment has two Containers:
-->
<p>输出显示 Deployment 中的 PodSpec 有两个容器:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>redis<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>patch-demo-ctr-2<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>patch-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span></code></pre></div><!--
View the Pods associated with your patched Deployment:
-->
<p>查看与 patch Deployment 相关的 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output shows that the running Pods have different names from the Pods that
were running previously. The Deployment terminated the old Pods and created two
new Pods that comply with the updated Deployment spec. The `2/2` indicates that
each Pod has two Containers:
-->
<p>输出显示正在运行的 Pod 与以前运行的 Pod 有不同的名称。Deployment 终止了旧的 Pod，并创建了两个
符合更新的部署规范的新 Pod。<code>2/2</code> 表示每个 Pod 有两个容器:</p>
<pre tabindex="0"><code>NAME                          READY     STATUS    RESTARTS   AGE
patch-demo-1081991389-2wrn5   2/2       Running   0          1m
patch-demo-1081991389-jmg7b   2/2       Running   0          1m
</code></pre><!--
Take a closer look at one of the patch-demo Pods:
-->
<p>仔细查看其中一个 patch-demo Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod &lt;your-pod-name&gt; --output yaml
</code></pre></div><!--
The output shows that the Pod has two Containers: one running nginx and one running redis:
-->
<p>输出显示 Pod 有两个容器:一个运行 nginx，一个运行 redis:</p>
<pre tabindex="0"><code>containers:
- image: redis
  ...
- image: nginx
  ...
</code></pre><!--
### Notes on the strategic merge patch

The patch you did in the preceding exercise is called a *strategic merge patch*.
Notice that the patch did not replace the `containers` list. Instead it added a new
Container to the list. In other words, the list in the patch was merged with the
existing list. This is not always what happens when you use a strategic merge patch on a list.
In some cases, the list is replaced, not merged.
-->
<h3 id="策略性合并类的-patch-的说明">策略性合并类的 patch 的说明</h3>
<p>你在前面的练习中所做的 patch 称为<code>策略性合并 patch（Strategic Merge Patch)</code>。
请注意，patch 没有替换<code>containers</code> 列表。相反，它向列表中添加了一个新 Container。换句话说，
patch 中的列表与现有列表合并。当你在列表中使用策略性合并 patch 时，并不总是这样。
在某些情况下，列表是替换的，而不是合并的。</p>
<!--
With a strategic merge patch, a list is either replaced or merged depending on its
patch strategy. The patch strategy is specified by the value of the `patchStrategy` key
in a field tag in the Kubernetes source code. For example, the `Containers` field of `PodSpec`
struct has a `patchStrategy` of `merge`:
-->
<p>对于策略性合并 patch，列表可以根据其 patch 策略进行替换或合并。
patch 策略由 Kubernetes 源代码中字段标记中的 <code>patchStrategy</code> 键的值指定。
例如，<code>PodSpec</code> 结构体的 <code>Containers</code> 字段的 <code>patchStrategy</code> 为 <code>merge</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a2f;font-weight:bold">type</span> PodSpec <span style="color:#a2f;font-weight:bold">struct</span> {
  <span style="color:#666">...</span>
  Containers []Container <span style="color:#b44">`json:&#34;containers&#34; patchStrategy:&#34;merge&#34; patchMergeKey:&#34;name&#34; ...`</span>
</code></pre></div><!--
You can also see the patch strategy in the
[OpenApi spec](https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json):
-->
<p>你还可以在 <a href="https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json">OpenApi spec</a>
规范中看到 patch 策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#b44">&#34;io.k8s.api.core.v1.PodSpec&#34;</span><span style="">:</span> {
    <span style="">...</span>
     <span style="color:#008000;font-weight:bold">&#34;containers&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;description&#34;</span>: <span style="color:#b44">&#34;List of containers belonging to the pod. ...
</span><span style="color:#b44">      },
</span><span style="color:#b44">      &#34;</span><span style="">x-kubernetes-patch-merge-key</span><span style="color:#b44">&#34;: &#34;</span><span style="">name</span><span style="color:#b44">&#34;,
</span><span style="color:#b44">      &#34;</span><span style="">x-kubernetes-patch-strategy</span><span style="color:#b44">&#34;: &#34;</span><span style="">merge&#34;</span>
     },
</code></pre></div><!--
And you can see the patch strategy in the
[Kubernetes API documentation](/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core).
-->
<p>你可以在 <a href="/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core">Kubernetes API 文档</a>
中看到 patch 策略。</p>
<!--
Create a file named `patch-file-tolerations.yaml` that has this content:
-->
<p>创建一个名为 <code>patch-file-tolerations.yaml</code> 的文件。内容如下:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>disktype<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>ssd<span style="color:#bbb">
</span></code></pre></div><!--
Patch your Deployment:
-->
<p>对 Deployment 执行 patch 操作：</p>
<pre tabindex="0"><code>kubectl patch deployment patch-demo --patch &quot;$(cat patch-file-containers.yaml)&quot;
</code></pre><!--
View the patched Deployment:
-->
<p>查看修补后的 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment patch-demo --output yaml
</code></pre></div><!--
The output shows that the PodSpec in the Deployment has only one Toleration:
-->
<p>输出结果显示 Deployment 中的 PodSpec 只有一个容忍度设置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">
containers:
- image: redis
  imagePullPolicy: Always
  name: patch-demo-ctr-2
  ...
- image: nginx
  imagePullPolicy: Always
  name: patch-demo-ctr
  ...
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">

tolerations:
      - effect: NoSchedule
        key: disktype
        value: ssd
</code></pre></div><!--
Notice that the `tolerations` list in the PodSpec was replaced, not merged. This is because
the Tolerations field of PodSpec does not have a `patchStrategy` key in its field tag. So the
strategic merge patch uses the default patch strategy, which is `replace`.
-->
<p>请注意，PodSpec 中的 <code>tolerations</code> 列表被替换，而不是合并。这是因为 PodSpec 的 <code>tolerations</code>
的字段标签中没有 <code>patchStrategy</code> 键。所以策略合并 patch 操作使用默认的 patch 策略，也就是 <code>replace</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a2f;font-weight:bold">type</span> PodSpec <span style="color:#a2f;font-weight:bold">struct</span> {
  <span style="color:#666">...</span>
  Tolerations []Toleration <span style="color:#b44">`json:&#34;tolerations,omitempty&#34; protobuf:&#34;bytes,22,opt,name=tolerations&#34;`</span>
</code></pre></div><!--
## Use a JSON merge patch to update a Deployment

A strategic merge patch is different from a
[JSON merge patch](https://tools.ietf.org/html/rfc7386).
With a JSON merge patch, if you
want to update a list, you have to specify the entire new list. And the new list completely
replaces the existing list.
-->
<h2 id="使用-json-合并-patch-更新-deployment">使用 JSON 合并 patch 更新 Deployment</h2>
<p>策略性合并 patch 不同于 <a href="https://tools.ietf.org/html/rfc7386">JSON 合并 patch</a>。
使用 JSON 合并 patch，如果你想更新列表，你必须指定整个新列表。新的列表完全取代现有的列表。</p>
<!--
The `kubectl patch` command has a `type` parameter that you can set to one of these values:
-->
<p><code>kubectl patch</code> 命令有一个 <code>type</code> 参数，你可以将其设置为以下值之一:</p>
<table>
  <!-- tr><th>Parameter value</th><th>Merge type</th></tr -->
  <tr><th>参数值</th><th>合并类型</th></tr>
  <tr><td>json</td><td><a href="https://tools.ietf.org/html/rfc6902">JSON Patch, RFC 6902</a></td></tr>
  <tr><td>merge</td><td><a href="https://tools.ietf.org/html/rfc7386">JSON Merge Patch, RFC 7386</a></td></tr>
  <!-- tr><td>strategic</td><td>Strategic merge patch</td></tr -->
  <tr><td>strategic</td><td>策略合并 patch</td></tr>
</table>
<!--
For a comparison of JSON patch and JSON merge patch, see
[JSON Patch and JSON Merge Patch](https://erosb.github.io/post/json-patch-vs-merge-patch/).
-->
<p>有关 JSON patch 和 JSON 合并 patch 的比较，查看
<a href="https://erosb.github.io/post/json-patch-vs-merge-patch/">JSON patch 和 JSON 合并 patch</a>。</p>
<!--
The default value for the `type` parameter is `strategic`. So in the preceding exercise, you
did a strategic merge patch.
-->
<p><code>type</code> 参数的默认值是 <code>strategic</code>。在前面的练习中，我们做了一个策略性的合并 patch。</p>
<!--
Next, do a JSON merge patch on your same Deployment. Create a file named `patch-file-2.yaml`
that has this content:
-->
<p>下一步，在相同的 Deployment 上执行 JSON 合并 patch。创建一个名为 <code>patch-file-2</code> 的文件。内容如下:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>patch-demo-ctr-3<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span></code></pre></div><!--
In your patch command, set `type` to `merge`:
-->
<p>在 patch 命令中，将 <code>type</code> 设置为 <code>merge</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch deployment patch-demo --type merge --patch <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat patch-file-2.yaml<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
</code></pre></div><!--
View the patched Deployment:
-->
<p>查看修补后的 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment patch-demo --output yaml
</code></pre></div><!--
The `containers` list that you specified in the patch has only one Container.
The output shows that your list of one Container replaced the existing `containers` list.
-->
<p>patch 中指定的<code>containers</code>列表只有一个 Container。
输出显示你所给出的 Contaier 列表替换了现有的 <code>containers</code> 列表。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>patch-demo-ctr-3<span style="color:#bbb">
</span></code></pre></div><!--
List the running Pods:
-->
<p>列表中运行的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
In the output, you can see that the existing Pods were terminated, and new Pods
were created. The `1/1` indicates that each new Pod is running only one Container.
-->
<p>在输出中，你可以看到已经终止了现有的 Pod，并创建了新的 Pod。<code>1/1</code> 表示每个新 Pod只运行一个容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                          READY     STATUS    RESTARTS   AGE
patch-demo-1307768864-69308   1/1       Running   <span style="color:#666">0</span>          1m
patch-demo-1307768864-c86dc   1/1       Running   <span style="color:#666">0</span>          1m
</code></pre></div><!--
## Use strategic merge patch to update a Deployment using the retainKeys strategy

Here's the configuration file for a Deployment that uses the `RollingUpdate` strategy:
-->
<h2 id="使用带-retainkeys-策略的策略合并-patch-更新-deployment">使用带 retainKeys 策略的策略合并 patch 更新 Deployment</h2>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/deployment-retainkeys.yaml" download="application/deployment-retainkeys.yaml"><code>application/deployment-retainkeys.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-deployment-retainkeys-yaml')" title="Copy application/deployment-retainkeys.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-deployment-retainkeys-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>retainkeys-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rollingUpdate</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxSurge</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span>%<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>retainkeys-demo-ctr<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- Create the deployment: -->
<p>创建 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/deployment-retainkeys.yaml
</code></pre></div><!--
At this point, the deployment is created and is using the `RollingUpdate` strategy.

Create a file named `patch-file-no-retainkeys.yaml` that has this content:
-->
<p>这时，Deployment 被创建，并使用 <code>RollingUpdate</code> 策略。</p>
<p>创建一个名为 <code>patch-file-no-retainkeys.yaml</code> 的文件，内容如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate<span style="color:#bbb">
</span></code></pre></div><!-- Patch your Deployment: -->
<p>修补你的 Deployment:</p>
<ul class="nav nav-tabs" id="kubectl-retainkeys-example" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#kubectl-retainkeys-example-0" role="tab" aria-controls="kubectl-retainkeys-example-0" aria-selected="true">Bash</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#kubectl-retainkeys-example-1" role="tab" aria-controls="kubectl-retainkeys-example-1">PowerShell</a></li></ul>
<div class="tab-content" id="kubectl-retainkeys-example"><div id="kubectl-retainkeys-example-0" class="tab-pane show active" role="tabpanel" aria-labelledby="kubectl-retainkeys-example-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
kubectl patch deployment retainkeys-demo --type merge --patch <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat patch-file-no-retainkeys.yaml<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
</code></pre></div></div>
  <div id="kubectl-retainkeys-example-1" class="tab-pane" role="tabpanel" aria-labelledby="kubectl-retainkeys-example-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-posh" data-lang="posh">
kubectl patch deployment <span style="color:#a2f">retainkeys-demo</span> --type merge --patch $(<span style="color:#a2f">Get-Content</span> <span style="color:#a2f">patch-file</span>-no-retainkeys.yaml -Raw)
</code></pre></div></div></div>

<!--
In the output, you can see that it is not possible to set `type` as `Recreate` when a value is defined for `spec.strategy.rollingUpdate`:
-->
<p>在输出中，你可以看到，当 <code>spec.strategy.rollingUpdate</code> 已经拥有取值定义时，
将其 <code>type</code> 设置为 <code>Recreate</code> 是不可能的。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">The Deployment <span style="color:#b44">&#34;retainkeys-demo&#34;</span> is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy <span style="color:#b44">`</span><span style="color:#a2f">type</span><span style="color:#b44">`</span> is <span style="color:#b44">&#39;Recreate&#39;</span>
</code></pre></div><!--
The way to remove the value for `spec.strategy.rollingUpdate` when updating the value for `type` is to use the `retainKeys` strategy for the strategic merge.

Create another file named `patch-file-retainkeys.yaml` that has this content:
-->
<p>更新 <code>type</code> 取值的同时移除 <code>spec.strategy.rollingUpdate</code> 现有值的方法是
为策略性合并操作设置 <code>retainKeys</code> 策略：</p>
<p>创建另一个名为 <code>patch-file-retainkeys.yaml</code> 的文件，内容如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">$retainKeys</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- type<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate<span style="color:#bbb">
</span></code></pre></div><!--
With this patch, we indicate that we want to retain only the `type` key of the `strategy` object. Thus, the `rollingUpdate` will be removed during the patch operation.

Patch your Deployment again with this new patch:
-->
<p>使用此 patch，我们表达了希望只保留 <code>strategy</code> 对象的 <code>type</code> 键。
这样，在 patch 操作期间 <code>rollingUpdate</code> 会被删除。</p>
<p>使用新的 patch 重新修补 Deployment：</p>
<ul class="nav nav-tabs" id="kubectl-retainkeys2-example" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#kubectl-retainkeys2-example-0" role="tab" aria-controls="kubectl-retainkeys2-example-0" aria-selected="true">Bash</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#kubectl-retainkeys2-example-1" role="tab" aria-controls="kubectl-retainkeys2-example-1">PowerShell</a></li></ul>
<div class="tab-content" id="kubectl-retainkeys2-example"><div id="kubectl-retainkeys2-example-0" class="tab-pane show active" role="tabpanel" aria-labelledby="kubectl-retainkeys2-example-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">
kubectl patch deployment retainkeys-demo --type merge --patch <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat patch-file-retainkeys.yaml<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
</code></pre></div></div>
  <div id="kubectl-retainkeys2-example-1" class="tab-pane" role="tabpanel" aria-labelledby="kubectl-retainkeys2-example-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-posh" data-lang="posh">
kubectl patch deployment <span style="color:#a2f">retainkeys-demo</span> --type merge --patch $(<span style="color:#a2f">Get-Content</span> <span style="color:#a2f">patch-file</span>-retainkeys.yaml -Raw)
</code></pre></div></div></div>

<!-- Examine the content of the Deployment: -->
<p>检查 Deployment 的内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment retainkeys-demo --output yaml
</code></pre></div><!--
The output shows that the strategy object in the Deployment does not contain the `rollingUpdate` key anymore:
-->
<p>输出显示 Deployment 中的 <code>strategy</code> 对象不再包含 <code>rollingUpdate</code> 键：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">spec:
  strategy:
    type: Recreate
  template:
</code></pre></div><!--
### Notes on the strategic merge patch using the retainKeys strategy

The patch you did in the preceding exercise is called a *strategic merge patch with retainKeys strategy*. This method introduces a new directive `$retainKeys` that has the following strategies:

- It contains a list of strings.
- All fields needing to be preserved must be present in the `$retainKeys` list.
- The fields that are present will be merged with live object.
- All of the missing fields will be cleared when patching.
- All fields in the `$retainKeys` list must be a superset or the same as the fields present in the patch.
-->
<h3 id="关于使用-retainkeys-策略的策略合并-patch-操作的说明">关于使用 retainKeys 策略的策略合并 patch 操作的说明</h3>
<p>在前文练习中所执行的称作 <em>带 <code>retainKeys</code> 策略的策略合并 patch（Strategic Merge
Patch with retainKeys Strategy）</em>。
这种方法引入了一种新的 <code>$retainKey</code> 指令，具有如下策略：</p>
<ul>
<li>其中包含一个字符串列表；</li>
<li>所有需要被保留的字段必须在 <code>$retainKeys</code> 列表中给出；</li>
<li>对于已有的字段，会和对象上对应的内容合并；</li>
<li>在修补操作期间，未找到的字段都会被清除；</li>
<li>列表 <code>$retainKeys</code> 中的所有字段必须 patch 操作所给字段的超集，或者与之完全一致。</li>
</ul>
<!--
The `retainKeys` strategy does not work for all objects. It only works when the value of the `patchStrategy` key in a field tag in the Kubernetes source code contains `retainKeys`. For example, the `Strategy` field of the `DeploymentSpec` struct has a `patchStrategy` of `retainKeys`:
-->
<p>策略 <code>retainKeys</code> 并不能对所有对象都起作用。它仅对那些 Kubernetes 源码中
<code>patchStrategy</code> 字段标志值包含 <code>retainKeys</code> 的字段有用。
例如 <code>DeploymentSpec</code> 结构的 <code>Strategy</code> 字段就包含了 <code>patchStrategy</code> 为
<code>retainKeys</code> 的标志。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a2f;font-weight:bold">type</span> DeploymentSpec <span style="color:#a2f;font-weight:bold">struct</span> {
  <span style="color:#666">...</span>
  <span style="color:#080;font-style:italic">// +patchStrategy=retainKeys
</span><span style="color:#080;font-style:italic"></span>  Strategy DeploymentStrategy <span style="color:#b44">`json:&#34;strategy,omitempty&#34; patchStrategy:&#34;retainKeys&#34; ...`</span>
</code></pre></div><!--
You can also see the `retainKeys` strategy in the [OpenApi spec](https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json):
-->
<p>你也可以查看 <a href="https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json">OpenAPI 规范</a>中的 <code>retainKeys</code> 策略：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#b44">&#34;io.k8s.api.apps.v1.DeploymentSpec&#34;</span><span style="">:</span> {
   <span style="">...</span>
  <span style="color:#008000;font-weight:bold">&#34;strategy&#34;</span>: {
    <span style="color:#008000;font-weight:bold">&#34;$ref&#34;</span>: <span style="color:#b44">&#34;#/definitions/io.k8s.api.apps.v1.DeploymentStrategy&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;description&#34;</span>: <span style="color:#b44">&#34;The deployment strategy to use to replace existing pods with new ones.&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;x-kubernetes-patch-strategy&#34;</span>: <span style="color:#b44">&#34;retainKeys&#34;</span>
  },
</code></pre></div><!--
And you can see the `retainKeys` strategy in the
[Kubernetes API documentation](/docs/reference/generated/kubernetes-api/v1.22/#deploymentspec-v1-apps).
-->
<p>而且你也可以在
<a href="/docs/reference/generated/kubernetes-api/v1.22/#deploymentspec-v1-apps">Kubernetes API 文档</a>.
中看到 <code>retainKey</code> 策略。</p>
<!--
## Alternate forms of the kubectl patch command

The `kubectl patch` command takes YAML or JSON. It can take the patch as a file or
directly on the command line.
-->
<h2 id="kubectl-patch-命令的其他形式">kubectl patch 命令的其他形式</h2>
<p><code>kubectl patch</code> 命令使用 YAML 或 JSON。它可以接受以文件形式提供的补丁，也可以
接受直接在命令行中给出的补丁。</p>
<!--
Create a file named `patch-file.json` that has this content:
-->
<p>创建一个文件名称是 <code>patch-file.json</code> 内容如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   <span style="color:#008000;font-weight:bold">&#34;spec&#34;</span>: {
      <span style="color:#008000;font-weight:bold">&#34;template&#34;</span>: {
         <span style="color:#008000;font-weight:bold">&#34;spec&#34;</span>: {
            <span style="color:#008000;font-weight:bold">&#34;containers&#34;</span>: [
               {
                  <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;patch-demo-ctr-2&#34;</span>,
                  <span style="color:#008000;font-weight:bold">&#34;image&#34;</span>: <span style="color:#b44">&#34;redis&#34;</span>
               }
            ]
         }
      }
   }
}
</code></pre></div><!--
The following commands are equivalent:
-->
<p>以下命令是等价的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch deployment patch-demo --patch <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat patch-file.yaml<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
kubectl patch deployment patch-demo --patch <span style="color:#b44">&#39;spec:\n template:\n  spec:\n   containers:\n   - name: patch-demo-ctr-2\n     image: redis&#39;</span>

kubectl patch deployment patch-demo --patch <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat patch-file.json<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
kubectl patch deployment patch-demo --patch <span style="color:#b44">&#39;{&#34;spec&#34;: {&#34;template&#34;: {&#34;spec&#34;: {&#34;containers&#34;: [{&#34;name&#34;: &#34;patch-demo-ctr-2&#34;,&#34;image&#34;: &#34;redis&#34;}]}}}}&#39;</span>
</code></pre></div><!--
## Summary

In this exercise, you used `kubectl patch` to change the live configuration
of a Deployment object. You did not change the configuration file that you originally used to
create the Deployment object. Other commands for updating API objects include
[kubectl annotate](/docs/reference/generated/kubectl/kubectl-commands/#annotate),
[kubectl edit](/docs/reference/generated/kubectl/kubectl-commands/#edit),
[kubectl replace](/docs/reference/generated/kubectl/kubectl-commands/#replace),
[kubectl scale](/docs/reference/generated/kubectl/kubectl-commands/#scale),
and
[kubectl apply](/docs/reference/generated/kubectl/kubectl-commands/#apply).
-->
<h2 id="总结">总结</h2>
<p>在本练习中，你使用 <code>kubectl patch</code> 更改了 Deployment 对象的当前配置。
你没有更改最初用于创建 Deployment 对象的配置文件。
用于更新 API 对象的其他命令包括
<a href="/docs/reference/generated/kubectl/kubectl-commands/#annotate"><code>kubectl annotate</code></a>，
<a href="/docs/reference/generated/kubectl/kubectl-commands/#edit"><code>kubectl edit</code></a>，
<a href="/docs/reference/generated/kubectl/kubectl-commands/#replace"><code>kubectl replace</code></a>，
<a href="/docs/reference/generated/kubectl/kubectl-commands/#scale"><code>kubectl scale</code></a>，
和
<a href="/docs/reference/generated/kubectl/kubectl-commands/#apply"><code>kubectl apply</code></a>。</p>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> Strategic merge patch is not supported for custom resources.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 定制资源不支持策略性合并 patch。</div>
</blockquote>
<h2 id="接下来">接下来</h2>
<!--
* [Kubernetes Object Management](/docs/concepts/overview/working-with-objects/object-management/)
* [Managing Kubernetes Objects Using Imperative Commands](/docs/tasks/manage-kubernetes-objects/imperative-command/)
* [Imperative Management of Kubernetes Objects Using Configuration Files](/docs/tasks/manage-kubernetes-objects/imperative-config/)
* [Declarative Management of Kubernetes Objects Using Configuration Files](/docs/tasks/manage-kubernetes-objects/declarative-config/)
-->
<ul>
<li><a href="/zh/docs/concepts/overview/working-with-objects/object-management/">Kubernetes 对象管理</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-command/">使用指令式命令管理 Kubernetes 对象</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-config">使用配置文件执行 Kubernetes 对象的指令式管理</a></li>
<li><a href="/zh/docs/tasks/manage-kubernetes-objects/declarative-config/">使用配置文件对 Kubernetes 对象进行声明式管理</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-94f49ece137035764368f22a98942872">5 - 管理 Secrets</h1>
    <div class="lead">使用 Secrets 管理机密配置数据。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-0ed63ce3c9665aed7ff5a560ff1da843">5.1 - 使用 kubectl 管理 Secret</h1>
    <div class="lead">使用 kubectl 命令行创建 Secret 对象。</div>
	<!--
title: Managing Secrets using kubectl
content_type: task
weight: 10
description: Creating Secret objects using kubectl command line.
-->
<!-- overview -->
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!-- ## Create a Secret -->
<h2 id="create-a-secret">创建 Secret   </h2>
<!--
A `Secret` can contain user credentials required by pods to access a database.
For example, a database connection string consists of a username and password.
You can store the username in a file `./username.txt` and the password in a
file `./password.txt` on your local machine.
 -->
<p>一个 <code>Secret</code> 可以包含 Pod 访问数据库所需的用户凭证。
例如，由用户名和密码组成的数据库连接字符串。
你可以在本地计算机上，将用户名存储在文件 <code>./username.txt</code> 中，将密码存储在文件 <code>./password.txt</code> 中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> -n <span style="color:#b44">&#39;admin&#39;</span> &gt; ./username.txt
<span style="color:#a2f">echo</span> -n <span style="color:#b44">&#39;1f2d1e2e67df&#39;</span> &gt; ./password.txt
</code></pre></div><!--
In these commands, the `-n` flag ensures that the generated files do not have
an extra newline character at the end of the text. This is important because
when `kubectl` reads a file and encodes the content into a base64 string, the
extra newline character gets encoded too.
-->
<p>在这些命令中，<code>-n</code> 标志确保生成的文件在文本末尾不包含额外的换行符。
这一点很重要，因为当 <code>kubectl</code> 读取文件并将内容编码为 base64 字符串时，多余的换行符也会被编码。</p>
<!--
The `kubectl create secret` command packages these files into a Secret and creates
the object on the API server.
-->
<p><code>kubectl create secret</code> 命令将这些文件打包成一个 Secret 并在 API 服务器上创建对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic db-user-pass <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-file<span style="color:#666">=</span>./username.txt <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-file<span style="color:#666">=</span>./password.txt
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>secret/db-user-pass created
</code></pre><!--
The default key name is the filename. You can optionally set the key name using
`--from-file=[key=]source`. For example:
-->
<p>默认密钥名称是文件名。 你可以选择使用 <code>--from-file=[key=]source</code> 来设置密钥名称。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic db-user-pass <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-file<span style="color:#666">=</span><span style="color:#b8860b">username</span><span style="color:#666">=</span>./username.txt <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-file<span style="color:#666">=</span><span style="color:#b8860b">password</span><span style="color:#666">=</span>./password.txt
</code></pre></div><!--
You do not need to escape special characters in password strings that you 
include in a file.
-->
<p>你不需要对文件中包含的密码字符串中的特殊字符进行转义。</p>
<!--
You can also provide Secret data using the `--from-literal=<key>=<value>` tag.
This tag can be specified more than once to provide multiple key-value pairs.
Note that special characters such as `$`, `\`, `*`, `=`, and `!` will be
interpreted by your [shell](https://en.wikipedia.org/wiki/Shell_(computing))
and require escaping.

In most shells, the easiest way to escape the password is to surround it with
single quotes (`'`). For example, if your password is `S!B\*d$zDsb=`,
run the following command:
-->
<p>你还可以使用 <code>--from-literal=&lt;key&gt;=&lt;value&gt;</code> 标签提供 Secret 数据。
可以多次使用此标签，提供多个键值对。
请注意，特殊字符（例如：<code>$</code>，<code>\</code>，<code>*</code>，<code>=</code> 和 <code>!</code>）由你的 <a href="https://en.wikipedia.org/wiki/Shell_(computing)">shell</a>
解释执行，而且需要转义。</p>
<p>在大多数 shell 中，转义密码最简便的方法是用单引号括起来。
比如，如果你的密码是 <code>S!B\*d$zDsb=</code>，
可以像下面一样执行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic db-user-pass <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-literal<span style="color:#666">=</span><span style="color:#b8860b">username</span><span style="color:#666">=</span>devuser <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  --from-literal<span style="color:#666">=</span><span style="color:#b8860b">password</span><span style="color:#666">=</span><span style="color:#b44">&#39;S!B\*d$zDsb=&#39;</span>
</code></pre></div><!-- ## Verify the Secret -->
<h2 id="verify-the-secret">验证 Secret   </h2>
<!-- Check that the Secret was created: -->
<p>检查 secret 是否已创建：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secrets
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME                  TYPE                                  DATA      AGE
db-user-pass          Opaque                                2         51s
</code></pre><!-- You can view a description of the `Secret`: -->
<p>你可以查看 <code>Secret</code> 的描述：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe secrets/db-user-pass
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>Name:            db-user-pass
Namespace:       default
Labels:          &lt;none&gt;
Annotations:     &lt;none&gt;

Type:            Opaque

Data
====
password:    12 bytes
username:    5 bytes
</code></pre><!--
The commands `kubectl get` and `kubectl describe` avoid showing the contents
of a `Secret` by default. This is to protect the `Secret` from being exposed
accidentally, or from being stored in a terminal log.
-->
<p><code>kubectl get</code> 和 <code>kubectl describe</code> 命令默认不显示 <code>Secret</code> 的内容。
这是为了防止 <code>Secret</code> 被意外暴露或存储在终端日志中。</p>
<!-- ## Decoding the Secret  {#decoding-secret} -->
<h2 id="decoding-secret">解码 Secret </h2>
<!--
To view the contents of the Secret you created, run the following command:
-->
<p>要查看创建的 Secret 的内容，运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secret db-user-pass -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.data}&#39;</span>
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{<span style="color:#008000;font-weight:bold">&#34;password&#34;</span>:<span style="color:#b44">&#34;MWYyZDFlMmU2N2Rm&#34;</span>,<span style="color:#008000;font-weight:bold">&#34;username&#34;</span>:<span style="color:#b44">&#34;YWRtaW4=&#34;</span>}
</code></pre></div><!-- 
Now you can decode the `password` data:
-->
<p>现在你可以解码 <code>password</code> 的数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> <span style="color:#b44">&#39;MWYyZDFlMmU2N2Rm&#39;</span> | base64 --decode
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>1f2d1e2e67df
</code></pre><!-- ## Clean Up -->
<h2 id="clean-up">清理   </h2>
<!-- Delete the Secret you created: -->
<p>删除创建的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete secret db-user-pass
</code></pre></div><!-- discussion -->
<h2 id="接下来">接下来</h2>
<!--
- Read more about the [Secret concept](/docs/concepts/configuration/secret/)
- Learn how to [manage Secrets using config files](/docs/tasks/configmap-secret/managing-secret-using-config-file/)
- Learn how to [manage Secrets using kustomize](/docs/tasks/configmap-secret/managing-secret-using-kustomize/)
-->
<ul>
<li>进一步阅读 <a href="/zh/docs/concepts/configuration/secret/">Secret 概念</a></li>
<li>了解如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-config-file/">使用配置文件管理 Secret</a></li>
<li>了解如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kustomize/">使用 kustomize 管理 Secret</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e841cf91fd3566db1e86143ed7a9e13c">5.2 - 使用配置文件管理 Secret</h1>
    <div class="lead">使用资源配置文件创建 Secret 对象。</div>
	<!--  
title: Managing Secrets using Configuration File
content_type: task
weight: 20
description: Creating Secret objects using resource configuration file.
-->
<!-- overview -->
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!-- ## Create the Config file -->
<h2 id="create-the-config-file">创建配置文件   </h2>
<!-- 
You can create a Secret in a file first, in JSON or YAML format, and then
create that object.  The
[Secret](/docs/reference/generated/kubernetes-api/v1.22/#secret-v1-core)
resource contains two maps: `data` and `stringData`.
The `data` field is used to store arbitrary data, encoded using base64. The
`stringData` field is provided for convenience, and it allows you to provide
Secret data as unencoded strings.
The keys of `data` and `stringData` must consist of alphanumeric characters,
`-`, `_` or `.`. 
-->
<p>你可以先用 JSON 或 YAML 格式在文件中创建 Secret，然后创建该对象。
<a href="/docs/reference/generated/kubernetes-api/v1.22/#secret-v1-core">Secret</a>
资源包含2个键值对： <code>data</code> 和 <code>stringData</code>。
<code>data</code> 字段用来存储 base64 编码的任意数据。
提供 <code>stringData</code> 字段是为了方便，它允许 Secret 使用未编码的字符串。
<code>data</code> 和 <code>stringData</code> 的键必须由字母、数字、<code>-</code>，<code>_</code> 或 <code>.</code> 组成。</p>
<!--  
For example, to store two strings in a Secret using the `data` field, convert
the strings to base64 as follows:
-->
<p>例如，要使用 Secret 的 <code>data</code> 字段存储两个字符串，请将字符串转换为 base64 ，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> -n <span style="color:#b44">&#39;admin&#39;</span> | base64
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>YWRtaW4=
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> -n <span style="color:#b44">&#39;1f2d1e2e67df&#39;</span> | base64
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>MWYyZDFlMmU2N2Rm
</code></pre><!-- Write a Secret config file that looks like this: -->
<p>编写一个 Secret 配置文件，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>YWRtaW4=<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>MWYyZDFlMmU2N2Rm<span style="color:#bbb">
</span></code></pre></div><!-- 
Note that the name of a Secret object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names). 
-->
<p>注意，Secret 对象的名称必须是有效的 <a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>.</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--  
The serialized JSON and YAML values of Secret data are encoded as base64
strings. Newlines are not valid within these strings and must be omitted. When
using the `base64` utility on Darwin/macOS, users should avoid using the `-b`
option to split long lines. Conversely, Linux users *should* add the option
`-w 0` to `base64` commands or the pipeline `base64 | tr -d '\n'` if the `-w`
option is not available.
-->
<p>Secret 数据的 JSON 和 YAML 序列化结果是以 base64 编码的。
换行符在这些字符串中无效，必须省略。
在 Darwin/macOS 上使用 <code>base64</code> 工具时，用户不应该使用 <code>-b</code> 选项分割长行。
相反地，Linux 用户 <em>应该</em> 在 <code>base64</code> 地命令中添加 <code>-w 0</code> 选项，
或者在 <code>-w</code> 选项不可用的情况下，输入 <code>base64 | tr -d '\n'</code>。</div>
</blockquote>
<!-- 
For certain scenarios, you may wish to use the `stringData` field instead. This
field allows you to put a non-base64 encoded string directly into the Secret,
and the string will be encoded for you when the Secret is created or updated.
-->
<p>对于某些场景，你可能希望使用 <code>stringData</code> 字段。
这字段可以将一个非 base64 编码的字符串直接放入 Secret 中，
当创建或更新该 Secret 时，此字段将被编码。</p>
<!--  
A practical example of this might be where you are deploying an application
that uses a Secret to store a configuration file, and you want to populate
parts of that configuration file during your deployment process.
-->
<p>上述用例的实际场景可能是这样：当你部署应用时，使用 Secret 存储配置文件，
你希望在部署过程中，填入部分内容到该配置文件。</p>
<!-- For example, if your application uses the following configuration file: -->
<p>例如，如果你的应用程序使用以下配置文件:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiUrl</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;https://my.api.com/api/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&lt;user&gt;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&lt;password&gt;&#34;</span><span style="color:#bbb">
</span></code></pre></div><!-- You could store this in a Secret using the following definition: -->
<p>你可以使用以下定义将其存储在 Secret 中:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">stringData</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">config.yaml</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    apiUrl: &#34;https://my.api.com/api/v1&#34;
</span><span style="color:#b44;font-style:italic">    username: &lt;user&gt;
</span><span style="color:#b44;font-style:italic">    password: &lt;password&gt;</span><span style="color:#bbb">    
</span></code></pre></div><!-- ## Create the Secret object -->
<h2 id="create-the-secret-object">创建 Secret 对象   </h2>
<!-- Now create the Secret using [`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands#apply): -->
<p>现在使用 <a href="/docs/reference/generated/kubectl/kubectl-commands#apply"><code>kubectl apply</code></a> 创建 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f ./secret.yaml
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>secret/mysecret created
</code></pre><!-- ## Check the Secret -->
<h2 id="check-the-secret">检查 Secret  </h2>
<!--  
The `stringData` field is a write-only convenience field. It is never output when
retrieving Secrets. For example, if you run the following command:
-->
<p><code>stringData</code> 字段是只写的。获取 Secret 时，此字段永远不会输出。
例如，如果你运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secret mysecret -o yaml
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">config.yaml</span>:<span style="color:#bbb"> </span>YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHt7dXNlcm5hbWV9fQpwYXNzd29yZDoge3twYXNzd29yZH19<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2018-11-15T20:40:59Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;7225&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>c280ad2e-e916-11e8-98f2-025000000001<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span></code></pre></div><!--  
The commands `kubectl get` and `kubectl describe` avoid showing the contents of a `Secret` by
default. This is to protect the `Secret` from being exposed accidentally to an onlooker,
or from being stored in a terminal log.
To check the actual content of the encoded data, please refer to
[decoding secret](/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret).
-->
<p>命令 <code>kubectl get</code> 和 <code>kubectl describe</code> 默认不显示 <code>Secret</code> 的内容。
这是为了防止 <code>Secret</code> 意外地暴露给旁观者或者保存在终端日志中。
检查编码数据的实际内容，请参考<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret">解码 secret</a>.</p>
<!-- 
If a field, such as `username`, is specified in both `data` and `stringData`,
the value from `stringData` is used. For example, the following Secret definition: 
-->
<p>如果在 <code>data</code> 和 <code>stringData</code> 中都指定了一个字段，比如 <code>username</code>，字段值来自 <code>stringData</code>。
例如，下面的 Secret 定义:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>YWRtaW4=<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">stringData</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>administrator<span style="color:#bbb">
</span></code></pre></div><!-- Results in the following Secret: -->
<p>结果有以下 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>YWRtaW5pc3RyYXRvcg==<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2018-11-15T20:46:46Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysecret<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;7579&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>91460ecb-e917-11e8-98f2-025000000001<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Opaque<span style="color:#bbb">
</span></code></pre></div><!-- Where `YWRtaW5pc3RyYXRvcg==` decodes to `administrator`. -->
<p>其中 <code>YWRtaW5pc3RyYXRvcg==</code> 解码成 <code>administrator</code>。</p>
<!-- ## Clean Up -->
<h2 id="clean-up">清理   </h2>
<!-- To delete the Secret you have created: -->
<p>删除你创建的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete secret mysecret
</code></pre></div><h2 id="接下来">接下来</h2>
<!-- 
- Read more about the [Secret concept](/docs/concepts/configuration/secret/)
- Learn how to [manage Secrets with the `kubectl` command](/docs/tasks/configmap-secret/managing-secret-using-kubectl/)
- Learn how to [manage Secrets using kustomize](/docs/tasks/configmap-secret/managing-secret-using-kustomize/)
-->
<ul>
<li>进一步阅读 <a href="/zh/docs/concepts/configuration/secret/">Secret 概念</a></li>
<li>了解如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/">使用 <code>kubectl</code> 命令管理 Secret</a></li>
<li>了解如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kustomize/">使用 kustomize 管理 Secret</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a0ff2e3ba8af5670d5dc3d94c4bd0a68">5.3 - 使用 Kustomize 管理 Secret</h1>
    <div class="lead">使用 kustomization.yaml 文件创建 Secret 对象。</div>
	<!-- 
title: Managing Secrets using Kustomize
content_type: task
weight: 30
description: Creating Secret objects using kustomization.yaml file.
-->
<!-- overview -->
<!--  
Since Kubernetes v1.14, `kubectl` supports
[managing objects using Kustomize](/docs/tasks/manage-kubernetes-objects/kustomization/).
Kustomize provides resource Generators to create Secrets and ConfigMaps. The
Kustomize generators should be specified in a `kustomization.yaml` file inside
a directory. After generating the Secret, you can create the Secret on the API
server with `kubectl apply`.
-->
<p>从 kubernetes v1.14 开始，<code>kubectl</code> 支持<a href="/zh/docs/tasks/manage-kubernetes-objects/kustomization/">使用 Kustomize 管理对象</a>。
Kustomize 提供了资源生成器（Generators）来创建 Secret 和 ConfigMap。
Kustomize 生成器应该在某个目录的 <code>kustomization.yaml</code> 文件中指定。
生成 Secret 后，你可以使用 <code>kubectl apply</code> 在 API 服务器上创建该 Secret。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!-- ## Create the Kustomization file -->
<h2 id="create-the-kustomization-file">创建 Kustomization 文件   </h2>
<!--  
You can generate a Secret by defining a `secretGenerator` in a
`kustomization.yaml` file that references other existing files.
For example, the following kustomization file references the
`./username.txt` and the `./password.txt` files:
-->
<p>你可以在 <code>kustomization.yaml</code> 中定义 <code>secreteGenerator</code>，并在定义中引用其他现成的文件，生成 Secret。
例如：下面的 kustomization 文件 引用了 <code>./username.txt</code> 和 <code>./password.txt</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">secretGenerator</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>db-user-pass<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">files</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- username.txt<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- password.txt<span style="color:#bbb">
</span></code></pre></div><!--  
You can also define the `secretGenerator` in the `kustomization.yaml`
file by providing some literals.
For example, the following `kustomization.yaml` file contains two literals
for `username` and `password` respectively:
-->
<p>你也可以在 <code>kustomization.yaml</code> 文件中指定一些字面量定义 <code>secretGenerator</code>。
例如：下面的 <code>kustomization.yaml</code> 文件中包含了 <code>username</code> 和 <code>password</code> 两个字面量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">secretGenerator</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>db-user-pass<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">literals</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- username=admin<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- password=1f2d1e2e67df<span style="color:#bbb">
</span></code></pre></div><!-- 
You can also define the `secretGenerator` in the `kustomization.yaml`
file by providing `.env` files.
For example, the following `kustomization.yaml` file pulls in data from
`.env.secret` file:
-->
<p>你也可以使用 <code>.env</code> 文件在 <code>kustomization.yaml</code> 中定义 <code>secretGenerator</code>。
例如：下面的 <code>kustomization.yaml</code> 文件从 <code>.env.secret</code> 文件获取数据。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">secretGenerator</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>db-user-pass<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">envs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- .env.secret<span style="color:#bbb">
</span></code></pre></div><!--
Note that in all cases, you don't need to base64 encode the values.
-->
<p>注意，上面两种情况，你都不需要使用 base64 编码。</p>
<!-- ## Create the Secret -->
<h2 id="create-the-secret">创建 Secret   </h2>
<!-- Apply the directory containing the `kustomization.yaml` to create the Secret. -->
<p>使用 <code>kubectl apply</code> 命令应用包含 <code>kustomization.yaml</code> 文件的目录创建 Secret。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -k .
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>secret/db-user-pass-96mffmfh4k created
</code></pre><!-- 
Note that when a Secret is generated, the Secret name is created by hashing
the Secret data and appending the hash value to the name. This ensures that
a new Secret is generated each time the data is modified. 
-->
<p>请注意，生成 Secret 时，Secret 的名称最终是由 <code>name</code> 字段和数据的哈希值拼接而成。
这将保证每次修改数据时生成一个新的 Secret。</p>
<!-- ## Check the Secret created -->
<h2 id="check-the-secret-created">检查创建的 Secret   </h2>
<!-- You can check that the secret was created: -->
<p>你可以检查刚才创建的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secrets
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME                             TYPE                                  DATA      AGE
db-user-pass-96mffmfh4k          Opaque                                2         51s
</code></pre><!-- You can view a description of the secret: -->
<p>你可以看到 Secret 的描述：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe secrets/db-user-pass-96mffmfh4k
</code></pre></div><!-- The output is similar to: -->
<p>输出类似于：</p>
<pre tabindex="0"><code>Name:            db-user-pass-96mffmfh4k
Namespace:       default
Labels:          &lt;none&gt;
Annotations:     &lt;none&gt;

Type:            Opaque

Data
====
password.txt:    12 bytes
username.txt:    5 bytes
</code></pre><!-- 
The commands `kubectl get` and `kubectl describe` avoid showing the contents of a `Secret` by
default. This is to protect the `Secret` from being exposed accidentally to an onlooker,
or from being stored in a terminal log.
To check the actual content of the encoded data, please refer to
[decoding secret](/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret). 
-->
<p><code>kubectl get</code> 和 <code>kubectl describe</code> 命令默认不显示 <code>Secret</code> 的内容。
这是为了防止 <code>Secret</code> 被意外暴露给旁观者或存储在终端日志中。
检查编码后的实际内容，请参考<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/#decoding-secret">解码 secret</a>。
--&gt;</p>
<!-- ## Clean Up -->
<h2 id="clean-up">清理   </h2>
<!-- To delete the Secret you have created: -->
<p>删除你创建的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete secret db-user-pass-96mffmfh4k
</code></pre></div><!-- Optional section; add links to information related to this topic. -->
<h2 id="接下来">接下来</h2>
<!-- 
- Read more about the [Secret concept](/docs/concepts/configuration/secret/)
- Learn how to [manage Secrets with the `kubectl` command](/docs/tasks/configmap-secret/managing-secret-using-kubectl/)
- Learn how to [manage Secrets using config file](/docs/tasks/configmap-secret/managing-secret-using-config-file/) 
-->
<ul>
<li>进一步阅读 <a href="/zh/docs/concepts/configuration/secret/">Secret 概念</a></li>
<li>了解如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/">使用 <code>kubectl</code> 命令管理 Secret</a></li>
<li>了解如何<a href="/zh/docs/tasks/configmap-secret/managing-secret-using-config-file/">使用配置文件管理 Secret</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-866924fa095f897ede8dfdcab9e97942">6 - 给应用注入数据</h1>
    <div class="lead">给你的工作负载 Pod 指定配置和其他数据。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c9af1e81bb6e109f6c41febe44f0931b">6.1 - 为容器设置启动时要执行的命令和参数</h1>
    
	<!--
title: Define a Command and Arguments for a Container
content_type: task
weight: 10
-->
<!-- overview -->
<!--
This page shows how to define commands and arguments when you run a container
in a <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>.
-->
<p>本页将展示如何为 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a>
中容器设置启动时要执行的命令及其参数。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Define a command and arguments when you create a Pod

When you create a Pod, you can define a command and arguments for the
containers that run in the Pod. To define a command, include the `command`
field in the configuration file. To define arguments for the command, include
the `args` field in the configuration file. The command and arguments that
you define cannot be changed after the Pod is created.
-->
<h2 id="创建-pod-时设置命令及参数">创建 Pod 时设置命令及参数</h2>
<p>创建 Pod 时，可以为其下的容器设置启动时要执行的命令及其参数。如果要设置命令，就填写在配置文件的 <code>command</code> 字段下，如果要设置命令的参数，就填写在配置文件的 <code>args</code> 字段下。一旦 Pod 创建完成，该命令及其参数就无法再进行更改了。</p>
<!--
The command and arguments that you define in the configuration file
override the default command and arguments provided by the container image.
If you define args, but do not define a command, the default command is used
with your new arguments.
-->
<p>如果在配置文件中设置了容器启动时要执行的命令及其参数，那么容器镜像中自带的命令与参数将会被覆盖而不再执行。如果配置文件中只是设置了参数，却没有设置其对应的命令，那么容器镜像中自带的命令会使用该新参数作为其执行时的参数。</p>
<!--
The `command` field corresponds to `entrypoint` in some container
runtimes. Refer to the [Notes](#notes) below.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 在有些容器运行时中，<code>command</code> 字段对应 <code>entrypoint</code>，请参阅下面的
<a href="#notes">说明事项</a>。</div>
</blockquote>
<!--
In this exercise, you create a Pod that runs one container. The configuration
file for the Pod defines a command and two arguments:
-->
<p>本示例中，将创建一个只包含单个容器的 Pod。在 Pod 配置文件中设置了一个命令与两个参数：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/commands.yaml" download="pods/commands.yaml"><code>pods/commands.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-commands-yaml')" title="Copy pods/commands.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-commands-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>command-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">purpose</span>:<span style="color:#bbb"> </span>demonstrate-command<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>command-demo-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;printenv&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;HOSTNAME&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;KUBERNETES_PORT&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Create a Pod based on the YAML configuration file:
-->
<ol>
<li>
<p>基于 YAML 文件创建一个 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/commands.yaml
</code></pre></div></li>
</ol>
<!--
1. List the running Pods:
-->
<ol start="2">
<li>
<p>获取正在运行的 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output shows that the container that ran in the command-demo Pod has completed.
-->
<p>查询结果显示在 command-demo 这个 Pod 下运行的容器已经启动完成。</p>
</li>
</ol>
<!--
1. To see the output of the command that ran in the container, view the logs
from the Pod:
-->
<ol start="3">
<li>
<p>如果要获取容器启动时执行命令的输出结果，可以通过 Pod 的日志进行查看：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs command-demo
</code></pre></div><!--
The output shows the values of the HOSTNAME and KUBERNETES_PORT environment variables:
-->
<p>日志中显示了 HOSTNAME 与 KUBERNETES_PORT 这两个环境变量的值：</p>
<pre tabindex="0"><code>command-demo
tcp://10.3.240.1:443
</code></pre></li>
</ol>
<!--
## Use environment variables to define arguments

In the preceding example, you defined the arguments directly by
providing strings. As an alternative to providing strings directly,
you can define arguments by using environment variables:
-->
<h2 id="使用环境变量来设置参数">使用环境变量来设置参数</h2>
<p>在上面的示例中，我们直接将一串字符作为命令的参数。除此之外，我们还可以将环境变量作为命令的参数。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MESSAGE<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;hello world&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/echo&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;$(MESSAGE)&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
This means you can define an argument for a Pod using any of
the techniques available for defining environment variables, including
[ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/)
and
[Secrets](/docs/concepts/configuration/secret/).
-->
<p>这意味着你可以将那些用来设置环境变量的方法应用于设置命令的参数，其中包括了
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMaps</a> 与
<a href="/zh/docs/concepts/configuration/secret/">Secrets</a>。</p>
<!--
The environment variable appears in parentheses, `"$(VAR)"`. This is
required for the variable to be expanded in the `command` or `args` field.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 环境变量需要加上括号，类似于 <code>&quot;$(VAR)&quot;</code>。这是在 <code>command</code> 或 <code>args</code> 字段使用变量的格式要求。</div>
</blockquote>
<!--
## Run a command in a shell

In some cases, you need your command to run in a shell. For example, your
command might consist of several commands piped together, or it might be a shell
script. To run your command in a shell, wrap it like this:
-->
<h2 id="在-shell-来执行命令">在 Shell 来执行命令</h2>
<p>有时候，你需要在 Shell 脚本中运行命令。
例如，你要执行的命令可能由多个命令组合而成，或者它就是一个 Shell 脚本。
这时，就可以通过如下方式在 Shell 中执行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">command: <span style="color:#666">[</span><span style="color:#b44">&#34;/bin/sh&#34;</span><span style="color:#666">]</span>
args: <span style="color:#666">[</span><span style="color:#b44">&#34;-c&#34;</span>, <span style="color:#b44">&#34;while true; do echo hello; sleep 10;done&#34;</span><span style="color:#666">]</span>
</code></pre></div><!--
## Notes

This table summarizes the field names used by Docker and Kubernetes.

|              Description               |    Docker field name   | Kubernetes field name |
|----------------------------------------|------------------------|---------------------|
|  The command run by the container      |   Entrypoint           |      command        |
|  The arguments passed to the command   |   Cmd                  |      args           |
-->
<h2 id="notes">说明事项 </h2>
<p>下表给出了 Docker 与 Kubernetes 中对应的字段名称。</p>
<table>
<thead>
<tr>
<th>描述</th>
<th>Docker 字段名称</th>
<th>Kubernetes 字段名称</th>
</tr>
</thead>
<tbody>
<tr>
<td>容器执行的命令</td>
<td>Entrypoint</td>
<td>command</td>
</tr>
<tr>
<td>传给命令的参数</td>
<td>Cmd</td>
<td>args</td>
</tr>
</tbody>
</table>
<!--
When you override the default Entrypoint and Cmd, these rules apply:

* If you do not supply `command` or `args` for a Container, the defaults defined
in the Docker image are used.

* If you supply a `command` but no `args` for a Container, only the supplied
`command` is used. The default EntryPoint and the default Cmd defined in the Docker
image are ignored.

* If you supply only `args` for a Container, the default Entrypoint defined in
the Docker image is run with the `args` that you supplied.

* If you supply a `command` and `args`, the default Entrypoint and the default
Cmd defined in the Docker image are ignored. Your `command` is run with your
`args`.
-->
<p>如果要覆盖默认的 Entrypoint 与 Cmd，需要遵循如下规则：</p>
<ul>
<li>
<p>如果在容器配置中没有设置 <code>command</code> 或者 <code>args</code>，那么将使用 Docker 镜像自带的命令及其参数。</p>
</li>
<li>
<p>如果在容器配置中只设置了 <code>command</code> 但是没有设置 <code>args</code>，那么容器启动时只会执行该命令，
Docker 镜像中自带的命令及其参数会被忽略。</p>
</li>
<li>
<p>如果在容器配置中只设置了 <code>args</code>，那么 Docker 镜像中自带的命令会使用该新参数作为其执行时的参数。</p>
</li>
<li>
<p>如果在容器配置中同时设置了 <code>command</code> 与 <code>args</code>，那么 Docker 镜像中自带的命令及其参数会被忽略。
容器启动时只会执行配置中设置的命令，并使用配置中设置的参数作为命令的参数。</p>
</li>
</ul>
<!--
Here are some examples:

| Image Entrypoint   |    Image Cmd     | Container command   |  Container args    |    Command run   |
|--------------------|------------------|---------------------|--------------------|------------------|
|     `[/ep-1]`      |   `[foo bar]`    |   &lt;not set&gt;   |   &lt;not set&gt;  | `[ep-1 foo bar]` |
|     `[/ep-1]`      |   `[foo bar]`    |      `[/ep-2]`      |   &lt;not set&gt;  |     `[ep-2]`     |
|     `[/ep-1]`      |   `[foo bar]`    |   &lt;not set&gt;   |     `[zoo boo]`    | `[ep-1 zoo boo]` |
|     `[/ep-1]`      |   `[foo bar]`    |   `[/ep-2]`         |     `[zoo boo]`    | `[ep-2 zoo boo]` |
-->
<p>下面是一些例子：</p>
<table>
<thead>
<tr>
<th>镜像 Entrypoint</th>
<th>镜像 Cmd</th>
<th>容器 command</th>
<th>容器 args</th>
<th>命令执行</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>[/ep-1]</code></td>
<td><code>[foo bar]</code></td>
<td>&lt;not set&gt;</td>
<td>&lt;not set&gt;</td>
<td><code>[ep-1 foo bar]</code></td>
</tr>
<tr>
<td><code>[/ep-1]</code></td>
<td><code>[foo bar]</code></td>
<td><code>[/ep-2]</code></td>
<td>&lt;not set&gt;</td>
<td><code>[ep-2]</code></td>
</tr>
<tr>
<td><code>[/ep-1]</code></td>
<td><code>[foo bar]</code></td>
<td>&lt;not set&gt;</td>
<td><code>[zoo boo]</code></td>
<td><code>[ep-1 zoo boo]</code></td>
</tr>
<tr>
<td><code>[/ep-1]</code></td>
<td><code>[foo bar]</code></td>
<td><code>[/ep-2]</code></td>
<td><code>[zoo boo]</code></td>
<td><code>[ep-2 zoo boo]</code></td>
</tr>
</tbody>
</table>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [configuring pods and containers](/docs/tasks/).
* Learn more about [running commands in a container](/docs/tasks/debug-application-cluster/get-shell-running-container/).
* See [Container](/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core).
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/">配置 Pod 和容器</a></li>
<li>进一步了解<a href="/zh/docs/tasks/debug-application-cluster/get-shell-running-container/">在容器中运行命令</a></li>
<li>参阅 <a href="/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core">Container</a>
API 资源</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-82c93897176489678232542102daea40">6.2 - 为容器设置环境变量</h1>
    
	<!--
title: Define Environment Variables for a Container
content_type: task
weight: 20
-->
<!-- overview -->
<!--
This page shows how to define environment variables for a container
in a Kubernetes Pod. 
-->
<p>本页将展示如何为 kubernetes Pod 下的容器设置环境变量。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!--
## Define an environment variable for a container
-->
<h2 id="为容器设置一个环境变量">为容器设置一个环境变量</h2>
<!--
When you create a Pod, you can set environment variables for the containers
that run in the Pod. To set environment variables, include the `env` or
`envFrom` field in the configuration file.
-->
<p>创建 Pod 时，可以为其下的容器设置环境变量。通过配置文件的 <code>env</code> 或者 <code>envFrom</code> 字段来设置环境变量。</p>
<!--
In this exercise, you create a Pod that runs one container. The configuration
file for the Pod defines an environment variable with name `DEMO_GREETING` and
value `"Hello from the environment"`. Here is the configuration manifest for the
Pod:
-->
<p>本示例中，将创建一个只包含单个容器的 Pod。Pod 的配置文件中设置环境变量的名称为 <code>DEMO_GREETING</code>，
其值为 <code>&quot;Hello from the environment&quot;</code>。下面是 Pod 的配置清单：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/envars.yaml" download="pods/inject/envars.yaml"><code>pods/inject/envars.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-envars-yaml')" title="Copy pods/inject/envars.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-envars-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>envar-demo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">purpose</span>:<span style="color:#bbb"> </span>demonstrate-envars<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>envar-demo-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>DEMO_GREETING<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Hello from the environment&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>DEMO_FAREWELL<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Such a sweet sorrow&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Create a Pod based on that manifest:
-->
<ol>
<li>
<p>基于配置清单创建一个 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/inject/envars.yaml
</code></pre></div></li>
</ol>
<!--
1. List the running Pods:
-->
<ol start="2">
<li>
<p>获取一下当前正在运行的 Pods 信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">purpose</span><span style="color:#666">=</span>demonstrate-envars
</code></pre></div> <!--
 The output is similar to this:
 -->
<p>查询结果应为：</p>
<pre tabindex="0"><code>NAME            READY     STATUS    RESTARTS   AGE
envar-demo      1/1       Running   0          9s
</code></pre></li>
</ol>
<!--
1. List the Pod's container environment variables:
-->
<ol start="3">
<li>
<p>列出 Pod 容器的环境变量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> envar-demo -- printenv
</code></pre></div> <!--
 The output is similar to this:
 -->
<p>打印结果应为：</p>
<pre tabindex="0"><code>NODE_VERSION=4.4.2
EXAMPLE_SERVICE_PORT_8080_TCP_ADDR=10.3.245.237
HOSTNAME=envar-demo
...
DEMO_GREETING=Hello from the environment
DEMO_FAREWELL=Such a sweet sorrow
</code></pre></li>
</ol>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> The environment variables set using the <code>env</code> or <code>envFrom</code> field
override any environment variables specified in the container image.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 通过 <code>env</code> 或 <code>envFrom</code> 字段设置的环境变量将覆盖容器镜像中指定的所有环境变量。</div>
</blockquote>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> Environment variables may reference each other, however ordering is important.
Variables making use of others defined in the same context must come later in
the list. Similarly, avoid circular references.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 环境变量可以互相引用，但是顺序很重要。
使用在相同上下文中定义的其他变量的变量必须在列表的后面。
同样，请避免使用循环引用。</div>
</blockquote>
<!--
## Using environment variables inside of your config

Environment variables that you define in a Pod's configuration can be used
elsewhere in the configuration, for example in commands and arguments that
you set for the Pod's containers.
In the example configuration below, the `GREETING`, `HONORIFIC`, and
`NAME` environment variables are set to `Warm greetings to`, `The Most
Honorable`, and `Kubernetes`, respectively. Those environment variables
are then used in the CLI arguments passed to the `env-print-demo`
container.
-->
<h2 id="在配置中使用环境变量">在配置中使用环境变量</h2>
<p>您在 Pod 的配置中定义的环境变量可以在配置的其他地方使用，
例如可用在为 Pod 的容器设置的命令和参数中。
在下面的示例配置中，环境变量 <code>GREETING</code> ，<code>HONORIFIC</code> 和 <code>NAME</code> 分别设置为 <code>Warm greetings to</code> ，
<code>The Most Honorable</code> 和 <code>Kubernetes</code>。然后这些环境变量在传递给容器 <code>env-print-demo</code> 的 CLI 参数中使用。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>print-greeting<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>env-print-demo<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>bash<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>GREETING<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Warm greetings to&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>HONORIFIC<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;The Most Honorable&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>NAME<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Kubernetes&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;echo&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;$(GREETING) $(HONORIFIC) $(NAME)&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
Upon creation, the command `echo Warm greetings to The Most Honorable Kubernetes` is run on the container.
-->
<p>创建后，命令 <code>echo Warm greetings to The Most Honorable Kubernetes</code> 将在容器中运行。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/).
* Learn about [using secrets as environment variables](/docs/user-guide/secrets/#using-secrets-as-environment-variables).
* See [EnvVarSource](/docs/reference/generated/kubernetes-api/v1.22/#envvarsource-v1-core).
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">环境变量</a></li>
<li>进一步了解<a href="/zh/docs/concepts/configuration/secret/#using-secrets-as-environment-variables">通过环境变量来使用 Secret</a></li>
<li>关于 <a href="/docs/reference/generated/kubernetes-api/v1.22/#envvarsource-v1-core">EnvVarSource</a> 资源的信息。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eff97c25c917cdb414eda016df0e2bca">6.3 - 定义相互依赖的环境变量</h1>
    
	<!-- 
title: Define Dependent Environment Variables
-->
<!-- overview -->
<!-- 
This page shows how to define dependent environment variables for a container
in a Kubernetes Pod.
-->
<p>本页展示了如何为 Kubernetes Pod 中的容器定义相互依赖的环境变量。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!-- 
## Define an environment dependent variable for a container

When you create a Pod, you can set dependent environment variables for the containers that run in the Pod. To set dependent environment variables, you can use $(VAR_NAME) in the `value` of `env` in the configuration file.

In this exercise, you create a Pod that runs one container. The configuration
file for the Pod defines an dependent environment variable with common usage defined. Here is the configuration manifest for the
Pod:
-->
<h2 id="define-an-environment-dependent-variable-for-a-container">为容器定义相互依赖的环境变量  </h2>
<p>当创建一个 Pod 时，你可以为运行在 Pod 中的容器设置相互依赖的环境变量。
设置相互依赖的环境变量，你就可以在配置清单文件的 <code>env</code> 的 <code>value</code> 中使用 $(VAR_NAME)。</p>
<p>在本练习中，你会创建一个单容器的 Pod。
此 Pod 的配置文件定义了一个已定义常用用法的相互依赖的环境变量。
下面是 Pod 的配置清单：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/dependent-envars.yaml" download="pods/inject/dependent-envars.yaml"><code>pods/inject/dependent-envars.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-dependent-envars-yaml')" title="Copy pods/inject/dependent-envars.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-dependent-envars-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dependent-envars-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dependent-envars-demo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- while true; do echo -en &#39;\n&#39;; printf UNCHANGED_REFERENCE=$UNCHANGED_REFERENCE&#39;\n&#39;; printf SERVICE_ADDRESS=$SERVICE_ADDRESS&#39;\n&#39;;printf ESCAPED_REFERENCE=$ESCAPED_REFERENCE&#39;\n&#39;; sleep 30; done;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- sh<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SERVICE_PORT<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;80&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SERVICE_IP<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;172.17.0.1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>UNCHANGED_REFERENCE<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>PROTOCOL<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;https&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SERVICE_ADDRESS<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ESCAPED_REFERENCE<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;$$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 1. Create a Pod based on that manifest: -->
<ol>
<li>
<p>依据清单创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/inject/dependent-envars.yaml
</code></pre></div><pre tabindex="0"><code>pod/dependent-envars-demo created
</code></pre> <!-- 2. List the running Pods: -->
</li>
<li>
<p>列出运行的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods dependent-envars-demo
</code></pre></div><pre tabindex="0"><code>NAME                      READY     STATUS    RESTARTS   AGE
dependent-envars-demo     1/1       Running   0          9s
</code></pre> <!-- 3. Check the logs for the container running in your Pod: -->
</li>
<li>
<p>检查 Pod 中运行容器的日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs pod/dependent-envars-demo
</code></pre></div><pre tabindex="0"><code>
UNCHANGED_REFERENCE=$(PROTOCOL)://172.17.0.1:80
SERVICE_ADDRESS=https://172.17.0.1:80
ESCAPED_REFERENCE=$(PROTOCOL)://172.17.0.1:80
</code></pre></li>
</ol>
<!-- 
As shown above, you have defined the correct dependency reference of `SERVICE_ADDRESS`, bad dependency reference of `UNCHANGED_REFERENCE` and skip dependent references of `ESCAPED_REFERENCE`.

When an environment variable is already defined when being referenced,
the reference can be correctly resolved, such as in the `SERVICE_ADDRESS` case.
-->
<p>如上所示，你已经定义了 <code>SERVICE_ADDRESS</code> 的正确依赖引用，
<code>UNCHANGED_REFERENCE</code> 的错误依赖引用，
并跳过了 <code>ESCAPED_REFERENCE</code> 的依赖引用。</p>
<p>如果环境变量被引用时已事先定义，则引用可以正确解析，
比如 <code>SERVICE_ADDRESS</code> 的例子。</p>
<!-- 
When the environment variable is undefined or only includes some variables, the undefined environment variable is treated as a normal string, such as `UNCHANGED_REFERENCE`. Note that incorrectly parsed environment variables, in general, will not block the container from starting.

The `$(VAR_NAME)` syntax can be escaped with a double `$`, ie: `$$(VAR_NAME)`.
Escaped references are never expanded, regardless of whether the referenced variable
is defined or not. This can be seen from the `ESCAPED_REFERENCE` case above.
-->
<p>当环境变量未定义或仅包含部分变量时，未定义的变量会被当做普通字符串对待，
比如 <code>UNCHANGED_REFERENCE</code> 的例子。
注意，解析不正确的环境变量通常不会阻止容器启动。</p>
<p><code>$(VAR_NAME)</code> 这样的语法可以用两个 <code>$</code> 转义，既：<code>$$(VAR_NAME)</code>。
无论引用的变量是否定义，转义的引用永远不会展开。
这一点可以从上面 <code>ESCAPED_REFERENCE</code> 的例子得到印证。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/).
* See [EnvVarSource](/docs/reference/generated/kubernetes-api/v1.22/#envvarsource-v1-core).
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">环境变量</a>.</li>
<li>参阅 <a href="/docs/reference/generated/kubernetes-api/v1.22/#envvarsource-v1-core">EnvVarSource</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-66c0456fdbef5e5116dd606d1e6f73cc">6.4 - 通过环境变量将 Pod 信息呈现给容器</h1>
    
	<!--
title: Expose Pod Information to Containers Through Environment Variables
content_type: task
weight: 30
-->
<!-- overview -->
<!--
This page shows how a Pod can use environment variables to expose information
about itself to Containers running in the Pod. Environment variables can expose
Pod fields and Container fields.
-->
<p>此页面展示 Pod 如何使用环境变量把自己的信息呈现给 Pod 中运行的容器。
环境变量可以呈现 Pod 的字段和容器字段。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<h2 id="downward-api">Downward API</h2>
<!--
There are two ways to expose Pod and Container fields to a running Container:

* Environment variables
* [Volume Files](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api)

Together, these two ways of exposing Pod and Container fields are called the
*Downward API*.
-->
<p>有两种方式可以将 Pod 和 Container 字段呈现给运行中的容器：</p>
<ul>
<li>环境变量</li>
<li><a href="/zh/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api">卷文件</a></li>
</ul>
<p>这两种呈现 Pod 和 Container 字段的方式统称为 <em>Downward API</em>。</p>
<!--
## Use Pod fields as values for environment variables

In this exercise, you create a Pod that has one Container. Here is the
configuration file for the Pod:
-->
<h2 id="用-pod-字段作为环境变量的值">用 Pod 字段作为环境变量的值</h2>
<p>在这个练习中，你将创建一个包含一个容器的 Pod。这是该 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/dapi-envars-pod.yaml" download="pods/inject/dapi-envars-pod.yaml"><code>pods/inject/dapi-envars-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-dapi-envars-pod-yaml')" title="Copy pods/inject/dapi-envars-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-dapi-envars-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-envars-fieldref<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- while true; do<span style="color:#bbb">
</span><span style="color:#bbb">          </span>echo -en &#39;\n&#39;;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>sleep 10;<span style="color:#bbb">
</span><span style="color:#bbb">        </span>done;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_NODE_NAME<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>spec.nodeName<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_POD_NAME<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>metadata.name<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_POD_NAMESPACE<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>metadata.namespace<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_POD_IP<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>status.podIP<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_POD_SERVICE_ACCOUNT<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>spec.serviceAccountName<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see five environment variables. The `env`
field is an array of
[EnvVars](/docs/reference/generated/kubernetes-api/v1.22/#envvar-v1-core).
The first element in the array specifies that the `MY_NODE_NAME` environment
variable gets its value from the Pod's `spec.nodeName` field. Similarly, the
other environment variables get their names from Pod fields.
-->
<p>这个配置文件中，你可以看到五个环境变量。<code>env</code> 字段是一个
<a href="/docs/reference/generated/kubernetes-api/v1.22/#envvar-v1-core">EnvVars</a>.
对象的数组。
数组中第一个元素指定 <code>MY_NODE_NAME</code> 这个环境变量从 Pod 的 <code>spec.nodeName</code> 字段获取变量值。
同样，其它环境变量也是从 Pod 的字段获取它们的变量值。</p>
<!--
The fields in this example are Pod fields. They are not fields of the
Container in the Pod.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 本示例中的字段是 Pod 字段，不是 Pod 中 Container 的字段。</div>
</blockquote>
<!--
Create the Pod:
-->
<p>创建Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-pod.yaml
</code></pre></div><!--
Verify that the Container in the Pod is running:
-->
<p>验证 Pod 中的容器运行正常：</p>
<pre tabindex="0"><code>kubectl get pods
</code></pre><!--
View the Container's logs:
-->
<p>查看容器日志：</p>
<pre tabindex="0"><code>kubectl logs dapi-envars-fieldref
</code></pre><!--
The output shows the values of selected environment variables:
-->
<p>输出信息显示了所选择的环境变量的值：</p>
<pre tabindex="0"><code>minikube
dapi-envars-fieldref
default
172.17.0.4
default
</code></pre><!--
To see why these values are in the log, look at the `command` and `args` fields
in the configuration file. When the Container starts, it writes the values of
five environment variables to stdout. It repeats this every ten seconds.

Next, get a shell into the Container that is running in your Pod:
-->
<p>要了解为什么这些值在日志中，请查看配置文件中的<code>command</code> 和 <code>args</code>字段。
当容器启动时，它将五个环境变量的值写入 stdout。每十秒重复执行一次。</p>
<p>接下来，通过打开一个 Shell 进入 Pod 中运行的容器：</p>
<pre tabindex="0"><code>kubectl exec -it dapi-envars-fieldref -- sh
</code></pre><!--
In your shell, view the environment variables:
-->
<p>在 Shell 中，查看环境变量：</p>
<pre tabindex="0"><code>/# printenv
</code></pre><!--
The output shows that certain environment variables have been assigned the
values of Pod fields:
-->
<p>输出信息显示环境变量已经设置为 Pod 字段的值。</p>
<pre tabindex="0"><code>MY_POD_SERVICE_ACCOUNT=default
...
MY_POD_NAMESPACE=default
MY_POD_IP=172.17.0.4
...
MY_NODE_NAME=minikube
...
MY_POD_NAME=dapi-envars-fieldref
</code></pre><!--
## Use Container fields as values for environment variables

In the preceding exercise, you used Pod fields as the values for environment
variables. In this next exercise, you use Container fields as the values for
environment variables. Here is the configuration file for a Pod that has one
container:
-->
<h2 id="用-container-字段作为环境变量的值">用 Container 字段作为环境变量的值</h2>
<p>前面的练习中，你将 Pod 字段作为环境变量的值。
接下来这个练习中，你将用 Container 字段作为环境变量的值。这里是包含一个容器的 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/dapi-envars-container.yaml" download="pods/inject/dapi-envars-container.yaml"><code>pods/inject/dapi-envars-container.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-dapi-envars-container-yaml')" title="Copy pods/inject/dapi-envars-container.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-dapi-envars-container-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dapi-envars-resourcefieldref<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox:1.24<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb"> </span><span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- while true; do<span style="color:#bbb">
</span><span style="color:#bbb">          </span>echo -en &#39;\n&#39;;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>printenv MY_CPU_REQUEST MY_CPU_LIMIT;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>printenv MY_MEM_REQUEST MY_MEM_LIMIT;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>sleep 10;<span style="color:#bbb">
</span><span style="color:#bbb">        </span>done;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;32Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;125m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;64Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;250m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_CPU_REQUEST<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>requests.cpu<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_CPU_LIMIT<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>limits.cpu<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_MEM_REQUEST<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>requests.memory<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_MEM_LIMIT<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>limits.memory<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see four environment variables. The `env`
field is an array of
[EnvVars](/docs/reference/generated/kubernetes-api/v1.22/#envvar-v1-core).
The first element in the array specifies that the `MY_CPU_REQUEST` environment
variable gets its value from the `requests.cpu` field of a Container named
`test-container`. Similarly, the other environment variables get their values
from Container fields.

The fields in this example are Pod fields. They are not fields of the
Container in the Pod.

Create the Pod:
-->
<p>这个配置文件中，你可以看到四个环境变量。<code>env</code> 字段是一个
<a href="/docs/reference/generated/kubernetes-api/v1.22/#envvar-v1-core">EnvVars</a>.
对象的数组。数组中第一个元素指定 <code>MY_CPU_REQUEST</code> 这个环境变量从 Container 的 <code>requests.cpu</code>
字段获取变量值。同样，其它环境变量也是从 Container 的字段获取它们的变量值。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> 本例中使用的是 Container 的字段而不是 Pod 的字段。</div>
</blockquote>
<p>创建Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-container.yaml
</code></pre></div><!--
Verify that the Container in the Pod is running:
-->
<p>验证 Pod 中的容器运行正常：</p>
<pre tabindex="0"><code>kubectl get pods
</code></pre><!--
View the Container's logs:
-->
<p>查看容器日志：</p>
<pre tabindex="0"><code>kubectl logs dapi-envars-resourcefieldref
</code></pre><!--
The output shows the values of selected environment variables:
-->
<p>输出信息显示了所选择的环境变量的值：</p>
<pre tabindex="0"><code>1
1
33554432
67108864
</code></pre><h2 id="接下来">接下来</h2>
<!--
* [Defining Environment Variables for a Container](/docs/tasks/inject-data-application/define-environment-variable-container/)
* [PodSpec](/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core)
* [Container](/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core)
* [EnvVar](/docs/reference/generated/kubernetes-api/v1.22/#envvar-v1-core)
* [EnvVarSource](/docs/reference/generated/kubernetes-api/v1.22/#envvarsource-v1-core)
* [ObjectFieldSelector](/docs/reference/generated/kubernetes-api/v1.22/#objectfieldselector-v1-core)
* [ResourceFieldSelector](/docs/reference/generated/kubernetes-api/v1.22/#resourcefieldselector-v1-core)
-->
<ul>
<li><a href="/zh/docs/tasks/inject-data-application/define-environment-variable-container/">给容器定义环境变量</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core">PodSpec</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core">Container</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#envvar-v1-core">EnvVar</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#envvarsource-v1-core">EnvVarSource</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#objectfieldselector-v1-core">ObjectFieldSelector</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#resourcefieldselector-v1-core">ResourceFieldSelector</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bcf93d1cd019501fd0b7649e9fbcaf60">6.5 - 通过文件将 Pod 信息呈现给容器</h1>
    
	<!-- overview -->
<!--
This page shows how a Pod can use a DownwardAPIVolumeFile to expose information
about itself to Containers running in the Pod. A DownwardAPIVolumeFile can expose
Pod fields and Container fields.
-->
<p>此页面描述 Pod 如何使用 DownwardAPIVolumeFile 把自己的信息呈现给 Pod 中运行的容器。
DownwardAPIVolumeFile 可以呈现 Pod 的字段和容器字段。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## The Downward API

There are two ways to expose Pod and Container fields to a running Container:

* [Environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#the-downward-api)
* Volume Files
-->
<h2 id="downward-api">Downward API</h2>
<p>有两种方式可以将 Pod 和 Container 字段呈现给运行中的容器：</p>
<ul>
<li><a href="/zh/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#the-downward-api">环境变量</a></li>
<li>卷文件</li>
</ul>
<p>这两种呈现 Pod 和 Container 字段的方式都称为 <em>Downward API</em>。</p>
<!--
## Store Pod fields

In this exercise, you create a Pod that has one Container.
Here is the configuration file for the Pod:
-->
<h2 id="存储-pod-字段">存储 Pod 字段</h2>
<p>在这个练习中，你将创建一个包含一个容器的 Pod。Pod 的配置文件如下：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/dapi-volume.yaml" download="pods/inject/dapi-volume.yaml"><code>pods/inject/dapi-volume.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-dapi-volume-yaml')" title="Copy pods/inject/dapi-volume.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-dapi-volume-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kubernetes-downwardapi-volume-example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">zone</span>:<span style="color:#bbb"> </span>us-est-coast<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>test-cluster1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rack</span>:<span style="color:#bbb"> </span>rack-22<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">build</span>:<span style="color:#bbb"> </span>two<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">builder</span>:<span style="color:#bbb"> </span>john-doe<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>client-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- while true; do<span style="color:#bbb">
</span><span style="color:#bbb">          </span>if [[ -e /etc/podinfo/labels ]]; then<span style="color:#bbb">
</span><span style="color:#bbb">            </span>echo -en &#39;\n\n&#39;; cat /etc/podinfo/labels; fi;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>if [[ -e /etc/podinfo/annotations ]]; then<span style="color:#bbb">
</span><span style="color:#bbb">            </span>echo -en &#39;\n\n&#39;; cat /etc/podinfo/annotations; fi;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>sleep 5;<span style="color:#bbb">
</span><span style="color:#bbb">        </span>done;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>podinfo<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/podinfo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>podinfo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">downwardAPI</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;labels&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>metadata.labels<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;annotations&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>metadata.annotations<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a `downwardAPI` Volume,
and the Container mounts the Volume at `/etc/podinfo`.

Look at the `items` array under `downwardAPI`. Each element of the array is a
[DownwardAPIVolumeFile](/docs/reference/generated/kubernetes-api/v1.22/#downwardapivolumefile-v1-core).
The first element specifies that the value of the Pod's
`metadata.labels` field should be stored in a file named `labels`.
The second element specifies that the value of the Pod's `annotations`
field should be stored in a file named `annotations`.
-->
<p>在配置文件中，你可以看到 Pod 有一个 <code>downwardAPI</code> 类型的卷，并且挂载到容器中的
<code>/etc/podinfo</code> 目录。</p>
<p>查看 <code>downwardAPI</code> 下面的 <code>items</code> 数组。
每个数组元素都是一个
<a href="/docs/reference/generated/kubernetes-api/v1.22/#downwardapivolumefile-v1-core">DownwardAPIVolumeFile</a>
对象。
第一个元素指示 Pod 的 <code>metadata.labels</code> 字段的值保存在名为 <code>labels</code> 的文件中。
第二个元素指示 Pod 的 <code>annotations</code> 字段的值保存在名为 <code>annotations</code> 的文件中。</p>
<!--
The fields in this example are Pod fields. They are not
fields of the Container in the Pod.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 本示例中的字段是Pod字段，不是Pod中容器的字段。</div>
</blockquote>
<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume.yaml
</code></pre></div><!--
Verify that the Container in the Pod is running:
-->
<p>验证Pod中的容器运行正常：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
View the Container's logs:
-->
<p>查看容器的日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs kubernetes-downwardapi-volume-example
</code></pre></div><!--
The output shows the contents of the labels file and the annotations file:
-->
<p>输出显示 <code>labels</code> 和 <code>annotations</code> 文件的内容：</p>
<pre tabindex="0"><code>cluster=&quot;test-cluster1&quot;
rack=&quot;rack-22&quot;
zone=&quot;us-est-coast&quot;

build=&quot;two&quot;
builder=&quot;john-doe&quot;
</code></pre><!--
Get a shell into the Container that is running in your Pod:
-->
<p>进入 Pod 中运行的容器，打开一个 Shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it kubernetes-downwardapi-volume-example -- sh
</code></pre></div><!--
In your shell, view the `labels` file:
-->
<p>在该 Shell中，查看 <code>labels</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">/# cat /etc/podinfo/labels
</code></pre></div><!--
The output shows that all of the Pod's labels have been written
to the `labels` file:
-->
<p>输出显示 Pod 的所有标签都已写入 <code>labels</code> 文件。</p>
<pre tabindex="0"><code>cluster=&quot;test-cluster1&quot;
rack=&quot;rack-22&quot;
zone=&quot;us-est-coast&quot;
</code></pre><!--
Similarly, view the `annotations` file:
-->
<p>同样，查看<code>annotations</code>文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">/# cat /etc/podinfo/annotations
</code></pre></div><!--
View the files in the `/etc/podinfo` directory:
-->
<p>查看<code>/etc/podinfo</code>目录下的文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">/# ls -laR /etc/podinfo
</code></pre></div><!--
In the output, you can see that the `labels` and `annotations` files
are in a temporary subdirectory: in this example,
`..2982_06_02_21_47_53.299460680`. In the `/etc/podinfo` directory, `..data` is
a symbolic link to the temporary subdirectory. Also in the `/etc/podinfo` directory,
`labels` and `annotations` are symbolic links.
-->
<p>在输出中可以看到，<code>labels</code> 和 <code>annotations</code> 文件都在一个临时子目录中。
在这个例子，<code>..2982_06_02_21_47_53.299460680</code>。
在 <code>/etc/podinfo</code> 目录中，<code>..data</code> 是一个指向临时子目录
的符号链接。<code>/etc/podinfo</code> 目录中，<code>labels</code> 和 <code>annotations</code> 也是符号链接。</p>
<pre tabindex="0"><code>drwxr-xr-x  ... Feb 6 21:47 ..2982_06_02_21_47_53.299460680
lrwxrwxrwx  ... Feb 6 21:47 ..data -&gt; ..2982_06_02_21_47_53.299460680
lrwxrwxrwx  ... Feb 6 21:47 annotations -&gt; ..data/annotations
lrwxrwxrwx  ... Feb 6 21:47 labels -&gt; ..data/labels

/etc/podinfo/..2982_06_02_21_47_53.299460680:
total 8
-rw-r--r--  ... Feb  6 21:47 annotations
-rw-r--r--  ... Feb  6 21:47 labels
</code></pre><!--
Using symbolic links enables dynamic atomic refresh of the metadata; updates are
written to a new temporary directory, and the `..data` symlink is updated
atomically using
[rename(2)](http://man7.org/linux/man-pages/man2/rename.2.html).
-->
<p>用符号链接可实现元数据的动态原子性刷新；更新将写入一个新的临时目录，
然后通过使用<a href="http://man7.org/linux/man-pages/man2/rename.2.html">rename(2)</a>
完成 <code>..data</code> 符号链接的原子性更新。</p>
<!--
A container using Downward API as a
[subPath](/docs/concepts/storage/volumes/#using-subpath) volume mount will not
receive Downward API updates.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果容器以
<a href="/zh/docs/concepts/storage/volumes/#using-subpath">subPath</a>卷挂载方式来使用
Downward API，则该容器无法收到更新事件。</div>
</blockquote>
<p>退出 Shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">/# <span style="color:#a2f">exit</span>
</code></pre></div><h2 id="存储容器字段">存储容器字段</h2>
<!--
The preceding exercise, you stored Pod fields in a DownwardAPIVolumeFile.
In this next exercise, you store Container fields. Here is the configuration
file for a Pod that has one Container:
-->
<p>前面的练习中，你将 Pod 字段保存到 DownwardAPIVolumeFile 中。
接下来这个练习，你将存储 Container 字段。这里是包含一个容器的 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/dapi-volume-resources.yaml" download="pods/inject/dapi-volume-resources.yaml"><code>pods/inject/dapi-volume-resources.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-dapi-volume-resources-yaml')" title="Copy pods/inject/dapi-volume-resources.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-dapi-volume-resources-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kubernetes-downwardapi-volume-example-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>client-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/busybox:1.24<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- while true; do<span style="color:#bbb">
</span><span style="color:#bbb">          </span>echo -en &#39;\n&#39;;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>if [[ -e /etc/podinfo/cpu_limit ]]; then<span style="color:#bbb">
</span><span style="color:#bbb">            </span>echo -en &#39;\n&#39;; cat /etc/podinfo/cpu_limit; fi;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>if [[ -e /etc/podinfo/cpu_request ]]; then<span style="color:#bbb">
</span><span style="color:#bbb">            </span>echo -en &#39;\n&#39;; cat /etc/podinfo/cpu_request; fi;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>if [[ -e /etc/podinfo/mem_limit ]]; then<span style="color:#bbb">
</span><span style="color:#bbb">            </span>echo -en &#39;\n&#39;; cat /etc/podinfo/mem_limit; fi;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>if [[ -e /etc/podinfo/mem_request ]]; then<span style="color:#bbb">
</span><span style="color:#bbb">            </span>echo -en &#39;\n&#39;; cat /etc/podinfo/mem_request; fi;<span style="color:#bbb">
</span><span style="color:#bbb">          </span>sleep 5;<span style="color:#bbb">
</span><span style="color:#bbb">        </span>done;<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;32Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;125m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;64Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;250m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>podinfo<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/podinfo<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>podinfo<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">downwardAPI</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;cpu_limit&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>client-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>limits.cpu<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">divisor</span>:<span style="color:#bbb"> </span>1m<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;cpu_request&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>client-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>requests.cpu<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">divisor</span>:<span style="color:#bbb"> </span>1m<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;mem_limit&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>client-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>limits.memory<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">divisor</span>:<span style="color:#bbb"> </span>1Mi<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;mem_request&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">resourceFieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerName</span>:<span style="color:#bbb"> </span>client-container<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb"> </span>requests.memory<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">divisor</span>:<span style="color:#bbb"> </span>1Mi<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a `downwardAPI` Volume,
and the Container mounts the Volume at `/etc/podinfo`.

Look at the `items` array under `downwardAPI`. Each element of the array is a
DownwardAPIVolumeFile.

The first element specifies that in the Container named `client-container`,
the value of the `limits.cpu` field in the format specified by `1m` should be
stored in a file named `cpu_limit`. The `divisor` field is optional and has the
default value of `1` which means cores for cpu and bytes for memory.

Create the Pod:
-->
<p>在这个配置文件中，你可以看到 Pod 有一个 <code>downwardAPI</code> 类型的卷，并且挂载到容器的
<code>/etc/podinfo</code> 目录。</p>
<p>查看 <code>downwardAPI</code> 下面的 <code>items</code> 数组。每个数组元素都是一个 DownwardAPIVolumeFile。</p>
<p>第一个元素指定名为 <code>client-container</code> 的容器中 <code>limits.cpu</code> 字段的值应保存在名为
<code>cpu_limit</code> 的文件中。</p>
<p>创建Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume-resources.yaml
</code></pre></div><!--
Get a shell into the Container that is running in your Pod:
-->
<p>打开一个 Shell，进入 Pod 中运行的容器：</p>
<pre tabindex="0"><code>kubectl exec -it kubernetes-downwardapi-volume-example-2 -- sh
</code></pre><!--
In your shell, view the `cpu_limit` file:
-->
<p>在 Shell 中，查看 <code>cpu_limit</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">/# cat /etc/podinfo/cpu_limit
</code></pre></div><!--
You can use similar commands to view the `cpu_request`, `mem_limit` and
`mem_request` files.
-->
<p>你可以使用同样的命令查看 <code>cpu_request</code>、<code>mem_limit</code> 和 <code>mem_request</code> 文件.</p>
<!-- discussion -->
<!--
## Capabilities of the Downward API
-->
<h2 id="downward-api-的能力">Downward API 的能力</h2>
<!--
The following information is available to containers through environment
variables and `downwardAPI` volumes:

* Information available via `fieldRef`:
  * `metadata.name` - the pod's name
  * `metadata.namespace` - the pod's namespace
  * `metadata.uid` - the pod's UID
  * `metadata.labels['<KEY>']` - the value of the pod's label `<KEY>` (for example, `metadata.labels['mylabel']`)
  * `metadata.annotations['<KEY>']` - the value of the pod's annotation `<KEY>` (for example, `metadata.annotations['myannotation']`)
-->
<p>下面这些信息可以通过环境变量和 <code>downwardAPI</code> 卷提供给容器：</p>
<ul>
<li>能通过 <code>fieldRef</code> 获得的：
<ul>
<li><code>metadata.name</code> - Pod 名称</li>
<li><code>metadata.namespace</code> - Pod 名字空间</li>
<li><code>metadata.uid</code> - Pod 的 UID</li>
<li><code>metadata.labels['&lt;KEY&gt;']</code> - Pod 标签 <code>&lt;KEY&gt;</code> 的值 (例如, <code>metadata.labels['mylabel']</code>）</li>
<li><code>metadata.annotations['&lt;KEY&gt;']</code> - Pod 的注解 <code>&lt;KEY&gt;</code> 的值（例如, <code>metadata.annotations['myannotation']</code>）</li>
</ul>
</li>
</ul>
<!--
* Information available via `resourceFieldRef`:
  * A Container's CPU limit
  * A Container's CPU request
  * A Container's memory limit
  * A Container's memory request
  * A Container's hugepages limit (providing that the `DownwardAPIHugePages` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled)
  * A Container's hugepages request (providing that the `DownwardAPIHugePages` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled)
  * A Container's ephemeral-storage limit
  * A Container's ephemeral-storage request
-->
<ul>
<li>能通过 <code>resourceFieldRef</code> 获得的：
<ul>
<li>容器的 CPU 约束值</li>
<li>容器的 CPU 请求值</li>
<li>容器的内存约束值</li>
<li>容器的内存请求值</li>
<li>容器的巨页限制值（前提是启用了 <code>DownwardAPIHugePages</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>）</li>
<li>容器的巨页请求值（前提是启用了 <code>DownwardAPIHugePages</code> <a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>）</li>
<li>容器的临时存储约束值</li>
<li>容器的临时存储请求值</li>
</ul>
</li>
</ul>
<!--
In addition, the following information is available through
`downwardAPI` volume `fieldRef`:
-->
<p>此外，以下信息可通过 <code>downwardAPI</code> 卷从 <code>fieldRef</code> 获得：</p>
<!--
* `metadata.labels` - all of the pod’s labels, formatted as `label-key="escaped-label-value"` with one label per line
* `metadata.annotations` - all of the pod’s annotations, formatted as `annotation-key="escaped-annotation-value"` with one annotation per line
-->
<ul>
<li><code>metadata.labels</code> - Pod 的所有标签，以 <code>label-key=&quot;escaped-label-value&quot;</code> 格式显示，每行显示一个标签</li>
<li><code>metadata.annotations</code> - Pod 的所有注解，以 <code>annotation-key=&quot;escaped-annotation-value&quot;</code>
格式显示，每行显示一个标签</li>
</ul>
<!--
The following information is available through environment variables:
-->
<p>以下信息可通过环境变量获得：</p>
<ul>
<li><code>status.podIP</code> - 节点 IP</li>
<li><code>spec.serviceAccountName</code> - Pod 服务帐号名称, 版本要求 v1.4.0-alpha.3</li>
<li><code>spec.nodeName</code> - 节点名称, 版本要求 v1.4.0-alpha.3</li>
<li><code>status.hostIP</code> - 节点 IP, 版本要求 v1.7.0-alpha.1</li>
</ul>
<!--
If CPU and memory limits are not specified for a Container, the
Downward API defaults to the node allocatable value for CPU and memory.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果容器未指定 CPU 和内存限制，则 Downward API 默认将节点可分配值
视为容器的 CPU 和内存限制。</div>
</blockquote>
<!--
## Project keys to specific paths and file permissions

You can project keys to specific paths and specific permissions on a per-file
basis. For more information, see
[Secrets](/docs/concepts/configuration/secret/).
-->
<h2 id="投射键名到指定路径并且指定文件权限">投射键名到指定路径并且指定文件权限</h2>
<p>你可以将键名投射到指定路径并且指定每个文件的访问权限。
更多信息，请参阅<a href="/zh/docs/concepts/configuration/secret/">Secrets</a>.</p>
<!--
## Motivation for the Downward API

It is sometimes useful for a Container to have information about itself, without
being overly coupled to Kubernetes. The Downward API allows containers to consume
information about themselves or the cluster without using the Kubernetes client
or API server.

An example is an existing application that assumes a particular well-known
environment variable holds a unique identifier. One possibility is to wrap the
application, but that is tedious and error prone, and it violates the goal of low
coupling. A better option would be to use the Pod's name as an identifier, and
inject the Pod's name into the well-known environment variable.
-->
<h2 id="downward-api的动机">Downward API的动机</h2>
<p>对于容器来说，有时候拥有自己的信息是很有用的，可避免与 Kubernetes 过度耦合。
Downward API 使得容器使用自己或者集群的信息，而不必通过 Kubernetes 客户端或
API 服务器来获得。</p>
<p>一个例子是有一个现有的应用假定要用一个非常熟悉的环境变量来保存一个唯一标识。
一种可能是给应用增加处理层，但这样是冗余和易出错的，而且它违反了低耦合的目标。
更好的选择是使用 Pod 名称作为标识，把 Pod 名称注入这个环境变量中。</p>
<h2 id="接下来">接下来</h2>
<ul>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#podspec-v1-core">PodSpec</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#volume-v1-core">Volume</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#downwardapivolumesource-v1-core">DownwardAPIVolumeSource</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#downwardapivolumefile-v1-core">DownwardAPIVolumeFile</a></li>
<li><a href="/docs/reference/generated/kubernetes-api/v1.22/#resourcefieldselector-v1-core">ResourceFieldSelector</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7f9454a1e775548c23ee5b300a9218a3">6.6 - 使用 Secret 安全地分发凭证</h1>
    
	<!-- overview -->
<!--
This page shows how to securely inject sensitive data, such as passwords and
encryption keys, into Pods.
-->
<p>本文展示如何安全地将敏感数据（如密码和加密密钥）注入到 Pods 中。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
### Convert your secret data to a base-64 representation

Suppose you want to have two pieces of secret data: a username `my-app` and a password
`39528$vdg7Jb`. First, use a base64 encoding tool to convert your username and password to a base64 representation. Here's an example using the commonly available base64 program:
-->
<h3 id="将-secret-数据转换为-base-64-形式">将 secret 数据转换为 base-64 形式</h3>
<p>假设用户想要有两条 Secret 数据：用户名 <code>my-app</code> 和密码 <code>39528$vdg7Jb</code>。
首先使用 <a href="https://www.base64encode.org/">Base64 编码</a> 将用户名和密码转化为 base-64 形式。
下面是一个使用常用的 base64 程序的示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> -n <span style="color:#b44">&#39;my-app&#39;</span> | base64
<span style="color:#a2f">echo</span> -n <span style="color:#b44">&#39;39528$vdg7Jb&#39;</span> | base64
</code></pre></div><!--
The output shows that the base-64 representation of your username is `bXktYXBw`,
and the base-64 representation of your password is `Mzk1MjgkdmRnN0pi`.
-->
<p>结果显示 base-64 形式的用户名为 <code>bXktYXBw</code>，
base-64 形式的密码为 <code>Mzk1MjgkdmRnN0pi</code>。</p>
<!--
Use a local tool trusted by your OS to decrease the security risks of external tools.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 使用你的操作系统所能信任的本地工具以降低使用外部工具的风险。</div>
</blockquote>

<!-- steps -->
<!--
## Create a Secret

Here is a configuration file you can use to create a Secret that holds your
username and password:
-->
<h2 id="创建-secret">创建 Secret</h2>
<p>这里是一个配置文件，可以用来创建存有用户名和密码的 Secret:</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/secret.yaml" download="pods/inject/secret.yaml"><code>pods/inject/secret.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-secret-yaml')" title="Copy pods/inject/secret.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-secret-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>bXktYXBw<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>Mzk1MjgkdmRnN0pi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<ol>
<li>
<!--Create the Secret -->
<p>创建 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/inject/secret.yaml
</code></pre></div></li>
<li>
<!-- View information about the Secret -->
<p>查看 Secret 相关信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get secret test-secret
</code></pre></div><!-- Output: -->
<p>输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME          TYPE      DATA      AGE
test-secret   Opaque    <span style="color:#666">2</span>         1m
</code></pre></div></li>
<li>
<!-- View more detailed information about the Secret -->
<p>查看 Secret 相关的更多详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe secret test-secret
</code></pre></div><!-- Output: -->
<p>输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Name:       test-secret
Namespace:  default
Labels:     &lt;none&gt;
Annotations:    &lt;none&gt;

Type:   Opaque

<span style="color:#b8860b">Data</span>
<span style="color:#666">====</span>
password:   <span style="color:#666">13</span> bytes
username:   <span style="color:#666">7</span>  bytes
</code></pre></div></li>
</ol>
<!--
### Create a Secret directly with kubectl

If you want to skip the Base64 encoding step, you can create the
same Secret using the `kubectl create secret` command. For example:
-->
<h3 id="直接用-kubectl-创建-secret">直接用 kubectl 创建 Secret</h3>
<p>如果你希望略过 Base64 编码的步骤，你也可以使用 <code>kubectl create secret</code>
命令直接创建 Secret。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic test-secret --from-literal<span style="color:#666">=</span><span style="color:#b44">&#39;username=my-app&#39;</span> --from-literal<span style="color:#666">=</span><span style="color:#b44">&#39;password=39528$vdg7Jb&#39;</span>
</code></pre></div><!--
This is more convenient. The detailed approach shown earlier runs
through each step explicitly to demonstrate what is happening.
-->
<p>这是一种更为方便的方法。
前面展示的详细分解步骤有助于了解究竟发生了什么事情。</p>
<!--
## Create a Pod that has access to the secret data through a Volume

Here is a configuration file you can use to create a Pod:
-->
<h2 id="创建一个可以通过卷访问-secret-数据的-pod">创建一个可以通过卷访问 secret 数据的 Pod</h2>
<p>这里是一个可以用来创建 pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/secret-pod.yaml" download="pods/inject/secret-pod.yaml"><code>pods/inject/secret-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-secret-pod-yaml')" title="Copy pods/inject/secret-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-secret-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-test-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-container<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># name must match the volume name below</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># The secret data is exposed to Containers in the Pod through a Volume.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>secret-volume<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>test-secret<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<ol>
<li>
<!-- Create the Pod:-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f secret-pod.yaml
</code></pre></div></li>
<li>
<!-- Verify that your Pod is running: -->
<p>确认 Pod 正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod secret-test-pod
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME              READY     STATUS    RESTARTS   AGE
secret-test-pod   1/1       Running   <span style="color:#666">0</span>          42m
</code></pre></div></li>
<li>
<!-- Get a shell into the Container that is running in your Pod:-->
<p>获取一个 shell 进入 Pod 中运行的容器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it secret-test-pod -- /bin/bash
</code></pre></div></li>
<li>
<!-- The secret data is exposed to the Container through a Volume mounted under
`/etc/secret-volume`.

In your shell, list the files in the `/etc/secret-volume` directory:
-->
<p>Secret 数据通过挂载在 <code>/etc/secret-volume</code> 目录下的卷暴露在容器中。</p>
<p>在 shell 中，列举 <code>/etc/secret-volume</code> 目录下的文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ls /etc/secret-volume
</code></pre></div><!--
The output shows two files, one for each piece of secret data:
-->
<p>输出包含两个文件，每个对应一个 Secret 数据条目：</p>
<pre tabindex="0"><code>password username
</code></pre></li>
<li>
<!--
In your shell, display the contents of the `username` and `password` files:
-->
<p>在 Shell 中，显示 <code>username</code> 和 <code>password</code> 文件的内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在容器中 Shell 运行下面命令</span>
<span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat /etc/secret-volume/username<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
<span style="color:#a2f">echo</span> <span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>cat /etc/secret-volume/password<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>
</code></pre></div><!--
The output is your username and password:
-->
<p>输出为用户名和密码：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">my-app
39528<span style="color:#b8860b">$vdg7Jb</span>
</code></pre></div></li>
</ol>
<!--
## Define container environment variables using Secret data

### Define a container environment variable with data from a single Secret

-->
<h2 id="使用-secret-数据定义容器变量">使用 Secret 数据定义容器变量</h2>
<h3 id="使用来自-secret-中的数据定义容器变量">使用来自 Secret 中的数据定义容器变量</h3>
<!--
*  Define an environment variable as a key-value pair in a Secret:
-->
<ul>
<li>
<p>定义环境变量为 Secret 中的键值偶对：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic backend-user --from-literal<span style="color:#666">=</span>backend-username<span style="color:#666">=</span><span style="color:#b44">&#39;backend-admin&#39;</span>
</code></pre></div></li>
</ul>
<!--
*  Assign the `backend-username` value defined in the Secret to the `SECRET_USERNAME` environment variable in the Pod specification.
-->
<ul>
<li>
<p>在 Pod 规约中，将 Secret 中定义的值 <code>backend-username</code> 赋给 <code>SECRET_USERNAME</code> 环境变量</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/pod-single-secret-env-variable.yaml" download="pods/inject/pod-single-secret-env-variable.yaml"><code>pods/inject/pod-single-secret-env-variable.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-pod-single-secret-env-variable-yaml')" title="Copy pods/inject/pod-single-secret-env-variable.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-pod-single-secret-env-variable-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>env-single-secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>envars-test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>SECRET_USERNAME<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>backend-user<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>backend-username<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


</li>
</ul>
<!--
*  Create the Pod:
-->
<ul>
<li>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/inject/pod-single-secret-env-variable.yaml
</code></pre></div></li>
</ul>
<!--
*  In your shell, display the content of `SECRET_USERNAME` container environment variable
-->
<ul>
<li>
<p>在 Shell 中，显示容器环境变量 <code>SECRET_USERNAME</code> 的内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t env-single-secret -- /bin/sh -c <span style="color:#b44">&#39;echo $SECRET_USERNAME&#39;</span>
</code></pre></div><p>输出为：</p>
<pre tabindex="0"><code>backend-admin
</code></pre></li>
</ul>
<!--
### Define container environment variables with data from multiple Secrets
-->
<h3 id="使用来自多个-secret-的数据定义环境变量">使用来自多个 Secret 的数据定义环境变量</h3>
<!--
*  As with the previous example, create the Secrets first.
-->
<ul>
<li>
<p>和前面的例子一样，先创建 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic backend-user --from-literal<span style="color:#666">=</span>backend-username<span style="color:#666">=</span><span style="color:#b44">&#39;backend-admin&#39;</span>
kubectl create secret generic db-user --from-literal<span style="color:#666">=</span>db-username<span style="color:#666">=</span><span style="color:#b44">&#39;db-admin&#39;</span>
</code></pre></div></li>
</ul>
<!--
*  Define the environment variables in the Pod specification.
-->
<ul>
<li>
<p>在 Pod 规约中定义环境变量：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/pod-multiple-secret-env-variable.yaml" download="pods/inject/pod-multiple-secret-env-variable.yaml"><code>pods/inject/pod-multiple-secret-env-variable.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-pod-multiple-secret-env-variable-yaml')" title="Copy pods/inject/pod-multiple-secret-env-variable.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-pod-multiple-secret-env-variable-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>envvars-multiple-secrets<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>envars-test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>BACKEND_USERNAME<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>backend-user<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>backend-username<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>DB_USERNAME<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">secretKeyRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>db-user<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>db-username<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


</li>
</ul>
<!--
*  Create the Pod:
-->
<ul>
<li>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/inject/pod-multiple-secret-env-variable.yaml
</code></pre></div></li>
</ul>
<!--
*  In your shell, display the container environment variables
-->
<ul>
<li>
<p>在你的 Shell 中，显示容器环境变量的内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t envvars-multiple-secrets -- /bin/sh -c <span style="color:#b44">&#39;env | grep _USERNAME&#39;</span>
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>DB_USERNAME=db-admin
BACKEND_USERNAME=backend-admin
</code></pre></li>
</ul>
<!--
## Configure all key-value pairs in a Secret as container environment variables
-->
<h2 id="将-secret-中的所有键值偶对定义为环境变量">将 Secret 中的所有键值偶对定义为环境变量</h2>
<!--
This functionality is available in Kubernetes v1.6 and later.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 此功能在 Kubernetes 1.6 版本之后可用。</div>
</blockquote>
<!--
*  Create a Secret containing multiple key-value pairs
-->
<ul>
<li>
<p>创建包含多个键值偶对的 Secret：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create secret generic test-secret --from-literal<span style="color:#666">=</span><span style="color:#b8860b">username</span><span style="color:#666">=</span><span style="color:#b44">&#39;my-app&#39;</span> --from-literal<span style="color:#666">=</span><span style="color:#b8860b">password</span><span style="color:#666">=</span><span style="color:#b44">&#39;39528$vdg7Jb&#39;</span>
</code></pre></div></li>
</ul>
<!--
*  Use envFrom to define all of the Secret's data as container environment variables. The key from the Secret becomes the environment variable name in the Pod.
-->
<ul>
<li>
<p>使用 <code>envFrom</code> 来将 Secret 中的所有数据定义为环境变量。
Secret 中的键名成为容器中的环境变量名：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/inject/pod-secret-envFrom.yaml" download="pods/inject/pod-secret-envFrom.yaml"><code>pods/inject/pod-secret-envFrom.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-inject-pod-secret-envfrom-yaml')" title="Copy pods/inject/pod-secret-envFrom.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-inject-pod-secret-envfrom-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>envfrom-secret<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>envars-test-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">envFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">secretRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-secret<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


</li>
</ul>
<!--
*  Create the Pod:
-->
<ul>
<li>
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/pods/inject/pod-secret-envFrom.yaml
</code></pre></div></li>
</ul>
<!--
* In your shell, display `username` and `password` container environment variables
-->
<ul>
<li>
<p>在 Shell 中，显示环境变量 <code>username</code> 和 <code>password</code> 的内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -i -t envfrom-secret -- /bin/sh -c <span style="color:#b44">&#39;echo &#34;username: $username\npassword: $password\n&#34;&#39;</span>
</code></pre></div><p>输出为：</p>
<pre tabindex="0"><code>username: my-app
password: 39528$vdg7Jb
</code></pre></li>
</ul>
<!-- ### References -->
<h3 id="参考">参考</h3>
<ul>
<li><a href="/docs/api-reference/v1.22/#secret-v1-core">Secret</a></li>
<li><a href="/docs/api-reference/v1.22/#volume-v1-core">Volume</a></li>
<li><a href="/docs/api-reference/v1.22/#pod-v1-core">Pod</a></li>
</ul>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [Secrets](/docs/concepts/configuration/secret/).
* Learn about [Volumes](/docs/concepts/storage/volumes/).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/configuration/secret/">Secret</a>。</li>
<li>了解 <a href="/zh/docs/concepts/storage/volumes/">Volumes</a>。</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a78a5e7e765fd8c49c8f7c0d72499f72">7 - 运行应用</h1>
    <div class="lead">运行和管理无状态和有状态的应用程序。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-790ea02857492b3a822e981e93e3a98b">7.1 - 使用 Deployment 运行一个无状态应用</h1>
    
	<!-- overview -->
<!--
This page shows how to run an application using a Kubernetes Deployment object.
-->
<p>本文介绍如何通过 Kubernetes Deployment 对象去运行一个应用.</p>
<h2 id="教程目标">教程目标</h2>
<!--
* Create an nginx deployment.
* Use kubectl to list information about the deployment.
* Update the deployment.
-->
<ul>
<li>创建一个 nginx Deployment.</li>
<li>使用 kubectl 列举关于 Deployment 的信息.</li>
<li>更新 Deployment。</li>
</ul>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.9.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- lessoncontent -->
<!--
## Creating and exploring an nginx deployment

You can run an application by creating a Kubernetes Deployment object, and you
can describe a Deployment in a YAML file. For example, this YAML file describes
a Deployment that runs the nginx:1.14.2 Docker image:
-->
<h2 id="创建并了解一个-nginx-deployment">创建并了解一个 nginx Deployment</h2>
<p>你可以通过创建一个 Kubernetes Deployment 对象来运行一个应用, 且你可以在一个
YAML 文件中描述 Deployment。例如, 下面这个 YAML 文件描述了一个运行 nginx:1.14.2
Docker 镜像的 Deployment：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/deployment.yaml" download="application/deployment.yaml"><code>application/deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-deployment-yaml')" title="Copy application/deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># tells deployment to run 2 pods matching the template</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Create a Deployment based on the YAML file:
-->
<ol>
<li>
<p>通过 YAML 文件创建一个 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/deployment.yaml
</code></pre></div></li>
</ol>
<!--
1. Display information about the Deployment:
-->
<ol start="2">
<li>
<p>显示 Deployment 相关信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment nginx-deployment
</code></pre></div><!-- 
The output is similar to this:
-->
<p>输出类似于这样：</p>
<pre tabindex="0"><code>Name:     nginx-deployment
Namespace:    default
CreationTimestamp:  Tue, 30 Aug 2016 18:11:37 -0700
Labels:     app=nginx
Annotations:    deployment.kubernetes.io/revision=1
Selector:   app=nginx
Replicas:   2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:   RollingUpdate
MinReadySeconds:  0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx:1.7.9
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
OldReplicaSets:   &lt;none&gt;
NewReplicaSet:    nginx-deployment-1771418926 (2/2 replicas created)
No events.
</code></pre></li>
</ol>
<!--
1. List the Pods created by the deployment:
-->
<ol start="3">
<li>
<p>列出 Deployment 创建的 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于这样：</p>
<pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1771418926-7o5ns   1/1       Running   0          16h
nginx-deployment-1771418926-r18az   1/1       Running   0          16h
</code></pre></li>
</ol>
<!--
1. Display information about a Pod:
-->
<ol start="4">
<li>
<p>展示某一个 Pod 信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod &lt;pod-name&gt;
</code></pre></div><!--
where `<pod-name>` is the name of one of your Pods.
-->
<p>这里的 <code>&lt;pod-name&gt;</code> 是某一 Pod 的名称。</p>
</li>
</ol>
<!--
## Updating the deployment

You can update the deployment by applying a new YAML file. This YAML file
specifies that the deployment should be updated to use nginx 1.16.1.
-->
<h2 id="更新-deployment">更新 Deployment</h2>
<p>你可以通过更新一个新的 YAML 文件来更新 Deployment。下面的 YAML 文件指定该
Deployment 镜像更新为 nginx 1.16.1。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/deployment-update.yaml" download="application/deployment-update.yaml"><code>application/deployment-update.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-deployment-update-yaml')" title="Copy application/deployment-update.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-deployment-update-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.16.1<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Update the version of nginx from 1.14.2 to 1.16.1</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Apply the new YAML file:
-->
<ol>
<li>
<p>应用新的 YAML：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/deployment-update.yaml
</code></pre></div></li>
</ol>
<!--
1. Watch the deployment create pods with new names and delete the old pods:
-->
<ol start="2">
<li>
<p>查看该 Deployment 以新的名称创建 Pods 同时删除旧的 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div></li>
</ol>
<!--
## Scaling the application by increasing the replica count

You can increase the number of Pods in your Deployment by applying a new YAML
file. This YAML file sets `replicas` to 4, which specifies that the Deployment
should have four Pods:
-->
<h2 id="通过增加副本数来扩缩应用">通过增加副本数来扩缩应用</h2>
<p>你可以通过应用新的 YAML 文件来增加 Deployment 中 Pods 的数量。
下面的 YAML 文件将 <code>replicas</code> 设置为 4，指定该 Deployment 应有 4 个 Pods：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/deployment-scale.yaml" download="application/deployment-scale.yaml"><code>application/deployment-scale.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-deployment-scale-yaml')" title="Copy application/deployment-scale.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-deployment-scale-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Update the replicas from 2 to 4</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx:1.14.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Apply the new YAML file:
-->
<ol>
<li>
<p>应用新的 YAML 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/deployment-scale.yaml
</code></pre></div></li>
</ol>
<!--
1. Verify that the Deployment has four Pods:
-->
<ol start="2">
<li>
<p>验证 Deployment 有 4 个 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
The output is similar to this:
-->
<p>输出的结果类似于:</p>
<pre tabindex="0"><code>NAME                               READY     STATUS    RESTARTS   AGE
nginx-deployment-148880595-4zdqq   1/1       Running   0          25s
nginx-deployment-148880595-6zgi1   1/1       Running   0          25s
nginx-deployment-148880595-fxcez   1/1       Running   0          2m
nginx-deployment-148880595-rwovn   1/1       Running   0          2m
</code></pre></li>
</ol>
<!--
## Deleting a deployment

Delete the deployment by name:
-->
<h2 id="删除-deployment">删除 Deployment</h2>
<p>基于名称删除 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment nginx-deployment
</code></pre></div><!--
## ReplicationControllers -- the Old Way

The preferred way to create a replicated application is to use a Deployment,
which in turn uses a ReplicaSet. Before the Deployment and ReplicaSet were
added to Kubernetes, replicated applications were configured using a
[ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).
-->
<h2 id="replicationcontrollers-旧的方式">ReplicationControllers -- 旧的方式</h2>
<p>创建一个多副本应用首选方法是使用 Deployment，Deployment 内部使用 ReplicaSet。
在 Deployment 和 ReplicaSet 被引入到 Kubernetes 之前，多副本应用通过
<a href="/zh/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>
来配置。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [Deployment objects](/docs/concepts/workloads/controllers/deployment/).
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment 对象</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-43398a6f5dc7ce19df59f5f4c2e7922d">7.2 - 运行一个单实例有状态应用</h1>
    
	<!-- overview -->
<!--
This page shows you how to run a single-instance stateful application
in Kubernetes using a PersistentVolume and a Deployment. The
application is MySQL.
-->
<p>本文介绍在 Kubernetes 中如何使用 PersistentVolume 和 Deployment 运行一个单实例有状态应用。该应用是 MySQL.</p>
<h2 id="教程目标">教程目标</h2>
<!--
* Create a PersistentVolume referencing a disk in your environment.
* Create a MySQL Deployment.
* Expose MySQL to other pods in the cluster at a known DNS name.
-->
<ul>
<li>在你的环境中创建一个引用磁盘的 PersistentVolume</li>
<li>创建一个 MySQL Deployment.</li>
<li>在集群内以一个已知的 DNS 名称将 MySQL 暴露给其他 Pod</li>
</ul>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
<li><p>您需要有一个带有默认<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>的动态持续卷供应程序，或者自己<a href="/docs/user-guide/persistent-volumes/#provisioning">静态的提供持久卷</a>来满足这里使用的<a href="/docs/user-guide/persistent-volumes/#persistentvolumeclaims">持久卷请求</a>。</p>
<!--
You need to either have a dynamic PersistentVolume provisioner with a default
[StorageClass](/docs/concepts/storage/storage-classes/),
or [statically provision PersistentVolumes](/docs/user-guide/persistent-volumes/#provisioning)
yourself to satisfy the [PersistentVolumeClaims](/docs/user-guide/persistent-volumes/#persistentvolumeclaims)
used here.
-->
</li>
</ul>
<!-- lessoncontent -->
<h2 id="deploy-mysql">部署 MySQL  </h2>
<!--
You can run a stateful application by creating a Kubernetes Deployment
and connecting it to an existing PersistentVolume using a
PersistentVolumeClaim.  For example, this YAML file describes a
Deployment that runs MySQL and references the PersistentVolumeClaim. The file
defines a volume mount for /var/lib/mysql, and then creates a
PersistentVolumeClaim that looks for a 20G volume. This claim is
satisfied by any existing volume that meets the requirements,
or by a dynamic provisioner.
-->
<p>你可以通过创建一个 Kubernetes Deployment 并使用 PersistentVolumeClaim 将其连接到
某已有的 PV 卷来运行一个有状态的应用。
例如，这里的 YAML 描述的是一个运行 MySQL 的 Deployment，其中引用了 PVC 申领。
文件为 /var/lib/mysql 定义了加载卷，并创建了一个 PVC 申领，寻找一个 20G 大小的卷。
该申领可以通过现有的满足需求的卷来满足，也可以通过动态供应卷的机制来满足。</p>
<!--
Note: The password is defined in the config yaml, and this is insecure. See
[Kubernetes Secrets](/docs/concepts/configuration/secret/)
for a secure solution.
-->
<p>注意：在配置的 YAML 文件中定义密码的做法是不安全的。具体安全解决方案请参考
<a href="/zh/docs/concepts/configuration/secret/">Kubernetes Secrets</a>.</p>
<p>

 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/mysql/mysql-deployment.yaml" download="application/mysql/mysql-deployment.yaml"><code>application/mysql/mysql-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-mysql-mysql-deployment-yaml')" title="Copy application/mysql/mysql-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-mysql-mysql-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Recreate<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mysql:5.6<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># Use secret in real usage</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MYSQL_ROOT_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>password<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-persistent-storage<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>mysql-pv-claim<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>




 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/mysql/mysql-pv.yaml" download="application/mysql/mysql-pv.yaml"><code>application/mysql/mysql-pv.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-mysql-mysql-pv-yaml')" title="Copy application/mysql/mysql-pv.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-mysql-mysql-pv-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pv-volume<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>local<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/mnt/data&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-pv-claim<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>manual<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ReadWriteOnce<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>20Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>

</p>
<ol>
<li>
<!--Deploy the PV and PVC of the YAML file-->
<p>部署 YAML 文件中定义的 PV 和 PVC：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-pv.yaml
</code></pre></div></li>
<li>
<!-- Deploy the contents of the YAML file -->
<p>部署 YAML 文件中定义的 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-deployment.yaml
</code></pre></div></li>
<li>
<!-- Display information about the Deployment -->
<p>展示 Deployment 相关信息:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment mysql
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>Name:                 mysql
Namespace:            default
CreationTimestamp:    Tue, 01 Nov 2016 11:18:45 -0700
Labels:               app=mysql
Annotations:          deployment.kubernetes.io/revision=1
Selector:             app=mysql
Replicas:             1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:         Recreate
MinReadySeconds:      0
Pod Template:
  Labels:       app=mysql
  Containers:
   mysql:
    Image:      mysql:5.6
    Port:       3306/TCP
    Environment:
      MYSQL_ROOT_PASSWORD:      password
    Mounts:
      /var/lib/mysql from mysql-persistent-storage (rw)
  Volumes:
   mysql-persistent-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  mysql-pv-claim
    ReadOnly:   false
    Conditions:
      Type          Status  Reason
      ----          ------  ------
      Available     False   MinimumReplicasUnavailable
      Progressing   True    ReplicaSetUpdated
      OldReplicaSets:       &lt;none&gt;
      NewReplicaSet:        mysql-63082529 (1/1 replicas created)
      Events:
        FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
        ---------    --------    -----    ----                -------------    --------    ------            -------
        33s          33s         1        {deployment-controller }             Normal      ScalingReplicaSet Scaled up replica set mysql-63082529 to 1
</code></pre></li>
<li>
<!-- List the pods created by the Deployment -->
<p>列举出 Deployment 创建的 pods:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>mysql
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME                   READY     STATUS    RESTARTS   AGE
mysql-63082529-2z3ki   1/1       Running   0          3m
</code></pre></li>
<li>
<!-- Inspect the PersistentVolumeClaim -->
<p>查看 PersistentVolumeClaim：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pvc mysql-pv-claim
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>Name:         mysql-pv-claim
Namespace:    default
StorageClass:
Status:       Bound
Volume:       mysql-pv-volume
Labels:       &lt;none&gt;
Annotations:    pv.kubernetes.io/bind-completed=yes
                pv.kubernetes.io/bound-by-controller=yes
Capacity:     20Gi
Access Modes: RWO
Events:       &lt;none&gt;
</code></pre></li>
</ol>
<!--
## Accessing the MySQL instance

The preceding YAML file creates a service that
allows other Pods in the cluster to access the database. The Service option
`clusterIP: None` lets the Service DNS name resolve directly to the
Pod's IP address. This is optimal when you have only one Pod
behind a Service and you don't intend to increase the number of Pods.

Run a MySQL client to connect to the server:
-->
<h2 id="accessing-the-mysql-instance">访问 MySQL 实例  </h2>
<p>前面 YAML 文件中创建了一个允许集群内其他 Pod 访问的数据库服务。该服务中选项
<code>clusterIP: None</code> 让服务 DNS 名称直接解析为 Pod 的 IP 地址。
当在一个服务下只有一个 Pod 并且不打算增加 Pod 的数量这是最好的.</p>
<p>运行 MySQL 客户端以连接到服务器:</p>
<pre tabindex="0"><code>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword
</code></pre><!--
This command creates a new Pod in the cluster running a MySQL client
and connects it to the server through the Service. If it connects, you
know your stateful MySQL database is up and running.
-->
<p>此命令在集群内创建一个新的 Pod 并运行 MySQL 客户端，并通过 Service 连接到服务器。
如果连接成功，你就知道有状态的 MySQL 数据库正处于运行状态。</p>
<pre tabindex="0"><code>Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false
If you don't see a command prompt, try pressing enter.

mysql&gt;
</code></pre><!--
## Updating

The image or any other part of the Deployment can be updated as usual
with the `kubectl apply` command. Here are some precautions that are
specific to stateful apps:
-->
<h2 id="updating">更新  </h2>
<p>Deployment 中镜像或其他部分同往常一样可以通过 <code>kubectl apply</code> 命令更新。
以下是特定于有状态应用的一些注意事项:</p>
<!--
* Don't scale the app. This setup is for single-instance apps
  only. The underlying PersistentVolume can only be mounted to one
  Pod. For clustered stateful apps, see the
  [StatefulSet documentation](/docs/concepts/workloads/controllers/statefulset/).
* Use `strategy:` `type: Recreate` in the Deployment configuration
  YAML file. This instructs Kubernetes to _not_ use rolling
  updates. Rolling updates will not work, as you cannot have more than
  one Pod running at a time. The `Recreate` strategy will stop the
  first pod before creating a new one with the updated configuration.
-->
<ul>
<li>不要对应用进行规模扩缩。这里的设置仅适用于单实例应用。下层的 PersistentVolume
仅只能挂载到一个 Pod 上。对于集群级有状态应用，请参考
<a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet 文档</a>.</li>
<li>在 Deployment 的 YAML 文件中使用 <code>strategy:</code> <code>type: Recreate</code>。
该选项指示 Kubernetes <em>不</em> 使用滚动升级。滚动升级无法工作，因为这里一次不能
运行多个 Pod。在使用更新的配置文件创建新的 Pod 前，<code>Recreate</code> 策略将
保证先停止第一个 Pod。</li>
</ul>
<!--
## Deleting a deployment

Delete the deployed objects by name:
-->
<h2 id="deleting-a-deployment">删除 Deployment   </h2>
<p>通过名称删除部署的对象:</p>
<pre tabindex="0"><code>kubectl delete deployment,svc mysql
kubectl delete pvc mysql-pv-claim
kubectl delete pv mysql-pv-volume
</code></pre><!--
If you manually provisioned a PersistentVolume, you also need to manually
delete it, as well as release the underlying resource.
If you used a dynamic provisioner, it automatically deletes the
PersistentVolume when it sees that you deleted the PersistentVolumeClaim.
Some dynamic provisioners (such as those for EBS and PD) also release the
underlying resource upon deleting the PersistentVolume.
-->
<p>如果通过手动的方式供应 PersistentVolume, 那么也需要手动删除它以释放下层资源。
如果是用动态供应方式创建的 PersistentVolume，在删除 PersistentVolumeClaim 后
PersistentVolume 将被自动删除。
一些存储服务（比如 EBS 和 PD）也会在 PersistentVolume 被删除时自动回收下层资源。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [Deployment objects](/docs/concepts/workloads/controllers/deployment/).

* Learn more about [Deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/)

* [kubectl run documentation](/docs/reference/generated/kubectl/kubectl-commands/#run)

* [Volumes](/docs/concepts/storage/volumes/) and [Persistent Volumes](/docs/concepts/storage/persistent-volumes/)
-->
<ul>
<li>
<p>欲进一步了解 Deployment 对象，请参考 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment 对象</a></p>
</li>
<li>
<p>进一步了解<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署应用</a></p>
</li>
<li>
<p>参阅 <a href="/docs/reference/generated/kubectl/kubectl-commands/#run">kubectl run 文档</a></p>
</li>
<li>
<p>参阅<a href="/zh/docs/concepts/storage/volumes/">卷</a>和<a href="/zh/docs/concepts/storage/persistent-volumes/">持久卷</a></p>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-95b3d561509c573e53bec2368264cf6a">7.3 - 运行一个有状态的应用程序</h1>
    
	<!--
reviewers:
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
- smarterclayton
title: Run a Replicated Stateful Application
content_type: tutorial
weight: 30
-->
<!-- overview -->
<!--
This page shows how to run a replicated stateful application using a
[StatefulSet](/docs/concepts/workloads/controllers/statefulset/) controller.
This application is a replicated MySQL database. The example topology has a
single primary server and multiple replicas, using asynchronous row-based
replication.
-->
<p>本页展示如何使用 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>
控制器运行一个有状态的应用程序。此例是多副本的 MySQL 数据库。
示例应用的拓扑结构有一个主服务器和多个副本，使用异步的基于行（Row-Based）
的数据复制。</p>
<!--
**this is not a production configuration**.
In particular, MySQL settings remain on insecure defaults to keep the focus
on general patterns for running stateful applications in Kubernetes.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <strong>这不是生产环境下配置</strong>。
尤其注意，MySQL 设置都使用的是不安全的默认值，这是因为我们想把重点放在 Kubernetes
中运行有状态应用程序的一般模式上。</div>
</blockquote>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.

<p>您需要有一个带有默认<a href="/docs/concepts/storage/storage-classes/">StorageClass</a>的动态持续卷供应程序，或者自己<a href="/docs/user-guide/persistent-volumes/#provisioning">静态的提供持久卷</a>来满足这里使用的<a href="/docs/user-guide/persistent-volumes/#persistentvolumeclaims">持久卷请求</a>。</p>
<!--
You need to either have a dynamic PersistentVolume provisioner with a default
[StorageClass](/docs/concepts/storage/storage-classes/),
or [statically provision PersistentVolumes](/docs/user-guide/persistent-volumes/#provisioning)
yourself to satisfy the [PersistentVolumeClaims](/docs/user-guide/persistent-volumes/#persistentvolumeclaims)
used here.
-->
</p>
<!--
* This tutorial assumes you are familiar with
  [PersistentVolumes](/docs/concepts/storage/persistent-volumes/)
  and [StatefulSets](/docs/concepts/workloads/controllers/statefulset/),
  as well as other core concepts like [Pods](/docs/concepts/workloads/pods/),
  [Services](/docs/concepts/services-networking/service/), and
  [ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/).
* Some familiarity with MySQL helps, but this tutorial aims to present
  general patterns that should be useful for other systems.
* You are using the default namespace or another namespace that does not contain any conflicting objects.
-->
<ul>
<li>本教程假定你熟悉
<a href="/zh/docs/concepts/storage/persistent-volumes/">PersistentVolumes</a>
与 <a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>,
以及其他核心概念，例如 <a href="/zh/docs/concepts/workloads/pods/">Pod</a>、
<a href="/zh/docs/concepts/services-networking/service/">服务</a> 与
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>.</li>
<li>熟悉 MySQL 会有所帮助，但是本教程旨在介绍对其他系统应该有用的常规模式。</li>
<li>您正在使用默认命名空间或不包含任何冲突对象的另一个命名空间。</li>
</ul>
<h2 id="教程目标">教程目标</h2>
<!--
* Deploy a replicated MySQL topology with a StatefulSet controller.
* Send MySQL client traffic.
* Observe resistance to downtime.
* Scale the StatefulSet up and down.
-->
<ul>
<li>使用 StatefulSet 控制器部署多副本 MySQL 拓扑架构。</li>
<li>发送 MySQL 客户端请求</li>
<li>观察对宕机的抵抗力</li>
<li>扩缩 StatefulSet 的规模</li>
</ul>
<!-- lessoncontent -->
<!--
## Deploy MySQL

The example MySQL deployment consists of a ConfigMap, two Services,
and a StatefulSet.
-->
<h2 id="deploy-mysql">部署 MySQL </h2>
<p>MySQL 示例部署包含一个 ConfigMap、两个 Service 与一个 StatefulSet。</p>
<h3 id="configmap">ConfigMap</h3>
<!--
Create the ConfigMap from the following YAML configuration file:
-->
<p>使用以下的 YAML 配置文件创建 ConfigMap ：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/mysql/mysql-configmap.yaml" download="application/mysql/mysql-configmap.yaml"><code>application/mysql/mysql-configmap.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-mysql-mysql-configmap-yaml')" title="Copy application/mysql/mysql-configmap.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-mysql-mysql-configmap-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConfigMap<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">master.cnf</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    # Apply this config only on the master.
</span><span style="color:#b44;font-style:italic">    [mysqld]
</span><span style="color:#b44;font-style:italic">    log-bin</span><span style="color:#bbb">    
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">slave.cnf</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">    # Apply this config only on slaves.
</span><span style="color:#b44;font-style:italic">    [mysqld]
</span><span style="color:#b44;font-style:italic">    super-read-only</span><span style="color:#bbb">    
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml
</code></pre></div><!--
This ConfigMap provides `my.cnf` overrides that let you independently control
configuration on the primary MySQL server and replicas.
In this case, you want the primary server to be able to serve replication logs to replicas
and you want repicas to reject any writes that don't come via replication. 
-->
<p>这个 ConfigMap 提供 <code>my.cnf</code> 覆盖设置，使你可以独立控制 MySQL 主服务器和从服务器的配置。
在这里，你希望主服务器能够将复制日志提供给副本服务器，并且希望副本服务器拒绝任何不是通过
复制进行的写操作。</p>
<!--
There's nothing special about the ConfigMap itself that causes different
portions to apply to different Pods.
Each Pod decides which portion to look at as it's initializing,
based on information provided by the StatefulSet controller. 
-->
<p>ConfigMap 本身没有什么特别之处，因而也不会出现不同部分应用于不同的 Pod 的情况。
每个 Pod 都会在初始化时基于 StatefulSet 控制器提供的信息决定要查看的部分。</p>
<!--
### Services 

Create the Services from the following YAML configuration file: 
-->
<h3 id="services">服务 </h3>
<p>使用以下 YAML 配置文件创建服务：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/mysql/mysql-services.yaml" download="application/mysql/mysql-services.yaml"><code>application/mysql/mysql-services.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-mysql-mysql-services-yaml')" title="Copy application/mysql/mysql-services.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-mysql-mysql-services-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># Headless service for stable DNS entries of StatefulSet members.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># Client service for connecting to any MySQL instance for reads.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># For writes, you must instead connect to the master: mysql-0.mysql.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql-read<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml
</code></pre></div><!--
The Headless Service provides a home for the DNS entries that the StatefulSet
controller creates for each Pod that's part of the set.
Because the Headless Service is named `mysql`, the Pods are accessible by
resolving `<pod-name>.mysql` from within any other Pod in the same Kubernetes
cluster and namespace. 
-->
<p>这个无头服务给 StatefulSet 控制器为集合中每个 Pod 创建的 DNS 条目提供了一个宿主。
因为服务名为 <code>mysql</code>，所以可以通过在同一 Kubernetes 集群和名字中的任何其他 Pod
内解析 <code>&lt;Pod 名称&gt;.mysql</code> 来访问 Pod。</p>
<!--
The Client Service, called `mysql-read`, is a normal Service with its own
cluster IP that distributes connections across all MySQL Pods that report
being Ready. The set of potential endpoints includes the primary MySQL server and all
replicas. 
-->
<p>客户端服务称为 <code>mysql-read</code>，是一种常规服务，具有其自己的集群 IP。
该集群 IP 在报告就绪的所有MySQL Pod 之间分配连接。
可能的端点集合包括 MySQL 主节点和所有副本节点。</p>
<!--
Note that only read queries can use the load-balanced Client Service.
Because there is only one primary MySQL server, clients should connect directly to the
primary MySQL Pod (through its DNS entry within the Headless Service) to execute
writes. 
-->
<p>请注意，只有读查询才能使用负载平衡的客户端服务。
因为只有一个 MySQL 主服务器，所以客户端应直接连接到 MySQL 主服务器 Pod
（通过其在无头服务中的 DNS 条目）以执行写入操作。</p>
<h3 id="statefulset">StatefulSet</h3>
<!--
Finally, create the StatefulSet from the following YAML configuration file: 
-->
<p>最后，使用以下 YAML 配置文件创建 StatefulSet：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/mysql/mysql-statefulset.yaml" download="application/mysql/mysql-statefulset.yaml"><code>application/mysql/mysql-statefulset.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-mysql-mysql-statefulset-yaml')" title="Copy application/mysql/mysql-statefulset.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-mysql-mysql-statefulset-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StatefulSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceName</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">initContainers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>init-mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mysql:5.7<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- bash<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#b44">&#34;-c&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- |<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">          set -ex
</span><span style="color:#b44;font-style:italic">          # Generate mysql server-id from pod ordinal index.
</span><span style="color:#b44;font-style:italic">          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
</span><span style="color:#b44;font-style:italic">          ordinal=${BASH_REMATCH[1]}
</span><span style="color:#b44;font-style:italic">          echo [mysqld] &gt; /mnt/conf.d/server-id.cnf
</span><span style="color:#b44;font-style:italic">          # Add an offset to avoid reserved server-id=0 value.
</span><span style="color:#b44;font-style:italic">          echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf
</span><span style="color:#b44;font-style:italic">          # Copy appropriate conf.d files from config-map to emptyDir.
</span><span style="color:#b44;font-style:italic">          if [[ $ordinal -eq 0 ]]; then
</span><span style="color:#b44;font-style:italic">            cp /mnt/config-map/master.cnf /mnt/conf.d/
</span><span style="color:#b44;font-style:italic">          else
</span><span style="color:#b44;font-style:italic">            cp /mnt/config-map/slave.cnf /mnt/conf.d/
</span><span style="color:#b44;font-style:italic">          fi</span><span style="color:#bbb">          
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>conf<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/mnt/conf.d<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-map<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/mnt/config-map<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>clone-mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/xtrabackup:1.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- bash<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#b44">&#34;-c&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- |<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">          set -ex
</span><span style="color:#b44;font-style:italic">          # Skip the clone if data already exists.
</span><span style="color:#b44;font-style:italic">          [[ -d /var/lib/mysql/mysql ]] &amp;&amp; exit 0
</span><span style="color:#b44;font-style:italic">          # Skip the clone on master (ordinal index 0).
</span><span style="color:#b44;font-style:italic">          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
</span><span style="color:#b44;font-style:italic">          ordinal=${BASH_REMATCH[1]}
</span><span style="color:#b44;font-style:italic">          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0
</span><span style="color:#b44;font-style:italic">          # Clone data from previous peer.
</span><span style="color:#b44;font-style:italic">          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
</span><span style="color:#b44;font-style:italic">          # Prepare the backup.
</span><span style="color:#b44;font-style:italic">          xtrabackup --prepare --target-dir=/var/lib/mysql</span><span style="color:#bbb">          
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">subPath</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>conf<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/mysql/conf.d<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>mysql:5.7<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MYSQL_ALLOW_EMPTY_PASSWORD<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">3306</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">subPath</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>conf<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/mysql/conf.d<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>1Gi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;mysqladmin&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;ping&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># Check we can execute queries over TCP (skip-networking is off).</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;mysql&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-h&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;127.0.0.1&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-e&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;SELECT 1&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>xtrabackup<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/xtrabackup:1.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>xtrabackup<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">3307</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- bash<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#b44">&#34;-c&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- |<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">          set -ex
</span><span style="color:#b44;font-style:italic">          cd /var/lib/mysql
</span><span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">          # Determine binlog position of cloned data, if any.
</span><span style="color:#b44;font-style:italic">          if [[ -f xtrabackup_slave_info &amp;&amp; &#34;x$(&lt;xtrabackup_slave_info)&#34; != &#34;x&#34; ]]; then
</span><span style="color:#b44;font-style:italic">            # XtraBackup already generated a partial &#34;CHANGE MASTER TO&#34; query
</span><span style="color:#b44;font-style:italic">            # because we&#39;re cloning from an existing slave. (Need to remove the tailing semicolon!)
</span><span style="color:#b44;font-style:italic">            cat xtrabackup_slave_info | sed -E &#39;s/;$//g&#39; &gt; change_master_to.sql.in
</span><span style="color:#b44;font-style:italic">            # Ignore xtrabackup_binlog_info in this case (it&#39;s useless).
</span><span style="color:#b44;font-style:italic">            rm -f xtrabackup_slave_info xtrabackup_binlog_info
</span><span style="color:#b44;font-style:italic">          elif [[ -f xtrabackup_binlog_info ]]; then
</span><span style="color:#b44;font-style:italic">            # We&#39;re cloning directly from master. Parse binlog position.
</span><span style="color:#b44;font-style:italic">            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
</span><span style="color:#b44;font-style:italic">            rm -f xtrabackup_binlog_info xtrabackup_slave_info
</span><span style="color:#b44;font-style:italic">            echo &#34;CHANGE MASTER TO MASTER_LOG_FILE=&#39;${BASH_REMATCH[1]}&#39;,\
</span><span style="color:#b44;font-style:italic">                  MASTER_LOG_POS=${BASH_REMATCH[2]}&#34; &gt; change_master_to.sql.in
</span><span style="color:#b44;font-style:italic">          fi
</span><span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">          # Check if we need to complete a clone by starting replication.
</span><span style="color:#b44;font-style:italic">          if [[ -f change_master_to.sql.in ]]; then
</span><span style="color:#b44;font-style:italic">            echo &#34;Waiting for mysqld to be ready (accepting connections)&#34;
</span><span style="color:#b44;font-style:italic">            until mysql -h 127.0.0.1 -e &#34;SELECT 1&#34;; do sleep 1; done
</span><span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">            echo &#34;Initializing replication from clone position&#34;
</span><span style="color:#b44;font-style:italic">            mysql -h 127.0.0.1 \
</span><span style="color:#b44;font-style:italic">                  -e &#34;$(&lt;change_master_to.sql.in), \
</span><span style="color:#b44;font-style:italic">                          MASTER_HOST=&#39;mysql-0.mysql&#39;, \
</span><span style="color:#b44;font-style:italic">                          MASTER_USER=&#39;root&#39;, \
</span><span style="color:#b44;font-style:italic">                          MASTER_PASSWORD=&#39;&#39;, \
</span><span style="color:#b44;font-style:italic">                          MASTER_CONNECT_RETRY=10; \
</span><span style="color:#b44;font-style:italic">                        START SLAVE;&#34; || exit 1
</span><span style="color:#b44;font-style:italic">            # In case of container restart, attempt this at-most-once.
</span><span style="color:#b44;font-style:italic">            mv change_master_to.sql.in change_master_to.sql.orig
</span><span style="color:#b44;font-style:italic">          fi
</span><span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">          # Start a server to send backups when requested by peers.
</span><span style="color:#b44;font-style:italic">          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
</span><span style="color:#b44;font-style:italic">            &#34;xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root&#34;</span><span style="color:#bbb">          
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/mysql<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">subPath</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>conf<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/mysql/conf.d<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>conf<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config-map<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mysql<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumeClaimTemplates</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>data<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;ReadWriteOnce&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml
</code></pre></div><!--
You can watch the startup progress by running: 
-->
<p>你可以通过运行以下命令查看启动进度：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>mysql --watch
</code></pre></div><!--
After a while, you should see all 3 Pods become Running: 
-->
<p>一段时间后，你应该看到所有 3 个 Pod 进入 Running 状态：</p>
<pre tabindex="0"><code>NAME      READY     STATUS    RESTARTS   AGE
mysql-0   2/2       Running   0          2m
mysql-1   2/2       Running   0          1m
mysql-2   2/2       Running   0          1m
</code></pre><!--
Press **Ctrl+C** to cancel the watch.
If you don't see any progress, make sure you have a dynamic PersistentVolume
provisioner enabled as mentioned in the [prerequisites](#before-you-begin). 
-->
<p>输入 <strong>Ctrl+C</strong> 结束 watch 操作。
如果你看不到任何进度，确保已启用<a href="#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B">前提条件</a>
中提到的动态 PersistentVolume 预配器。</p>
<!--
This manifest uses a variety of techniques for managing stateful Pods as part of
a StatefulSet. The next section highlights some of these techniques to explain
what happens as the StatefulSet creates Pods. 
-->
<p>此清单使用多种技术来管理作为 StatefulSet 的一部分的有状态 Pod。
下一节重点介绍其中的一些技巧，以解释 StatefulSet 创建 Pod 时发生的状况。</p>
<!--
## Understanding stateful Pod initialization 

The StatefulSet controller starts Pods one at a time, in order by their
ordinal index.
It waits until each Pod reports being Ready before starting the next one. 
-->
<h2 id="了解有状态的-pod-初始化">了解有状态的 Pod 初始化</h2>
<p>StatefulSet 控制器按序数索引顺序地每次启动一个 Pod。
它一直等到每个 Pod 报告就绪才再启动下一个 Pod。</p>
<!--
In addition, the controller assigns each Pod a unique, stable name of the form
`<statefulset-name>-<ordinal-index>`, which results in Pods named `mysql-0`,
`mysql-1`, and `mysql-2`. 
-->
<p>此外，控制器为每个 Pod 分配一个唯一、稳定的名称，形如 <code>&lt;statefulset 名称&gt;-&lt;序数索引&gt;</code>，
其结果是 Pods 名为 <code>mysql-0</code>、<code>mysql-1</code> 和 <code>mysql-2</code>。</p>
<!--
The Pod template in the above StatefulSet manifest takes advantage of these
properties to perform orderly startup of MySQL replication. 
-->
<p>上述 StatefulSet 清单中的 Pod 模板利用这些属性来执行 MySQL 副本的有序启动。</p>
<!--
### Generating configuration 

Before starting any of the containers in the Pod spec, the Pod first runs any
[Init Containers](/docs/concepts/workloads/pods/init-containers/)
in the order defined. 
-->
<h3 id="生成配置">生成配置</h3>
<p>在启动 Pod 规约中的任何容器之前，Pod 首先按顺序运行所有的
<a href="/zh/docs/concepts/workloads/pods/init-containers/">Init 容器</a>。</p>
<!--
The first Init Container, named `init-mysql`, generates special MySQL config
files based on the ordinal index. 
-->
<p>第一个名为 <code>init-mysql</code> 的 Init 容器根据序号索引生成特殊的 MySQL 配置文件。</p>
<!--
The script determines its own ordinal index by extracting it from the end of
the Pod name, which is returned by the `hostname` command.
Then it saves the ordinal (with a numeric offset to avoid reserved values)
into a file called `server-id.cnf` in the MySQL `conf.d` directory.
This translates the unique, stable identity provided by the StatefulSet
controller into the domain of MySQL server IDs, which require the same
properties. 
-->
<p>该脚本通过从 Pod 名称的末尾提取索引来确定自己的序号索引，而 Pod 名称由 <code>hostname</code> 命令返回。
然后将序数（带有数字偏移量以避免保留值）保存到 MySQL conf.d 目录中的文件 server-id.cnf。
这一操作将 StatefulSet 所提供的唯一、稳定的标识转换为 MySQL 服务器的 ID，
而这些 ID 也是需要唯一性、稳定性保证的。</p>
<!--
The script in the `init-mysql` container also applies either `primary.cnf` or
`replica.cnf` from the ConfigMap by copying the contents into `conf.d`.
Because the example topology consists of a single primary MySQL server and any number of
replicas, the script assigns ordinal `0` to be the primary server, and everyone
else to be replicas. 

Combined with the StatefulSet controller's
[deployment order guarantee](/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees),
this ensures the primary MySQL server is Ready before creating replicas, so they can begin
replicating.
-->
<p>通过将内容复制到 conf.d 中，<code>init-mysql</code> 容器中的脚本也可以应用 ConfigMap 中的
<code>primary.cnf</code> 或 <code>replica.cnf</code>。
由于示例部署结构由单个 MySQL 主节点和任意数量的副本节点组成，
因此脚本仅将序数 <code>0</code> 指定为主节点，而将其他所有节点指定为副本节点。</p>
<p>与 StatefulSet 控制器的
<a href="/zh/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">部署顺序保证</a>
相结合，
可以确保 MySQL 主服务器在创建副本服务器之前已准备就绪，以便它们可以开始复制。</p>
<!--
### Cloning existing data 

In general, when a new Pod joins the set as a replica, it must assume the primary MySQL
server might already have data on it. It also must assume that the replication
logs might not go all the way back to the beginning of time. 
-->
<h3 id="克隆现有数据">克隆现有数据</h3>
<p>通常，当新 Pod 作为副本节点加入集合时，必须假定 MySQL 主节点可能已经有数据。
还必须假设复制日志可能不会一直追溯到时间的开始。</p>
<!--
These conservative assumptions are the key to allow a running StatefulSet
to scale up and down over time, rather than being fixed at its initial size. 
-->
<p>这些保守的假设是允许正在运行的 StatefulSet 随时间扩大和缩小而不是固定在其初始大小的关键。</p>
<!--
The second Init Container, named `clone-mysql`, performs a clone operation on
a replica Pod the first time it starts up on an empty PersistentVolume.
That means it copies all existing data from another running Pod,
so its local state is consistent enough to begin replicating from the primary server.
-->
<p>第二个名为 <code>clone-mysql</code> 的 Init 容器，第一次在带有空 PersistentVolume 的副本 Pod
上启动时，会在从属 Pod 上执行克隆操作。
这意味着它将从另一个运行中的 Pod 复制所有现有数据，使此其本地状态足够一致，
从而可以开始从主服务器复制。</p>
<!--
MySQL itself does not provide a mechanism to do this, so the example uses a
popular open-source tool called Percona XtraBackup.
During the clone, the source MySQL server might suffer reduced performance.
To minimize impact on the primary MySQL server, the script instructs each Pod to clone
from the Pod whose ordinal index is one lower.
This works because the StatefulSet controller always ensures Pod `N` is
Ready before starting Pod `N+1`. 
-->
<p>MySQL 本身不提供执行此操作的机制，因此本示例使用了一种流行的开源工具 Percona XtraBackup。
在克隆期间，源 MySQL 服务器性能可能会受到影响。
为了最大程度地减少对 MySQL 主服务器的影响，该脚本指示每个 Pod 从序号较低的 Pod 中克隆。
可以这样做的原因是 StatefulSet 控制器始终确保在启动 Pod N + 1 之前 Pod N 已准备就绪。</p>
<!--
### Starting replication 

After the Init Containers complete successfully, the regular containers run.
The MySQL Pods consist of a `mysql` container that runs the actual `mysqld`
server, and an `xtrabackup` container that acts as a
[sidecar](https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns). 
-->
<h3 id="开始复制">开始复制</h3>
<p>Init 容器成功完成后，应用容器将运行。
MySQL Pod 由运行实际 <code>mysqld</code> 服务的 <code>mysql</code> 容器和充当
<a href="https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns">辅助工具</a>
的 xtrabackup 容器组成。</p>
<!--
The `xtrabackup` sidecar looks at the cloned data files and determines if
it's necessary to initialize MySQL replication on the replica.
If so, it waits for `mysqld` to be ready and then executes the
`CHANGE MASTER TO` and `START SLAVE` commands with replication parameters
extracted from the XtraBackup clone files. 
-->
<p><code>xtrabackup</code> sidecar 容器查看克隆的数据文件，并确定是否有必要在副本服务器上初始化 MySQL 复制。
如果是这样，它将等待 <code>mysqld</code> 准备就绪，然后使用从 XtraBackup 克隆文件中提取的复制参数
执行  <code>CHANGE MASTER TO</code> 和 <code>START SLAVE</code> 命令。</p>
<!--
Once a replica begins replication, it remembers its primary MySQL server and
reconnects automatically if the server restarts or the connection dies.
Also, because replicas look for the primary server at its stable DNS name
(`mysql-0.mysql`), they automatically find the primary server even if it gets a new
Pod IP due to being rescheduled. 
-->
<p>一旦副本服务器开始复制后，它会记住其 MySQL 主服务器，并且如果服务器重新启动或
连接中断也会自动重新连接。
另外，因为副本服务器会以其稳定的 DNS 名称查找主服务器（<code>mysql-0.mysql</code>），
即使由于重新调度而获得新的 Pod IP，它们也会自动找到主服务器。</p>
<!--
Lastly, after starting replication, the `xtrabackup` container listens for
connections from other Pods requesting a data clone.
This server remains up indefinitely in case the StatefulSet scales up, or in
case the next Pod loses its PersistentVolumeClaim and needs to redo the clone. 
-->
<p>最后，开始复制后，<code>xtrabackup</code> 容器监听来自其他 Pod 的连接，处理其数据克隆请求。
如果 StatefulSet 扩大规模，或者下一个 Pod 失去其 PersistentVolumeClaim 并需要重新克隆，
则此服务器将无限期保持运行。</p>
<!--
## Sending client traffic 

You can send test queries to the primary MySQL server (hostname `mysql-0.mysql`)
by running a temporary container with the `mysql:5.7` image and running the
`mysql` client binary. 
-->
<h2 id="发送客户端请求">发送客户端请求</h2>
<p>你可以通过运行带有 <code>mysql:5.7</code> 镜像的临时容器并运行 <code>mysql</code> 客户端二进制文件，
将测试查询发送到 MySQL 主服务器（主机名 <code>mysql-0.mysql</code>）。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run mysql-client --image<span style="color:#666">=</span>mysql:5.7 -i --rm --restart<span style="color:#666">=</span>Never --<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  mysql -h mysql-0.mysql <span style="color:#b44">&lt;&lt;EOF
</span><span style="color:#b44">CREATE DATABASE test;
</span><span style="color:#b44">CREATE TABLE test.messages (message VARCHAR(250));
</span><span style="color:#b44">INSERT INTO test.messages VALUES (&#39;hello&#39;);
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Use the hostname `mysql-read` to send test queries to any server that reports
being Ready: 
-->
<p>使用主机名 <code>mysql-read</code> 将测试查询发送到任何报告为就绪的服务器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run mysql-client --image<span style="color:#666">=</span>mysql:5.7 -i -t --rm --restart<span style="color:#666">=</span>Never --<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  mysql -h mysql-read -e <span style="color:#b44">&#34;SELECT * FROM test.messages&#34;</span>
</code></pre></div><!--
You should get output like this: 
-->
<p>你应该获得如下输出：</p>
<pre tabindex="0"><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod &quot;mysql-client&quot; deleted
</code></pre><!--
To demonstrate that the `mysql-read` Service distributes connections across
servers, you can run `SELECT @@server_id` in a loop: 
-->
<p>为了演示 <code>mysql-read</code> 服务在服务器之间分配连接，你可以在循环中运行 <code>SELECT @@server_id</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run mysql-client-loop --image<span style="color:#666">=</span>mysql:5.7 -i -t --rm --restart<span style="color:#666">=</span>Never --<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  bash -ic <span style="color:#b44">&#34;while sleep 1; do mysql -h mysql-read -e &#39;SELECT @@server_id,NOW()&#39;; done&#34;</span>
</code></pre></div><!--
You should see the reported `@@server_id` change randomly, because a different
endpoint might be selected upon each connection attempt: 
-->
<p>你应该看到报告的 <code>@@server_id</code> 发生随机变化，因为每次尝试连接时都可能选择了不同的端点：</p>
<pre tabindex="0"><code>+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2006-01-02 15:04:05 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2006-01-02 15:04:06 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2006-01-02 15:04:07 |
+-------------+---------------------+
</code></pre><!--
You can press **Ctrl+C** when you want to stop the loop, but it's useful to keep
it running in another window so you can see the effects of the following steps. 
-->
<p>要停止循环时可以按 <strong>Ctrl+C</strong> ，但是让它在另一个窗口中运行非常有用，
这样你就可以看到以下步骤的效果。</p>
<!--
## Simulating Pod and Node downtime 

To demonstrate the increased availability of reading from the pool of replicas
instead of a single server, keep the `SELECT @@server_id` loop from above
running while you force a Pod out of the Ready state. 
-->
<h2 id="模拟-pod-和-node-的宕机时间">模拟 Pod 和 Node 的宕机时间</h2>
<p>为了证明从副本节点缓存而不是单个服务器读取数据的可用性提高，请在使 Pod 退出 Ready
状态时，保持上述 <code>SELECT @@server_id</code> 循环一直运行。</p>
<!--
### Break the Readiness Probe 

The [readiness probe](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes)
for the `mysql` container runs the command `mysql -h 127.0.0.1 -e 'SELECT 1'`
to make sure the server is up and able to execute queries. 
-->
<h3 id="破坏就绪态探测">破坏就绪态探测</h3>
<p><code>mysql</code> 容器的
<a href="/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes">就绪态探测</a>
运行命令 <code>mysql -h 127.0.0.1 -e 'SELECT 1'</code>，以确保服务器已启动并能够执行查询。</p>
<!--
One way to force this readiness probe to fail is to break that command: 
-->
<p>迫使就绪态探测失败的一种方法就是中止该命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off
</code></pre></div><!--
This reaches into the actual container's filesystem for Pod `mysql-2` and
renames the `mysql` command so the readiness probe can't find it.
After a few seconds, the Pod should report one of its containers as not Ready,
which you can check by running: 
-->
<p>此命令会进入 Pod <code>mysql-2</code> 的实际容器文件系统，重命名 <code>mysql</code> 命令，导致就绪态探测无法找到它。
几秒钟后， Pod 会报告其中一个容器未就绪。你可以通过运行以下命令进行检查：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod mysql-2
</code></pre></div><!--
Look for `1/2` in the `READY` column: 
-->
<p>在 <code>READY</code> 列中查找 <code> 1/2</code> ：</p>
<pre tabindex="0"><code>NAME      READY     STATUS    RESTARTS   AGE
mysql-2   1/2       Running   0          3m
</code></pre><!--
At this point, you should see your `SELECT @@server_id` loop continue to run,
although it never reports `102` anymore.
Recall that the `init-mysql` script defined `server-id` as `100 + $ordinal`,
so server ID `102` corresponds to Pod `mysql-2`. 
-->
<p>此时，你应该会看到 <code>SELECT @@server_id</code> 循环继续运行，尽管它不再报告 <code>102</code>。
回想一下，<code>init-mysql</code> 脚本将 <code>server-id</code> 定义为 <code>100 + $ordinal</code>，
因此服务器 ID <code>102</code> 对应于 Pod <code>mysql-2</code>。</p>
<!--
Now repair the Pod and it should reappear in the loop output
after a few seconds: 
-->
<p>现在修复 Pod，几秒钟后它应该重新出现在循环输出中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql
</code></pre></div><!--
### Delete Pods 

The StatefulSet also recreates Pods if they're deleted, similar to what a
ReplicaSet does for stateless Pods. 
-->
<h3 id="删除-pods">删除 Pods</h3>
<p>如果删除了 Pod，则 StatefulSet 还会重新创建 Pod，类似于 ReplicaSet 对无状态 Pod 所做的操作。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod mysql-2
</code></pre></div><!--
The StatefulSet controller notices that no `mysql-2` Pod exists anymore,
and creates a new one with the same name and linked to the same
PersistentVolumeClaim.
You should see server ID `102` disappear from the loop output for a while
and then return on its own. 
-->
<p>StatefulSet 控制器注意到不再存在 <code>mysql-2</code> Pod，于是创建一个具有相同名称并链接到相同
PersistentVolumeClaim 的新 Pod。
你应该看到服务器 ID <code>102</code> 从循环输出中消失了一段时间，然后又自行出现。</p>
<!--
### Drain a Node 

If your Kubernetes cluster has multiple Nodes, you can simulate Node downtime
(such as when Nodes are upgraded) by issuing a
[drain](/docs/reference/generated/kubectl/kubectl-commands/#drain). 
-->
<h3 id="drain-a-node">腾空节点  </h3>
<p>如果你的 Kubernetes 集群具有多个节点，则可以通过发出以下
<a href="/docs/reference/generated/kubectl/kubectl-commands/#drain">drain</a>
命令来模拟节点停机（就好像节点在被升级）。</p>
<!--
First determine which Node one of the MySQL Pods is on: 
-->
<p>首先确定 MySQL Pod 之一在哪个节点上：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod mysql-2 -o wide
</code></pre></div><!--
The Node name should show up in the last column: 
-->
<p>节点名称应显示在最后一列中：</p>
<pre tabindex="0"><code>NAME      READY     STATUS    RESTARTS   AGE       IP            NODE
mysql-2   2/2       Running   0          15m       10.244.5.27   kubernetes-node-9l2t
</code></pre><!--
Then drain the Node by running the following command, which cordons it so
no new Pods may schedule there, and then evicts any existing Pods.
Replace `<node-name>` with the name of the Node you found in the last step. 
-->
<p>然后通过运行以下命令腾空节点，该命令将其保护起来，以使新的 Pod 不能调度到该节点，
然后逐出所有现有的 Pod。将 <code>&lt;节点名称&gt;</code> 替换为在上一步中找到的节点名称。</p>
<!--
This might impact other applications on the Node, so it's best to
**only do this in a test cluster**. 

```shell
kubectl drain <node-name> --force --delete-local-data --ignore-daemonsets
```
-->
<p>这可能会影响节点上的其他应用程序，因此最好 <strong>仅在测试集群中执行此操作</strong>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl drain &lt;节点名称&gt; --force --delete-local-data --ignore-daemonsets
</code></pre></div><!--
Now you can watch as the Pod reschedules on a different Node: 
-->
<p>现在，你可以看到 Pod 被重新调度到其他节点上：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod mysql-2 -o wide --watch
</code></pre></div><!--
It should look something like this: 
-->
<p>它看起来应该像这样：</p>
<pre tabindex="0"><code>NAME      READY   STATUS          RESTARTS   AGE       IP            NODE
mysql-2   2/2     Terminating     0          15m       10.244.1.56   kubernetes-node-9l2t
[...]
mysql-2   0/2     Pending         0          0s        &lt;none&gt;        kubernetes-node-fjlm
mysql-2   0/2     Init:0/2        0          0s        &lt;none&gt;        kubernetes-node-fjlm
mysql-2   0/2     Init:1/2        0          20s       10.244.5.32   kubernetes-node-fjlm
mysql-2   0/2     PodInitializing 0          21s       10.244.5.32   kubernetes-node-fjlm
mysql-2   1/2     Running         0          22s       10.244.5.32   kubernetes-node-fjlm
mysql-2   2/2     Running         0          30s       10.244.5.32   kubernetes-node-fjlm
</code></pre><!--
And again, you should see server ID `102` disappear from the
`SELECT @@server_id` loop output for a while and then return. 
-->
<p>再次，你应该看到服务器 ID <code>102</code> 从 <code>SELECT @@server_id</code> 循环输出
中消失一段时间，然后自行出现。</p>
<!--
Now uncordon the Node to return it to a normal state: 

```shell
kubectl uncordon <node-name>
```
-->
<p>现在去掉节点保护（Uncordon），使其恢复为正常模式:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl uncordon &lt;节点名称&gt;
</code></pre></div><!--
## Scaling the number of replicas

With MySQL replication, you can scale your read query capacity by adding replicas.
With StatefulSet, you can do this with a single command: 
-->
<h2 id="扩展副本节点数量">扩展副本节点数量</h2>
<p>使用 MySQL 复制，你可以通过添加副本节点来扩展读取查询的能力。
使用 StatefulSet，你可以使用单个命令执行此操作：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale statefulset mysql --replicas<span style="color:#666">=</span><span style="color:#666">5</span>
</code></pre></div><!--
Watch the new Pods come up by running: 
-->
<p>查看新的 Pod 的运行情况：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>mysql --watch
</code></pre></div><!--
Once they're up, you should see server IDs `103` and `104` start appearing in
the `SELECT @@server_id` loop output.

You can also verify that these new servers have the data you added before they
existed: 
-->
<p>一旦 Pod 启动，你应该看到服务器 IDs <code>103</code> 和 <code>104</code> 开始出现在 <code>SELECT @@server_id</code> 循环输出中。</p>
<p>你还可以验证这些新服务器在存在之前已添加了数据：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run mysql-client --image<span style="color:#666">=</span>mysql:5.7 -i -t --rm --restart<span style="color:#666">=</span>Never --<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>  mysql -h mysql-3.mysql -e <span style="color:#b44">&#34;SELECT * FROM test.messages&#34;</span>
</code></pre></div><pre tabindex="0"><code>Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod &quot;mysql-client&quot; deleted
</code></pre><!--
Scaling back down is also seamless: 
-->
<p>向下缩容操作也是很平滑的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale statefulset mysql --replicas<span style="color:#666">=</span><span style="color:#666">3</span>
</code></pre></div><!--
Note, however, that while scaling up creates new PersistentVolumeClaims
automatically, scaling down does not automatically delete these PVCs.
This gives you the choice to keep those initialized PVCs around to make
scaling back up quicker, or to extract data before deleting them. 
-->
<p>但是请注意，按比例扩大会自动创建新的 PersistentVolumeClaims，而按比例缩小不会自动删除这些 PVC。
这使你可以选择保留那些初始化的 PVC，以更快地进行缩放，或者在删除它们之前提取数据。</p>
<!--
You can see this by running: 
-->
<p>你可以通过运行以下命令查看此信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>mysql
</code></pre></div><!--
Which shows that all 5 PVCs still exist, despite having scaled the
StatefulSet down to 3: 
-->
<p>这表明，尽管将 StatefulSet 缩小为3，所有5个 PVC 仍然存在：</p>
<pre tabindex="0"><code>NAME           STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
data-mysql-0   Bound     pvc-8acbf5dc-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-1   Bound     pvc-8ad39820-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-2   Bound     pvc-8ad69a6d-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-3   Bound     pvc-50043c45-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
data-mysql-4   Bound     pvc-500a9957-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
</code></pre><!--
If you don't intend to reuse the extra PVCs, you can delete them: 
-->
<p>如果你不打算重复使用多余的 PVC，则可以删除它们：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pvc data-mysql-3
kubectl delete pvc data-mysql-4
</code></pre></div><h2 id="清理现场">清理现场</h2>
<!--
1. Cancel the `SELECT @@server_id` loop by pressing **Ctrl+C** in its terminal,
   or running the following from another terminal: 
-->
<ol>
<li>
<p>通过在终端上按 <strong>Ctrl+C</strong> 取消 <code>SELECT @@server_id</code> 循环，或从另一个终端运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod mysql-client-loop --now
</code></pre></div></li>
</ol>
<!--
1. Delete the StatefulSet. This also begins terminating the Pods. 
-->
<ol start="2">
<li>
<p>删除 StatefulSet。这也会开始终止 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulset mysql
</code></pre></div></li>
</ol>
<!--
1. Verify that the Pods disappear.
   They might take some time to finish terminating. 
-->
<ol start="3">
<li>
<p>验证 Pod 消失。他们可能需要一些时间才能完成终止。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>mysql
</code></pre></div><!-- You'll know the Pods have terminated when the above returns: -->
<p>当上述命令返回如下内容时，你就知道 Pod 已终止：</p>
<pre tabindex="0"><code>No resources found.
</code></pre></li>
</ol>
<!--
1. Delete the ConfigMap, Services, and PersistentVolumeClaims. 
-->
<ol start="4">
<li>
<p>删除 ConfigMap、Services 和 PersistentVolumeClaims。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete configmap,service,pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>mysql
</code></pre></div></li>
</ol>
<!--
1. If you manually provisioned PersistentVolumes, you also need to manually
   delete them, as well as release the underlying resources.
   If you used a dynamic provisioner, it automatically deletes the
   PersistentVolumes when it sees that you deleted the PersistentVolumeClaims.
   Some dynamic provisioners (such as those for EBS and PD) also release the
   underlying resources upon deleting the PersistentVolumes. 
-->
<ol start="5">
<li>如果你手动供应 PersistentVolume，则还需要手动删除它们，并释放下层资源。
如果你使用了动态预配器，当得知你删除 PersistentVolumeClaims 时，它将自动删除 PersistentVolumes。
一些动态预配器（例如用于 EBS 和 PD 的预配器）也会在删除 PersistentVolumes 时释放下层资源。</li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [scaling a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).
* Learn more about [debugging a StatefulSet](/docs/tasks/debug-application-cluster/debug-stateful-set/).
* Learn more about [deleting a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).
* Learn more about [force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).
* Look in the [Helm Charts repository](https://artifacthub.io/)
  for other stateful application examples.
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/run-application/scale-stateful-set/">为 StatefulSet 扩缩容</a>.</li>
<li>进一步了解<a href="/zh/docs/tasks/debug-application-cluster/debug-stateful-set/">调试 StatefulSet</a>.</li>
<li>进一步了解<a href="/zh/docs/tasks/run-application/delete-stateful-set/">删除 StatefulSet</a>.</li>
<li>进一步了解<a href="/zh/docs/tasks/run-application/force-delete-stateful-set-pod/">强制删除 StatefulSet Pods</a>.</li>
<li>在 <a href="https://artifacthub.io/">Helm Charts 仓库</a>中查找其他有状态的应用程序示例。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c43537b0ee1da992ecb7488f87e6c934">7.4 - 删除 StatefulSet</h1>
    
	<!--
reviewers:
- bprashanth
- erictune
- foxish
- janetkuo
- smarterclayton
title: Delete a StatefulSet
content_type: task
weight: 60
-->
<!-- overview -->
<!--
This task shows you how to delete a StatefulSet.
-->
<p>本任务展示如何删除 StatefulSet。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* This task assumes you have an application running on your cluster represented by a StatefulSet.
-->
<ul>
<li>本任务假设在你的集群上已经运行了由 StatefulSet 创建的应用。</li>
</ul>
<!-- steps -->
<h2 id="deleting-a-statefulset">删除 StatefulSet  </h2>
<!--
You can delete a StatefulSet in the same way you delete other resources in Kubernetes: use the `kubectl delete` command, and specify the StatefulSet either by file or by name.
-->
<p>你可以像删除 Kubernetes 中的其他资源一样删除 StatefulSet：使用 <code>kubectl delete</code> 命令，并按文件或者名字指定 StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -f &lt;file.yaml&gt;
</code></pre></div><!--
```shell
kubectl delete statefulsets <statefulset-name>
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete statefulsets &lt;statefulset 名称&gt;
</code></pre></div><!--
You may need to delete the associated headless service separately after the StatefulSet itself is deleted.

```shell
kubectl delete service <service-name>
```
-->
<p>删除 StatefulSet 之后，你可能需要单独删除关联的无头服务。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete service &lt;服务名称&gt;
</code></pre></div><!--
When deleting a StatefulSet through `kubectl`, the StatefulSet scales down to 0. All Pods that are part of this workload are also deleted. If you want to delete only the StatefulSet and not the Pods, use `--cascade=orphan`.
For example:
--->
<p>当通过 <code>kubectl</code> 删除 StatefulSet 时，StatefulSet 会被缩容为 0。
属于该 StatefulSet 的所有 Pod 也被删除。
如果你只想删除 StatefulSet 而不删除 Pod，使用 <code>--cascade=orphan</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -f &lt;file.yaml&gt; --cascade<span style="color:#666">=</span>orphan
</code></pre></div><!--
By passing `--cascade=orphan` to `kubectl delete`, the Pods managed by the StatefulSet are left behind even after the StatefulSet object itself is deleted. If the pods have a label `app=myapp`, you can then delete them as follows:
--->
<p>通过将 <code>--cascade=orphan</code> 传递给 <code>kubectl delete</code>，在删除 StatefulSet 对象之后，
StatefulSet 管理的 Pod 会被保留下来。如果 Pod 具有标签 <code>app=myapp</code>，则可以按照
如下方式删除它们：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>myapp
</code></pre></div><!--
### Persistent Volumes

Deleting the Pods in a StatefulSet will not delete the associated volumes. This is to ensure that you have the chance to copy data off the volume before deleting it. Deleting the PVC after the pods have left the [terminating state](/docs/concepts/workloads/pods/pod/#termination-of-pods) might trigger deletion of the backing Persistent Volumes depending on the storage class and reclaim policy. You should never assume ability to access a volume after claim deletion.
-->
<h3 id="persistent-volumes">持久卷 </h3>
<p>删除 StatefulSet 管理的 Pod 并不会删除关联的卷。这是为了确保你有机会在删除卷之前从卷中复制数据。
在 Pod 离开<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">终止状态</a>
后删除 PVC 可能会触发删除背后的 PV 持久卷，具体取决于存储类和回收策略。
永远不要假定在 PVC 删除后仍然能够访问卷。</p>
<!--
Use caution when deleting a PVC, as it may lead to data loss.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 删除 PVC 时要谨慎，因为这可能会导致数据丢失。</div>
</blockquote>
<!--
### Complete deletion of a StatefulSet

To simply delete everything in a StatefulSet, including the associated pods, you can run a series of commands similar to the following:
-->
<h3 id="complete-deletion-of-a-statefulset">完全删除 StatefulSet </h3>
<p>要删除 StatefulSet 中的所有内容，包括关联的 pods，你可以运行
一系列如下所示的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">grace</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pods &lt;stateful-set-pod&gt; --template <span style="color:#b44">&#39;{{.spec.terminationGracePeriodSeconds}}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
kubectl delete statefulset -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>myapp
sleep <span style="color:#b8860b">$grace</span>
kubectl delete pvc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>myapp
</code></pre></div><!--
In the example above, the Pods have the label `app=myapp`; substitute your own label as appropriate.
-->
<p>在上面的例子中，Pod 的标签为 <code>app=myapp</code>；适当地替换你自己的标签。</p>
<!--
### Force deletion of StatefulSet pods

If you find that some pods in your StatefulSet are stuck in the 'Terminating' or 'Unknown' states for an extended period of time, you may need to manually intervene to forcefully delete the pods from the apiserver. This is a potentially dangerous task. Refer to [Deleting StatefulSet Pods](/docs/tasks/manage-stateful-set/delete-pods/) for details.
-->
<h3 id="强制删除-statefulset-的-pod">强制删除 StatefulSet 的 Pod</h3>
<p>如果你发现 StatefulSet 的某些 Pod 长时间处于 'Terminating' 或者 'Unknown' 状态，
则可能需要手动干预以强制从 API 服务器中删除这些 Pod。
这是一项有点危险的任务。详细信息请阅读
<a href="/zh/docs/tasks/run-application/force-delete-stateful-set-pod/">删除 StatefulSet 类型的 Pods</a>。</p>
<h2 id="接下来">接下来</h2>
<!--
Learn more about [force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).
-->
<p>进一步了解<a href="/zh/docs/tasks/run-application/force-delete-stateful-set-pod/">强制删除 StatefulSet 的 Pods</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f5f2f7a74377a9d45325c5253353fa8f">7.5 - 强制删除 StatefulSet 中的 Pods</h1>
    
	<!--
reviewers:
- bprashanth
- erictune
- foxish
- smarterclayton
title: Force Delete StatefulSet Pods
content_type: task
weight: 70
-->
<!-- overview -->
<!--
This page shows how to delete Pods which are part of a <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='stateful set'>stateful set</a>, and explains the considerations to keep in mind when doing so.
-->
<p>本文介绍如何删除 <a class='glossary-tooltip' title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet</a>
管理的 Pods，并解释这样操作时需要记住的注意事项。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* This is a fairly advanced task and has the potential to violate some of the properties inherent to StatefulSet.
* Before proceeding, make yourself familiar with the considerations enumerated below.
-->
<ul>
<li>这是一项相当高级的任务，并且可能会违反 StatefulSet 固有的某些属性。</li>
<li>继续任务之前，请熟悉下面列举的注意事项。</li>
</ul>
<!-- steps -->
<!--
## StatefulSet considerations

In normal operation of a StatefulSet, there is **never** a need to force delete a StatefulSet Pod. The [StatefulSet controller](/docs/concepts/workloads/controllers/statefulset/) is responsible for creating, scaling and deleting members of the StatefulSet. It tries to ensure that the specified number of Pods from ordinal 0 through N-1 are alive and ready. StatefulSet ensures that, at any time, there is at most one Pod with a given identity running in a cluster. This is referred to as *at most one* semantics provided by a StatefulSet.
-->
<h2 id="statefulset-注意事项">StatefulSet 注意事项</h2>
<p>在 StatefulSet 的正常操作中，<strong>永远不</strong>需要强制删除 StatefulSet 管理的 Pod。
<a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet 控制器</a>负责创建、
扩缩和删除 StatefulSet 管理的 Pods。它尝试确保指定数量的从序数 0 到 N-1 的 Pod
处于活跃状态并准备就绪。StatefulSet 确保在任何时候，集群中最多只有一个具有给定标识的 Pod。
这就是所谓的由 StatefulSet 提供的*最多一个（At Most One）*的语义。</p>
<!--
Manual force deletion should be undertaken with caution, as it has the potential to violate the at most one semantics inherent to StatefulSet. StatefulSets may be used to run distributed and clustered applications which have a need for a stable network identity and stable storage. These applications often have configuration which relies on an ensemble of a fixed number of members with fixed identities. Having multiple members with the same identity can be disastrous and may lead to data loss (e.g. split brain scenario in quorum-based systems).
-->
<p>应谨慎进行手动强制删除操作，因为它可能会违反 StatefulSet 固有的至多一个的语义。
StatefulSets 可用于运行分布式和集群级的应用，这些应用需要稳定的网络标识和可靠的存储。
这些应用通常配置为具有固定标识固定数量的成员集合。
具有相同身份的多个成员可能是灾难性的，并且可能导致数据丢失 (例如：票选系统中的脑裂场景)。</p>
<!--
## Delete Pods

You can perform a graceful pod deletion with the following command:
-->
<h2 id="delete-pods">删除 Pods  </h2>
<p>你可以使用下面的命令执行体面地删除 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pods &lt;pod&gt;
</code></pre></div><!--
For the above to lead to graceful termination, the Pod **must not** specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. The practice of setting a `pod.Spec.TerminationGracePeriodSeconds` of 0 seconds is unsafe and strongly discouraged for StatefulSet Pods. Graceful deletion is safe and will ensure that the [Pod shuts down gracefully](/docs/user-guide/pods/#termination-of-pods) before the kubelet deletes the name from the apiserver.
-->
<p>为了让上面操作能够体面地终止 Pod，Pod <strong>一定不能</strong> 设置 <code>pod.Spec.TerminationGracePeriodSeconds</code> 为 0。
将 <code>pod.Spec.TerminationGracePeriodSeconds</code> 设置为 0s 的做法是不安全的，强烈建议 StatefulSet 类型的
Pod 不要使用。体面删除是安全的，并且会在 kubelet 从 API 服务器中删除资源名称之前确保
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">体面地结束 pod </a>。</p>
<!--
A Pod is not deleted automatically when a Node is unreachable.
The Pods running on an unreachable Node enter the 'Terminating' or 'Unknown' state after a
[timeout](/docs/concepts/architecture/nodes/#condition).
Pods may also enter these states when the user attempts graceful deletion of a Pod
on an unreachable Node.
The only ways in which a Pod in such a state can be removed from the apiserver are as follows:
-->
<p>当某个节点不可达时，不会引发自动删除 Pod。
在无法访问的节点上运行的 Pod 在
<a href="/zh/docs/concepts/architecture/nodes/#condition">超时</a>
后会进入'Terminating' 或者 'Unknown' 状态。
当用户尝试体面地删除无法访问的节点上的 Pod 时 Pod 也可能会进入这些状态。
从 API 服务器上删除处于这些状态 Pod 的仅有可行方法如下：</p>
<!--
   * The Node object is deleted (either by you, or by the [Node Controller](/docs/admin/node)).<br/>
   * The kubelet on the unresponsive Node starts responding, kills the Pod and removes the entry from the apiserver.<br/>
   * Force deletion of the Pod by the user.
-->
<ul>
<li>删除 Node 对象（要么你来删除, 要么<a href="/zh/docs/concepts/architecture/nodes/#node-controller">节点控制器</a>
来删除）</li>
<li>无响应节点上的 kubelet 开始响应，杀死 Pod 并从 API 服务器上移除 Pod 对象</li>
<li>用户强制删除 pod</li>
</ul>
<!--
The recommended best practice is to use the first or second approach. If a Node is confirmed to be dead (e.g. permanently disconnected from the network, powered down, etc), then delete the Node object. If the Node is suffering from a network partition, then try to resolve this or wait for it to resolve. When the partition heals, the kubelet will complete the deletion of the Pod and free up its name in the apiserver.
-->
<p>推荐使用第一种或者第二种方法。如果确认节点已经不可用了 (比如，永久断开网络、断电等)，
则应删除 Node 对象。
如果节点遇到网裂问题，请尝试解决该问题或者等待其解决。
当网裂愈合时，kubelet 将完成 Pod 的删除并从 API 服务器上释放其名字。</p>
<!--
Normally, the system completes the deletion once the Pod is no longer running on a Node, or the Node is deleted by an administrator. You may override this by force deleting the Pod.
-->
<p>通常，Pod 一旦不在节点上运行，或者管理员删除了节点，系统就会完成其删除动作。
你也可以通过强制删除 Pod 来绕过这一机制。</p>
<!--
### Force Deletion

Force deletions **do not** wait for confirmation from the kubelet that the Pod has been terminated. Irrespective of whether a force deletion is successful in killing a Pod, it will immediately free up the name from the apiserver. This would let the StatefulSet controller create a replacement Pod with that same identity; this can lead to the duplication of a still-running Pod, and if said Pod can still communicate with the other members of the StatefulSet, will violate the at most one semantics that StatefulSet is designed to guarantee.
-->
<h3 id="force-deletion">强制删除   </h3>
<p>强制删除<strong>不会</strong>等待来自 kubelet 对 Pod 已终止的确认消息。
无论强制删除是否成功杀死了 Pod，它都会立即从 API 服务器中释放该名字。
这将让 StatefulSet 控制器创建一个具有相同标识的替身 Pod；因而可能导致正在运行 Pod 的重复，
并且如果所述 Pod 仍然可以与 StatefulSet 的成员通信，则将违反 StatefulSet 所要保证的
最多一个的语义。</p>
<!--
When you force delete a StatefulSet pod, you are asserting that the Pod in question will never again make contact with other Pods in the StatefulSet and its name can be safely freed up for a replacement to be created.
-->
<p>当你强制删除 StatefulSet 类型的 Pod 时，你要确保有问题的 Pod 不会再和 StatefulSet 管理的其他
Pod 通信并且可以安全地释放其名字以便创建替代 Pod。</p>
<!--
If you want to delete a Pod forcibly using kubectl version >= 1.5, do the following:
-->
<p>如果要使用 kubectl 1.5 以上版本强制删除 Pod，请执行下面命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pods &lt;pod&gt; --grace-period<span style="color:#666">=</span><span style="color:#666">0</span> --force
</code></pre></div><!--
If you're using any version of kubectl <= 1.4, you should omit the `--force` option and use:
-->
<p>如果你使用 kubectl 的 1.4 以下版本，则应省略 <code>--force</code> 选项：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pods &lt;pod&gt; --grace-period<span style="color:#666">=</span><span style="color:#666">0</span>
</code></pre></div><!--
If even after these commands the pod is stuck on `Unknown` state, use the following command to remove the pod from the cluster:
-->
<p>如果在这些命令后 Pod 仍处于 <code>Unknown</code> 状态，请使用以下命令从集群中删除 Pod:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch pod &lt;pod&gt; -p <span style="color:#b44">&#39;{&#34;metadata&#34;:{&#34;finalizers&#34;:null}}&#39;</span>
</code></pre></div><!--
Always perform force deletion of StatefulSet Pods carefully and with complete knowledge of the risks involved.
-->
<p>请始终谨慎地执行强制删除 StatefulSet 类型的 pods，并完全了解所涉及地风险。</p>
<h2 id="接下来">接下来</h2>
<!--
Learn more about [debugging a StatefulSet](/docs/tasks/debug-application-cluster/debug-stateful-set/).
-->
<p>进一步了解<a href="/zh/docs/tasks/debug-application-cluster/debug-stateful-set/">调试 StatefulSet</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0c0bb1bd76d2a9069e50e2cec6d20c2a">7.6 - Pod 水平自动扩缩</h1>
    
	<!-- overview -->
<!--
The Horizontal Pod Autoscaler automatically scales the number of Pods
in a replication controller, deployment, replica set or stateful set based on observed CPU utilization (or, with
[custom metrics](https://git.k8s.io/community/contributors/design-proposals/instrumentation/custom-metrics-api.md)
support, on some other application-provided metrics). Note that Horizontal
Pod Autoscaling does not apply to objects that can't be scaled, for example, DaemonSets.
-->
<p>Pod 水平自动扩缩（Horizontal Pod Autoscaler）
可以基于 CPU 利用率自动扩缩 ReplicationController、Deployment、ReplicaSet 和
StatefulSet 中的 Pod 数量。
除了 CPU 利用率，也可以基于其他应程序提供的
<a href="https://git.k8s.io/community/contributors/design-proposals/instrumentation/custom-metrics-api.md">自定义度量指标</a>
来执行自动扩缩。
Pod 自动扩缩不适用于无法扩缩的对象，比如 DaemonSet。</p>
<!--
The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller.
The resource determines the behavior of the controller.
The controller periodically adjusts the number of replicas in a replication controller or deployment
to match the observed metrics such as average CPU utilisation, average memory utilisation or any other custom metric to the target specified by the user.
-->
<p>Pod 水平自动扩缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。
控制器会周期性地调整副本控制器或 Deployment 中的副本数量，以使得类似 Pod 平均 CPU
利用率、平均内存利用率这类观测到的度量值与用户所设定的目标值匹配。</p>
<!-- body -->
<!--
## How does the Horizontal Pod Autoscaler work?
-->
<h2 id="pod-水平自动扩缩工作机制">Pod 水平自动扩缩工作机制</h2>
<p><img src="/images/docs/horizontal-pod-autoscaler.svg" alt="水平自动扩缩示意图"></p>
<!--
The Horizontal Pod Autoscaler is implemented as a control loop, with a period controlled
by the controller manager's `--horizontal-pod-autoscaler-sync-period` flag (with a default
value of 15 seconds).
-->
<p>Pod 水平自动扩缩器的实现是一个控制回路，由控制器管理器的 <code>--horizontal-pod-autoscaler-sync-period</code> 参数指定周期（默认值为 15 秒）。</p>
<!--
During each period, the controller manager queries the resource utilization against the
metrics specified in each HorizontalPodAutoscaler definition.  The controller manager
obtains the metrics from either the resource metrics API (for per-pod resource metrics),
or the custom metrics API (for all other metrics).
-->
<p>每个周期内，控制器管理器根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。
控制器管理器可以从资源度量指标 API（按 Pod 统计的资源用量）和自定义度量指标
API（其他指标）获取度量值。</p>
<!--
* For per-pod resource metrics (like CPU), the controller fetches the metrics
  from the resource metrics API for each Pod targeted by the HorizontalPodAutoscaler.
  Then, if a target utilization value is set, the controller calculates the utilization
  value as a percentage of the equivalent resource request on the containers in
  each Pod.  If a target raw value is set, the raw metric values are used directly.
  The controller then takes the mean of the utilization or the raw value (depending on the type
  of target specified) across all targeted Pods, and produces a ratio used to scale
  the number of desired replicas.
-->
<ul>
<li>
<p>对于按 Pod 统计的资源指标（如 CPU），控制器从资源指标 API 中获取每一个
HorizontalPodAutoscaler 指定的 Pod 的度量值，如果设置了目标使用率，
控制器获取每个 Pod 中的容器资源使用情况，并计算资源使用率。
如果设置了 target 值，将直接使用原始数据（不再计算百分比）。
接下来，控制器根据平均的资源使用率或原始值计算出扩缩的比例，进而计算出目标副本数。</p>
<!--
Please note that if some of the Pod's containers do not have the relevant resource request set,
CPU utilization for the Pod will not be defined and the autoscaler will
not take any action for that metric. See the [algorithm
details](#algorithm-details) section below for more information about
how the autoscaling algorithm works.
-->
<p>需要注意的是，如果 Pod 某些容器不支持资源采集，那么控制器将不会使用该 Pod 的 CPU 使用率。
下面的<a href="#algorithm-details">算法细节</a>章节将会介绍详细的算法。</p>
</li>
</ul>
<!--
* For per-pod custom metrics, the controller functions similarly to per-pod resource metrics,
  except that it works with raw values, not utilization values.
-->
<ul>
<li>如果 Pod 使用自定义指示，控制器机制与资源指标类似，区别在于自定义指标只使用
原始值，而不是使用率。</li>
</ul>
<!--
* For object metrics and external metrics, a single metric is fetched, which describes
  the object in question. This metric is compared to the target
  value, to produce a ratio as above. In the `autoscaling/v2beta2` API
  version, this value can optionally be divided by the number of Pods before the
  comparison is made.
-->
<ul>
<li>如果 Pod 使用对象指标和外部指标（每个指标描述一个对象信息）。
这个指标将直接根据目标设定值相比较，并生成一个上面提到的扩缩比例。
在 <code>autoscaling/v2beta2</code> 版本 API 中，这个指标也可以根据 Pod 数量平分后再计算。</li>
</ul>
<!--
The HorizontalPodAutoscaler normally fetches metrics from a series of aggregated APIs (`metrics.k8s.io`,
`custom.metrics.k8s.io`, and `external.metrics.k8s.io`).  The `metrics.k8s.io` API is usually provided by
metrics-server, which needs to be launched separately. See
[metrics-server](/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server)
for instructions. The HorizontalPodAutoscaler can also fetch metrics directly from Heapster.
-->
<p>通常情况下，控制器将从一系列的聚合 API（<code>metrics.k8s.io</code>、<code>custom.metrics.k8s.io</code>
和 <code>external.metrics.k8s.io</code>）中获取度量值。
<code>metrics.k8s.io</code> API 通常由 Metrics 服务器（需要额外启动）提供。
可以从 <a href="/zh/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server">metrics-server</a> 获取更多信息。
另外，控制器也可以直接从 Heapster 获取指标。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes 1.11 [deprecated]</code>
</div>
<!--
Fetching metrics from Heapster is deprecated as of Kubernetes 1.11.
-->
<p>自 Kubernetes 1.11 起，从 Heapster 获取指标特性已废弃。</div>
</blockquote>
<!--
See [Support for metrics APIs](#support-for-metrics-apis) for more details.
-->
<p>关于指标 API 更多信息，请参考<a href="#support-for-metrics-apis">度量值指标 API 的支持</a>。</p>
<!--
The autoscaler accesses corresponding scalable controllers (such as replication controllers, deployments, and replica sets)
by using the scale sub-resource. Scale is an interface that allows you to dynamically set the number of replicas and examine
each of their current states. More details on scale sub-resource can be found
[here](https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#scale-subresource).
-->
<p>自动扩缩控制器使用 scale 子资源访问相应可支持扩缩的控制器（如副本控制器、
Deployment 和 ReplicaSet）。
<code>scale</code> 是一个可以动态设定副本数量和检查当前状态的接口。
关于 scale 子资源的更多信息，请参考<a href="https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#scale-subresource">这里</a>.</p>
<!--
### Algorithm Details

From the most basic perspective, the Horizontal Pod Autoscaler controller
operates on the ratio between desired metric value and current metric
value:
-->
<h3 id="algorithm-details">算法细节  </h3>
<p>从最基本的角度来看，Pod 水平自动扩缩控制器根据当前指标和期望指标来计算扩缩比例。</p>
<!--
```
desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
```
-->
<pre tabindex="0"><code>期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)]
</code></pre><!--
For example, if the current metric value is `200m`, and the desired value
is `100m`, the number of replicas will be doubled, since `200.0 / 100.0 ==
2.0` If the current value is instead `50m`, we'll halve the number of
replicas, since `50.0 / 100.0 == 0.5`.  We'll skip scaling if the ratio is
sufficiently close to 1.0 (within a globally-configurable tolerance, from
the `--horizontal-pod-autoscaler-tolerance` flag, which defaults to 0.1).
-->
<p>例如，当前度量值为 <code>200m</code>，目标设定值为 <code>100m</code>，那么由于 <code>200.0/100.0 == 2.0</code>，
副本数量将会翻倍。
如果当前指标为 <code>50m</code>，副本数量将会减半，因为<code>50.0/100.0 == 0.5</code>。
如果计算出的扩缩比例接近 1.0
（根据<code>--horizontal-pod-autoscaler-tolerance</code> 参数全局配置的容忍值，默认为 0.1），
将会放弃本次扩缩。</p>
<!--
When a `targetAverageValue` or `targetAverageUtilization` is specified,
the `currentMetricValue` is computed by taking the average of the given
metric across all Pods in the HorizontalPodAutoscaler's scale target.
Before checking the tolerance and deciding on the final values, we take
pod readiness and missing metrics into consideration, however.
-->
<p>如果 HorizontalPodAutoscaler 指定的是 <code>targetAverageValue</code> 或 <code>targetAverageUtilization</code>，
那么将会把指定 Pod 度量值的平均值做为 <code>currentMetricValue</code>。
然而，在检查容忍度和决定最终扩缩值前，我们仍然会把那些无法获取指标的 Pod 统计进去。</p>
<!--
All Pods with a deletion timestamp set (i.e. Pods in the process of being
shut down) and all failed Pods are discarded.

If a particular Pod is missing metrics, it is set aside for later; Pods
with missing metrics will be used to adjust the final scaling amount.
-->
<p>所有被标记了删除时间戳（Pod 正在关闭过程中）的 Pod 和失败的 Pod 都会被忽略。</p>
<p>如果某个 Pod 缺失度量值，它将会被搁置，只在最终确定扩缩数量时再考虑。</p>
<!--
When scaling on CPU, if any pod has yet to become ready (i.e. it's still
initializing) *or* the most recent metric point for the pod was before it
became ready, that pod is set aside as well.
-->
<p>当使用 CPU 指标来扩缩时，任何还未就绪（例如还在初始化）状态的 Pod <em>或</em> 最近的指标
度量值采集于就绪状态前的 Pod，该 Pod 也会被搁置。</p>
<!--
Due to technical constraints, the HorizontalPodAutoscaler controller
cannot exactly determine the first time a pod becomes ready when
determining whether to set aside certain CPU metrics. Instead, it
considers a Pod "not yet ready" if it's unready and transitioned to
unready within a short, configurable window of time since it started.
This value is configured with the `--horizontal-pod-autoscaler-initial-readiness-delay` flag, and its default is 30
seconds.  Once a pod has become ready, it considers any transition to
ready to be the first if it occurred within a longer, configurable time
since it started. This value is configured with the `--horizontal-pod-autoscaler-cpu-initialization-period` flag, and its
default is 5 minutes.
-->
<p>由于受技术限制，Pod 水平扩缩控制器无法准确的知道 Pod 什么时候就绪，
也就无法决定是否暂时搁置该 Pod。
<code>--horizontal-pod-autoscaler-initial-readiness-delay</code> 参数（默认为 30s）用于设置 Pod 准备时间，
在此时间内的 Pod 统统被认为未就绪。
<code>--horizontal-pod-autoscaler-cpu-initialization-period</code> 参数（默认为5分钟）
用于设置 Pod 的初始化时间，
在此时间内的 Pod，CPU 资源度量值将不会被采纳。</p>
<!--
The `currentMetricValue / desiredMetricValue` base scale ratio is then
calculated using the remaining pods not set aside or discarded from above.
-->
<p>在排除掉被搁置的 Pod 后，扩缩比例就会根据 <code>currentMetricValue/desiredMetricValue</code>
计算出来。</p>
<!--
If there were any missing metrics, we recompute the average more
conservatively, assuming those pods were consuming 100% of the desired
value in case of a scale down, and 0% in case of a scale up.  This dampens
the magnitude of any potential scale.
-->
<p>如果缺失任何的度量值，我们会更保守地重新计算平均值，
在需要缩小时假设这些 Pod 消耗了目标值的 100%，
在需要放大时假设这些 Pod 消耗了 0% 目标值。
这可以在一定程度上抑制扩缩的幅度。</p>
<!--
Furthermore, if any not-yet-ready pods were present, and we would have
scaled up without factoring in missing metrics or not-yet-ready pods, we
conservatively assume the not-yet-ready pods are consuming 0% of the
desired metric, further dampening the magnitude of a scale up.
-->
<p>此外，如果存在任何尚未就绪的 Pod，我们可以在不考虑遗漏指标或尚未就绪的 Pod 的情况下进行扩缩，
我们保守地假设尚未就绪的 Pod 消耗了期望指标的 0%，从而进一步降低了扩缩的幅度。</p>
<!--
After factoring in the not-yet-ready pods and missing metrics, we
recalculate the usage ratio.  If the new ratio reverses the scale
direction, or is within the tolerance, we skip scaling.  Otherwise, we use
the new ratio to scale.
-->
<p>在扩缩方向（缩小或放大）确定后，我们会把未就绪的 Pod 和缺少指标的 Pod 考虑进来再次计算使用率。
如果新的比率与扩缩方向相反，或者在容忍范围内，则跳过扩缩。
否则，我们使用新的扩缩比例。</p>
<!--
Note that the *original* value for the average utilization is reported
back via the HorizontalPodAutoscaler status, without factoring in the
not-yet-ready pods or missing metrics, even when the new usage ratio is
used.
-->
<p>注意，平均利用率的<em>原始</em>值会通过 HorizontalPodAutoscaler 的状态体现（
即使使用了新的使用率，也不考虑未就绪 Pod 和 缺少指标的 Pod)。</p>
<!--
If multiple metrics are specified in a HorizontalPodAutoscaler, this
calculation is done for each metric, and then the largest of the desired
replica counts is chosen. If any of these metrics cannot be converted
into a desired replica count (e.g. due to an error fetching the metrics
from the metrics APIs) and a scale down is suggested by the metrics which
can be fetched, scaling is skipped. This means that the HPA is still capable
of scaling up if one or more metrics give a `desiredReplicas` greater than
the current value.
-->
<p>如果创建 HorizontalPodAutoscaler 时指定了多个指标，
那么会按照每个指标分别计算扩缩副本数，取最大值进行扩缩。
如果任何一个指标无法顺利地计算出扩缩副本数（比如，通过 API 获取指标时出错），
并且可获取的指标建议缩容，那么本次扩缩会被跳过。
这表示，如果一个或多个指标给出的 <code>desiredReplicas</code> 值大于当前值，HPA 仍然能实现扩容。</p>
<!--
Finally, right before HPA scales the target, the scale recommendation is recorded.  The
controller considers all recommendations within a configurable window choosing the
highest recommendation from within that window. This value can be configured using the `--horizontal-pod-autoscaler-downscale-stabilization` flag, which defaults to 5 minutes.
This means that scaledowns will occur gradually, smoothing out the impact of rapidly
fluctuating metric values.
-->
<p>最后，在 HPA 控制器执行扩缩操作之前，会记录扩缩建议信息。
控制器会在操作时间窗口中考虑所有的建议信息，并从中选择得分最高的建议。
这个值可通过 <code>kube-controller-manager</code> 服务的启动参数
<code>--horizontal-pod-autoscaler-downscale-stabilization</code> 进行配置，
默认值为 5 分钟。
这个配置可以让系统更为平滑地进行缩容操作，从而消除短时间内指标值快速波动产生的影响。</p>
<!--
## API Object

The Horizontal Pod Autoscaler is an API resource in the Kubernetes `autoscaling` API group.
The current stable version, which only includes support for CPU autoscaling,
can be found in the `autoscaling/v1` API version.
-->
<h2 id="api-object">API 对象  </h2>
<p>HorizontalPodAutoscaler 是 Kubernetes <code>autoscaling</code> API 组的资源。
在当前稳定版本（<code>autoscaling/v1</code>）中只支持基于 CPU 指标的扩缩。</p>
<!--
The beta version, which includes support for scaling on memory and custom metrics,
can be found in `autoscaling/v2beta2`. The new fields introduced in `autoscaling/v2beta2`
are preserved as annotations when working with `autoscaling/v1`.
-->
<p>API 的 beta 版本（<code>autoscaling/v2beta2</code>）引入了基于内存和自定义指标的扩缩。
在 <code>autoscaling/v2beta2</code> 版本中新引入的字段在 <code>autoscaling/v1</code> 版本中以注解
的形式得以保留。</p>
<!--
When you create a HorizontalPodAutoscaler API object, make sure the name specified is a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
More details about the API object can be found at
[HorizontalPodAutoscaler Object](/docs/reference/generated/kubernetes-api/v1.22/#horizontalpodautoscaler-v1-autoscaling).
-->
<p>创建 HorizontalPodAutoscaler 对象时，需要确保所给的名称是一个合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#dns-subdomain-names">DNS 子域名</a>。
有关 API 对象的更多信息，请查阅
<a href="/docs/reference/generated/kubernetes-api/v1.22/#horizontalpodautoscaler-v1-autoscaling">HorizontalPodAutoscaler 对象设计文档</a>。</p>
<!--
## Support for Horizontal Pod Autoscaler in kubectl

Horizontal Pod Autoscaler, like every API resource, is supported in a standard way by `kubectl`.
We can create a new autoscaler using `kubectl create` command.
We can list autoscalers by `kubectl get hpa` and get detailed description by `kubectl describe hpa`.
Finally, we can delete an autoscaler using `kubectl delete hpa`.
-->
<h2 id="kubectl-对-horizontal-pod-autoscaler-的支持">kubectl 对 Horizontal Pod Autoscaler 的支持</h2>
<p>与其他 API 资源类似，<code>kubectl</code> 以标准方式支持 HPA。
我们可以通过 <code>kubectl create</code> 命令创建一个 HPA 对象，
通过 <code>kubectl get hpa</code> 命令来获取所有 HPA 对象，
通过 <code>kubectl describe hpa</code> 命令来查看 HPA 对象的详细信息。
最后，可以使用 <code>kubectl delete hpa</code> 命令删除对象。</p>
<!--
In addition, there is a special `kubectl autoscale` command for creating a HorizontalPodAutoscaler.
For instance, executing `kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80`
will create an autoscaler for replication set *foo*, with target CPU utilization set to `80%`
and the number of replicas between 2 and 5.
The detailed documentation of `kubectl autoscale` can be found [here](/docs/reference/generated/kubectl/kubectl-commands/#autoscale).
-->
<p>此外，还有个简便的命令 <code>kubectl autoscale</code> 来创建 HPA 对象。
例如，命令 <code>kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80</code> 将会为名
为 <em>foo</em> 的 ReplicationSet 创建一个 HPA 对象，
目标 CPU 使用率为 <code>80%</code>，副本数量配置为 2 到 5 之间。</p>
<!--
## Autoscaling during rolling update

Kubernetes lets you perform a rolling update on a Deployment. In that
case, the Deployment manages the underlying ReplicaSets for you.
When you configure autoscaling for a Deployment, you bind a
HorizontalPodAutoscaler to a single Deployment. The HorizontalPodAutoscaler
manages the `replicas` field of the Deployment. The deployment controller is responsible
for setting the `replicas` of the underlying ReplicaSets so that they add up to a suitable
number during the rollout and also afterwards.
-->
<h2 id="autoscaling-during-rolling-update">滚动升级时扩缩  </h2>
<p>Kubernetes 允许你在 Deployment 上执行滚动更新。在这种情况下，Deployment 为你管理下层的 ReplicaSet。
当你为一个 Deployment 配置自动扩缩时，你要为每个 Deployment 绑定一个 HorizontalPodAutoscaler。
HorizontalPodAutoscaler 管理 Deployment 的 <code>replicas</code> 字段。
Deployment Controller 负责设置下层 ReplicaSet 的 <code>replicas</code> 字段，
以便确保在上线及后续过程副本个数合适。</p>
<!--
If you perform a rolling update of a StatefulSet that has an autoscaled number of
replicas, the StatefulSet directly manages its set of Pods (there is no intermediate resource
similar to ReplicaSet).
-->
<p>如果你对一个副本个数被自动扩缩的 StatefulSet 执行滚动更新， 该 StatefulSet
会直接管理它的 Pod 集合 （不存在类似 ReplicaSet 这样的中间资源）。</p>
<!--
## Support for cooldown/delay

When managing the scale of a group of replicas using the Horizontal Pod Autoscaler,
it is possible that the number of replicas keeps fluctuating frequently due to the
dynamic nature of the metrics evaluated. This is sometimes referred to as *thrashing*.
-->
<h2 id="冷却-延迟支持">冷却/延迟支持</h2>
<p>当使用 Horizontal Pod Autoscaler 管理一组副本扩缩时，
有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为
<em>抖动（Thrashing）</em>。</p>
<!--
Starting from v1.6, a cluster operator can mitigate this problem by tuning
the global HPA settings exposed as flags for the `kube-controller-manager` component:
-->
<p>从 v1.6 版本起，集群操作员可以调节某些 <code>kube-controller-manager</code> 的全局参数来
缓解这个问题。</p>
<!--
Starting from v1.12, a new algorithmic update removes the need for the
upscale delay.
-->
<p>从 v1.12 开始，算法调整后，扩容操作时的延迟就不必设置了。</p>
<!--
- `--horizontal-pod-autoscaler-downscale-stabilization`: Specifies the duration of the
  downscale stabilization time window. Horizontal Pod Autoscaler remembers
  this historical recommended sizes and only acts on the largest size within this time window.
  The default value is 5 minutes (`5m0s`).
-->
<ul>
<li><code>--horizontal-pod-autoscaler-downscale-stabilization</code>: 设置缩容冷却时间窗口长度。
水平 Pod
扩缩器能够记住过去建议的负载规模，并仅对此时间窗口内的最大规模执行操作。
默认值是 5 分钟（<code>5m0s</code>）。</li>
</ul>
<!--
When tuning these parameter values, a cluster operator should be aware of the possible
consequences. If the delay (cooldown) value is set too long, there could be complaints
that the Horizontal Pod Autoscaler is not responsive to workload changes. However, if
the delay value is set too short, the scale of the replicas set may keep thrashing as
usual.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 当调整这些参数时，集群操作员需要明白其可能的影响。
如果延迟（冷却）时间设置的太长，Horizontal Pod Autoscaler 可能会不能很好的改变负载。
如果延迟（冷却）时间设置的太短，那么副本数量有可能跟以前一样出现抖动。</div>
</blockquote>
<!--
## Support for resource metrics

Any HPA target can be scaled based on the resource usage of the pods in the scaling target.
When defining the pod specification the resource requests like `cpu` and `memory` should
be specified. This is used to determine the resource utilization and used by the HPA controller
to scale the target up or down. To use resource utilization based scaling specify a metric source
like this:
-->
<h2 id="support-for-resource-metrics">对资源指标的支持  </h2>
<p>HPA 的任何目标资源都可以基于其中的 Pods 的资源用量来实现扩缩。
在定义 Pod 规约时，类似 <code>cpu</code> 和 <code>memory</code> 这类资源请求必须被设定。
这些设定值被用来确定资源利用量并被 HPA 控制器用来对目标资源完成扩缩操作。
要使用基于资源利用率的扩缩，可以像下面这样指定一个指标源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Resource<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Utilization<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></code></pre></div><!--
With this metric the HPA controller will keep the average utilization of the pods in the scaling
target at 60%. Utilization is the ratio between the current usage of resource to the requested
resources of the pod. See [Algorithm](#algorithm-details) for more details about how the utilization
is calculated and averaged.
-->
<p>基于这一指标设定，HPA 控制器会维持扩缩目标中的 Pods 的平均资源利用率在 60%。
利用率是 Pod 的当前资源用量与其请求值之间的比值。关于如何计算利用率以及如何计算平均值
的细节可参考<a href="#algorithm-details">算法</a>小节。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
Since the resource usages of all the containers are summed up the total pod utilization may not
accurately represent the individual container resource usage. This could lead to situations where
a single container might be running with high usage and the HPA will not scale out because the overall
pod usage is still within acceptable limits.
-->
<p>由于所有的容器的资源用量都会被累加起来，Pod 的总体资源用量值可能不会精确体现
各个容器的资源用量。这一现象也会导致一些问题，例如某个容器运行时的资源用量非常
高，但因为 Pod 层面的资源用量总值让人在可接受的约束范围内，HPA 不会执行扩大
目标对象规模的操作。</div>
</blockquote>
<!--
### Container Resource Metrics
-->
<h3 id="container-resource-metrics">容器资源指标  </h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>

<!--
`HorizontalPodAutoscaler` also supports a container metric source where the HPA can track the
resource usage of individual containers across a set of Pods, in order to scale the target resource.
This lets you configure scaling thresholds for the containers that matter most in a particular Pod.
For example, if you have a web application and a logging sidecar, you can scale based on the resource
use of the web application, ignoring the sidecar container and its resource use.
-->
<p><code>HorizontalPodAutoscaler</code> 也支持容器指标源，这时 HPA 可以跟踪记录一组 Pods 中各个容器的
资源用量，进而触发扩缩目标对象的操作。
容器资源指标的支持使得你可以为特定 Pod 中最重要的容器配置规模缩放阈值。
例如，如果你有一个 Web 应用和一个执行日志操作的边车容器，你可以基于 Web 应用的
资源用量来执行扩缩，忽略边车容器的存在及其资源用量。</p>
<!--
If you revise the target resource to have a new Pod specification with a different set of containers,
you should revise the HPA spec if that newly added container should also be used for
scaling. If the specified container in the metric source is not present or only present in a subset
of the pods then those pods are ignored and the recommendation is recalculated. See [Algorithm](#algorithm-details)
for more details about the calculation. To use container resources for autoscaling define a metric
source as follows:
-->
<p>如果你更改缩放目标对象，令其使用新的、包含一组不同的容器的 Pod 规约，你就需要
修改 HPA 的规约才能基于新添加的容器来执行规模扩缩操作。
如果指标源中指定的容器不存在或者仅存在于部分 Pods 中，那么这些 Pods 会被忽略，
HPA 会重新计算资源用量值。参阅<a href="#algorithm-details">算法</a>小节进一步了解计算细节。
要使用容器资源用量来完成自动扩缩，可以像下面这样定义指标源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>ContainerResource<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">containerResource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">container</span>:<span style="color:#bbb"> </span>application<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Utilization<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></code></pre></div><!--
In the above example the HPA controller scales the target such that the average utilization of the cpu
in the `application` container of all the pods is 60%.
-->
<p>在上面的例子中，HPA 控制器会对目标对象执行扩缩操作以确保所有 Pods 中
<code>application</code> 容器的平均 CPU 用量为 60%。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
If you change the name of a container that a HorizontalPodAutoscaler is tracking, you can
make that change in a specific order to ensure scaling remains available and effective
whilst the change is being applied. Before you update the resource that defines the container
(such as a Deployment), you should update the associated HPA to track both the new and
old container names. This way, the HPA is able to calculate a scaling recommendation
throughout the update process.
-->
<p>如果你要更改 HorizontalPodAutoscaler 所跟踪记录的容器的名称，你可以按一定顺序
来执行这一更改，确保在应用更改的过程中用来判定扩缩行为的容器可用。
在更新定义容器的资源（如 Deployment）之前，你需要更新相关的 HPA，使之能够同时
跟踪记录新的和老的容器名称。这样，HPA 就能够在整个更新过程中继续计算并提供扩缩操作建议。</p>
<!--
Once you have rolled out the container name change to the workload resource, tidy up by removing
the old container name from the HPA specification.
-->
<p>一旦你已经将容器名称变更这一操作应用到整个负载对象至上，就可以从 HPA
的规约中去掉老的容器名称，完成清理操作。</p>
</div>
</blockquote>
<!--
## Support for multiple metrics

Kubernetes 1.6 adds support for scaling based on multiple metrics. You can use the `autoscaling/v2beta2` API
version to specify multiple metrics for the Horizontal Pod Autoscaler to scale on. Then, the Horizontal Pod
Autoscaler controller will evaluate each metric, and propose a new scale based on that metric. The largest of the
proposed scales will be used as the new scale.
-->
<h2 id="support-for-multiple-metrics">多指标支持  </h2>
<p>Kubernetes 1.6 开始支持基于多个度量值进行扩缩。
你可以使用 <code>autoscaling/v2beta2</code> API 来为 Horizontal Pod Autoscaler 指定多个指标。
Horizontal Pod Autoscaler 会根据每个指标计算，并生成一个扩缩建议。
幅度最大的扩缩建议会被采纳。</p>
<!--
## Support for custom metrics

Kubernetes 1.2 added alpha support for scaling based on application-specific metrics using special annotations.
Support for these annotations was removed in Kubernetes 1.6 in favor of the new autoscaling API.  While the old method for collecting
custom metrics is still available, these metrics will not be available for use by the Horizontal Pod Autoscaler, and the former
annotations for specifying which custom metrics to scale on are no longer honored by the Horizontal Pod Autoscaler controller.
-->
<h2 id="support-for-custom-metrics">自定义指标支持  </h2>
<blockquote class="note callout">
  <div><strong>说明：</strong> 在 Kubernetes 1.2 增加了支持基于使用特殊注解表达的、特定于具体应用的扩缩能力，
此能力处于 Alpha 阶段。
从 Kubernetes 1.6 起，由于新的 autoscaling API 的引入，这些 annotation 就被废弃了。
虽然收集自定义指标的旧方法仍然可用，Horizontal Pod Autoscaler 调度器将不会再使用这些度量值。
同时，Horizontal Pod Autoscaler 也不再使用之前用于指定用户自定义指标的注解。</div>
</blockquote>
<!--
Kubernetes 1.6 adds support for making use of custom metrics in the Horizontal Pod Autoscaler.
You can add custom metrics for the Horizontal Pod Autoscaler to use in the `autoscaling/v2beta2` API.
Kubernetes then queries the new custom metrics API to fetch the values of the appropriate custom metrics.
-->
<p>自 Kubernetes 1.6 起，Horizontal Pod Autoscaler 支持使用自定义指标。
你可以使用 <code>autoscaling/v2beta2</code> API 为 Horizontal Pod Autoscaler 指定用户自定义指标。
Kubernetes 会通过用户自定义指标 API 来获取相应的指标。</p>
<!--
See [Support for metrics APIs](#support-for-metrics-apis) for the requirements.
-->
<p>关于指标 API 的要求，请参阅<a href="#support-for-metrics-apis">对 Metrics API 的支持</a>。</p>
<!--
## Support for metrics APIs

By default, the HorizontalPodAutoscaler controller retrieves metrics from a series of APIs.  In order for it to access these
APIs, cluster administrators must ensure that:
-->
<h2 id="support-for-metrics-apis">对 Metrics API 的支持  </h2>
<p>默认情况下，HorizontalPodAutoscaler 控制器会从一系列的 API 中检索度量值。
集群管理员需要确保下述条件，以保证 HPA 控制器能够访问这些 API：</p>
<!--
* The [API aggregation layer](/docs/tasks/extend-kubernetes/configure-aggregation-layer/) is enabled.

* The corresponding APIs are registered:

   * For resource metrics, this is the `metrics.k8s.io` API, generally provided by [metrics-server](https://github.com/kubernetes-sigs/metrics-server).
     It can be launched as a cluster addon.

   * For custom metrics, this is the `custom.metrics.k8s.io` API.  It's provided by "adapter" API servers provided by metrics solution vendors.
     Check with your metrics pipeline, or the [list of known solutions](https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api).
     If you would like to write your own, check out the [boilerplate](https://github.com/kubernetes-sigs/custom-metrics-apiserver) to get started.

   * For external metrics, this is the `external.metrics.k8s.io` API.  It may be provided by the custom metrics adapters provided above.

* The `--horizontal-pod-autoscaler-use-rest-clients` is `true` or unset.  Setting this to false switches to Heapster-based autoscaling, which is deprecated.
-->
<ul>
<li>
<p>启用了 <a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">API 聚合层</a></p>
</li>
<li>
<p>相应的 API 已注册：</p>
<ul>
<li>
<p>对于资源指标，将使用 <code>metrics.k8s.io</code> API，一般由 <a href="https://github.com/kubernetes-incubator/metrics-server">metrics-server</a> 提供。
它可以作为集群插件启动。</p>
</li>
<li>
<p>对于自定义指标，将使用 <code>custom.metrics.k8s.io</code> API。
它由其他度量指标方案厂商的“适配器（Adapter）” API 服务器提供。
确认你的指标流水线，或者查看<a href="https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api">已知方案列表</a>。
如果你想自己编写，请从 <a href="https://github.com/kubernetes-sigs/custommetrics-apiserver">boilerplate</a>开始。</p>
</li>
<li>
<p>对于外部指标，将使用 <code>external.metrics.k8s.io</code> API。可能由上面的自定义指标适配器提供。</p>
</li>
</ul>
</li>
<li>
<p><code>--horizontal-pod-autoscaler-use-rest-clients</code> 参数设置为 <code>true</code> 或者不设置。
如果设置为 false，则会切换到基于 Heapster 的自动扩缩，这个特性已经被弃用了。</p>
</li>
</ul>
<!--  
For more information on these different metrics paths and how they differ please see the relevant design proposals for
[the HPA V2](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/hpa-v2.md),
[custom.metrics.k8s.io](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/custom-metrics-api.md)
and [external.metrics.k8s.io](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/external-metrics-api.md).
-->
<p>关于指标来源以及其区别的更多信息，请参阅相关的设计文档，
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/hpa-v2.md">the HPA V2</a>、
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/custom-metrics-api.md">custom.metrics.k8s.io</a> 和
<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/external-metrics-api.md">external.metrics.k8s.io</a>。</p>
<!--
For examples of how to use them see [the walkthrough for using custom metrics](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics)
and [the walkthrough for using external metrics](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects).
-->
<p>关于如何使用它们的示例，请参考
<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">使用自定义指标的教程</a>
和<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects">使用外部指标的教程</a>。</p>
<!--  
## Support for configurable scaling behavior

Starting from
[v1.18](https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md)
the `v2beta2` API allows scaling behavior to be configured through the HPA
`behavior` field. Behaviors are specified separately for scaling up and down in
`scaleUp` or `scaleDown` section under the `behavior` field. A stabilization
window can be specified for both directions which prevents the flapping of the
number of the replicas in the scaling target. Similarly specifying scaling
policies controls the rate of change of replicas while scaling.
-->
<h2 id="support-for-configurable-scaling-behaviour">支持可配置的扩缩</h2>
<p>从 <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md">v1.18</a>
开始，<code>v2beta2</code> API 允许通过 HPA 的 <code>behavior</code> 字段配置扩缩行为。
在 <code>behavior</code> 字段中的 <code>scaleUp</code> 和 <code>scaleDown</code> 分别指定扩容和缩容行为。
可以两个方向指定一个稳定窗口，以防止扩缩目标中副本数量的波动。
类似地，指定扩缩策略可以控制扩缩时副本数的变化率。</p>
<!--  
### Scaling Policies

One or more scaling policies can be specified in the `behavior` section of the spec.
When multiple policies are specified the policy which allows the highest amount of
change is the policy which is selected by default. The following example shows this behavior
while scaling down:
-->
<h3 id="scaling-policies">扩缩策略</h3>
<p>在 spec 字段的 <code>behavior</code> 部分可以指定一个或多个扩缩策略。
当指定多个策略时，默认选择允许更改最多的策略。
下面的例子展示了缩容时的行为:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">behavior</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleDown</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">policies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></code></pre></div><!--  
`periodSeconds` indicates the length of time in the past for which the policy must hold true.
The first policy _(Pods)_ allows at most 4 replicas to be scaled down in one minute. The second policy
_(Percent)_ allows at most 10% of the current replicas to be scaled down in one minute.

Since by default the policy which allows the highest amount of change is selected, the second policy will
only be used when the number of pod replicas is more than 40. With 40 or less replicas, the first policy will be applied.
For instance if there are 80 replicas and the target has to be scaled down to 10 replicas
then during the first step 8 replicas will be reduced. In the next iteration when the number
of replicas is 72, 10% of the pods is 7.2 but the number is rounded up to 8. On each loop of
the autoscaler controller the number of pods to be change is re-calculated based on the number
of current replicas. When the number of replicas falls below 40 the first policy _(Pods)_ is applied
and 4 replicas will be reduced at a time.
-->
<p><code>periodSeconds</code> 表示在过去的多长时间内要求策略值为真。
第一个策略（Pods）允许在一分钟内最多缩容 4 个副本。第二个策略（Percent）
允许在一分钟内最多缩容当前副本个数的百分之十。</p>
<p>由于默认情况下会选择容许更大程度作出变更的策略，只有 Pod 副本数大于 40 时，
第二个策略才会被采用。如果副本数为 40 或者更少，则应用第一个策略。
例如，如果有 80 个副本，并且目标必须缩小到 10 个副本，那么在第一步中将减少 8 个副本。
在下一轮迭代中，当副本的数量为 72 时，10% 的 Pod 数为 7.2，但是这个数字向上取整为 8。
在 autoscaler 控制器的每个循环中，将根据当前副本的数量重新计算要更改的 Pod 数量。
当副本数量低于 40 时，应用第一个策略（Pods），一次减少 4 个副本。</p>
<!--  
The policy selection can be changed by specifying the `selectPolicy` field for a scaling
direction. By setting the value to `Min` which would select the policy which allows the
smallest change in the replica count. Setting the value to `Disabled` completely disables
scaling in that direction.
-->
<p>可以指定扩缩方向的 <code>selectPolicy</code> 字段来更改策略选择。
通过设置 <code>Min</code> 的值，它将选择副本数变化最小的策略。
将该值设置为 <code>Disabled</code> 将完全禁用该方向的缩放。</p>
<!--  
### Stabilization Window

The stabilization window is used to restrict the flapping of replicas when the metrics
used for scaling keep fluctuating. The stabilization window is used by the autoscaling
algorithm to consider the computed desired state from the past to prevent scaling. In
the following example the stabilization window is specified for `scaleDown`.
-->
<h3 id="stabilization-window">稳定窗口</h3>
<p>当用于扩缩的指标持续抖动时，使用稳定窗口来限制副本数上下振动。
自动扩缩算法使用稳定窗口来考虑过去计算的期望状态，以防止扩缩。
在下面的例子中，稳定化窗口被指定为 <code>scaleDown</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">scaleDown</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">300</span><span style="color:#bbb">
</span></code></pre></div><!--  
When the metrics indicate that the target should be scaled down the algorithm looks
into previously computed desired states and uses the highest value from the specified
interval. In above example all desired states from the past 5 minutes will be considered.
-->
<p>当指标显示目标应该缩容时，自动扩缩算法查看之前计算的期望状态，并使用指定时间间隔内的最大值。
在上面的例子中，过去 5 分钟的所有期望状态都会被考虑。</p>
<!--  
### Default Behavior

To use the custom scaling not all fields have to be specified. Only values which need to be
customized can be specified. These custom values are merged with default values. The default values
match the existing behavior in the HPA algorithm.
-->
<h3 id="default-behavior">默认行为</h3>
<p>要使用自定义扩缩，不必指定所有字段。
只有需要自定义的字段才需要指定。
这些自定义值与默认值合并。
默认值与 HPA 算法中的现有行为匹配。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">behavior</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleDown</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">300</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">policies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleUp</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">policies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">4</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">selectPolicy</span>:<span style="color:#bbb"> </span>Max<span style="color:#bbb">
</span></code></pre></div><!--  
For scaling down the stabilization window is _300_ seconds (or the value of the
`--horizontal-pod-autoscaler-downscale-stabilization` flag if provided). There is only a single policy
for scaling down which allows a 100% of the currently running replicas to be removed which
means the scaling target can be scaled down to the minimum allowed replicas.
For scaling up there is no stabilization window. When the metrics indicate that the target should be
scaled up the target is scaled up immediately. There are 2 policies where 4 pods or a 100% of the currently
running replicas will be added every 15 seconds till the HPA reaches its steady state.
-->
<p>用于缩小稳定窗口的时间为 <em>300</em>  秒(或是 <code>--horizontal-pod-autoscaler-downscale-stabilization</code>
参数设定值)。
只有一种缩容的策略，允许 100% 删除当前运行的副本，这意味着扩缩目标可以缩小到允许的最小副本数。
对于扩容，没有稳定窗口。当指标显示目标应该扩容时，目标会立即扩容。
这里有两种策略，每 15 秒添加 4 个 Pod 或 100% 当前运行的副本数，直到 HPA 达到稳定状态。</p>
<!--  
### Example: change downscale stabilization window

To provide a custom downscale stabilization window of 1 minute, the following
behavior would be added to the HPA:
--> 
<h3 id="示例-更改缩容稳定窗口">示例：更改缩容稳定窗口</h3>
<p>将下面的 behavior 配置添加到 HPA 中，可提供一个 1 分钟的自定义缩容稳定窗口：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">behavior</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleDown</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">stabilizationWindowSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></code></pre></div><!--  
### Example: limit scale down rate

To limit the rate at which pods are removed by the HPA to 10% per minute, the
following behavior would be added to the HPA:
-->
<h3 id="示例-限制缩容速率">示例：限制缩容速率</h3>
<p>将下面的 behavior 配置添加到 HPA 中，可限制 Pod 被 HPA 删除速率为每分钟 10%：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">behavior</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleDown</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">policies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span></code></pre></div><!--  
To ensure that no more than 5 Pods are removed per minute, you can add a second scale-down
policy with a fixed size of 5, and set `selectPolicy` to minimum. Setting `selectPolicy` to `Min` means
that the autoscaler chooses the policy that affects the smallest number of Pods:
-->
<p>为了确保每分钟删除的 Pod 数不超过 5 个，可以添加第二个缩容策略，大小固定为 5，
并将 <code>selectPolicy</code> 设置为最小值。
将 <code>selectPolicy</code> 设置为 <code>Min</code> 意味着 autoscaler 会选择影响 Pod 数量最小的策略:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">behavior</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleDown</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">policies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Percent<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">periodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">selectPolicy</span>:<span style="color:#bbb"> </span>Min<span style="color:#bbb">
</span></code></pre></div><!--  
### Example: disable scale down

The `selectPolicy` value of `Disabled` turns off scaling the given direction.
So to prevent downscaling the following policy would be used:
-->
<h3 id="示例-禁用缩容">示例：禁用缩容</h3>
<p><code>selectPolicy</code> 的值 <code>Disabled</code> 会关闭对给定方向的缩容。
因此使用以下策略，将会阻止缩容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">behavior</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleDown</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">selectPolicy</span>:<span style="color:#bbb"> </span>Disabled<span style="color:#bbb">
</span></code></pre></div><!--
## Implicit maintenance-mode deactivation

You can implicitly deactivate the HPA for a target without the
need to change the HPA configuration itself. If the target's desired replica count
is set to 0, and the HPA's minimum replica count is greater than 0, the HPA 
stops adjusting the target (and sets the `ScalingActive` Condition on itself
to `false`) until you reactivate it by manually adjusting the target's desired
replica count or HPA's minimum replica count.
-->
<h2 id="隐式维护状态禁用">隐式维护状态禁用</h2>
<p>你可以在不必更改 HPA 配置的情况下隐式地为某个目标禁用 HPA。
如果此目标的期望副本个数被设置为 0，而 HPA 的最小副本个数大于 0，
则 HPA 会停止调整目标（并将其自身的 <code>ScalingActive</code> 状况设置为 <code>false</code>），
直到你通过手动调整目标的期望副本个数或 HPA 的最小副本个数来重新激活。</p>
<h2 id="接下来">接下来</h2>
<!--
* Design documentation: [Horizontal Pod Autoscaling](https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md).
* kubectl autoscale command: [kubectl autoscale](/docs/reference/generated/kubectl/kubectl-commands/#autoscale).
* Usage example of [Horizontal Pod Autoscaler](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/).
-->
<ul>
<li>设计文档：<a href="https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md">Horizontal Pod Autoscaling</a></li>
<li><code>kubectl autoscale</code> 命令：<a href="/docs/reference/generated/kubectl/kubectl-commands/#autoscale">kubectl autoscale</a>.</li>
<li>使用示例：<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Horizontal Pod Autoscaler</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8138226ce9660ac8e3e82ff86fff8ad2">7.7 - Horizontal Pod Autoscaler 演练</h1>
    
	<!--
reviewers:
- fgrzadkowski
- jszczepkowski
- justinsb
- directxman12
title: Horizontal Pod Autoscaler Walkthrough
content_type: task
weight: 100
-->
<!-- overview -->
<!--
Horizontal Pod Autoscaler automatically scales the number of pods
in a replication controller, deployment or replica set or statefulset based on observed CPU utilization
(or, with beta support, on some other, application-provided metrics).
-->
<p>Horizontal Pod Autoscaler 可以根据 CPU 利用率自动扩缩 ReplicationController、
Deployment、ReplicaSet 或 StatefulSet 中的 Pod 数量
（也可以基于其他应用程序提供的度量指标，目前这一功能处于 beta 版本）。</p>
<!--
This document walks you through an example of enabling Horizontal Pod Autoscaler for the php-apache server.
For more information on how Horizontal Pod Autoscaler behaves, see the
[Horizontal Pod Autoscaler user guide](/docs/tasks/run-application/horizontal-pod-autoscale/).
-->
<p>本文将引领你了解如何为 php-apache 服务器配置和使用 Horizontal Pod Autoscaler。
与 Horizontal Pod Autoscaler 相关的更多信息请参阅
<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler 用户指南</a>。</p>
<h2 id="准备开始">准备开始</h2>
<!--
This example requires a running Kubernetes cluster and kubectl, version 1.2 or later.
[metrics-server](https://github.com/kubernetes-incubator/metrics-server/) monitoring needs to be deployed
in the cluster to provide metrics through the [Metrics API](https://github.com/kubernetes/metrics).
Horizontal Pod Autoscaler uses this API to collect metrics. To learn how to deploy the metrics-server,
see the [metrics-server documentation](https://github.com/kubernetes-sigs/metrics-server#deployment).
-->
<p>本文示例需要一个运行中的 Kubernetes 集群以及 kubectl，版本为 1.2 或更高。
<a href="https://github.com/kubernetes-incubator/metrics-server/">Metrics 服务器</a>
需要被部署到集群中，以便通过 <a href="https://github.com/kubernetes/metrics">Metrics API</a>
提供度量数据。
Horizontal Pod Autoscaler 根据此 API 来获取度量数据。
要了解如何部署 metrics-server，请参考
<a href="https://github.com/kubernetes-incubator/metrics-server/">metrics-server 文档</a> 。</p>
<!--
To specify multiple resource metrics for a Horizontal Pod Autoscaler, you must have a
Kubernetes cluster and kubectl at version 1.6 or later. To make use of custom metrics, your cluster
must be able to communicate with the API server providing the custom metrics API.
Finally, to use metrics not related to any Kubernetes object you must have a
Kubernetes cluster at version 1.10 or later, and you must be able to communicate with
the API server that provides the external metrics API.
See the [Horizontal Pod Autoscaler user guide](/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics) for more details.
-->
<p>如果需要为 Horizontal Pod Autoscaler 指定多种资源度量指标，你的 Kubernetes
集群以及 kubectl 至少需要达到 1.6 版本。
此外，如果要使用自定义度量指标，你的 Kubernetes 集群还必须能够与提供这些自定义指标
的 API 服务器通信。
最后，如果要使用与 Kubernetes 对象无关的度量指标，则 Kubernetes 集群版本至少需要
达到 1.10 版本，同样，需要保证集群能够与提供这些外部指标的 API 服务器通信。
更多详细信息，请参阅
<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics">Horizontal Pod Autoscaler 用户指南</a>。</p>
<!-- steps -->
<!--
## Run & expose php-apache server
-->
<h2 id="运行-php-apache-服务器并暴露服务">运行 php-apache 服务器并暴露服务</h2>
<!--
To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image.
The Dockerfile has the following content:
-->
<p>为了演示 Horizontal Pod Autoscaler，我们将使用一个基于 php-apache 镜像的
定制 Docker 镜像。Dockerfile 内容如下：</p>
<pre tabindex="0"><code>FROM php:5-apache
COPY index.php /var/www/html/index.php
RUN chmod a+rx index.php
</code></pre><!--
It defines an index.php page which performs some CPU intensive computations:
-->
<p>该文件定义了一个 index.php 页面来执行一些 CPU 密集型计算：</p>
<pre tabindex="0"><code>&lt;?php
  $x = 0.0001;
  for ($i = 0; $i &lt;= 1000000; $i++) {
    $x += sqrt($x);
  }
  echo &quot;OK!&quot;;
?&gt;
</code></pre><!--
First, we will start a deployment running the image and expose it as a service
using the following configuration:
-->
<p>首先，我们使用下面的配置启动一个 Deployment 来运行这个镜像并暴露一个服务：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/php-apache.yaml" download="application/php-apache.yaml"><code>application/php-apache.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-php-apache-yaml')" title="Copy application/php-apache.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-php-apache-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/hpa-example<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>200m<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Run the following command:
-->
<p>运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/php-apache.yaml
</code></pre></div><pre tabindex="0"><code>deployment.apps/php-apache created
service/php-apache created
</code></pre><!--
## Create Horizontal Pod Autoscaler

Now that the server is running, we will create the autoscaler using
[kubectl autoscale](/docs/reference/generated/kubectl/kubectl-commands#autoscale).
The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods
controlled by the php-apache deployment we created in the first step of these instructions.
Roughly speaking, HPA will increase and decrease the number of replicas
(via the deployment) to maintain an average CPU utilization across all Pods of 50%
(since each pod requests 200 milli-cores by `kubectl run`), this means average CPU usage of 100 milli-cores).
See [here](/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details) for more details on the algorithm.
-->
<h2 id="create-horizontal-pod-autoscaler">创建 Horizontal Pod Autoscaler </h2>
<p>现在，php-apache 服务器已经运行，我们将通过
<a href="/docs/reference/generated/kubectl/kubectl-commands#autoscale">kubectl autoscale</a>
命令创建 Horizontal Pod Autoscaler。
以下命令将创建一个 Horizontal Pod Autoscaler 用于控制我们上一步骤中创建的
Deployment，使 Pod 的副本数量维持在 1 到 10 之间。
大致来说，HPA 将（通过 Deployment）增加或者减少 Pod 副本的数量以保持所有 Pod
的平均 CPU 利用率在 50% 左右（由于每个 Pod 请求 200 毫核的 CPU，这意味着平均
CPU 用量为 100 毫核）。
算法的详情请参阅<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details">相关文档</a>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl autoscale deployment php-apache --cpu-percent<span style="color:#666">=</span><span style="color:#666">50</span> --min<span style="color:#666">=</span><span style="color:#666">1</span> --max<span style="color:#666">=</span><span style="color:#666">10</span>
</code></pre></div><pre tabindex="0"><code>horizontalpodautoscaler.autoscaling/php-apache autoscaled
</code></pre><!--
We may check the current status of autoscaler by running:
-->
<p>我们可以通过以下命令查看 Autoscaler 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get hpa
</code></pre></div><pre tabindex="0"><code>NAME         REFERENCE                     TARGET    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%  1         10        1          18s

</code></pre><!--
Please note that the current CPU consumption is 0% as we are not sending any requests to the server
(the ``CURRENT`` column shows the average across all the pods controlled by the corresponding deployment).
-->
<p>请注意当前的 CPU 利用率是 0%，这是由于我们尚未发送任何请求到服务器
（<code>CURRENT</code> 列显示了相应 Deployment 所控制的所有 Pod 的平均 CPU 利用率）。</p>
<!--
## Increase load

Now, we will see how the autoscaler reacts to increased load.
We will start a container, and send an infinite loop of queries to the php-apache service (please run it in a different terminal):
-->
<h2 id="increase-load">增加负载 </h2>
<p>现在，我们将看到 Autoscaler 如何对增加负载作出反应。
我们将启动一个容器，并通过一个循环向 php-apache 服务器发送无限的查询请求
（请在另一个终端中运行以下命令）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run -i --tty load-generator --rm --image<span style="color:#666">=</span>busybox --restart<span style="color:#666">=</span>Never -- /bin/sh -c <span style="color:#b44">&#34;while sleep 0.01; do wget -q -O- http://php-apache; done&#34;</span>
</code></pre></div><!--
Within a minute or so, we should see the higher CPU load by executing:
-->
<p>一分钟时间左右之后，通过以下命令，我们可以看到 CPU 负载升高了：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get hpa
</code></pre></div><pre tabindex="0"><code>NAME         REFERENCE                     TARGET      MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   305% / 50%  1         10        1          3m
</code></pre><!--
Here, CPU consumption has increased to 305% of the request.
As a result, the deployment was resized to 7 replicas:
-->
<p>这时，由于请求增多，CPU 利用率已经升至请求值的 305%。
可以看到，Deployment 的副本数量已经增长到了 7：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment php-apache
</code></pre></div><pre tabindex="0"><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   7/7      7           7           19m
</code></pre><!--
It may take a few minutes to stabilize the number of replicas. Since the amount
of load is not controlled in any way it may happen that the final number of replicas
will differ from this example.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 有时最终副本的数量可能需要几分钟才能稳定下来。由于环境的差异，
不同环境中最终的副本数量可能与本示例中的数量不同。</div>
</blockquote>
<!--
## Stop load

We will finish our example by stopping the user load.

In the terminal where we created the container with `busybox` image, terminate
the load generation by typing `<Ctrl> + C`.

Then we will verify the result state (after a minute or so):
-->
<h2 id="停止负载">停止负载</h2>
<p>我们将通过停止负载来结束我们的示例。</p>
<p>在我们创建 busybox 容器的终端中，输入<code>&lt;Ctrl&gt; + C</code> 来终止负载的产生。</p>
<p>然后我们可以再次检查负载状态（等待几分钟时间）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get hpa
</code></pre></div><pre tabindex="0"><code>NAME         REFERENCE                     TARGET       MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%     1         10        1          11m
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment php-apache
</code></pre></div><pre tabindex="0"><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
php-apache   1/1     1            1           27m
</code></pre><!--
Here CPU utilization dropped to 0, and so HPA autoscaled the number of replicas back down to 1.
-->
<p>这时，CPU 利用率已经降到 0，所以 HPA 将自动缩减副本数量至 1。</p>
<!--
Autoscaling the replicas may take a few minutes.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 自动扩缩完成副本数量的改变可能需要几分钟的时间。</div>
</blockquote>
<!-- discussion -->
<!--
## Autoscaling on multiple metrics and custom metrics

You can introduce additional metrics to use when autoscaling the `php-apache` Deployment
by making use of the `autoscaling/v2beta2` API version.
-->
<h2 id="autoscaling-on-multiple-metrics-and-custom-metrics">基于多项度量指标和自定义度量指标自动扩缩</h2>
<p>利用 <code>autoscaling/v2beta2</code> API 版本，你可以在自动扩缩 php-apache 这个
Deployment 时使用其他度量指标。</p>
<!--
First, get the YAML of your HorizontalPodAutoscaler in the `autoscaling/v2beta2` form:
-->
<p>首先，将 HorizontalPodAutoscaler 的 YAML 文件改为 <code>autoscaling/v2beta2</code> 格式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get hpa php-apache -o yaml &gt; /tmp/hpa-v2.yaml
</code></pre></div><!--
Open the `/tmp/hpa-v2.yaml` file in an editor, and you should see YAML which looks like this:
-->
<p>在编辑器中打开 <code>/tmp/hpa-v2.yaml</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>autoscaling/v2beta2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>HorizontalPodAutoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleTargetRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metrics</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Resource<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Utilization<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">observedGeneration</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">lastScaleTime</span>:<span style="color:#bbb"> </span>&lt;some-time&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">currentReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">desiredReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">currentMetrics</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Resource<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">current</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">averageValue</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span></code></pre></div><!--
Notice that the `targetCPUUtilizationPercentage` field has been replaced with an array called `metrics`.
The CPU utilization metric is a *resource metric*, since it is represented as a percentage of a resource
specified on pod containers.  Notice that you can specify other resource metrics besides CPU.  By default,
the only other supported resource metric is memory.  These resources do not change names from cluster
to cluster, and should always be available, as long as the `metrics.k8s.io` API is available.
-->
<p>需要注意的是，<code>targetCPUUtilizationPercentage</code> 字段已经被名为 <code>metrics</code> 的数组所取代。
CPU 利用率这个度量指标是一个 <em>resource metric</em>（资源度量指标），因为它表示容器上指定资源的百分比。
除 CPU 外，你还可以指定其他资源度量指标。默认情况下，目前唯一支持的其他资源度量指标为内存。
只要 <code>metrics.k8s.io</code> API 存在，这些资源度量指标就是可用的，并且他们不会在不同的 Kubernetes 集群中改变名称。</p>
<!--
You can also specify resource metrics in terms of direct values, instead of as percentages of the
requested value, by using a `target.type` of `AverageValue` instead of `Utilization`, and
setting the corresponding `target.averageValue` field instead of the `target.averageUtilization`.
-->
<p>你还可以指定资源度量指标使用绝对数值，而不是百分比，你需要将 <code>target.type</code> 从
<code>Utilization</code> 替换成 <code>AverageValue</code>，同时设置 <code>target.averageValue</code>
而非 <code>target.averageUtilization</code> 的值。</p>
<!--
There are two other types of metrics, both of which are considered *custom metrics*: pod metrics and
object metrics.  These metrics may have names which are cluster specific, and require a more
advanced cluster monitoring setup.
-->
<p>还有两种其他类型的度量指标，他们被认为是 <em>custom metrics</em>（自定义度量指标）：
即 Pod 度量指标和 Object 度量指标。
这些度量指标可能具有特定于集群的名称，并且需要更高级的集群监控设置。</p>
<!--
The first of these alternative metric types is *pod metrics*.  These metrics describe pods, and
are averaged together across pods and compared with a target value to determine the replica count.
They work much like resource metrics, except that they *only* support a `target` type of `AverageValue`.
-->
<p>第一种可选的度量指标类型是 Pod 度量指标。这些指标从某一方面描述了 Pod，
在不同 Pod 之间进行平均，并通过与一个目标值比对来确定副本的数量。
它们的工作方式与资源度量指标非常相像，只是它们仅支持 <code>target</code> 类型为 <code>AverageValue</code>。</p>
<!--
Pod metrics are specified using a metric block like this:
-->
<p>pod 度量指标通过如下代码块定义：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metric</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>packets-per-second<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>AverageValue<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">averageValue</span>:<span style="color:#bbb"> </span>1k<span style="color:#bbb">
</span></code></pre></div><!--
The second alternative metric type is *object metrics*. These metrics describe a different
object in the same namespace, instead of describing pods. The metrics are not necessarily
fetched from the object; they only describe it. Object metrics support `target` types of
both `Value` and `AverageValue`.  With `Value`, the target is compared directly to the returned
metric from the API. With `AverageValue`, the value returned from the custom metrics API is divided
by the number of pods before being compared to the target. The following example is the YAML
representation of the `requests-per-second` metric.
-->
<p>第二种可选的度量指标类型是对象（Object）度量指标。这些度量指标用于描述
在相同名字空间中的别的对象，而非 Pods。
请注意这些度量指标不一定来自某对象，它们仅用于描述这些对象。
对象度量指标支持的 <code>target</code> 类型包括 <code>Value</code> 和 <code>AverageValue</code>。
如果是 <code>Value</code> 类型，<code>target</code> 值将直接与 API 返回的度量指标比较，
而对于 <code>AverageValue</code> 类型，API 返回的度量值将按照 Pod 数量拆分，
然后再与 <code>target</code> 值比较。
下面的 YAML 文件展示了一个表示 <code>requests-per-second</code> 的度量指标。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">object</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metric</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>requests-per-second<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">describedObject</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>main-route<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Value<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>2k<span style="color:#bbb">
</span></code></pre></div><!--
If you provide multiple such metric blocks, the HorizontalPodAutoscaler will consider each metric in turn.
The HorizontalPodAutoscaler will calculate proposed replica counts for each metric, and then choose the
one with the highest replica count.
-->
<p>如果你指定了多个上述类型的度量指标，HorizontalPodAutoscaler 将会依次考量各个指标。
HorizontalPodAutoscaler 将会计算每一个指标所提议的副本数量，然后最终选择一个最高值。</p>
<!--
For example, if you had your monitoring system collecting metrics about network traffic,
you could update the definition above using `kubectl edit` to look like this:
-->
<p>比如，如果你的监控系统能够提供网络流量数据，你可以通过 <code>kubectl edit</code> 命令
将上述 Horizontal Pod Autoscaler 的定义更改为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>autoscaling/v2beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>HorizontalPodAutoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleTargetRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metrics</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Resource<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>AverageUtilization<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Pods<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metric</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>packets-per-second<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>AverageValue<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">averageValue</span>:<span style="color:#bbb"> </span>1k<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">object</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metric</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>requests-per-second<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">describedObject</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>main-route<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Value<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>10k<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">observedGeneration</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">lastScaleTime</span>:<span style="color:#bbb"> </span>&lt;some-time&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">currentReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">desiredReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">currentMetrics</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Resource<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resource</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cpu<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">current</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">averageUtilization</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">averageValue</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">object</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metric</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>requests-per-second<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">describedObject</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>main-route<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">current</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>10k<span style="color:#bbb">
</span></code></pre></div><!--
Then, your HorizontalPodAutoscaler would attempt to ensure that each pod was consuming roughly
50% of its requested CPU, serving 1000 packets per second, and that all pods behind the main-route
Ingress were serving a total of 10000 requests per second.
-->
<p>这样，你的 HorizontalPodAutoscaler 将会尝试确保每个 Pod 的 CPU 利用率在 50% 以内，
每秒能够服务 1000 个数据包请求，
并确保所有在 Ingress 后的 Pod 每秒能够服务的请求总数达到 10000 个。</p>
<!--
### Autoscaling on more specific metrics

Many metrics pipelines allow you to describe metrics either by name or by a set of additional
descriptors called _labels_. For all non-resource metric types (pod, object, and external,
described below), you can specify an additional label selector which is passed to your metric
pipeline. For instance, if you collect a metric `http_requests` with the `verb`
label, you can specify the following metric block to scale only on GET requests:
-->
<h3 id="autoscaing-on-more-specific-metrics">基于更特别的度量值来扩缩  </h3>
<p>许多度量流水线允许你通过名称或附加的 <em>标签</em> 来描述度量指标。
对于所有非资源类型度量指标（Pod、Object 和后面将介绍的 External），
可以额外指定一个标签选择算符。例如，如果你希望收集包含 <code>verb</code> 标签的
<code>http_requests</code> 度量指标，可以按如下所示设置度量指标块，使得扩缩操作仅针对
GET 请求执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">object</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metric</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>`http_requests`<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb"> </span>`verb=GET`<span style="color:#bbb">
</span></code></pre></div><!--
This selector uses the same syntax as the full Kubernetes label selectors. The monitoring pipeline
determines how to collapse multiple series into a single value, if the name and selector
match multiple series. The selector is additive, and cannot select metrics
that describe objects that are **not** the target object (the target pods in the case of the `Pods`
type, and the described object in the case of the `Object` type).
-->
<p>这个选择算符使用与 Kubernetes 标签选择算符相同的语法。
如果名称和标签选择算符匹配到多个系列，监测管道会决定如何将多个系列合并成单个值。
选择算符是可以累加的，它不会选择目标以外的对象（类型为 <code>Pods</code> 的目标 Pods 或者
类型为 <code>Object</code> 的目标对象）。</p>
<!--
### Autoscaling on metrics not related to Kubernetes objects

Applications running on Kubernetes may need to autoscale based on metrics that don't have an obvious
relationship to any object in the Kubernetes cluster, such as metrics describing a hosted service with
no direct correlation to Kubernetes namespaces. In Kubernetes 1.10 and later, you can address this use case
with *external metrics*.
-->
<h3 id="基于与-kubernetes-对象无关的度量指标执行扩缩">基于与 Kubernetes 对象无关的度量指标执行扩缩</h3>
<p>运行在 Kubernetes 上的应用程序可能需要基于与 Kubernetes 集群中的任何对象
没有明显关系的度量指标进行自动扩缩，
例如那些描述与任何 Kubernetes 名字空间中的服务都无直接关联的度量指标。
在 Kubernetes 1.10 及之后版本中，你可以使用外部度量指标（external metrics）。</p>
<!--
Using external metrics requires knowledge of your monitoring system; the setup is
similar to that required when using custom metrics. External metrics allow you to autoscale your cluster
based on any metric available in your monitoring system. Provide a `metric` block with a
`name` and `selector`, as above, and use the `External` metric type instead of `Object`.
If multiple time series are matched by the `metricSelector`,
the sum of their values is used by the HorizontalPodAutoscaler.
External metrics support both the `Value` and `AverageValue` target types, which function exactly the same
as when you use the `Object` type.
-->
<p>使用外部度量指标时，需要了解你所使用的监控系统，相关的设置与使用自定义指标时类似。
外部度量指标使得你可以使用你的监控系统的任何指标来自动扩缩你的集群。
你需要在 <code>metric</code> 块中提供 <code>name</code> 和 <code>selector</code>，同时将类型由 <code>Object</code> 改为 <code>External</code>。
如果 <code>metricSelector</code> 匹配到多个度量指标，HorizontalPodAutoscaler 将会把它们加和。
外部度量指标同时支持 <code>Value</code> 和 <code>AverageValue</code> 类型，这与 <code>Object</code> 类型的度量指标相同。</p>
<!--
For example if your application processes tasks from a hosted queue service, you could add the following
section to your HorizontalPodAutoscaler manifest to specify that you need one worker per 30 outstanding tasks.
-->
<p>例如，如果你的应用程序处理来自主机上消息队列的任务，
为了让每 30 个任务有 1 个工作者实例，你可以将下面的内容添加到
HorizontalPodAutoscaler 的配置中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>External<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">external</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metric</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>queue_messages_ready<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">queue</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;worker_tasks&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">target</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>AverageValue<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">averageValue</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span></code></pre></div><!--
When possible, it's preferable to use the custom metric target types instead of external metrics, since it's
easier for cluster administrators to secure the custom metrics API.  The external metrics API potentially allows
access to any metric, so cluster administrators should take care when exposing it.
-->
<p>如果可能，还是推荐定制度量指标而不是外部度量指标，因为这便于让系统管理员加固定制度量指标 API。
而外部度量指标 API 可以允许访问所有的度量指标。
当暴露这些服务时，系统管理员需要仔细考虑这个问题。</p>
<!--
## Appendix: Horizontal Pod Autoscaler Status Conditions

When using the `autoscaling/v2beta2` form of the HorizontalPodAutoscaler, you will be able to see
*status conditions* set by Kubernetes on the HorizontalPodAutoscaler.  These status conditions indicate
whether or not the HorizontalPodAutoscaler is able to scale, and whether or not it is currently restricted
in any way.
-->
<h2 id="附录-horizontal-pod-autoscaler-状态条件">附录：Horizontal Pod Autoscaler 状态条件</h2>
<p>使用 <code>autoscaling/v2beta2</code> 格式的 HorizontalPodAutoscaler 时，你将可以看到
Kubernetes 为 HorizongtalPodAutoscaler 设置的状态条件（Status Conditions）。
这些状态条件可以显示当前 HorizontalPodAutoscaler 是否能够执行扩缩以及是否受到一定的限制。</p>
<!--
The conditions appear in the `status.conditions` field.  To see the conditions affecting a HorizontalPodAutoscaler,
we can use `kubectl describe hpa`:
-->
<p><code>status.conditions</code> 字段展示了这些状态条件。
可以通过 <code>kubectl describe hpa</code> 命令查看当前影响 HorizontalPodAutoscaler
的各种状态条件信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe hpa cm-test
</code></pre></div><pre tabindex="0"><code>Name:                           cm-test
Namespace:                      prom
Labels:                         &lt;none&gt;
Annotations:                    &lt;none&gt;
CreationTimestamp:              Fri, 16 Jun 2017 18:09:22 +0000
Reference:                      ReplicationController/cm-test
Metrics:                        ( current / target )
  &quot;http_requests&quot; on pods:      66m / 500m
Min replicas:                   1
Max replicas:                   4
ReplicationController pods:     1 current / 1 desired
Conditions:
  Type                  Status  Reason                  Message
  ----                  ------  ------                  -------
  AbleToScale           True    ReadyForNewScale        the last scale time was sufficiently old as to warrant a new scale
  ScalingActive         True    ValidMetricFound        the HPA was able to successfully calculate a replica count from pods metric http_requests
  ScalingLimited        False   DesiredWithinRange      the desired replica count is within the acceptable range
Events:
</code></pre><!--
For this HorizontalPodAutoscaler, we can see several conditions in a healthy state.  The first,
`AbleToScale`, indicates whether or not the HPA is able to fetch and update scales, as well as
whether or not any backoff-related conditions would prevent scaling.  The second, `ScalingActive`,
indicates whether or not the HPA is enabled (i.e. the replica count of the target is not zero) and
is able to calculate desired scales. When it is `False`, it generally indicates problems with
fetching metrics.  Finally, the last condition, `ScalingLimited`, indicates that the desired scale
was capped by the maximum or minimum of the HorizontalPodAutoscaler.  This is an indication that
you may wish to raise or lower the minimum or maximum replica count constraints on your
HorizontalPodAutoscaler.
-->
<p>对于上面展示的这个 HorizontalPodAutoscaler，我们可以看出有若干状态条件处于健康状态。
首先，<code>AbleToScale</code> 表明 HPA 是否可以获取和更新扩缩信息，以及是否存在阻止扩缩的各种回退条件。
其次，<code>ScalingActive</code> 表明 HPA 是否被启用（即目标的副本数量不为零） 以及是否能够完成扩缩计算。
当这一状态为 <code>False</code> 时，通常表明获取度量指标存在问题。
最后一个条件 <code>ScalingLimitted</code> 表明所需扩缩的值被 HorizontalPodAutoscaler
所定义的最大或者最小值所限制（即已经达到最大或者最小扩缩值）。
这通常表明你可能需要调整 HorizontalPodAutoscaler 所定义的最大或者最小副本数量的限制了。</p>
<!--
## Appendix: Quantities

All metrics in the HorizontalPodAutoscaler and metrics APIs are specified using
a special whole-number notation known in Kubernetes as a
<a class='glossary-tooltip' title='使用全数字来表示较小数值或使用 SI 后缀表示较大数值的表示法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-quantity' target='_blank' aria-label='quantity'>quantity</a>.  For example,
the quantity `10500m` would be written as `10.5` in decimal notation.  The metrics APIs
will return whole numbers without a suffix when possible, and will generally return
quantities in milli-units otherwise.  This means you might see your metric value fluctuate
between `1` and `1500m`, or `1` and `1.5` when written in decimal notation.
-->
<h2 id="appendix-quantities">附录：量纲   </h2>
<p>HorizontalPodAutoscaler 和 度量指标 API 中的所有的度量指标使用 Kubernetes 中称为
<a class='glossary-tooltip' title='使用全数字来表示较小数值或使用 SI 后缀表示较大数值的表示法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-quantity' target='_blank' aria-label='量纲（Quantity）'>量纲（Quantity）</a>
的特殊整数表示。
例如，数量 <code>10500m</code> 用十进制表示为 <code>10.5</code>。
如果可能的话，度量指标 API 将返回没有后缀的整数，否则返回以千分单位的数量。
这意味着你可能会看到你的度量指标在 <code>1</code> 和 <code>1500m</code> （也就是在十进制记数法中的 <code>1</code> 和 <code>1.5</code>）之间波动。</p>
<!--
## Appendix: Other possible scenarios

### Creating the autoscaler declaratively
-->
<h2 id="appendix-other-possible-scenarios">附录：其他可能的情况  </h2>
<h3 id="creating-the-autoscaler-declaratively">以声明式方式创建 Autoscaler    </h3>
<!--
Instead of using `kubectl autoscale` command to create a HorizontalPodAutoscaler imperatively we
can use the following file to create it declaratively:
-->
<p>除了使用 <code>kubectl autoscale</code> 命令，也可以文件创建 HorizontalPodAutoscaler：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/hpa/php-apache.yaml" download="application/hpa/php-apache.yaml"><code>application/hpa/php-apache.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-hpa-php-apache-yaml')" title="Copy application/hpa/php-apache.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-hpa-php-apache-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>autoscaling/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>HorizontalPodAutoscaler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scaleTargetRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>php-apache<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxReplicas</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">targetCPUUtilizationPercentage</span>:<span style="color:#bbb"> </span><span style="color:#666">50</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
We will create the autoscaler by executing the following command:
-->
<p>使用如下命令创建 autoscaler：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/application/hpa/php-apache.yaml
</code></pre></div><pre tabindex="0"><code>horizontalpodautoscaler.autoscaling/php-apache created
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fbe2744f00d1aa4df4cdf4eea6a082d4">7.8 - 为应用程序设置干扰预算（Disruption Budget）</h1>
    
	<!--
title: Specifying a Disruption Budget for your Application
content_type: task
weight: 110
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code>
</div>

<!--
This page shows how to limit the number of concurrent disruptions
that your application experiences, allowing for higher availability
while permitting the cluster administrator to manage the clusters
nodes.
-->
<p>本文展示如何限制应用程序的并发干扰数量，在允许集群管理员管理集群节点的同时保证高可用。</p>
<h2 id="准备开始">准备开始</h2>


您的 Kubernetes 服务器版本必须不低于版本 v1.21.
 要获知版本信息，请输入 <code>kubectl version</code>.

<!--
* You are the owner of an application running on a Kubernetes cluster that requires
  high availability.
* You should know how to deploy [Replicated Stateless Applications](/docs/tasks/run-application/run-stateless-application-deployment/)
  and/or [Replicated Stateful Applications](/docs/tasks/run-application/run-replicated-stateful-application/).
* You should have read about [Pod Disruptions](/docs/concepts/workloads/pods/disruptions/).
* You should confirm with your cluster owner or service provider that they respect
  Pod Disruption Budgets.
-->
<ul>
<li>你是 Kubernetes 集群中某应用的所有者，该应用有高可用要求。</li>
<li>你应了解如何部署<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">无状态应用</a>
和/或<a href="/zh/docs/tasks/run-application/run-replicated-stateful-application/">有状态应用</a>。</li>
<li>你应当已经阅读过关于 <a href="/zh/docs/concepts/workloads/pods/disruptions/">Pod 干扰</a> 的文档。</li>
<li>用户应当与集群所有者或服务提供者确认其遵从 Pod 干扰预算（Pod Disruption Budgets）的规则。</li>
</ul>
<!-- steps -->
<!--
## Protecting an Application with a PodDisruptionBudget

1. Identify what application you want to protect with a PodDisruptionBudget (PDB).
1. Think about how your application reacts to disruptions.
1. Create a PDB definition as a YAML file.
1. Create the PDB object from the YAML file.
-->
<h2 id="用-poddisruptionbudget-来保护应用">用 PodDisruptionBudget 来保护应用</h2>
<ol>
<li>确定想要使用 PodDisruptionBudget (PDB) 来保护的应用。</li>
<li>考虑应用对干扰的反应。</li>
<li>以 YAML 文件形式定义 PDB 。</li>
<li>通过 YAML 文件创建 PDB 对象。</li>
</ol>
<!-- discussion -->
<!--
## Identify an Application to Protect

The most common use case when you want to protect an application
specified by one of the built-in Kubernetes controllers:
-->
<h2 id="确定要保护的应用">确定要保护的应用</h2>
<p>用户想要保护通过内置的 Kubernetes 控制器指定的应用，这是最常见的使用场景：</p>
<ul>
<li>Deployment</li>
<li>ReplicationController</li>
<li>ReplicaSet</li>
<li>StatefulSet</li>
</ul>
<!--
In this case, make a note of the controller's `.spec.selector`; the same
selector goes into the PDBs `.spec.selector`.
-->
<p>在这种情况下，在控制器的 <code>.spec.selector</code> 字段中做记录，并在 PDB 的
<code>.spec.selector</code> 字段中加入同样的选择算符。</p>
<!--
From version 1.15 PDBs support custom controllers where the [scale subresource](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource) is enabled.
-->
<p>从 1.15 版本开始，PDB 支持启用
<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource">scale 子资源</a>
的自定义控制器。</p>
<!--
You can also use PDBs with pods which are not controlled by one of the above
controllers, or arbitrary groups of pods, but there are some restrictions,
described in [Arbitrary Controllers and Selectors](#arbitrary-controllers-and-selectors).
-->
<p>用户也可以用 PDB 来保护不受上述控制器控制的 Pod，或任意的 Pod 集合，但是正如
<a href="#arbitrary-controllers-and-selectors">任意控制器和选择算符</a>中描述的，这里存在一些限制。</p>
<!--
## Think about how your application reacts to disruptions

Decide how many instances can be down at the same time for a short period
due to a voluntary disruption.
-->
<h2 id="考虑应用对干扰的反应">考虑应用对干扰的反应</h2>
<p>确定在自发干扰时，多少实例可以在短时间内同时关闭。</p>
<!--
- Stateless frontends:
  - Concern: don't reduce serving capacity by more than 10%.
    - Solution: use PDB with minAvailable 90% for example.
- Single-instance Stateful Application:
  - Concern: do not terminate this application without talking to me.
    - Possible Solution 1: Do not use a PDB and tolerate occasional downtime.
    - Possible Solution 2: Set PDB with maxUnavailable=0.  Have an understanding
      (outside of Kubernetes) that the cluster operator needs to consult you before
      termination.  When the cluster operator contacts you, prepare for downtime,
      and then delete the PDB to indicate readiness for disruption.  Recreate afterwards.
- Multiple-instance Stateful application such as Consul, ZooKeeper, or etcd:
  - Concern: Do not reduce number of instances below quorum, otherwise writes fail.
    - Possible Solution 1: set maxUnavailable to 1 (works with varying scale of application).
    - Possible Solution 2: set minAvailable to quorum-size (e.g. 3 when scale is 5).  (Allows more disruptions at once).
- Restartable Batch Job:
  - Concern: Job needs to complete in case of voluntary disruption.
    - Possible solution: Do not create a PDB.  The Job controller will create a replacement pod.
-->
<ul>
<li>无状态的前端：
<ul>
<li>关注：不能降低服务能力 10% 以上。
<ul>
<li>解决方案：例如，使用 PDB，指定其 minAvailable 值为 90%。</li>
</ul>
</li>
</ul>
</li>
<li>单实例有状态应用：
<ul>
<li>关注：不要在不通知的情况下终止该应用。
<ul>
<li>可能的解决方案 1：不使用 PDB，并忍受偶尔的停机。</li>
<li>可能的解决方案 2：设置 maxUnavailable=0 的 PDB。
意为（Kubernetes 范畴之外的）集群操作人员需要在终止应用前与用户协商，
协商后准备停机，然后删除 PDB 表示准备接受干扰，后续再重新创建。</li>
</ul>
</li>
</ul>
</li>
<li>多实例有状态应用，如 Consul、ZooKeeper 或 etcd：
<ul>
<li>关注：不要将实例数量减少至低于仲裁规模，否则将出现写入失败。
<ul>
<li>可能的解决方案 1：设置 maxUnavailable 值为 1 (适用于不同规模的应用)。</li>
<li>可能的解决方案 2：设置 minAvailable 值为仲裁规模（例如规模为 5 时设置为 3）。
(允许同时出现更多的干扰)。</li>
</ul>
</li>
</ul>
</li>
<li>可重新启动的批处理任务：
<ul>
<li>关注：自发干扰的情况下，需要确保任务完成。
<ul>
<li>可能的解决方案：不创建 PDB。 任务控制器会创建一个替换 Pod。</li>
</ul>
</li>
</ul>
</li>
</ul>
<!--
### Rounding logic when specifying percentages

Values for `minAvailable` or `maxUnavailable` can be expressed as integers or as a percentage.
-->
<h3 id="指定百分比时的舍入逻辑">指定百分比时的舍入逻辑</h3>
<p><code>minAvailable</code> 或 <code>maxUnavailable</code> 的值可以表示为整数或百分比。</p>
<!--
- When you specify an integer, it represents a number of Pods. For instance, if you set `minAvailable` to 10, then 10
  Pods must always be available, even during a disruption.
- When you specify a percentage by setting the value to a string representation of a percentage (eg. `"50%"`), it represents a percentage of
  total Pods. For instance, if you set `minUnavailable` to `"50%"`, then only 50% of the Pods can be unavailable during a
  disruption.
-->
<ul>
<li>指定整数值时，它表示 Pod 个数。例如，如果将 minAvailable 设置为 10，
那么即使在干扰期间，也必须始终有 10 个Pod可用。</li>
<li>通过将值设置为百分比的字符串表示形式（例如 “50％”）来指定百分比时，它表示占总 Pod 数的百分比。
例如，如果将 &quot;minUnavailable&quot; 设置为 “50％”，则干扰期间只允许 50％ 的 Pod 不可用。</li>
</ul>
<!--
When you specify the value as a percentage, it may not map to an exact number
of Pods. For example, if you have 7 Pods and you set `minAvailable` to
`"50%"`, it's not immediately obvious whether that means 3 Pods or 4 Pods must
be available.  Kubernetes rounds up to the nearest integer, so in this case, 4
Pods must be available. You can examine the
[code](https://github.com/kubernetes/kubernetes/blob/23be9587a0f8677eb8091464098881df939c44a9/pkg/controller/disruption/disruption.go#L539)
that controls this behavior.
-->
<p>如果将值指定为百分比，则可能无法映射到确切数量的 Pod。例如，如果你有 7 个 Pod，
并且你将 <code>minAvailable</code> 设置为 <code>&quot;50％&quot;</code>，具体是 3 个 Pod 或 4 个 Pod 必须可用
并非显而易见。
Kubernetes 采用向上取整到最接近的整数的办法，因此在这种情况下，必须有 4 个 Pod。
你可以检查控制此行为的
<a href="https://github.com/kubernetes/kubernetes/blob/23be9587a0f8677eb8091464098881df939c44a9/pkg/controller/disruption/disruption.go#L539">代码</a>。</p>
<!--
## Specifying a PodDisruptionBudget

A `PodDisruptionBudget` has three fields: 
-->
<h2 id="指定-poddisruptionbudget">指定 PodDisruptionBudget</h2>
<p>一个 <code>PodDisruptionBudget</code> 有 3 个字段：</p>
<!--
* A label selector `.spec.selector` to specify the set of
pods to which it applies. This field is required.
* `.spec.minAvailable` which is a description of the number of pods from that
set that must still be available after the eviction, even in the absence
of the evicted pod. `minAvailable` can be either an absolute number or a percentage.
* `.spec.maxUnavailable` (available in Kubernetes 1.7 and higher) which is a description
of the number of pods from that set that can be unavailable after the eviction.
It can be either an absolute number or a percentage.
-->
<ul>
<li>标签选择算符 <code>.spec.selector</code> 用于指定其所作用的 Pod 集合，该字段为必需字段。</li>
<li><code>.spec.minAvailable</code> 表示驱逐后仍须保证可用的 Pod 数量。即使因此影响到 Pod 驱逐
（即该条件在和 Pod 驱逐发生冲突时优先保证）。
<code>minAvailable</code> 值可以是绝对值，也可以是百分比。</li>
<li><code>.spec.maxUnavailable</code> （Kubernetes 1.7 及更高的版本中可用）表示驱逐后允许不可用的
Pod 的最大数量。其值可以是绝对值或是百分比。</li>
</ul>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
The behavior for an empty selector differs between the policy/v1beta1 and policy/v1 APIs for
PodDisruptionBudgets. For policy/v1beta1 an empty selector matches zero pods, while
for policy/v1 an empty selector matches every pod in the namespace.
-->
<p><code>policy/v1beta1</code> 和 <code>policy/v1</code> API 中 PodDisruptionBudget 的空选择算符的行为
略有不同。在 <code>policy/v1beta1</code> 中，空的选择算符不会匹配任何 Pods，而
<code>policy/v1</code> 中，空的选择算符会匹配名字空间中所有 Pods。</div>
</blockquote>
<!--
You can specify only one of `maxUnavailable` and `minAvailable` in a single `PodDisruptionBudget`.
`maxUnavailable` can only be used to control the eviction of pods
that have an associated controller managing them. In the examples below, "desired replicas"
is the `scale` of the controller managing the pods being selected by the
`PodDisruptionBudget`.
-->
<p>用户在同一个 <code>PodDisruptionBudget</code> 中只能够指定 <code>maxUnavailable</code> 和 <code>minAvailable</code> 中的一个。
<code>maxUnavailable</code> 只能够用于控制存在相应控制器的 Pod 的驱逐（即不受控制器控制的 Pod 不在
<code>maxUnavailable</code> 控制范围内）。在下面的示例中，
“所需副本” 指的是相应控制器的 <code>scale</code>，控制器对 <code>PodDisruptionBudget</code> 所选择的 Pod 进行管理。</p>
<!--
Example 1: With a `minAvailable` of 5, evictions are allowed as long as they leave behind
5 or more healthy pods among those selected by the PodDisruptionBudget's `selector`.
-->
<p>示例 1：设置 <code>minAvailable</code> 值为 5 的情况下，驱逐时需保证 PodDisruptionBudget 的 <code>selector</code>
选中的 Pod 中 5 个或 5 个以上处于健康状态。</p>
<!--
Example 2: With a `minAvailable` of 30%, evictions are allowed as long as at least 30%
of the number of desired replicas are healthy. 
-->
<p>示例 2：设置 <code>minAvailable</code> 值为 30% 的情况下，驱逐时需保证 Pod 所需副本的至少 30% 处于健康状态。</p>
<!--
Example 3: With a `maxUnavailable` of 5, evictions are allowed as long as there are at most 5
unhealthy replicas among the total number of desired replicas.
-->
<p>示例 3：设置 <code>maxUnavailable</code> 值为 5 的情况下，驱逐时需保证所需副本中最多 5 个处于不可用状态。</p>
<!--
Example 4: With a `maxUnavailable` of 30%, evictions are allowed as long as no more than 30%
of the desired replicas are unhealthy.
-->
<p>示例 4：设置 <code>maxUnavailable</code> 值为 30% 的情况下，驱逐时需保证所需副本中最多 30% 处于不可用状态。</p>
<!--
In typical usage, a single budget would be used for a collection of pods managed by
a controller—for example, the pods in a single ReplicaSet or StatefulSet. 
-->
<p>在典型用法中，干扰预算会被用于一个控制器管理的一组 Pod 中 —— 例如：一个 ReplicaSet 或 StatefulSet
中的 Pod。</p>
<!--
A disruption budget does not truly guarantee that the specified
number/percentage of pods will always be up.  For example, a node that hosts a
pod from the collection may fail when the collection is at the minimum size
specified in the budget, thus bringing the number of available pods from the
collection below the specified size. The budget can only protect against
voluntary evictions, not all causes of unavailability.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 干扰预算并不能真正保证指定数量/百分比的 Pod 一直处于运行状态。例如： 当 Pod 集合的
规模处于预算指定的最小值时，承载集合中某个 Pod 的节点发生了故障，这样就导致集合中可用 Pod 的
数量低于预算指定值。预算只能够针对自发的驱逐提供保护，而不能针对所有 Pod 不可用的诱因。</div>
</blockquote>
<!--
A `maxUnavailable` of 0% (or 0) or a `minAvailable` of 100% (or equal to the
number of replicas) may block node drains entirely. This is permitted as per the
semantics of `PodDisruptionBudget`.
-->
<p>设置 <code>maxUnavailable</code> 值为 0%（或 0）或设置 <code>minAvailable</code> 值为 100%（或等于副本数）
可能会阻塞节点，导致资源耗尽。按照 <code>PodDisruptionBudget</code> 的语义，这是允许的。</p>
<!--
You can find examples of pod disruption budgets defined below. They match pods with the label
`app: zookeeper`.
-->
<p>用户可以在下面看到 pod 干扰预算定义的示例，它们与带有 <code>app: zookeeper</code> 标签的 pod 相匹配：</p>
<!--
Example PDB Using minAvailable:
-->
<p>使用 minAvailable 的PDB 示例：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/policy/zookeeper-pod-disruption-budget-minavailable.yaml" download="policy/zookeeper-pod-disruption-budget-minavailable.yaml"><code>policy/zookeeper-pod-disruption-budget-minavailable.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('policy-zookeeper-pod-disruption-budget-minavailable-yaml')" title="Copy policy/zookeeper-pod-disruption-budget-minavailable.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="policy-zookeeper-pod-disruption-budget-minavailable-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>policy/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PodDisruptionBudget<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-pdb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">minAvailable</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zookeeper<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Example PDB Using maxUnavailable:
-->
<p>使用 maxUnavailable 的 PDB 示例：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/policy/zookeeper-pod-disruption-budget-maxunavailable.yaml" download="policy/zookeeper-pod-disruption-budget-maxunavailable.yaml"><code>policy/zookeeper-pod-disruption-budget-maxunavailable.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('policy-zookeeper-pod-disruption-budget-maxunavailable-yaml')" title="Copy policy/zookeeper-pod-disruption-budget-maxunavailable.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="policy-zookeeper-pod-disruption-budget-maxunavailable-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>policy/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PodDisruptionBudget<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-pdb<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>zookeeper<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
For example, if the above `zk-pdb` object selects the pods of a StatefulSet of size 3, both
specifications have the exact same meaning. The use of `maxUnavailable` is recommended as it
automatically responds to changes in the number of replicas of the corresponding controller.
-->
<p>例如，如果上述 <code>zk-pdb</code> 选择的是一个规格为 3 的 StatefulSet 对应的 Pod，
那么上面两种规范的含义完全相同。
推荐使用 <code>maxUnavailable</code> ，因为它自动响应控制器副本数量的变化。</p>
<!--
## Create the PDB object

You can create the PDB object with a command like `kubectl apply -f mypdb.yaml`.
-->
<h2 id="创建-pdb-对象">创建 PDB 对象</h2>
<p>你可以通过类似 <code>kubectl apply -f mypdb.yaml</code> 的命令来创建 PDB。</p>
<!--
You cannot update PDB objects.  They must be deleted and re-created.
-->
<p>PDB 对象无法更新，必须删除后重新创建。</p>
<!--
## Check the status of the PDB

Use kubectl to check that your PDB is created.
-->
<h2 id="检查-pdb-的状态">检查 PDB 的状态</h2>
<p>使用 kubectl 来确认 PDB 被创建。</p>
<!--
Assuming you don't actually have pods matching `app: zookeeper` in your namespace,
then you'll see something like this:
-->
<p>假设用户的名字空间下没有匹配 <code>app: zookeeper</code> 的 Pod，用户会看到类似下面的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get poddisruptionbudgets
</code></pre></div><pre tabindex="0"><code>NAME     MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
zk-pdb   2               N/A               0                     7s
</code></pre><!--
If there are matching pods (say, 3), then you would see something like this:
-->
<p>假设有匹配的 Pod (比如说 3 个), 那么用户会看到类似下面的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get poddisruptionbudgets
</code></pre></div><pre tabindex="0"><code>NAME     MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
zk-pdb   2               N/A               1                     7s

</code></pre><!--
The non-zero value for `ALLOWED-DISRUPTIONS` means that the disruption controller has seen the pods,
counted the matching pods, and updated the status of the PDB.

You can get more information about the status of a PDB with this command:
-->
<p><code>ALLOWED-DISRUPTIONS</code> 值非 0 意味着干扰控制器已经感知到相应的 Pod，对匹配的 Pod 进行统计，
并更新了 PDB 的状态。</p>
<p>用户可以通过以下命令获取更多 PDB 状态相关信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get poddisruptionbudgets zk-pdb -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>policy/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PodDisruptionBudget<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">anntation</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2020-03-04T04:22:56Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>zk-pdb<span style="color:#bbb">
</span><span style="color:#bbb"></span>…<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">currentHealthy</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">desiredHealthy</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">disruptionsAllowed</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">expectedPods</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">observedGeneration</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></code></pre></div><!--
## Arbitrary Controllers and Selectors

You can skip this section if you only use PDBs with the built-in
application controllers (Deployment, ReplicationController, ReplicaSet, and StatefulSet),
with the PDB selector matching the controller's selector.
-->
<h2 id="arbitrary-controllers-and-selectors">任意控制器和选择算符  </h2>
<p>如果你只使用与内置的应用控制器（Deployment、ReplicationController、ReplicaSet 和 StatefulSet）
对应的 PDB，也就是 PDB 的选择算符与 控制器的选择算符相匹配，那么可以跳过这一节。</p>
<!--
You can use a PDB with pods controlled by another type of controller, by an
"operator", or bare pods, but with these restrictions:
-->
<p>你可以使用这样的 PDB：它对应的 Pod 可能由其他类型的控制器控制，可能由 &quot;operator&quot; 控制，
也可能为“裸的（不受控制器控制）” Pod，但该类 PDB 存在以下限制：</p>
<!--
- only `.spec.minAvailable` can be used, not `.spec.maxUnavailable`.
- only an integer value can be used with `.spec.minAvailable`, not a percentage.
-->
<ul>
<li>只能够使用 <code>.spec.minAvailable</code> ，而不能够使用 <code>.spec.maxUnavailable。</code></li>
<li>只能够使用整数作为 <code>.spec.minAvailable</code> 的值，而不能使用百分比。</li>
</ul>
<!--
You can use a selector which selects a subset or superset of the pods belonging to a built-in
controller.  However, when there are multiple PDBs in a namespace, you must be careful not
to create PDBs whose selectors overlap.
-->
<p>你可以令选择算符选择一个内置控制器所控制 Pod 的子集或父集。
然而，当名字空间下存在多个 PDB 时，用户必须小心，保证 PDB 的选择算符之间不重叠。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-52cd10ee3fc7c74a6c31043a2d489878">7.9 - 从 Pod 中访问 Kubernetes API</h1>
    
	<!--
title: Accessing the Kubernetes API from a Pod
content_type: task
weight: 120
-->
<!-- overview -->
<!--
This guide demonstrates how to access the Kubernetes API from within a pod.
-->
<p>本指南演示了如何从 Pod 中访问 Kubernetes API。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!--
## Accessing the API from within a Pod

When accessing the API from within a Pod, locating and authenticating
to the API server are slightly different to the external client case.
-->
<h3 id="accessing-the-api-from-within-a-pod">从 Pod 中访问 API  </h3>
<p>从 Pod 内部访问 API 时，定位 API 服务器和向服务器认证身份的操作
与外部客户端场景不同。</p>
<!--
The easiest way to use the Kubernetes API from a Pod is to use
one of the official [client libraries](/docs/reference/using-api/client-libraries/). These
libraries can automatically discover the API server and authenticate.
-->
<p>从 Pod 使用 Kubernetes API 的最简单的方法就是使用官方的
<a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>。
这些库可以自动发现 API 服务器并进行身份验证。</p>
<!--
### Using Official Client Libraries

From within a Pod, the recommended ways to connect to the Kubernetes API are:

  - For a Go client, use the official [Go client library](https://github.com/kubernetes/client-go/).
    The `rest.InClusterConfig()` function handles API host discovery and authentication automatically.
    See [an example here](https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go).

  - For a Python client, use the official [Python client library](https://github.com/kubernetes-client/python/).
    The `config.load_incluster_config()` function handles API host discovery and authentication automatically.
    See [an example here](https://github.com/kubernetes-client/python/blob/master/examples/in_cluster_config.py).

  - There are a number of other libraries available, please refer to the [Client Libraries](/docs/reference/using-api/client-libraries/) page.

In each case, the service account credentials of the Pod are used to communicate
securely with the API server.
-->
<h4 id="using-official-client-libraries">使用官方客户端库  </h4>
<p>从一个 Pod 内部连接到 Kubernetes API 的推荐方式为：</p>
<ul>
<li>
<p>对于 Go 语言客户端，使用官方的 <a href="https://github.com/kubernetes/client-go/">Go 客户端库</a>。
函数 <code>rest.InClusterConfig()</code> 自动处理 API 主机发现和身份认证。
参见<a href="https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go">这里的一个例子</a>。</p>
</li>
<li>
<p>对于 Python 客户端，使用官方的 <a href="https://github.com/kubernetes-client/python/">Python 客户端库</a>。
函数 <code>config.load_incluster_config()</code> 自动处理 API 主机的发现和身份认证。
参见<a href="https://github.com/kubernetes-client/python/blob/master/examples/in_cluster_config.py">这里的一个例子</a>。</p>
</li>
<li>
<p>还有一些其他可用的客户端库，请参阅<a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>页面。</p>
</li>
</ul>
<p>在以上场景中，客户端库都使用 Pod 的服务账号凭据来与 API 服务器安全地通信。</p>
<!--
### Directly accessing the REST API

While running in a Pod, the Kubernetes apiserver is accessible via a Service named
`kubernetes` in the `default` namespace. Therefore, Pods can use the
`kubernetes.default.svc` hostname to query the API server. Official client libraries
do this automatically.
-->
<h4 id="directly-accessing-the-rest-api">直接访问 REST API  </h4>
<p>在运行在 Pod 中时，可以通过 <code>default</code> 命名空间中的名为 <code>kubernetes</code> 的服务访问
Kubernetes API 服务器。也就是说，Pod 可以使用 <code>kubernetes.default.svc</code> 主机名
来查询 API 服务器。官方客户端库自动完成这个工作。</p>
<!--
The recommended way to authenticate to the API server is with a
[service account](/docs/tasks/configure-pod-container/configure-service-account/) credential. By default, a Pod
is associated with a service account, and a credential (token) for that
service account is placed into the filesystem tree of each container in that Pod,
at `/var/run/secrets/kubernetes.io/serviceaccount/token`.
-->
<p>向 API 服务器进行身份认证的推荐做法是使用
<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">服务账号</a>凭据。
默认情况下，每个 Pod 与一个服务账号关联，该服务账户的凭证（令牌）放置在此 Pod 中
每个容器的文件系统树中的 <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code> 处。</p>
<!--
If available, a certificate bundle is placed into the filesystem tree of each
container at `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt`, and should be
used to verify the serving certificate of the API server.
-->
<p>如果证书包可用，则凭证包被放入每个容器的文件系统树中的
<code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code> 处，
且将被用于验证 API 服务器的服务证书。</p>
<!--
Finally, the default namespace to be used for namespaced API operations is placed in a file
at `/var/run/secrets/kubernetes.io/serviceaccount/namespace` in each container.
-->
<p>最后，用于命名空间域 API 操作的默认命名空间放置在每个容器中的
<code>/var/run/secrets/kubernetes.io/serviceaccount/namespace</code> 文件中。</p>
<!--
### Using kubectl proxy

If you would like to query the API without an official client library, you can run `kubectl proxy`
as the [command](/docs/tasks/inject-data-application/define-command-argument-container/)
of a new sidecar container in the Pod. This way, `kubectl proxy` will authenticate
to the API and expose it on the `localhost` interface of the Pod, so that other containers
in the Pod can use it directly.
-->
<h4 id="use-kubectl-proxy">使用 kubectl proxy  </h4>
<p>如果你希望不使用官方客户端库就完成 API 查询，可以将 <code>kubectl proxy</code> 作为
<a href="/zh/docs/tasks/inject-data-application/define-command-argument-container/">command</a>
在 Pod 中启动一个边车（Sidecar）容器。这样，<code>kubectl proxy</code> 自动完成对 API
的身份认证，并将其暴露到 Pod 的 <code>localhost</code> 接口，从而 Pod 中的其他容器可以
直接使用 API。</p>
<!--
### Without using a proxy

It is possible to avoid using the kubectl proxy by passing the authentication token
directly to the API server.  The internal certificate secures the connection.
-->
<h3 id="without-using-a-proxy">不使用代理  </h3>
<p>通过将认证令牌直接发送到 API 服务器，也可以避免运行 kubectl proxy 命令。
内部的证书机制能够为链接提供保护。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 指向内部 API 服务器的主机名</span>
<span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span>https://kubernetes.default.svc

<span style="color:#080;font-style:italic"># 服务账号令牌的路径</span>
<span style="color:#b8860b">SERVICEACCOUNT</span><span style="color:#666">=</span>/var/run/secrets/kubernetes.io/serviceaccount

<span style="color:#080;font-style:italic"># 读取 Pod 的名字空间</span>
<span style="color:#b8860b">NAMESPACE</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>cat <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">SERVICEACCOUNT</span><span style="color:#b68;font-weight:bold">}</span>/namespace<span style="color:#a2f;font-weight:bold">)</span>

<span style="color:#080;font-style:italic"># 读取服务账号的持有者令牌</span>
<span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>cat <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">SERVICEACCOUNT</span><span style="color:#b68;font-weight:bold">}</span>/token<span style="color:#a2f;font-weight:bold">)</span>

<span style="color:#080;font-style:italic"># 引用内部证书机构（CA）</span>
<span style="color:#b8860b">CACERT</span><span style="color:#666">=</span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">SERVICEACCOUNT</span><span style="color:#b68;font-weight:bold">}</span>/ca.crt

<span style="color:#080;font-style:italic"># 使用令牌访问 API</span>
curl --cacert <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CACERT</span><span style="color:#b68;font-weight:bold">}</span> --header <span style="color:#b44">&#34;Authorization: Bearer </span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">TOKEN</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">&#34;</span> -X GET <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">APISERVER</span><span style="color:#b68;font-weight:bold">}</span>/api
</code></pre></div><!--
The output will be similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;APIVersions&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7a9b5779e228083ba3fdeaf414fe704e">7.10 - 扩缩 StatefulSet</h1>
    
	<!-- overview -->
<!--
This task shows how to scale a StatefulSet. Scaling a StatefulSet refers to increasing or decreasing the number of replicas.
-->
<p>本文介绍如何扩缩StatefulSet。StatefulSet 的扩缩指的是增加或者减少副本个数。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* StatefulSets are only available in Kubernetes version 1.5 or later.
  To check your version of Kubernetes, run `kubectl version`.

* Not all stateful applications scale nicely. If you are unsure about whether to scale your StatefulSets, see [StatefulSet concepts](/docs/concepts/workloads/controllers/statefulset/) or [StatefulSet tutorial](/docs/tutorials/stateful-application/basic-stateful-set/) for further information.

* You should perform scaling only when you are confident that your stateful application
  cluster is completely healthy.
-->
<ul>
<li>
<p>StatefulSets 仅适用于 Kubernetes 1.5 及以上版本。</p>
</li>
<li>
<p>不是所有 Stateful 应用都能很好地执行扩缩操作。
如果你不是很确定是否要扩缩你的 StatefulSet，可先参阅
<a href="/zh/docs/concepts/workloads/controllers/statefulset/">StatefulSet 概念</a>
或者 <a href="/zh/docs/tutorials/stateful-application/basic-stateful-set/">StatefulSet 教程</a>。</p>
</li>
<li>
<p>仅当你确定你的有状态应用的集群是完全健康的，才可执行扩缩操作.</p>
</li>
</ul>
<!-- steps -->
<!--
## Scaling StatefulSets

### Use kubectl to scale StatefulSets

First, find the StatefulSet you want to scale.

```shell
kubectl get statefulsets <stateful-set-name>
```
-->
<h2 id="scaling-statefulset">扩缩 StatefulSet  </h2>
<h2 id="使用-kubectl-扩缩-statefulset">使用 <code>kubectl</code> 扩缩 StatefulSet</h2>
<p>首先，找到你要扩缩的 StatefulSet。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get statefulsets &lt;statefulset 名称&gt;
</code></pre></div><!--
Change the number of replicas of your StatefulSet:

```shell
kubectl scale statefulsets <stateful-set-name> --replicas=<new-replicas>
```
-->
<p>更改 StatefulSet 中副本个数：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale statefulsets &lt;statefulset 名称&gt; --replicas<span style="color:#666">=</span>&lt;新的副本数&gt;
</code></pre></div><!--
### Make in-place updates on your StatefulSets

Alternatively, you can do [in-place updates](/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources) on your StatefulSets.

If your StatefulSet was initially created with `kubectl apply`,
update `.spec.replicas` of the StatefulSet manifests, and then do a `kubectl apply`:
-->
<h3 id="对-statefulset-执行就地更新">对 StatefulSet 执行就地更新</h3>
<p>另外, 你可以<a href="/zh/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources">就地更新</a> StatefulSet。</p>
<p>如果你的 StatefulSet 最初通过 <code>kubectl apply</code> 或 <code>kubectl create --save-config</code> 创建,
你可以更新 StatefulSet 清单中的 <code>.spec.replicas</code>, 然后执行命令 <code>kubectl apply</code>:</p>
<!--
```shell
kubectl apply -f <stateful-set-file-updated>
```

Otherwise, edit that field with `kubectl edit`:

```shell
kubectl edit statefulsets <stateful-set-name>
```

Or use `kubectl patch`:

```shell
kubectl patch statefulsets <stateful-set-name> -p '{"spec":{"replicas":<new-replicas>}}'
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f &lt;更新后的 statefulset 文件&gt;
</code></pre></div><p>否则，可以使用 <code>kubectl edit</code> 编辑副本字段：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit statefulsets &lt;statefulset 名称&gt;
</code></pre></div><p>或者使用 <code>kubectl patch</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl patch statefulsets &lt;statefulset 名称&gt; -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;replicas&#34;:&lt;new-replicas&gt;}}&#39;</span>
</code></pre></div><!--
## Troubleshooting

### Scaling down does not work right
-->
<h2 id="troubleshooting">故障排查 </h2>
<h3 id="缩容操作无法正常工作">缩容操作无法正常工作</h3>
<!--
You cannot scale down a StatefulSet when any of the stateful Pods it manages is unhealthy. Scaling down only takes place
after those stateful Pods become running and ready.

If spec.replicas > 1, Kubernetes cannot determine the reason for an unhealthy Pod. It might be the result of a permanent fault or of a transient fault. A transient fault can be caused by a restart required by upgrading or maintenance.
-->
<p>当 Stateful 所管理的任何 Pod 不健康时，你不能对该 StatefulSet 执行缩容操作。
仅当 StatefulSet 的所有 Pod 都处于运行状态和 Ready 状况后才可缩容.</p>
<p>如果 <code>spec.replicas</code> 大于 1，Kubernetes 无法判定 Pod 不健康的原因。
Pod 不健康可能是由于永久性故障造成也可能是瞬态故障。
瞬态故障可能是节点升级或维护而引起的节点重启造成的。</p>
<!--
If the Pod is unhealthy due to a permanent fault, scaling
without correcting the fault may lead to a state where the StatefulSet membership
drops below a certain minimum number of replicas that are needed to function
correctly. This may cause your StatefulSet to become unavailable.
-->
<p>如果该 Pod 不健康是由于永久性故障导致, 则在不纠正该故障的情况下进行缩容可能会导致
StatefulSet 进入一种状态，其成员 Pod 数量低于应正常运行的副本数。
这种状态也许会导致 StatefulSet 不可用。</p>
<!--
If the Pod is unhealthy due to a transient fault and the Pod might become available again,
the transient error may interfere with your scale-up or scale-down operation. Some distributed
databases have issues when nodes join and leave at the same time. It is better
to reason about scaling operations at the application level in these cases, and
perform scaling only when you are sure that your stateful application cluster is
completely healthy.
-->
<p>如果由于瞬态故障而导致 Pod 不健康并且 Pod 可能再次变为可用，那么瞬态错误可能会干扰
你对 StatefulSet 的扩容/缩容操作。 一些分布式数据库在同时有节点加入和离开时
会遇到问题。在这些情况下，最好是在应用级别进行分析扩缩操作的状态, 并且只有在确保
Stateful 应用的集群是完全健康时才执行扩缩操作。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [deleting a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).
-->
<ul>
<li>进一步了解<a href="/zh/docs/tasks/run-application/delete-stateful-set/">删除 StatefulSet</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ca3bc4e31dfe46d5044a3b93eb804ee9">8 - 运行 Jobs</h1>
    <div class="lead">使用并行处理运行 Jobs。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-964bdff888520740e5e221695245678d">8.1 - 使用 CronJob 运行自动化任务</h1>
    
	<!--
title: Running Automated Tasks with a CronJob
reviewers:
- chenopis
content_type: task
weight: 10
min-kubernetes-server-version: v1.21
-->
<!-- overview -->
<!--

CronJobs was promoted to general availability in Kubernetes v1.21. If you are using an older version of
Kubernetes, please refer to the documentation for the version of Kubernetes that you are using,
so that you see accurate information. Older Kubernetes versions do not support the `batch/v1` CronJob API.

You can use [CronJobs](/docs/concepts/workloads/controllers/cron-jobs) to run jobs on a time-based schedule.
These automated jobs run like [Cron](https://en.wikipedia.org/wiki/Cron) tasks on a Linux or UNIX system.

Cron jobs are useful for creating periodic and recurring tasks, like running backups or sending emails.
Cron jobs can also schedule individual tasks for a specific time, such as if you want to schedule a job for a low activity period.
-->
<p>在Kubernetes v1.21 版本中，CronJob 被提升为通用版本。如果你使用的是旧版本的 Kubernetes，请参考你正在使用的 Kubernetes 版本的文档，这样你就能看到准确的信息。旧的 Kubernetes 版本不支持<code>batch/v1</code> CronJob API。
你可以利用 <a href="/zh/docs/concepts/workloads/controllers/cron-jobs">CronJobs</a> 执行基于时间调度的任务。这些自动化任务和 Linux 或者 Unix 系统的 <a href="https://en.wikipedia.org/wiki/Cron">Cron</a> 任务类似。</p>
<p>CronJobs 在创建周期性以及重复性的任务时很有帮助，例如执行备份操作或者发送邮件。CronJobs 也可以在特定时间调度单个任务，例如你想调度低活跃周期的任务。</p>
<!--
Cron jobs have limitations and idiosyncrasies.
For example, in certain circumstances, a single cron job can create multiple jobs.
Therefore, jobs should be idempotent.
For more limitations, see [CronJobs](/docs/concepts/workloads/controllers/cron-jobs).
-->
<p>CronJobs 有一些限制和特点。
例如，在特定状况下，同一个 CronJob 可以创建多个任务。
因此，任务应该是幂等的。
查看更多限制，请参考 <a href="/zh/docs/concepts/workloads/controllers/cron-jobs">CronJobs</a>。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.21.
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!-- steps -->
<!--
## Creating a Cron Job

Cron jobs require a config file.
This example cron job config `.spec` file prints the current time and a hello message every minute:
-->
<h2 id="创建-cronjob">创建 CronJob</h2>
<p>CronJob 需要一个配置文件。
本例中 CronJob 的<code>.spec</code> 配置文件每分钟打印出当前时间和一个问好信息：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/cronjob.yaml" download="application/job/cronjob.yaml"><code>application/job/cronjob.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-cronjob-yaml')" title="Copy application/job/cronjob.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-cronjob-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronJob<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">schedule</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;*/1 * * * *&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">jobTemplate</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>IfNotPresent<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- /bin/sh<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- -c<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- date; echo Hello from the Kubernetes cluster<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Run the example cron job by downloading the example file and then running this command:
-->
<p>想要运行示例的 CronJob，可以下载示例文件并执行命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/application/job/cronjob.yaml
</code></pre></div><pre tabindex="0"><code>cronjob.batch/hello created
</code></pre><!--
After creating the cron job, get its status using this command:
-->
<p>创建好 CronJob 后，使用下面的命令来获取其状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get cronjob hello
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     0        50s             75s
</code></pre><!--
As you can see from the results of the command, the cron job has not scheduled or run any jobs yet.
Watch for the job to be created in around one minute:
-->
<p>就像你从命令返回结果看到的那样，CronJob 还没有调度或执行任何任务。大约需要一分钟任务才能创建好。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get <span style="color:#a2f">jobs</span> --watch
</code></pre></div><pre tabindex="0"><code>NAME               COMPLETIONS   DURATION   AGE
hello-4111706356   0/1                      0s
hello-4111706356   0/1           0s         0s
hello-4111706356   1/1           5s         5s
</code></pre><!--
Now you've seen one running job scheduled by the "hello" cron job.
You can stop watching the job and view the cron job again to see that it scheduled the job:
-->
<p>现在你已经看到了一个运行中的任务被 “hello” CronJob 调度。
你可以停止监视这个任务，然后再次查看 CronJob 就能看到它调度任务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get cronjob hello
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     0        50s             75s
</code></pre><!--
You should see that the cron job "hello" successfully scheduled a job at the time specified in `LAST-SCHEDULE`.
There are currently 0 active jobs, meaning that the job has completed or failed.

Now, find the pods that the last scheduled job created and view the standard output of one of the pods.
Note that the job name and pod name are different.
-->
<p>你应该能看到 “hello” CronJob 在 <code>LAST-SCHEDULE</code> 声明的时间点成功的调度了一次任务。
有 0 个活跃的任务意味着任务执行完毕或者执行失败。</p>
<p>现在，找到最后一次调度任务创建的 Pod 并查看一个 Pod 的标准输出。请注意任务名称和 Pod 名称是不同的。</p>
<!--
The job name and pod name are different.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> Job 名称和 Pod 名称不同。</div>
</blockquote>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在你的系统上将 &#34;hello-4111706356&#34; 替换为 Job 名称</span>
<span style="color:#b8860b">pods</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get pods --selector<span style="color:#666">=</span>job-name<span style="color:#666">=</span>hello-4111706356 --output<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">={</span>.items..metadata.name<span style="color:#666">}</span><span style="color:#a2f;font-weight:bold">)</span>
</code></pre></div><!--
Show pod log:
-->
<p>查看 Pod 日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs <span style="color:#b8860b">$pods</span>
</code></pre></div><pre tabindex="0"><code>Fri Feb 22 11:02:09 UTC 2019
Hello from the Kubernetes cluster
</code></pre><!--
## Deleting a Cron Job

When you don't need a cron job any more, delete it with `kubectl delete cronjob <cronjob name>`：
-->
<h2 id="删除-cronjob">删除 CronJob</h2>
<p>当你不再需要 CronJob 时，可以用 <code>kubectl delete cronjob &lt;cronjob name&gt;</code> 删掉它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete cronjob hello
</code></pre></div><!--
Deleting the cron job removes all the jobs and pods it created and stops it from creating additional jobs.
You can read more about removing jobs in [garbage collection](/docs/concepts/workloads/controllers/garbage-collection/).
-->
<p>删除 CronJob 会清除它创建的所有任务和 Pod，并阻止它创建额外的任务。你可以查阅
<a href="/zh/docs/concepts/workloads/controllers/garbage-collection/">垃圾收集</a>。</p>
<!--
## Writing a Cron Job Spec

As with all other Kubernetes configs, a cron job needs `apiVersion`, `kind`, and `metadata` fields. For general
information about working with config files, see [deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),
and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.

A cron job config also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
-->
<h2 id="编写-cronjob-声明信息">编写 CronJob 声明信息</h2>
<p>像 Kubernetes 的其他配置一样，CronJob 需要 <code>apiVersion</code>、<code>kind</code>、和 <code>metadata</code> 域。
配置文件的一般信息，请参考
<a href="/zh/docs/tasks/run-application/run-stateless-application-deployment/">部署应用</a> 和
<a href="/zh/docs/concepts/overview/working-with-objects/object-management/">使用 kubectl 管理资源</a>.</p>
<p>CronJob 配置也需要包括
<a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status"><code>.spec</code></a>.</p>
<!--
All modifications to a cron job, especially its `.spec`, are applied only to the following runs.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 对 CronJob 的所有改动，特别是它的 <code>.spec</code>，只会影响将来的运行实例。</div>
</blockquote>
<!--
### Schedule

The `.spec.schedule` is a required field of the `.spec`.
It takes a [Cron](https://en.wikipedia.org/wiki/Cron) format string, such as `0 * * * *` or `@hourly`, as schedule time of its jobs to be created and executed.
-->
<h3 id="时间安排">时间安排</h3>
<p><code>.spec.schedule</code> 是 <code>.spec</code> 需要的域。它使用了 <a href="https://en.wikipedia.org/wiki/Cron">Cron</a>
格式串，例如 <code>0 * * * *</code> or <code>@hourly</code> ，作为它的任务被创建和执行的调度时间。</p>
<!--
The format also includes extended "Vixie cron" step values. As explained in the [FreeBSD manual](https://www.freebsd.org/cgi/man.cgi?crontab%285%29):
-->
<p>该格式也包含了扩展的 &quot;Vixie cron&quot; 步长值。
<a href="https://www.freebsd.org/cgi/man.cgi?crontab%285%29">FreeBSD 手册</a>中解释如下:</p>
<!--
> Step values can be	used in	conjunction with ranges.  Following a range
> with `/<number>` specifies skips	of the number's	value through the
> range.  For example, `0-23/2` can be used in the	hours field to specify
> command execution every other hour	(the alternative in the	V7 standard is
> `0,2,4,6,8,10,12,14,16,18,20,22`).  Steps are also permitted after an
> asterisk, so if you want to say "every two hours", just use `*/2`.
-->
<blockquote>
<p>步长可被用于范围组合。范围后面带有 <code>/&lt;数字&gt;</code> 可以声明范围内的步幅数值。
例如，<code>0-23/2</code> 可被用在小时域来声明命令在其他数值的小时数执行
（ V7 标准中对应的方法是<code>0,2,4,6,8,10,12,14,16,18,20,22</code>）。
步长也可以放在通配符后面，因此如果你想表达 &quot;每两小时&quot;，就用 <code>*/2</code> 。</p>
</blockquote>
<!--
A question mark (`?`) in the schedule has the same meaning as an asterisk `*`, that is, it stands for any of available value for a given field.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 调度中的问号 (<code>?</code>) 和星号  <code>*</code> 含义相同，表示给定域的任何可用值。</div>
</blockquote>
<!--
### Job Template

The `.spec.jobTemplate` is the template for the job, and it is required.
It has exactly the same schema as a [Job](/docs/concepts/workloads/controllers/job/), 
except that it is nested and does not have an `apiVersion` or `kind`.
For information about writing a job `.spec`, see
[Writing a Job Spec](/docs/concepts/workloads/controllers/job/#writing-a-job-spec).
-->
<h3 id="任务模版">任务模版</h3>
<p><code>.spec.jobTemplate</code>是任务的模版，它是必须的。它和
<a href="/zh/docs/concepts/workloads/controllers/job/">Job</a>的语法完全一样，
除了它是嵌套的没有 <code>apiVersion</code> 和 <code>kind</code>。
编写任务的 <code>.spec</code> ，请参考
<a href="/zh/docs/concepts/workloads/controllers/job/#writing-a-job-spec">编写 Job 的Spec</a>。</p>
<!--
### Starting Deadline

The `.spec.startingDeadlineSeconds` field is optional.
It stands for the deadline in seconds for starting the job if it misses its scheduled time for any reason.
After the deadline, the cron job does not start the job.
Jobs that do not meet their deadline in this way count as failed jobs.
If this field is not specified, the jobs have no deadline.
-->
<h3 id="starting-deadline">开始的最后期限  </h3>
<p><code>.spec.startingDeadlineSeconds</code> 域是可选的。
它表示任务如果由于某种原因错过了调度时间，开始该任务的截止时间的秒数。过了截止时间，CronJob 就不会开始任务。
不满足这种最后期限的任务会被统计为失败任务。如果该域没有声明，那任务就没有最后期限。</p>
<!--
If the `.spec.startingDeadlineSeconds` field is set (not null), the CronJob
controller measures the time between when a job is expected to be created and
now. If the difference is higher than that limit, it will skip this execution.

For example, if it is set to `200`, it allows a job to be created for up to 200
seconds after the actual schedule.
-->
<p>如果<code>.spec.startingDeadlineSeconds</code>字段被设置(非空)，CronJob 控制器会计算从预期创建 Job 到当前时间的时间差。
如果时间差大于该限制，则跳过此次执行。</p>
<p>例如，如果将其设置为 <code>200</code>，则 Job 控制器允许在实际调度之后最多 200 秒内创建 Job。</p>
<!--
### Concurrency Policy

The `.spec.concurrencyPolicy` field is also optional.
It specifies how to treat concurrent executions of a job that is created by this cron job.
the spec may specify only one of the following concurrency policies:

* `Allow` (default): The cron job allows concurrently running jobs
* `Forbid`: The cron job does not allow concurrent runs; if it is time for a new job run and the previous job run hasn't finished yet, the cron job skips the new job run
* `Replace`: If it is time for a new job run and the previous job run hasn't finished yet, the cron job replaces the currently running job run with a new job run

Note that concurrency policy only applies to the jobs created by the same cron job.
If there are multiple cron jobs, their respective jobs are always allowed to run concurrently.
-->
<h3 id="并发性规则">并发性规则</h3>
<p><code>.spec.concurrencyPolicy</code> 也是可选的。它声明了 CronJob 创建的任务执行时发生重叠如何处理。
spec 仅能声明下列规则中的一种：</p>
<ul>
<li><code>Allow</code> (默认)：CronJob 允许并发任务执行。</li>
<li><code>Forbid</code>： CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行。</li>
<li><code>Replace</code>：如果新任务的执行时间到了而老任务没有执行完，CronJob 会用新任务替换当前正在运行的任务。</li>
</ul>
<p>请注意，并发性规则仅适用于相同 CronJob 创建的任务。如果有多个 CronJob，它们相应的任务总是允许并发执行的。</p>
<!--
### Suspend

The `.spec.suspend` field is also optional.
If it is set to `true`, all subsequent executions are suspended.
This setting does not apply to already started executions.
Defaults to false.
-->
<h3 id="挂起">挂起</h3>
<p><code>.spec.suspend</code>域也是可选的。如果设置为 <code>true</code> ，后续发生的执行都会挂起。
这个设置对已经开始的执行不起作用。默认是关闭的。</p>
<!--
Executions that are suspended during their scheduled time count as missed jobs.
When `.spec.suspend` changes from `true` to `false` on an existing cron job without a [starting deadline](#starting-deadline), the missed jobs are scheduled immediately.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 在调度时间内挂起的执行都会被统计为错过的任务。当 <code>.spec.suspend</code> 从 <code>true</code> 改为 <code>false</code> 时，
且没有 <a href="#starting-deadline">开始的最后期限</a>，错过的任务会被立即调度。</div>
</blockquote>

<!--
### Jobs History Limits

The `.spec.successfulJobsHistoryLimit` and `.spec.failedJobsHistoryLimit` fields are optional.
These fields specify how many completed and failed jobs should be kept.
By default, they are set to 3 and 1 respectively.  Setting a limit to `0` corresponds to keeping none of the corresponding kind of jobs after they finish.
-->
<h3 id="任务历史限制">任务历史限制</h3>
<p><code>.spec.successfulJobsHistoryLimit</code> 和 <code>.spec.failedJobsHistoryLimit</code>是可选的。
这两个字段指定应保留多少已完成和失败的任务。
默认设置为3和1。限制设置为0代表相应类型的任务完成后不会保留。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1058efa4d70f13c015e6a2094ff85068">8.2 - 使用工作队列进行粗粒度并行处理</h1>
    
	<!--
---
title: Coarse Parallel Processing Using a Work Queue
min-kubernetes-server-version: v1.8
content_type: task
weight: 20
---
-->
<!-- overview -->
<!--
In this example, we will run a Kubernetes Job with multiple parallel
worker processes.

In this example, as each pod is created, it picks up one unit of work
from a task queue, completes it, deletes it from the queue, and exits.

Here is an overview of the steps in this example:

1. **Start a message queue service.**  In this example, we use RabbitMQ, but you could use another
  one.  In practice you would set up a message queue service once and reuse it for many jobs.
1. **Create a queue, and fill it with messages.**  Each message represents one task to be done.  In
   this example, a message is an integer that we will do a lengthy computation on.
1. **Start a Job that works on tasks from the queue**.  The Job starts several pods.  Each pod takes
  one task from the message queue, processes it, and repeats until the end of the queue is reached.
-->
<p>本例中，我们会运行包含多个并行工作进程的 Kubernetes Job。</p>
<p>本例中，每个 Pod 一旦被创建，会立即从任务队列中取走一个工作单元并完成它，然后将工作单元从队列中删除后再退出。</p>
<p>下面是本次示例的主要步骤：</p>
<ol>
<li>
<p><strong>启动一个消息队列服务</strong>  本例中，我们使用 RabbitMQ，你也可以用其他的消息队列服务。在实际工作环境中，你可以创建一次消息队列服务然后在多个任务中重复使用。</p>
</li>
<li>
<p><strong>创建一个队列，放上消息数据</strong>  每个消息表示一个要执行的任务。本例中，每个消息是一个整数值。我们将基于这个整数值执行很长的计算操作。</p>
</li>
<li>
<p><strong>启动一个在队列中执行这些任务的 Job</strong>。该 Job 启动多个 Pod。每个 Pod 从消息队列中取走一个任务，处理它，然后重复执行，直到队列的队尾。</p>
</li>
</ol>
<h2 id="准备开始">准备开始</h2>
<!--
Be familiar with the basic,
non-parallel, use of [Job](/docs/concepts/jobs/run-to-completion-finite-workloads/).
-->
<p>要熟悉 Job 基本用法（非并行的），请参考
<a href="/zh/docs/concepts/workloads/controllers/job/">Job</a>。</p>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.8.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Starting a message queue service

This example uses RabbitMQ, however, you can adapt the example to use another AMQP-type message service.

In practice you could set up a message queue service once in a
cluster and reuse it for many jobs, as well as for long-running services.

Start RabbitMQ as follows:
-->
<h2 id="启动消息队列服务">启动消息队列服务</h2>
<p>本例使用了 RabbitMQ，但你可以更改该示例，使用其他 AMQP 类型的消息服务。</p>
<p>在实际工作中，在集群中一次性部署某个消息队列服务，之后在很多 Job 中复用，包括需要长期运行的服务。</p>
<p>按下面的方法启动 RabbitMQ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.3/examples/celery-rabbitmq/rabbitmq-service.yaml
</code></pre></div><pre tabindex="0"><code>service &quot;rabbitmq-service&quot; created
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.3/examples/celery-rabbitmq/rabbitmq-controller.yaml
</code></pre></div><pre tabindex="0"><code>replicationcontroller &quot;rabbitmq-controller&quot; created
</code></pre><!--
We will only use the rabbitmq part from the [celery-rabbitmq example](https://github.com/kubernetes/kubernetes/tree/release-1.3/examples/celery-rabbitmq).
-->
<p>我们仅用到 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.3/examples/celery-rabbitmq">celery-rabbitmq 示例</a> 中描述的部分功能。</p>
<!--
## Testing the message queue service

Now, we can experiment with accessing the message queue.  We will
create a temporary interactive pod, install some tools on it,
and experiment with queues.

First create a temporary interactive Pod.
-->
<h2 id="测试消息队列服务">测试消息队列服务</h2>
<p>现在，我们可以试着访问消息队列。我们将会创建一个临时的可交互的 Pod，在它上面安装一些工具，然后用队列做实验。</p>
<p>首先创建一个临时的可交互的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 创建一个临时的可交互的 Pod</span>
kubectl run -i --tty temp --image ubuntu:14.04
</code></pre></div><pre tabindex="0"><code>Waiting for pod default/temp-loe07 to be running, status is Pending, pod ready: false
... [ previous line repeats several times .. hit return when it stops ] ...
</code></pre><!--
Note that your pod name and command prompt will be different.

Next install the `amqp-tools` so we can work with message queues.
-->
<p>请注意你的 Pod 名称和命令提示符将会不同。</p>
<p>接下来安装 <code>amqp-tools</code> ，这样我们就能用消息队列了。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 安装一些工具</span>
root@temp-loe07:/# apt-get update
.... <span style="color:#666">[</span> lots of output <span style="color:#666">]</span> ....
root@temp-loe07:/# apt-get install -y curl ca-certificates amqp-tools python dnsutils
.... <span style="color:#666">[</span> lots of output <span style="color:#666">]</span> ....
</code></pre></div><!--
Later, we will make a docker image that includes these packages.

Next, we will check that we can discover the rabbitmq service:
-->
<p>后续，我们将制作一个包含这些包的 Docker 镜像。</p>
<p>接着，我们将要验证我们发现 RabbitMQ 服务：</p>
<!--
# Note the rabbitmq-service has a DNS name, provided by Kubernetes:
-->
<pre tabindex="0"><code># 请注意 rabbitmq-service 有Kubernetes 提供的 DNS 名称，

root@temp-loe07:/# nslookup rabbitmq-service
Server:        10.0.0.10
Address:    10.0.0.10#53

Name:    rabbitmq-service.default.svc.cluster.local
Address: 10.0.147.152

# 你的 IP 地址会不同
</code></pre><!--
If Kube-DNS is not setup correctly, the previous step may not work for you.
You can also find the service IP in an env var:
-->
<p>如果 Kube-DNS 没有正确安装，上一步可能会出错。
你也可以在环境变量中找到服务 IP。</p>
<!--
# Your address will vary.
-->
<pre tabindex="0"><code># env | grep RABBIT | grep HOST
RABBITMQ_SERVICE_SERVICE_HOST=10.0.147.152

# 你的 IP 地址会有所不同
</code></pre><!--
Next we will verify we can create a queue, and publish and consume messages.
-->
<p>接着我们将要确认可以创建队列，并能发布消息和消费消息。</p>
<!--
# In the next line, rabbitmq-service is the hostname where the rabbitmq-service
# can be reached.  5672 is the standard port for rabbitmq.

# If you could not resolve "rabbitmq-service" in the previous step,
# then use this command instead:
# root@temp-loe07:/# BROKER_URL=amqp://guest:guest@$RABBITMQ_SERVICE_SERVICE_HOST:5672
# Now create a queue:
# and publish a message to it:
# and get it back.
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 下一行，rabbitmq-service 是访问 rabbitmq-service 的主机名。5672是 rabbitmq 的标准端口。</span>

root@temp-loe07:/# <span style="color:#a2f">export</span> <span style="color:#b8860b">BROKER_URL</span><span style="color:#666">=</span>amqp://guest:guest@rabbitmq-service:5672

<span style="color:#080;font-style:italic"># 如果上一步中你不能解析 &#34;rabbitmq-service&#34;，可以用下面的命令替换：</span>
<span style="color:#080;font-style:italic"># root@temp-loe07:/# BROKER_URL=amqp://guest:guest@$RABBITMQ_SERVICE_SERVICE_HOST:5672</span>

<span style="color:#080;font-style:italic"># 现在创建队列：</span>

root@temp-loe07:/# /usr/bin/amqp-declare-queue --url<span style="color:#666">=</span><span style="color:#b8860b">$BROKER_URL</span> -q foo -d foo

<span style="color:#080;font-style:italic"># 向它推送一条消息:</span>

root@temp-loe07:/# /usr/bin/amqp-publish --url<span style="color:#666">=</span><span style="color:#b8860b">$BROKER_URL</span> -r foo -p -b Hello

<span style="color:#080;font-style:italic"># 然后取回它.</span>

root@temp-loe07:/# /usr/bin/amqp-consume --url<span style="color:#666">=</span><span style="color:#b8860b">$BROKER_URL</span> -q foo -c <span style="color:#666">1</span> cat <span style="color:#666">&amp;&amp;</span> <span style="color:#a2f">echo</span>
Hello
root@temp-loe07:/#
</code></pre></div><!--
In the last command, the `amqp-consume` tool takes one message (`-c 1`)
from the queue, and passes that message to the standard input of an arbitrary command.  In this case, the program `cat` prints out the characters read from standard input, and the echo adds a carriage
return so the example is readable.
-->
<p>最后一个命令中， <code>amqp-consume</code> 工具从队列中取走了一个消息，并把该消息传递给了随机命令的标准输出。
在这种情况下，<code>cat</code> 会打印它从标准输入中读取的字符，echo 会添加回车符以便示例可读。</p>
<!--
## Filling the Queue with tasks

Now let's fill the queue with some "tasks".  In our example, our tasks are strings to be
printed.

In a practice, the content of the messages might be:

- names of files to that need to be processed
- extra flags to the program
- ranges of keys in a database table
- configuration parameters to a simulation
- frame numbers of a scene to be rendered
-->
<h2 id="为队列增加任务">为队列增加任务</h2>
<p>现在让我们给队列增加一些任务。在我们的示例中，任务是多个待打印的字符串。</p>
<p>实践中，消息的内容可以是：</p>
<ul>
<li>待处理的文件名</li>
<li>程序额外的参数</li>
<li>数据库表的关键字范围</li>
<li>模拟任务的配置参数</li>
<li>待渲染的场景的帧序列号</li>
</ul>
<!--
In practice, if there is large data that is needed in a read-only mode by all pods
of the Job, you will typically put that in a shared file system like NFS and mount
that readonly on all the pods, or the program in the pod will natively read data from
a cluster file system like HDFS.

For our example, we will create the queue and fill it using the amqp command line tools.
In practice, you might write a program to fill the queue using an amqp client library.
-->
<p>本例中，如果有大量的数据需要被 Job 的所有 Pod 读取，典型的做法是把它们放在一个共享文件系统中，如NFS，并以只读的方式挂载到所有 Pod，或者 Pod 中的程序从类似 HDFS 的集群文件系统中读取。</p>
<p>例如，我们创建队列并使用 amqp 命令行工具向队列中填充消息。实践中，你可以写个程序来利用 amqp 客户端库来填充这些队列。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">/usr/bin/amqp-declare-queue --url<span style="color:#666">=</span><span style="color:#b8860b">$BROKER_URL</span> -q job1  -d job1

<span style="color:#a2f;font-weight:bold">for</span> f in apple banana cherry date fig grape lemon melon 
<span style="color:#a2f;font-weight:bold">do</span>
  /usr/bin/amqp-publish --url<span style="color:#666">=</span><span style="color:#b8860b">$BROKER_URL</span> -r job1 -p -b <span style="color:#b8860b">$f</span>
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
So, we filled the queue with 8 messages.

## Create an Image

Now we are ready to create an image that we will run as a job.

We will use the `amqp-consume` utility to read the message
from the queue and run our actual program.  Here is a very simple
example program:
-->
<p>这样，我们给队列中填充了8个消息。</p>
<h2 id="创建镜像">创建镜像</h2>
<p>现在我们可以创建一个做为 Job 来运行的镜像。</p>
<p>我们将用 <code>amqp-consume</code> 来从队列中读取消息并实际运行我们的程序。这里给出一个非常简单的示例程序：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/rabbitmq/worker.py" download="application/job/rabbitmq/worker.py"><code>application/job/rabbitmq/worker.py</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-rabbitmq-worker-py')" title="Copy application/job/rabbitmq/worker.py to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-rabbitmq-worker-py">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic">#!/usr/bin/env python</span>

<span style="color:#080;font-style:italic"># Just prints standard out and sleeps for 10 seconds.</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">sys</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">time</span>
<span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Processing &#34;</span> <span style="color:#666">+</span> sys<span style="color:#666">.</span>stdin<span style="color:#666">.</span>readlines()[<span style="color:#666">0</span>])
time<span style="color:#666">.</span>sleep(<span style="color:#666">10</span>)
</code></pre></div>
    </div>
</div>


<!--
Now, build an image.  If you are working in the source
tree, then change directory to `examples/job/work-queue-1`.
Otherwise, make a temporary directory, change to it,
download the [Dockerfile](/examples/application/job/rabbitmq/Dockerfile),
and [worker.py](/examples/application/job/rabbitmq/worker.py).  In either case,
build the image with this command:
-->
<p>现在，编译镜像。如果你在用源代码树，那么切换到目录 <code>examples/job/work-queue-1</code>。
否则的话，创建一个临时目录，切换到这个目录。下载
<a href="/examples/application/job/rabbitmq/Dockerfile">Dockerfile</a>，和
<a href="/examples/application/job/rabbitmq/worker.py">worker.py</a>。
无论哪种情况，都可以用下面的命令编译镜像</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker build -t job-wq-1 .
</code></pre></div><!--
For the [Docker Hub](https://hub.docker.com/), tag your app image with
your username and push to the Hub with the below commands. Replace
`<username>` with your Hub username.
-->
<p>对于 <a href="https://hub.docker.com/">Docker Hub</a>, 给你的应用镜像打上标签，
标签为你的用户名，然后用下面的命令推送到 Hub。用你的 Hub 用户名替换 <code>&lt;username&gt;</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker tag job-wq-1 &lt;username&gt;/job-wq-1
docker push &lt;username&gt;/job-wq-1
</code></pre></div><!--
If you are using [Google Container
Registry](https://cloud.google.com/tools/container-registry/), tag
your app image with your project ID, and push to GCR. Replace
`<project>` with your project ID.
-->
<p>如果你在用<a href="https://cloud.google.com/tools/container-registry/">谷歌容器仓库</a>，
用你的项目 ID 作为标签打到你的应用镜像上，然后推送到 GCR。
用你的项目 ID 替换 <code>&lt;project&gt;</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker tag job-wq-1 gcr.io/&lt;project&gt;/job-wq-1
gcloud docker -- push gcr.io/&lt;project&gt;/job-wq-1
</code></pre></div><!--
## Defining a Job

Here is a job definition.  You'll need to make a copy of the Job and edit the
image to match the name you used, and call it `./job.yaml`.
-->
<h2 id="定义-job">定义 Job</h2>
<p>这里给出一个 Job 定义 yaml文件。你需要拷贝一份并编辑镜像以匹配你使用的名称，保存为 <code>./job.yaml</code>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/rabbitmq/job.yaml" download="application/job/rabbitmq/job.yaml"><code>application/job/rabbitmq/job.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-rabbitmq-job-yaml')" title="Copy application/job/rabbitmq/job.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-rabbitmq-job-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>job-wq-1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">8</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>job-wq-1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>c<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/&lt;project&gt;/job-wq-1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>BROKER_URL<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>amqp://guest:guest@rabbitmq-service:5672<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>QUEUE<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>job1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In this example, each pod works on one item from the queue and then exits.
So, the completion count of the Job corresponds to the number of work items
done.  So we set, `.spec.completions: 8` for the example, since we put 8 items in the queue.

## Running the Job

So, now run the Job:
-->
<p>本例中，每个 Pod 使用队列中的一个消息然后退出。这样，Job 的完成计数就代表了完成的工作项的数量。本例中我们设置 <code>.spec.completions: 8</code>，因为我们放了8项内容在队列中。</p>
<h2 id="运行-job">运行 Job</h2>
<p>现在我们运行 Job：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./job.yaml
</code></pre></div><!--
Now wait a bit, then check on the job.
-->
<p>稍等片刻，然后检查 Job。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe jobs/job-wq-1
</code></pre></div><pre tabindex="0"><code>Name:             job-wq-1
Namespace:        default
Selector:         controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f
Labels:           controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f
                  job-name=job-wq-1
Annotations:      &lt;none&gt;
Parallelism:      2
Completions:      8
Start Time:       Wed, 06 Sep 2017 16:42:02 +0800
Pods Statuses:    0 Running / 8 Succeeded / 0 Failed
Pod Template:
  Labels:       controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f
                job-name=job-wq-1
  Containers:
   c:
    Image:      gcr.io/causal-jigsaw-637/job-wq-1
    Port:
    Environment:
      BROKER_URL:       amqp://guest:guest@rabbitmq-service:5672
      QUEUE:            job1
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen  LastSeen   Count    From    SubobjectPath    Type      Reason              Message
  ─────────  ────────   ─────    ────    ─────────────    ──────    ──────              ───────
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-hcobb
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-weytj
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-qaam5
  27s        27s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-b67sr
  26s        26s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-xe5hj
  15s        15s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-w2zqe
  14s        14s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-d6ppa
  14s        14s        1        {job }                   Normal    SuccessfulCreate    Created pod: job-wq-1-p17e0
</code></pre><!--
All our pods succeeded.  Yay.
-->
<p>我们所有的 Pod 都成功了。耶！</p>
<!-- discussion -->
<!--
## Alternatives

This approach has the advantage that you
do not need to modify your "worker" program to be aware that there is a work queue.

It does require that you run a message queue service.
If running a queue service is inconvenient, you may
want to consider one of the other [job patterns](/docs/concepts/jobs/run-to-completion-finite-workloads/#job-patterns).
-->
<h2 id="替代方案">替代方案</h2>
<p>本文所讲述的处理方法的好处是你不需要修改你的 &quot;worker&quot; 程序使其知道工作队列的存在。</p>
<p>本文所描述的方法需要你运行一个消息队列服务。如果不方便运行消息队列服务，你也许会考虑另外一种
<a href="/zh/docs/concepts/workloads/controllers/job/#job-patterns">任务模式</a>。</p>
<!--
This approach creates a pod for every work item.  If your work items only take a few seconds,
though, creating a Pod for every work item may add a lot of overhead.  Consider another
[example](/docs/tasks/job/fine-parallel-processing-work-queue/), that executes multiple work items per Pod.

In this example, we used use the `amqp-consume` utility to read the message
from the queue and run our actual program.  This has the advantage that you
do not need to modify your program to be aware of the queue.
A [different example](/docs/tasks/job/fine-parallel-processing-work-queue/), shows how to
communicate with the work queue using a client library.
-->
<p>本文所述的方法为每个工作项创建了一个 Pod。
如果你的工作项仅需数秒钟，为每个工作项创建 Pod会增加很多的常规消耗。
可以考虑另外的方案请参考<a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">示例</a>，
这种方案可以实现每个 Pod 执行多个工作项。</p>
<p>示例中，我们使用 <code>amqp-consume</code> 从消息队列读取消息并执行我们真正的程序。
这样的好处是你不需要修改你的程序使其知道队列的存在。
要了解怎样使用客户端库和工作队列通信，请参考
<a href="/zh/docs/tasks/job/fine-parallel-processing-work-queue/">不同的示例</a>。</p>
<!--
## Caveats

If the number of completions is set to less than the number of items in the queue, then
not all items will be processed.

If the number of completions is set to more than the number of items in the queue,
then the Job will not appear to be completed, even though all items in the queue
have been processed.  It will start additional pods which will block waiting
for a message.

There is an unlikely race with this pattern.  If the container is killed in between the time
that the message is acknowledged by the amqp-consume command and the time that the container
exits with success, or if the node crashes before the kubelet is able to post the success of the pod
back to the api-server, then the Job will not appear to be complete, even though all items
in the queue have been processed.
-->
<h2 id="友情提醒">友情提醒</h2>
<p>如果设置的完成数量小于队列中的消息数量，会导致一部分消息项不会被执行。</p>
<p>如果设置的完成数量大于队列中的消息数量，当队列中所有的消息都处理完成后，
Job 也会显示为未完成。Job 将创建 Pod 并阻塞等待消息输入。</p>
<p>当发生下面两种情况时，即使队列中所有的消息都处理完了，Job 也不会显示为完成状态：</p>
<ul>
<li>在 amqp-consume 命令拿到消息和容器成功退出之间的时间段内，执行杀死容器操作；</li>
<li>在 kubelet 向 api-server 传回 Pod 成功运行之前，发生节点崩溃。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-457c9dd93aed2b05615ed28dc38075d3">8.3 - 使用工作队列进行精细的并行处理</h1>
    
	<!--
title: Fine Parallel Processing Using a Work Queue
content_type: task
weight: 30
min-kubernetes-server-version: v1.8
-->
<!-- overview -->
<!--
In this example, we will run a Kubernetes Job with multiple parallel
worker processes in a given pod.
-->
<p>在这个例子中，我们会运行一个Kubernetes Job，其中的 Pod 会运行多个并行工作进程。</p>
<!--
In this example, as each pod is created, it picks up one unit of work
from a task queue, processes it, and repeats until the end of the queue is reached.

Here is an overview of the steps in this example:
-->
<p>在这个例子中，当每个pod被创建时，它会从一个任务队列中获取一个工作单元，处理它，然后重复，直到到达队列的尾部。</p>
<p>下面是这个示例的步骤概述：</p>
<!--
1. **Start a storage service to hold the work queue.**  In this example, we use Redis to store
  our work items.  In the previous example, we used RabbitMQ.  In this example, we use Redis and
  a custom work-queue client library because AMQP does not provide a good way for clients to
  detect when a finite-length work queue is empty.  In practice you would set up a store such
  as Redis once and reuse it for the work queues of many jobs, and other things.
-->
<ol>
<li><strong>启动存储服务用于保存工作队列。</strong> 在这个例子中，我们使用 Redis 来存储工作项。
在上一个例子中，我们使用了 RabbitMQ。
在这个例子中，由于 AMQP 不能为客户端提供一个良好的方法来检测一个有限长度的工作队列是否为空，
我们使用了 Redis 和一个自定义的工作队列客户端库。
在实践中，你可能会设置一个类似于 Redis 的存储库，并将其同时用于多项任务或其他事务的工作队列。</li>
</ol>
<!--
1. **Create a queue, and fill it with messages.**  Each message represents one task to be done.  In
   this example, a message is an integer that we will do a lengthy computation on.
-->
<ol start="2">
<li><strong>创建一个队列，然后向其中填充消息。</strong> 每个消息表示一个将要被处理的工作任务。
在这个例子中，消息是一个我们将用于进行长度计算的整数。</li>
</ol>
<!--
1. **Start a Job that works on tasks from the queue**.  The Job starts several pods.  Each pod takes
  one task from the message queue, processes it, and repeats until the end of the queue is reached.
-->
<ol start="3">
<li><strong>启动一个 Job 对队列中的任务进行处理</strong>。这个 Job 启动了若干个 Pod 。
每个 Pod 从消息队列中取出一个工作任务，处理它，然后重复，直到到达队列的尾部。</li>
</ol>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.8.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
Be familiar with the basic,
non-parallel, use of [Job](/docs/concepts/workloads/controllers/job/).
-->
<p>熟悉基本的、非并行的 <a href="/zh/docs/concepts/workloads/controllers/job/">Job</a>。</p>
<!-- steps -->
<!--
## Starting Redis

For this example, for simplicity, we will start a single instance of Redis.
See the [Redis Example](https://github.com/kubernetes/examples/tree/master/guestbook) for an example
of deploying Redis scalably and redundantly.
-->
<h2 id="启动-redis">启动 Redis</h2>
<p>对于这个例子，为了简单起见，我们将启动一个单实例的 Redis。
了解如何部署一个可伸缩、高可用的 Redis 例子，请查看
<a href="https://github.com/kubernetes/examples/tree/master/guestbook">Redis 示例</a></p>
<!--
You could also download the following files directly:
-->
<p>你也可以直接下载如下文件：</p>
<ul>
<li><a href="/examples/application/job/redis/redis-pod.yaml"><code>redis-pod.yaml</code></a></li>
<li><a href="/examples/application/job/redis/redis-service.yaml"><code>redis-service.yaml</code></a></li>
<li><a href="/examples/application/job/redis/Dockerfile"><code>Dockerfile</code></a></li>
<li><a href="/examples/application/job/redis/job.yaml"><code>job.yaml</code></a></li>
<li><a href="/examples/application/job/redis/rediswq.py"><code>rediswq.py</code></a></li>
<li><a href="/examples/application/job/redis/worker.py"><code>worker.py</code></a></li>
</ul>
<!--
## Filling the Queue with tasks

Now let's fill the queue with some "tasks".  In our example, our tasks are strings to be
printed.

Start a temporary interactive pod for running the Redis CLI.
-->
<h2 id="使用任务填充队列">使用任务填充队列</h2>
<p>现在，让我们往队列里添加一些“任务”。在这个例子中，我们的任务是一些将被打印出来的字符串。</p>
<p>启动一个临时的可交互的 pod 用于运行 Redis 命令行界面。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run -i --tty temp --image redis --command <span style="color:#b44">&#34;/bin/sh&#34;</span>
</code></pre></div><pre tabindex="0"><code>Waiting for pod default/redis2-c7h78 to be running, status is Pending, pod ready: false
Hit enter for command prompt
</code></pre><!--
Now hit enter, start the redis CLI, and create a list with some work items in it.
-->
<p>现在按回车键，启动 redis 命令行界面，然后创建一个存在若干个工作项的列表。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># redis-cli -h redis</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;apple&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">1</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;banana&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">2</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;cherry&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">3</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;date&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">4</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;fig&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">5</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;grape&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">6</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;lemon&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">7</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;melon&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">8</span>
redis:6379&gt; rpush job2 <span style="color:#b44">&#34;orange&#34;</span>
<span style="color:#666">(</span>integer<span style="color:#666">)</span> <span style="color:#666">9</span>
redis:6379&gt; lrange job2 <span style="color:#666">0</span> -1
1<span style="color:#666">)</span> <span style="color:#b44">&#34;apple&#34;</span>
2<span style="color:#666">)</span> <span style="color:#b44">&#34;banana&#34;</span>
3<span style="color:#666">)</span> <span style="color:#b44">&#34;cherry&#34;</span>
4<span style="color:#666">)</span> <span style="color:#b44">&#34;date&#34;</span>
5<span style="color:#666">)</span> <span style="color:#b44">&#34;fig&#34;</span>
6<span style="color:#666">)</span> <span style="color:#b44">&#34;grape&#34;</span>
7<span style="color:#666">)</span> <span style="color:#b44">&#34;lemon&#34;</span>
8<span style="color:#666">)</span> <span style="color:#b44">&#34;melon&#34;</span>
9<span style="color:#666">)</span> <span style="color:#b44">&#34;orange&#34;</span>
</code></pre></div><!--
So, the list with key `job2` will be our work queue.
-->
<p>因此，这个键为 <code>job2</code> 的列表就是我们的工作队列。</p>
<!--
Note: if you do not have Kube DNS setup correctly, you may need to change
the first step of the above block to `redis-cli -h $REDIS_SERVICE_HOST`.
-->
<p>注意：如果你还没有正确地配置 Kube DNS，你可能需要将上面的第一步改为
<code>redis-cli -h $REDIS_SERVICE_HOST</code>。</p>
<!--
## Create an Image

Now we are ready to create an image that we will run.

We will use a python worker program with a redis client to read
the messages from the message queue.

A simple Redis work queue client library is provided,
called rediswq.py ([Download](/examples/application/job/redis/rediswq.py)).
-->
<h2 id="创建镜像">创建镜像</h2>
<p>现在我们已经准备好创建一个我们要运行的镜像</p>
<p>我们会使用一个带有 redis 客户端的 python 工作程序从消息队列中读出消息。</p>
<p>这里提供了一个简单的 Redis 工作队列客户端库，叫 rediswq.py (<a href="/examples/application/job/redis/rediswq.py">下载</a>)。</p>
<!--
The "worker" program in each Pod of the Job uses the work queue
client library to get work.  Here it is:
-->
<p>Job 中每个 Pod 内的 “工作程序” 使用工作队列客户端库获取工作。如下：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/redis/worker.py" download="application/job/redis/worker.py"><code>application/job/redis/worker.py</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-redis-worker-py')" title="Copy application/job/redis/worker.py to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-redis-worker-py">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic">#!/usr/bin/env python</span>

<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">time</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">rediswq</span>

host<span style="color:#666">=</span><span style="color:#b44">&#34;redis&#34;</span>
<span style="color:#080;font-style:italic"># Uncomment next two lines if you do not have Kube-DNS working.</span>
<span style="color:#080;font-style:italic"># import os</span>
<span style="color:#080;font-style:italic"># host = os.getenv(&#34;REDIS_SERVICE_HOST&#34;)</span>

q <span style="color:#666">=</span> rediswq<span style="color:#666">.</span>RedisWQ(name<span style="color:#666">=</span><span style="color:#b44">&#34;job2&#34;</span>, host<span style="color:#666">=</span>host)
<span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Worker with sessionID: &#34;</span> <span style="color:#666">+</span>  q<span style="color:#666">.</span>sessionID())
<span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Initial queue state: empty=&#34;</span> <span style="color:#666">+</span> <span style="color:#a2f">str</span>(q<span style="color:#666">.</span>empty()))
<span style="color:#a2f;font-weight:bold">while</span> <span style="color:#a2f;font-weight:bold">not</span> q<span style="color:#666">.</span>empty():
  item <span style="color:#666">=</span> q<span style="color:#666">.</span>lease(lease_secs<span style="color:#666">=</span><span style="color:#666">10</span>, block<span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">True</span>, timeout<span style="color:#666">=</span><span style="color:#666">2</span>)
  <span style="color:#a2f;font-weight:bold">if</span> item <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> <span style="color:#a2f;font-weight:bold">None</span>:
    itemstr <span style="color:#666">=</span> item<span style="color:#666">.</span>decode(<span style="color:#b44">&#34;utf-8&#34;</span>)
    <span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Working on &#34;</span> <span style="color:#666">+</span> itemstr)
    time<span style="color:#666">.</span>sleep(<span style="color:#666">10</span>) <span style="color:#080;font-style:italic"># Put your actual work here instead of sleep.</span>
    q<span style="color:#666">.</span>complete(item)
  <span style="color:#a2f;font-weight:bold">else</span>:
    <span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Waiting for work&#34;</span>)
<span style="color:#a2f">print</span>(<span style="color:#b44">&#34;Queue empty, exiting&#34;</span>)
</code></pre></div>
    </div>
</div>


<!--
You could download [`worker.py`](/examples/application/job/redis/worker.py), [`rediswq.py`](/examples/application/job/redis/rediswq.py), and [`Dockerfile`](/examples/application/job/redis/Dockerfile)
using above links. Then build the image:
-->
<p>你也可以下载 <a href="/examples/application/job/redis/worker.py"><code>worker.py</code></a>、
<a href="/examples/application/job/redis/rediswq.py"><code>rediswq.py</code></a> 和
<a href="/examples/application/job/redis/Dockerfile"><code>Dockerfile</code></a>。然后构建镜像：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker build -t job-wq-2 .
</code></pre></div><!--
### Push the image

For the [Docker Hub](https://hub.docker.com/), tag your app image with
your username and push to the Hub with the below commands. Replace
`<username>` with your Hub username.
-->
<h3 id="push-镜像">Push 镜像</h3>
<p>对于 <a href="https://hub.docker.com/">Docker Hub</a>，请先用你的用户名给镜像打上标签，
然后使用下面的命令 push 你的镜像到仓库。请将 <code>&lt;username&gt;</code> 替换为你自己的 Hub 用户名。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker tag job-wq-2 &lt;username&gt;/job-wq-2
docker push &lt;username&gt;/job-wq-2
</code></pre></div><!--
You need to push to a public repository or [configure your cluster to be able to access
your private repository](/docs/concepts/containers/images/).
-->
<p>你需要将镜像 push 到一个公共仓库或者
<a href="/zh/docs/concepts/containers/images/">配置集群访问你的私有仓库</a>。</p>
<!--
If you are using [Google Container
Registry](https://cloud.google.com/tools/container-registry/), tag
your app image with your project ID, and push to GCR. Replace
`<project>` with your project ID.
-->
<p>如果你使用的是 <a href="https://cloud.google.com/tools/container-registry/">Google Container Registry</a>，
请先用你的 project ID 给你的镜像打上标签，然后 push 到 GCR 。请将 <code>&lt;project&gt;</code> 替换为你自己的 project ID</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker tag job-wq-2 gcr.io/&lt;project&gt;/job-wq-2
gcloud docker -- push gcr.io/&lt;project&gt;/job-wq-2
</code></pre></div><!--
## Defining a Job

Here is the job definition:
-->
<h2 id="定义一个-job">定义一个 Job</h2>
<p>这是 job 定义：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/redis/job.yaml" download="application/job/redis/job.yaml"><code>application/job/redis/job.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-redis-job-yaml')" title="Copy application/job/redis/job.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-redis-job-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>job-wq-2<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>job-wq-2<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>c<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/myproject/job-wq-2<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Be sure to edit the job template to
change `gcr.io/myproject` to your own path.
-->
<p>请确保将 job 模板中的 <code>gcr.io/myproject</code> 更改为你自己的路径。</p>
<!--
In this example, each pod works on several items from the queue and then exits when there are no more items.
Since the workers themselves detect when the workqueue is empty, and the Job controller does not
know about the workqueue, it relies on the workers to signal when they are done working.
The workers signal that the queue is empty by exiting with success.  So, as soon as any worker
exits with success, the controller knows the work is done, and the Pods will exit soon.
So, we set the completion count of the Job to 1.  The job controller will wait for the other pods to complete
too.
-->
<p>在这个例子中，每个 pod 处理了队列中的多个项目，直到队列中没有项目时便退出。
因为是由工作程序自行检测工作队列是否为空，并且 Job 控制器不知道工作队列的存在，
这依赖于工作程序在完成工作时发出信号。
工作程序以成功退出的形式发出信号表示工作队列已经为空。
所以，只要有任意一个工作程序成功退出，控制器就知道工作已经完成了，所有的 Pod 将很快会退出。
因此，我们将 Job 的完成计数（Completion Count）设置为 1 。
尽管如此，Job 控制器还是会等待其它 Pod 完成。</p>
<!--
## Running the Job

So, now run the Job:
-->
<h2 id="运行-job">运行 Job</h2>
<p>现在运行这个 Job ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f ./job.yaml
</code></pre></div><!--
Now wait a bit, then check on the job.
-->
<p>稍等片刻，然后检查这个 Job。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe jobs/job-wq-2
</code></pre></div><pre tabindex="0"><code>Name:             job-wq-2
Namespace:        default
Selector:         controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f
Labels:           controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f
                  job-name=job-wq-2
Annotations:      &lt;none&gt;
Parallelism:      2
Completions:      &lt;unset&gt;
Start Time:       Mon, 11 Jan 2016 17:07:59 -0800
Pods Statuses:    1 Running / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f
                job-name=job-wq-2
  Containers:
   c:
    Image:              gcr.io/exampleproject/job-wq-2
    Port:
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen    LastSeen    Count    From            SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----            -------------    --------    ------            -------
  33s          33s         1        {job-controller }                Normal      SuccessfulCreate  Created pod: job-wq-2-lglf8
</code></pre><p>查看日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs pods/job-wq-2-7r7b2
</code></pre></div><pre tabindex="0"><code>Worker with sessionID: bbd72d0a-9e5c-4dd6-abf6-416cc267991f
Initial queue state: empty=False
Working on banana
Working on date
Working on lemon
</code></pre><!--
As you can see, one of our pods worked on several work units.
-->
<p>你可以看到，其中的一个 pod 处理了若干个工作单元。</p>
<!-- discussion -->
<!--
## Alternatives
-->
<h2 id="替代方案">替代方案</h2>
<!--
If running a queue service or modifying your containers to use a work queue is inconvenient, you may
want to consider one of the other [job patterns](/docs/concepts/jobs/run-to-completion-finite-workloads/#job-patterns).
-->
<p>如果你不方便运行一个队列服务或者修改你的容器用于运行一个工作队列，你可以考虑其它的
<a href="/zh/docs/concepts/workloads/controllers/job/#job-patterns">Job 模式</a>。</p>
<!--
If you have a continuous stream of background processing work to run, then
consider running your background workers with a `ReplicaSet` instead,
and consider running a background processing library such as
[https://github.com/resque/resque](https://github.com/resque/resque).
-->
<p>如果你有持续的后台处理业务，那么可以考虑使用 <code>ReplicaSet</code> 来运行你的后台业务，
和运行一个类似 <a href="https://github.com/resque/resque">https://github.com/resque/resque</a>
的后台处理库。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-da7c2b067953d239eb4457e8978ad8f6">8.4 - 使用展开的方式进行并行处理</h1>
    
	<!--
title: Parallel Processing using Expansions
content_type: task
min-kubernetes-server-version: v1.8
weight: 50
-->
<!-- overview -->
<!--
This task demonstrates running multiple <a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a>
based on a common template. You can use this approach to process batches of work in
parallel.

For this example there are only three items: _apple_, _banana_, and _cherry_.
The sample Jobs process each item by printing a string then pausing.

See [using Jobs in real workloads](#using-jobs-in-real-workloads) to learn about how
this pattern fits more realistic use cases.
-->
<p>本任务展示基于一个公共的模板运行多个<a class='glossary-tooltip' title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/job/' target='_blank' aria-label='Jobs'>Jobs</a>。
你可以用这种方法来并行执行批处理任务。</p>
<p>在本任务示例中，只有三个工作条目：<em>apple</em>、<em>banana</em> 和 <em>cherry</em>。
示例任务处理每个条目时打印一个字符串之后结束。</p>
<p>参考<a href="#using-jobs-in-real-workloads">在真实负载中使用 Job</a>了解更适用于真实使用场景的模式。</p>
<h2 id="准备开始">准备开始</h2>
<!--
You should be familiar with the basic,
non-parallel, use of [Job](/docs/concepts/workloads/controllers/job/).
-->
<p>你应先熟悉基本的、非并行的 <a href="/zh/docs/concepts/workloads/controllers/job/">Job</a>
的用法。</p>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
For basic templating you need the command-line utility `sed`.

To follow the advanced templating example, you need a working installation of
[Python](https://www.python.org/), and the Jinja2 template
library for Python.

Once you have Python set up, you can install Jinja2 by running:
-->
<p>任务中的基本模板示例要求安装命令行工具 <code>sed</code>。
要使用较高级的模板示例，你需要安装 <a href="https://www.python.org/">Python</a>，
并且要安装 Jinja2 模板库。</p>
<p>一旦 Python 已经安装好，你可以运行下面的命令安装 Jinja2：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">pip install --user jinja2
</code></pre></div><!-- steps -->
<!--
## Create Jobs based on a template
-->
<h2 id="create-jobs-based-on-a-template">基于模板创建 Job </h2>
<!--
First, download the following template of a job to a file called `job-tmpl.yaml`
-->
<p>首先，将以下作业模板下载到名为 <code>job-tmpl.yaml</code> 的文件中。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/job/job-tmpl.yaml" download="application/job/job-tmpl.yaml"><code>application/job/job-tmpl.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-job-job-tmpl-yaml')" title="Copy application/job/job-tmpl.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-job-job-tmpl-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>process-item-$ITEM<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">jobgroup</span>:<span style="color:#bbb"> </span>jobexample<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>jobexample<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">jobgroup</span>:<span style="color:#bbb"> </span>jobexample<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>c<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;sh&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;echo Processing item $ITEM &amp;&amp; sleep 5&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"> <span style="color:#080;font-style:italic"># 使用 curl 下载 job-tmpl.yaml</span>
curl -L -s -O https://k8s.io/examples/application/job/job-tmpl.yaml
</code></pre></div><!--
The file you downloaded is not yet a valid Kubernetes
<a class='glossary-tooltip' title='一个或多个 Kubernetes API 对象的序列化规范。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-manifest' target='_blank' aria-label='manifest'>manifest</a>.
Instead that template is a YAML representation of a Job object with some placeholders
that need to be filled in before it can be used.  The `$ITEM` syntax is not meaningful to Kubernetes.
-->
<p>你所下载的文件不是一个合法的 Kubernetes <a class='glossary-tooltip' title='一个或多个 Kubernetes API 对象的序列化规范。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-manifest' target='_blank' aria-label='清单'>清单</a>。
这里的模板只是 Job 对象的 yaml 表示，其中包含一些占位符，在使用它之前需要被填充。
<code>$ITEM</code> 语法对 Kubernetes 没有意义。</p>
<!--
### Create manifests from the template

The following shell snippet uses `sed` to replace the string `$ITEM` with the loop
variable, writing into a temporary directory named `jobs`. Run this now:
-->
<h3 id="基于模板创建清单">基于模板创建清单</h3>
<p>下面的 Shell 代码片段使用 <code>sed</code> 将字符串 <code>$ITEM</code> 替换为循环变量，并将结果
写入到一个名为 <code>jobs</code> 的临时目录。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 展开模板文件到多个文件中，每个文件对应一个要处理的条目</span>
mkdir ./jobs
<span style="color:#a2f;font-weight:bold">for</span> i in apple banana cherry
<span style="color:#a2f;font-weight:bold">do</span>
  cat job-tmpl.yaml | sed <span style="color:#b44">&#34;s/\$ITEM/</span><span style="color:#b8860b">$i</span><span style="color:#b44">/&#34;</span> &gt; ./jobs/job-<span style="color:#b8860b">$i</span>.yaml
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
Check if it worked:
-->
<p>检查上述脚本的输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ls jobs/
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>job-apple.yaml
job-banana.yaml
job-cherry.yaml
</code></pre><!--
You could use any type of template language (for example: Jinja2; ERB), or
write a program to generate the Job manifests.
-->
<p>你可以使用任何一种模板语言（例如：Jinja2、ERB），或者编写一个程序来
生成 Job 清单。</p>
<!--
### Create Jobs from the manifests

Next, create all the Jobs with one kubectl command:
-->
<h3 id="基于清单创建-job">基于清单创建 Job</h3>
<p>接下来用一个 kubectl 命令创建所有的 Job：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f ./jobs
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>job.batch/process-item-apple created
job.batch/process-item-banana created
job.batch/process-item-cherry created
</code></pre><!--
Now, check on the jobs:
-->
<p>现在检查 Job：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get <span style="color:#a2f">jobs</span> -l <span style="color:#b8860b">jobgroup</span><span style="color:#666">=</span>jobexample
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME                  COMPLETIONS   DURATION   AGE
process-item-apple    1/1           14s        22s
process-item-banana   1/1           12s        21s
process-item-cherry   1/1           12s        20s
</code></pre><!--
Using the `-l` option to kubectl selects only the Jobs that are part
of this group of jobs (there might be other unrelated jobs in the system).

You can check on the Pods as well using the same
<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='label selector'>label selector</a>:
-->
<p>使用 kubectl 的 <code>-l</code> 选项可以仅选择属于当前 Job 组的对象
（系统中可能存在其他不相关的 Job）。</p>
<p>你可以使用相同的 <a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签选择算符'>标签选择算符</a>
来过滤 Pods：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">jobgroup</span><span style="color:#666">=</span>jobexample
</code></pre></div><!--
The output is similar to:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>NAME                        READY     STATUS      RESTARTS   AGE
process-item-apple-kixwv    0/1       Completed   0          4m
process-item-banana-wrsf7   0/1       Completed   0          4m
process-item-cherry-dnfu9   0/1       Completed   0          4m
</code></pre><!--
We can use this single command to check on the output of all jobs at once:
-->
<p>我们可以用下面的命令查看所有 Job 的输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs -f -l <span style="color:#b8860b">jobgroup</span><span style="color:#666">=</span>jobexample
</code></pre></div><!--
The output should be:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>Processing item apple
Processing item banana
Processing item cherry
</code></pre><!--
### Clean up {#cleanup-1}
-->
<h3 id="cleanup-1">清理</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 删除所创建的 Job</span>
<span style="color:#080;font-style:italic"># 集群会自动清理 Job 对应的 Pod</span>
kubectl delete job -l <span style="color:#b8860b">jobgroup</span><span style="color:#666">=</span>jobexample
</code></pre></div><!--
## Use advanced template parameters

In the [first example](#create-jobs-based-on-a-template), each instance of the template had one
parameter, and that parameter was also used in the Job's name. However,
[names](/docs/concepts/overview/working-with-objects/names/#names) are restricted
to contain only certain characters.
-->
<h2 id="使用高级模板参数">使用高级模板参数</h2>
<p>在<a href="#create-jobs-based-on-a-template">第一个例子</a>中，模板的每个示例都有一个参数
而该参数也用在 Job 名称中。不过，对象
<a href="/zh/docs/concepts/overview/working-with-objects/names/#names">名称</a>
被限制只能使用某些字符。</p>
<!--
This slightly more complex example uses the
[Jinja template language](https://palletsprojects.com/p/jinja/) to generate manifests
and then objects from those manifests, with a multiple parameters for each Job.

For this part of the task, you are going to use a one-line Python script to
convert the template to a set of manifests.

First, copy and paste the following template of a Job object, into a file called `job.yaml.jinja2`:
-->
<p>这里的略微复杂的例子使用 <a href="https://palletsprojects.com/p/jinja/">Jinja 模板语言</a>
来生成清单，并基于清单来生成对象，每个 Job 都有多个参数。</p>
<p>在本任务中，你将会使用一个一行的 Python 脚本，将模板转换为一组清单文件。</p>
<p>首先，复制下面的 Job 对象模板到一个名为 <code>job.yaml.jinja2</code> 的文件。</p>
<pre tabindex="0"><code class="language-liquid" data-lang="liquid">{%- set params = [{ &quot;name&quot;: &quot;apple&quot;, &quot;url&quot;: &quot;http://dbpedia.org/resource/Apple&quot;, },
                  { &quot;name&quot;: &quot;banana&quot;, &quot;url&quot;: &quot;http://dbpedia.org/resource/Banana&quot;, },
                  { &quot;name&quot;: &quot;cherry&quot;, &quot;url&quot;: &quot;http://dbpedia.org/resource/Cherry&quot; }]
%}
{%- for p in params %}
{%- set name = p[&quot;name&quot;] %}
{%- set url = p[&quot;url&quot;] %}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: jobexample-{{ name }}
  labels:
    jobgroup: jobexample
spec:
  template:
    metadata:
      name: jobexample
      labels:
        jobgroup: jobexample
    spec:
      containers:
      - name: c
        image: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo Processing URL {{ url }} &amp;&amp; sleep 5&quot;]
      restartPolicy: Never
{%- endfor %}
</code></pre><!--
The above template defines two parameters for each Job object using a list of
python dicts (lines 1-4). A `for` loop emits one Job manifest for each
set of parameters (remaining lines).

This example relies on a feature of YAML. One YAML file can contain multiple
documents (Kubernetes manifests, in this case), separated by `---` on a line
by itself.
You can pipe the output directly to `kubectl` to create the Jobs.

Next, use this one-line Python program to expand the template:
-->
<p>上面的模板使用 python 字典列表（第 1-4 行）定义每个作业对象的参数。
然后使用 for 循环为每组参数（剩余行）生成一个作业 yaml 对象。
我们利用了多个 YAML 文档（这里的 Kubernetes 清单）可以用 <code>---</code> 分隔符连接的事实。
我们可以将输出直接传递给 kubectl 来创建对象。</p>
<p>接下来我们用单行的 Python 程序将模板展开。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">alias</span> <span style="color:#b8860b">render_template</span><span style="color:#666">=</span><span style="color:#b44">&#39;python -c &#34;from jinja2 import Template; import sys; print(Template(sys.stdin.read()).render());&#34;&#39;</span>
</code></pre></div><!--
Use `render_template` to convert the parameters and template into a single
YAML file containing Kubernetes manifests:
-->
<p>使用 <code>render_template</code> 将参数和模板转换成一个 YAML 文件，其中包含 Kubernetes
资源清单：</p>
<!--
```shell
# This requires the alias you defined earlier
cat job.yaml.jinja2 | render_template > jobs.yaml
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 此命令需要之前定义的别名</span>
cat job.yaml.jinja2 | render_template &gt; jobs.yaml
</code></pre></div><!--
You can view `jobs.yaml` to verify that the `render_template` script worked
correctly.

Once you are happy that `render_template` is working how you intend,
you can pipe its output into `kubectl`:
-->
<p>你可以查看 <code>jobs.yaml</code> 以验证 <code>render_template</code> 脚本是否正常工作。</p>
<p>当你对输出结果比较满意时，可以用管道将其输出发送给 kubectl，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat job.yaml.jinja2 | render_template | kubectl apply -f -
</code></pre></div><!--
Kubernetes accepts and runs the Jobs you created.
-->
<p>Kubernetes 接收清单文件并执行你所创建的 Job。</p>
<!--
### Clean up {#cleanup-2}
```shell
# Remove the Jobs you created
# Your cluster automatically cleans up their Pods
kubectl delete job -l jobgroup=jobexample
```
-->
<h3 id="cleanup-2">清理</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 删除所创建的 Job</span>
<span style="color:#080;font-style:italic"># 集群会自动清理 Job 对应的 Pod</span>
kubectl delete job -l <span style="color:#b8860b">jobgroup</span><span style="color:#666">=</span>jobexample
</code></pre></div><!-- discussion -->
<!--
## Using Jobs in real workloads

In a real use case, each Job performs some substantial computation, such as rendering a frame
of a movie, or processing a range of rows in a database. If you were rendering a movie
you would set `$ITEM` to the frame number. If you were processing rows from a database
table, you would set `$ITEM` to represent the range of database rows to process.

In the task, you ran a command to collect the output from Pods by fetching
their logs. In a real use case, each Pod for a Job writes its output to
durable storage before completing. You can use a PersistentVolume for each Job,
or an external storage service. For example, if you are rendering frames for a movie,
use HTTP to `PUT` the rendered frame data to a URL, using a different URL for each
frame.
-->
<h2 id="using-jobs-in-real-workloads">在真实负载中使用 Job</h2>
<p>在真实的负载中，每个 Job 都会执行一些重要的计算，例如渲染电影的一帧，
或者处理数据库中的若干行。这时，<code>$ITEM</code> 参数将指定帧号或行范围。</p>
<p>在此任务中，你运行一个命令通过取回 Pod 的日志来收集其输出。
在真实应用场景中，Job 的每个 Pod 都会在结束之前将其输出写入到某持久性存储中。
你可以为每个 Job 指定 PersistentVolume 卷，或者使用其他外部存储服务。
例如，如果你在渲染视频帧，你可能会使用 HTTP 协议将渲染完的帧数据
用 'PUT' 请求发送到某 URL，每个帧使用不同的 URl。</p>
<!--
## Labels on Jobs and Pods

After you create a Job, Kubernetes automatically adds additional
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='labels'>labels</a> that
distinguish one Job's pods from another Job's pods.

In this example, each Job and its Pod template have a label:
`jobgroup=jobexample`.

Kubernetes itself pays no attention to labels named `jobgroup`. Setting a label
for all the Jobs you create from a template makes it convenient to operate on all
those Jobs at once.
In the [first example](#create-jobs-based-on-a-template) you used a template to
create several Jobs. The template ensures that each Pod also gets the same label, so
you can check on all Pods for these templated Jobs with a single command.
-->
<h2 id="job-和-pod-上的标签">Job 和 Pod 上的标签</h2>
<p>你创建了 Job 之后，Kubernetes 自动为 Job 的 Pod 添加
<a class='glossary-tooltip' title='用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='标签'>标签</a>，以便能够将一个 Job
的 Pod 与另一个 Job 的 Pod 区分开来。</p>
<p>在本例中，每个 Job 及其 Pod 模板有一个标签: <code>jobgroup=jobexample</code>。</p>
<p>Kubernetes 自身对标签名 <code>jobgroup</code> 没有什么要求。
为创建自同一模板的所有 Job 使用同一标签使得我们可以方便地同时操作组中的所有作业。
在<a href="#create-jobs-based-on-a-template">第一个例子</a>中，你使用模板来创建了若干 Job。
模板确保每个 Pod 都能够获得相同的标签，这样你可以用一条命令检查这些模板化
Job 所生成的全部 Pod。</p>
<!--
The label key `jobgroup` is not special or reserved.
You can pick your own labelling scheme.
There are [recommended labels](/docs/concepts/overview/working-with-objects/common-labels/#labels)
that you can use if you wish.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 标签键 <code>jobgroup</code> 没什么特殊的，也不是保留字。 你可以选择你自己的标签方案。
如果愿意，有一些<a href="/zh/docs/concepts/overview/working-with-objects/common-labels/#labels">建议的标签</a>
可供使用。</div>
</blockquote>
<!--
## Alternatives

If you plan to create a large number of Job objects, you may find that:
-->
<h2 id="替代方案">替代方案</h2>
<p>如果你有计划创建大量 Job 对象，你可能会发现：</p>
<!--
- Even using labels, managing so many Job objects is cumbersome.
- If you create many Jobs in a batch, you might place high load
  on the Kubernetes control plane. Alternatively, the Kubernetes API
  server could rate limit you, temporarily rejecting your requests with a 429 status.
- You are limited by a <a class='glossary-tooltip' title='资源配额提供了限制每个命名空间的资源消耗总和的约束。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/policy/resource-quotas/' target='_blank' aria-label='resource quota'>resource quota</a>
  on Jobs: the API server permanently rejects some of your requests
  when you create a great deal of work in one batch.
-->
<ul>
<li>即使使用标签，管理这么多 Job 对象也很麻烦。</li>
<li>如果你一次性创建很多 Job，很可能会给 Kubernetes 控制面带来很大压力。
一种替代方案是，Kubernetes API 可能对请求施加速率限制，通过 429 返回
状态值临时拒绝你的请求。</li>
<li>你可能会受到 Job 相关的<a class='glossary-tooltip' title='资源配额提供了限制每个命名空间的资源消耗总和的约束。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/policy/resource-quotas/' target='_blank' aria-label='资源配额'>资源配额</a>
限制：如果你在一个批量请求中触发了太多的任务，API 服务器会永久性地拒绝你的某些请求。</li>
</ul>
<!--
There are other [job patterns](/docs/concepts/workloads/controllers/job/#job-patterns)
that you can use to process large amounts of work without creating very many Job
objects.

You could also consider writing your own [controller](/docs/concepts/architecture/controller/)
to manage Job objects automatically.
-->
<p>还有一些其他<a href="/zh/docs/concepts/workloads/controllers/job/#job-patterns">作业模式</a>
可供选择，这些模式都能用来处理大量任务而又不会创建过多的 Job 对象。</p>
<p>你也可以考虑编写自己的<a href="/zh/docs/concepts/architecture/controller/">控制器</a>
来自动管理 Job 对象。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b74b959f5a531003dd0653dfbfc2e88b">9 - 访问集群中的应用程序</h1>
    <div class="lead">配置负载平衡、端口转发或设置防火墙或 DNS 配置，以访问集群中的应用程序。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-777447042cd4e81df3fa5beb3357a485">9.1 - Web 界面 (Dashboard)</h1>
    
	<!--
reviewers:
- bryk
- mikedanese
- rf232
title: Web UI (Dashboard)
content_type: concept
weight: 10
card:
  name: tasks
  weight: 30
  title: Use the Web UI Dashboard
-->
<!-- overview -->
<!--
Dashboard is a web-based Kubernetes user interface.
You can use Dashboard to deploy containerized applications to a Kubernetes cluster,
troubleshoot your containerized application, and manage the cluster resources.
You can use Dashboard to get an overview of applications running on your cluster,
as well as for creating or modifying individual Kubernetes resources
(such as Deployments, Jobs, DaemonSets, etc).
For example, you can scale a Deployment, initiate a rolling update, restart a pod
or deploy new applications using a deploy wizard.

Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.
-->
<p>Dashboard 是基于网页的 Kubernetes 用户界面。
你可以使用 Dashboard 将容器应用部署到 Kubernetes 集群中，也可以对容器应用排错，还能管理集群资源。
你可以使用 Dashboard 获取运行在集群中的应用的概览信息，也可以创建或者修改 Kubernetes 资源
（如 Deployment，Job，DaemonSet 等等）。
例如，你可以对 Deployment 实现弹性伸缩、发起滚动升级、重启 Pod 或者使用向导创建新的应用。</p>
<p>Dashboard 同时展示了 Kubernetes 集群中的资源状态信息和所有报错信息。</p>
<p><img src="/images/docs/ui-dashboard.png" alt="Kubernetes Dashboard UI"></p>
<!-- body -->
<!--
## Deploying the Dashboard UI

The Dashboard UI is not deployed by default. To deploy it, run the following command:
-->
<h2 id="部署-dashboard-ui">部署 Dashboard UI</h2>
<p>默认情况下不会部署 Dashboard。可以通过以下命令部署：</p>
<pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml
</code></pre><!--
## Accessing the Dashboard UI

To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default.
Currently, Dashboard only supports logging in with a Bearer Token.
To create a token for this demo, you can follow our guide on
[creating a sample user](https://github.com/kubernetes/dashboard/wiki/Creating-sample-user).
-->
<h2 id="访问-dashboard-ui">访问 Dashboard UI</h2>
<p>为了保护你的集群数据，默认情况下，Dashboard 会使用最少的 RBAC 配置进行部署。
当前，Dashboard 仅支持使用 Bearer 令牌登录。
要为此样本演示创建令牌，你可以按照
<a href="https://github.com/kubernetes/dashboard/wiki/Creating-sample-user">创建示例用户</a>
上的指南进行操作。</p>
<!--
The sample user created in the tutorial will have administrative privileges and is for educational purposes only.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 在教程中创建的样本用户将具有管理特权，并且仅用于教育目的。</div>
</blockquote>

<!--
### Command line proxy

You can access Dashboard using the kubectl command-line tool by running the following command:
-->
<h3 id="命令行代理">命令行代理</h3>
<p>你可以使用 kubectl 命令行工具访问 Dashboard，命令如下：</p>
<pre tabindex="0"><code>kubectl proxy
</code></pre><!--
Kubectl will make Dashboard available at [http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/](http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/).
-->
<p>kubectl 会使得 Dashboard 可以通过 <a href="http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</a> 访问。</p>
<!--
The UI can _only_ be accessed from the machine where the command is executed. See `kubectl proxy --help` for more options.
-->
<p>UI <em>只能</em> 通过执行这条命令的机器进行访问。更多选项参见 <code>kubectl proxy --help</code>。</p>
<!--
Kubeconfig Authentication method does NOT support external identity providers or x509 certificate-based authentication.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> Kubeconfig 身份验证方法不支持外部身份提供程序或基于 x509 证书的身份验证。</div>
</blockquote>
<!--
## Welcome view
-->
<h2 id="欢迎界面">欢迎界面</h2>
<!--
When you access Dashboard on an empty cluster, you'll see the welcome page. This page contains a link to this document as well as a button to deploy your first application. In addition, you can view which system applications are running by default in the `kube-system` [namespace](/docs/tasks/administer-cluster/namespaces/) of your cluster, for example the Dashboard itself.
 -->
<p>当访问空集群的 Dashboard 时，你会看到欢迎界面。
页面包含一个指向此文档的链接，以及一个用于部署第一个应用程序的按钮。
此外，你可以看到在默认情况下有哪些默认系统应用运行在 <code>kube-system</code>
<a href="/zh/docs/tasks/administer-cluster/namespaces/">名字空间</a> 中，比如 Dashboard 自己。</p>
<!--
![Kubernetes Dashboard welcome page](/images/docs/ui-dashboard-zerostate.png)
 -->
<p><img src="/images/docs/ui-dashboard-zerostate.png" alt="Kubernetes Dashboard 欢迎页面"></p>
<!--
## Deploying containerized applications

Dashboard lets you create and deploy a containerized application as a Deployment and optional Service with a simple wizard. You can either manually specify application details, or upload a YAML or JSON file containing application configuration.
-->
<h2 id="部署容器化应用">部署容器化应用</h2>
<p>通过一个简单的部署向导，你可以使用 Dashboard 将容器化应用作为一个 Deployment 和可选的 Service 进行创建和部署。可以手工指定应用的详细配置，或者上传一个包含应用配置的 YAML 或 JSON 文件。</p>
<!--
Click the **CREATE** button in the upper right corner of any page to begin.
-->
<p>点击任何页面右上角的 <strong>CREATE</strong> 按钮以开始。</p>
<!--
### Specifying application details

The deploy wizard expects that you provide the following information:
-->
<h3 id="指定应用的详细配置">指定应用的详细配置</h3>
<p>部署向导需要你提供以下信息：</p>
<!--
- **App name** (mandatory): Name for your application. A [label](/docs/concepts/overview/working-with-objects/labels/) with the name will be added to the Deployment and Service, if any, that will be deployed.
-->
<ul>
<li>
<p><strong>应用名称</strong>（必填）：应用的名称。内容为<code>应用名称</code>的
<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a>
会被添加到任何将被部署的 Deployment 和 Service。</p>
<!--
The application name must be unique within the selected Kubernetes [namespace](/docs/tasks/administer-cluster/namespaces/). It must start with a lowercase character, and end with a lowercase character or a number, and contain only lowercase letters, numbers and dashes (-). It is limited to 24 characters. Leading and trailing spaces are ignored.
-->
<p>在选定的 Kubernetes <a href="/zh/docs/tasks/administer-cluster/namespaces/">名字空间</a> 中，
应用名称必须唯一。必须由小写字母开头，以数字或者小写字母结尾，
并且只含有小写字母、数字和中划线（-）。小于等于24个字符。开头和结尾的空格会被忽略。</p>
</li>
</ul>
<!--
- **Container image** (mandatory): The URL of a public Docker [container image](/docs/concepts/containers/images/) on any registry, or a private image (commonly hosted on the Google Container Registry or Docker Hub). The container image specification must end with a colon.
 -->
<ul>
<li><strong>容器镜像</strong>（必填）：公共镜像仓库上的 Docker
<a href="/zh/docs/concepts/containers/images/">容器镜像</a> 或者私有镜像仓库
（通常是 Google Container Registry 或者 Docker Hub）的 URL。容器镜像参数说明必须以冒号结尾。</li>
</ul>
<!--
- **Number of pods** (mandatory): The target number of Pods you want your application to be deployed in. The value must be a positive integer.
-->
<ul>
<li>
<p><strong>Pod 的数量</strong>（必填）：你希望应用程序部署的 Pod 的数量。值必须为正整数。</p>
<!--
A [Deployment](/docs/concepts/workloads/controllers/deployment/) will be created to
maintain the desired number of Pods across your cluster.
-->
<p>系统会创建一个 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a>
以保证集群中运行期望的 Pod 数量。</p>
</li>
</ul>
<!--
- **Service** (optional): For some parts of your application (e.g. frontends) you may want to expose a [Service](/docs/concepts/services-networking/service/) onto an external, maybe public IP address outside of your cluster (external Service).
 -->
<ul>
<li>
<p><strong>服务</strong>（可选）：对于部分应用（比如前端），你可能想对外暴露一个
<a href="/zh/docs/concepts/services-networking/service/">Service</a> ，这个 Service
可能用的是集群之外的公网 IP 地址（外部 Service）。</p>
<!-- 
For external Services, you may need to open up one or more ports to do so.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 对于外部服务，你可能需要开放一个或多个端口才行。</div>
</blockquote>
<!--
Other Services that are only visible from inside the cluster are called internal Services.
-->
<p>其它只能对集群内部可见的 Service 称为内部 Service。</p>
<!--
Irrespective of the Service type, if you choose to create a Service and your container listens
on a port (incoming), you need to specify two ports.
The Service will be created mapping the port (incoming) to the target port seen by the container.
This Service will route to your deployed Pods. Supported protocols are TCP and UDP.
The internal DNS name for this Service will be the value you specified as application name above.
-->
<p>不管哪种 Service 类型，如果你选择创建一个 Service，而且容器在一个端口上开启了监听（入向的），
那么你需要定义两个端口。创建的 Service 会把（入向的）端口映射到容器可见的目标端口。
该 Service 会把流量路由到你部署的 Pod。支持 TCP 协议和 UDP 协议。
这个 Service 的内部 DNS 解析名就是之前你定义的应用名称的值。</p>
</li>
</ul>
<!--
If needed, you can expand the **Advanced options** section where you can specify more settings:
 -->
<p>如果需要，你可以打开 <strong>Advanced Options</strong> 部分，这里你可以定义更多设置：</p>
<!--
- **Description**: The text you enter here will be added as an
  [annotation](/docs/concepts/overview/working-with-objects/annotations/)
  to the Deployment and displayed in the application's details.
 -->
<ul>
<li><strong>描述</strong>：这里你输入的文本会作为一个
<a href="/zh/docs/concepts/overview/working-with-objects/annotations/">注解</a>
添加到 Deployment，并显示在应用的详细信息中。</li>
</ul>
<!--
- **Labels**: Default [labels](/docs/concepts/overview/working-with-objects/labels/) to be used for your application are application name and version. You can specify additional labels to be applied to the Deployment, Service (if any), and Pods, such as release, environment, tier, partition, and release track.
-->
<ul>
<li>
<p><strong>标签</strong>：应用默认使用的
<a href="/zh/docs/concepts/overview/working-with-objects/labels/">标签</a> 是应用名称和版本。
你可以为 Deployment、Service（如果有）定义额外的标签，比如 release（版本）、
environment（环境）、tier（层级）、partition（分区） 和 release track（版本跟踪）。</p>
<!-- Example: -->
<p>例子：</p>
<pre tabindex="0"><code class="language-conf" data-lang="conf">release=1.0
tier=frontend
environment=pod
track=stable
</code></pre></li>
</ul>
<!--
- **Namespace**: Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called [namespaces](/docs/tasks/administer-cluster/namespaces/). They let you partition resources into logically named groups.
-->
<ul>
<li>
<p><strong>名字空间</strong>：Kubernetes 支持多个虚拟集群依附于同一个物理集群。
这些虚拟集群被称为
<a href="/zh/docs/tasks/administer-cluster/namespaces/">名字空间</a>，
可以让你将资源划分为逻辑命名的组。</p>
<!--
Dashboard offers all available namespaces in a dropdown list, and allows you to create a new namespace.
The namespace name may contain a maximum of 63 alphanumeric characters and dashes (-) 
but can not contain capital letters.
-->
<p>Dashboard 通过下拉菜单提供所有可用的名字空间，并允许你创建新的名字空间。
名字空间的名称最长可以包含 63 个字母或数字和中横线（-），但是不能包含大写字母。</p>
<!--
Namespace names should not consist of only numbers.
If the name is set as a number, such as 10, the pod will be put in the default namespace.
-->
<p>名字空间的名称不能只包含数字。如果名字被设置成一个数字，比如 10，pod 就</p>
<!--
In case the creation of the namespace is successful, it is selected by default.
If the creation fails, the first namespace is selected.
-->
<p>在名字空间创建成功的情况下，默认会使用新创建的名字空间。如果创建失败，那么第一个名字空间会被选中。</p>
</li>
</ul>
<!--
- **Image Pull Secret**: In case the specified Docker container image is private, it may require [pull secret](/docs/concepts/configuration/secret/) credentials.
-->
<ul>
<li>
<p><strong>镜像拉取 Secret</strong>：如果要使用私有的 Docker 容器镜像，需要拉取
<a href="/zh/docs/concepts/configuration/secret/">Secret</a> 凭证。</p>
<!--
Dashboard offers all available secrets in a dropdown list, and allows you to create a new secret.
The secret name must follow the DNS domain name syntax, e.g. `new.image-pull.secret`.
The content of a secret must be base64-encoded and specified in a
[`.dockercfg`](/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod) file.
The secret name may consist of a maximum of 253 characters.
-->
<p>Dashboard 通过下拉菜单提供所有可用的 Secret，并允许你创建新的 Secret。
Secret 名称必须遵循 DNS 域名语法，比如 <code>new.image-pull.secret</code>。
Secret 的内容必须是 base64 编码的，并且在一个
<a href="/zh/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"><code>.dockercfg</code></a>
文件中声明。Secret 名称最大可以包含 253 个字符。</p>
<!--
In case the creation of the image pull secret is successful, it is selected by default.
If the creation fails, no secret is applied.
-->
<p>在镜像拉取 Secret 创建成功的情况下，默认会使用新创建的 Secret。
如果创建失败，则不会使用任何 Secret。</p>
</li>
</ul>
<!--
- **CPU requirement (cores)** and **Memory requirement (MiB)**: You can specify the minimum [resource limits](/docs/tasks/configure-pod-container/limit-range/) for the container. By default, Pods run with unbounded CPU and memory limits.
 -->
<ul>
<li><strong>CPU 需求（核数）<strong>和</strong>内存需求（MiB）</strong>：你可以为容器定义最小的
<a href="/zh/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">资源限制</a>。
默认情况下，Pod 没有 CPU 和内存限制。</li>
</ul>
<!--
- **Run command** and **Run command arguments**: By default, your containers run the specified Docker image's default [entrypoint command](/docs/user-guide/containers/#containers-and-commands). You can use the command options and arguments to override the default.
 -->
<ul>
<li><strong>运行命令</strong>和<strong>运行命令参数</strong>：默认情况下，你的容器会运行 Docker 镜像的默认
<a href="/zh/docs/tasks/inject-data-application/define-command-argument-container/">入口命令</a>。
你可以使用 command 选项覆盖默认值。</li>
</ul>
<!--
- **Run as privileged**: This setting determines whether processes in [privileged containers](/docs/user-guide/pods/#privileged-mode-for-pod-containers) are equivalent to processes running as root on the host. Privileged containers can make use of capabilities like manipulating the network stack and accessing devices.
 -->
<ul>
<li><strong>以特权模式运行</strong>：这个设置决定了在
<a href="/zh/docs/concepts/workloads/pods/#privileged-mode-for-containers">特权容器</a>
中运行的进程是否像主机中使用 root 运行的进程一样。
特权容器可以使用诸如操纵网络堆栈和访问设备的功能。</li>
</ul>
<!--
- **Environment variables**: Kubernetes exposes Services through [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/). You can compose environment variable or pass arguments to your commands using the values of environment variables. They can be used in applications to find a Service. Values can reference other variables using the `$(VAR_NAME)` syntax.
 -->
<ul>
<li><strong>环境变量</strong>：Kubernetes 通过
<a href="/zh/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">环境变量</a>
暴露 Service。你可以构建环境变量，或者将环境变量的值作为参数传递给你的命令。
它们可以被应用用于查找 Service。值可以通过  <code>$(VAR_NAME)</code> 语法关联其他变量。</li>
</ul>
<!--
### Uploading a YAML or JSON file

Kubernetes supports declarative configuration. In this style, all configuration is stored in YAML or JSON configuration files using the Kubernetes [API](/docs/concepts/overview/kubernetes-api/) resource schemas.
-->
<h3 id="上传-yaml-或者-json-文件">上传 YAML 或者 JSON 文件</h3>
<p>Kubernetes 支持声明式配置。所有的配置都存储在遵循 Kubernetes
<a href="/zh/docs/concepts/overview/kubernetes-api/">API</a> 规范的 YAML 或者 JSON 配置文件中。</p>
<!--
As an alternative to specifying application details in the deploy wizard, you can define your application in YAML or JSON files, and upload the files using Dashboard:
-->
<p>作为一种替代在部署向导中指定应用详情的方式，你可以在 YAML 或者 JSON 文件中定义应用，并且使用 Dashboard 上传文件：</p>
<!--
## Using Dashboard

Following sections describe views of the Kubernetes Dashboard UI; what they provide and how can they be used.
-->
<h2 id="使用-dashboard">使用 Dashboard</h2>
<p>以下各节描述了 Kubernetes Dashboard UI 视图；包括它们提供的内容，以及怎么使用它们。</p>
<!--
### Navigation

When there are Kubernetes objects defined in the cluster, Dashboard shows them in the initial view. By default only objects from the _default_ namespace are shown and this can be changed using the namespace selector located in the navigation menu.
-->
<h3 id="导航">导航</h3>
<p>当在集群中定义 Kubernetes 对象时，Dashboard 会在初始视图中显示它们。
默认情况下只会显示 <em>默认</em> 名字空间中的对象，可以通过更改导航栏菜单中的名字空间筛选器进行改变。</p>
<!--
Dashboard shows most Kubernetes object kinds and groups them in a few menu categories.
-->
<p>Dashboard 展示大部分 Kubernetes 对象，并将它们分组放在几个菜单类别中。</p>
<!--
#### Admin Overview

For cluster and namespace administrators, Dashboard lists Nodes, Namespaces and Persistent Volumes and has detail views for them. Node list view contains CPU and memory usage metrics aggregated across all Nodes. The details view shows the metrics for a Node, its specification, status, allocated resources, events and pods running on the node.
-->
<h4 id="管理概述">管理概述</h4>
<p>集群和名字空间管理的视图, Dashboard 会列出节点、名字空间和持久卷，并且有它们的详细视图。
节点列表视图包含从所有节点聚合的 CPU 和内存使用的度量值。
详细信息视图显示了一个节点的度量值，它的规格、状态、分配的资源、事件和这个节点上运行的 Pod。</p>
<!--
#### Workloads
Shows all applications running in the selected namespace. The view lists applications by workload kind (e.g., Deployments, Replica Sets, Stateful Sets, etc.) and each workload kind can be viewed separately. The lists summarize actionable information about the workloads, such as the number of ready pods for a Replica Set or current memory usage for a Pod.
 -->
<h4 id="负载">负载</h4>
<p>显示选中的名字空间中所有运行的应用。
视图按照负载类型（如 Deployment、ReplicaSet、StatefulSet 等）罗列应用，并且每种负载都可以单独查看。
列表总结了关于负载的可执行信息，比如一个 ReplicaSet 的准备状态的 Pod 数量，或者目前一个 Pod 的内存使用量。</p>
<!--
Detail views for workloads show status and specification information and surface relationships between objects. For example, Pods that Replica Set is controlling or New Replica Sets and Horizontal Pod Autoscalers for Deployments.
-->
<p>工作负载的详情视图展示了对象的状态、详细信息和相互关系。
例如，ReplicaSet 所控制的 Pod，或者 Deployment 关联的 新 ReplicaSet 和 Pod 水平扩展控制器。</p>
<!--
#### Services
Shows Kubernetes resources that allow for exposing services to external world and discovering them within a cluster. For that reason, Service and Ingress views show Pods targeted by them, internal endpoints for cluster connections and external endpoints for external users.
-->
<h4 id="服务">服务</h4>
<p>展示允许暴露给外网服务和允许集群内部发现的 Kubernetes 资源。
因此，Service 和 Ingress 视图展示他们关联的 Pod、给集群连接使用的内部端点和给外部用户使用的外部端点。</p>
<!--
#### Storage

Storage view shows Persistent Volume Claim resources which are used by applications for storing data.
-->
<h4 id="存储">存储</h4>
<p>存储视图展示持久卷申领（PVC）资源，这些资源被应用程序用来存储数据。</p>
<!--
#### Config Maps and Secrets

Shows all Kubernetes resources that are used for live configuration of applications running in clusters. The view allows for editing and managing config objects and displays secrets hidden by default.
-->
<h4 id="configmap-和-secret">ConfigMap 和 Secret</h4>
<p>展示的所有 Kubernetes 资源是在集群中运行的应用程序的实时配置。
通过这个视图可以编辑和管理配置对象，并显示那些默认隐藏的 secret。</p>
<!--
#### Logs viewer

Pod lists and detail pages link to logs viewer that is built into Dashboard. The viewer allows for drilling down logs from containers belonging to a single Pod.
-->
<h4 id="日志查看器">日志查看器</h4>
<p>Pod 列表和详细信息页面可以链接到 Dashboard 内置的日志查看器。查看器可以钻取属于同一个 Pod 的不同容器的日志。</p>
<!--
![Logs viewer](/images/docs/ui-dashboard-logs-view.png)
 -->
<p><img src="/images/docs/ui-dashboard-logs-view.png" alt="日志浏览"></p>
<h2 id="接下来">接下来</h2>
<!--
For more information, see the
[Kubernetes Dashboard project page](https://github.com/kubernetes/dashboard).
-->
<p>更多信息，参见 <a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard 项目页面</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-6a8d9e9e05f2b6825afbb8889c957370">9.2 - 访问集群</h1>
    
	<!--
title: Accessing Clusters
weight: 20
content_type: concept
-->
<!-- overview -->
<!--
This topic discusses multiple ways to interact with clusters.
-->
<p>本文阐述多种与集群交互的方法。</p>
<nav id="TableOfContents">
  <ul>
    <li><a href="#使用-kubectl-完成集群的第一次访问">使用 kubectl 完成集群的第一次访问</a></li>
    <li><a href="#直接访问-rest-api">直接访问 REST API</a>
      <ul>
        <li><a href="#使用-kubectl-proxy">使用 kubectl proxy</a></li>
        <li><a href="#不使用-kubectl-proxy">不使用 kubectl proxy</a></li>
      </ul>
    </li>
    <li><a href="#以编程方式访问-api">以编程方式访问 API</a>
      <ul>
        <li><a href="#go-客户端">Go 客户端</a></li>
        <li><a href="#python-客户端">Python 客户端</a></li>
        <li><a href="#其它语言">其它语言</a></li>
        <li><a href="#accessing-the-api-from-a-pod">从 Pod 中访问 API  </a></li>
      </ul>
    </li>
    <li><a href="#accessing-services-running-on-the-cluster">访问集群中正在运行的服务 </a>
      <ul>
        <li><a href="#ways-to-connect">连接的方法  </a></li>
        <li><a href="#发现内建服务">发现内建服务</a></li>
        <li><a href="#使用-web-浏览器访问运行在集群上的服务">使用 web 浏览器访问运行在集群上的服务</a></li>
      </ul>
    </li>
    <li><a href="#请求重定向">请求重定向</a></li>
    <li><a href="#多种代理">多种代理</a></li>
  </ul>
</nav>
<!-- body -->
<!--
## Accessing for the first time with kubectl

When accessing the Kubernetes API for the first time, we suggest using the
Kubernetes CLI, `kubectl`.

To access a cluster, you need to know the location of the cluster and have credentials
to access it.  Typically, this is automatically set-up when you work through
a [Getting started guide](/docs/setup/),
or someone else setup the cluster and provided you with credentials and a location.

Check the location and credentials that kubectl knows about with this command:
-->
<h2 id="使用-kubectl-完成集群的第一次访问">使用 kubectl 完成集群的第一次访问</h2>
<p>当你第一次访问 Kubernetes API 的时候，我们建议你使用 Kubernetes CLI，<code>kubectl</code>。</p>
<p>访问集群时，你需要知道集群的地址并且拥有访问的凭证。通常，这些在你通过
<a href="/zh/docs/setup/">启动安装</a>安装集群时都是自动安装好的，或者其他人安装时
也应该提供了凭证和集群地址。</p>
<p>通过以下命令检查 kubectl 是否知道集群地址及凭证：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><!--
Many of the [examples](/docs/user-guide/kubectl-cheatsheet) provide an introduction to using
kubectl and complete documentation is found in the [kubectl manual](/docs/user-guide/kubectl-overview).
-->
<p>有许多 <a href="/zh/docs/reference/kubectl/cheatsheet/">例子</a> 介绍了如何使用 kubectl，
可以在 <a href="/zh/docs/reference/kubectl/overview/">kubectl手册</a> 中找到更完整的文档。</p>
<!--
## Directly accessing the REST API

Kubectl handles locating and authenticating to the apiserver.
If you want to directly access the REST API with an http client like
curl or wget, or a browser, there are several ways to locate and authenticate:

  - Run kubectl in proxy mode.
    - Recommended approach.
    - Uses stored apiserver location.
    - Verifies identity of apiserver using self-signed cert.  No MITM possible.
    - Authenticates to apiserver.
    - In future, may do intelligent client-side load-balancing and failover.
  - Provide the location and credentials directly to the http client.
    - Alternate approach.
    - Works with some types of client code that are confused by using a proxy.
    - Need to import a root cert into your browser to protect against MITM.
-->
<h2 id="直接访问-rest-api">直接访问 REST API</h2>
<p>Kubectl 处理 apiserver 的定位和身份验证。
如果要使用 curl 或 wget 等 http 客户端或浏览器直接访问 REST API，可以通过
多种方式查找和验证：</p>
<ul>
<li>以代理模式运行 kubectl。
<ul>
<li>推荐此方式。</li>
<li>使用已存储的 apiserver 地址。</li>
<li>使用自签名的证书来验证 apiserver 的身份。杜绝 MITM 攻击。</li>
<li>对 apiserver 进行身份验证。</li>
<li>未来可能会实现智能化的客户端负载均衡和故障恢复。</li>
</ul>
</li>
<li>直接向 http 客户端提供位置和凭据。
<ul>
<li>可选的方案。</li>
<li>适用于代理可能引起混淆的某些客户端类型。</li>
<li>需要引入根证书到你的浏览器以防止 MITM 攻击。</li>
</ul>
</li>
</ul>
<!--
### Using kubectl proxy

The following command runs kubectl in a mode where it acts as a reverse proxy.  It handles
locating the apiserver and authenticating.
Run it like this:
-->
<h3 id="使用-kubectl-proxy">使用 kubectl proxy</h3>
<p>以下命令以反向代理的模式运行 kubectl。它处理 apiserver 的定位和验证。
像这样运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl proxy --port<span style="color:#666">=</span><span style="color:#666">8080</span> &amp;
</code></pre></div><!--
See [kubectl proxy](/docs/reference/generated/kubectl/kubectl-commands/#proxy) for more details.

Then you can explore the API with curl, wget, or a browser, replacing localhost
with [::1] for IPv6, like so:
-->
<p>参阅 <a href="/docs/reference/generated/kubectl/kubectl-commands/#proxy">kubectl proxy</a>
获取更多详细信息。</p>
<p>然后，你可以使用 curl、wget 或浏览器访问 API，如果是 IPv6 则用 [::1] 替换 localhost，
如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl http://localhost:8080/api/
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;APIVersions&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><!--
### Without kubectl proxy

In Kubernetes version 1.3 or later, `kubectl config view` no longer displays the token. Use `kubectl describe secret...` to get the token for the default service account, like this:
-->
<h3 id="不使用-kubectl-proxy">不使用 kubectl proxy</h3>
<p>在 Kubernetes 1.3 或更高版本中，<code>kubectl config view</code> 不再显示 token。
使用 <code>kubectl describe secret ...</code> 来获取默认服务帐户的 token，如下所示：</p>
<p><code>grep/cut</code> 方法实现：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl config view | grep server | cut -f 2- -d <span style="color:#b44">&#34;:&#34;</span> | tr -d <span style="color:#b44">&#34; &#34;</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl describe secret <span style="color:#a2f;font-weight:bold">$(</span>kubectl get secrets | grep default | cut -f1 -d <span style="color:#b44">&#39; &#39;</span><span style="color:#a2f;font-weight:bold">)</span> | grep -E <span style="color:#b44">&#39;^token&#39;</span> | cut -f2 -d<span style="color:#b44">&#39;:&#39;</span> | tr -d <span style="color:#b44">&#39; &#39;</span><span style="color:#a2f;font-weight:bold">)</span>
curl <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">&#34;Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">&#34;</span> --insecure
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;APIVersions&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><p><code>jsonpath</code> 方法实现：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">APISERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl config view --minify -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.clusters[0].cluster.server}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
<span style="color:#b8860b">TOKEN</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl get secret <span style="color:#a2f;font-weight:bold">$(</span>kubectl get serviceaccount default -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.secrets[0].name}&#39;</span><span style="color:#a2f;font-weight:bold">)</span> -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.data.token}&#39;</span> | base64 --decode <span style="color:#a2f;font-weight:bold">)</span>
curl <span style="color:#b8860b">$APISERVER</span>/api --header <span style="color:#b44">&#34;Authorization: Bearer </span><span style="color:#b8860b">$TOKEN</span><span style="color:#b44">&#34;</span> --insecure
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;APIVersions&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;versions&#34;</span>: [
    <span style="color:#b44">&#34;v1&#34;</span>
  ],
  <span style="color:#008000;font-weight:bold">&#34;serverAddressByClientCIDRs&#34;</span>: [
    {
      <span style="color:#008000;font-weight:bold">&#34;clientCIDR&#34;</span>: <span style="color:#b44">&#34;0.0.0.0/0&#34;</span>,
      <span style="color:#008000;font-weight:bold">&#34;serverAddress&#34;</span>: <span style="color:#b44">&#34;10.0.1.149:443&#34;</span>
    }
  ]
}
</code></pre></div><!--
The above examples use the `--insecure` flag.  This leaves it subject to MITM
attacks.  When kubectl accesses the cluster it uses a stored root certificate
and client certificates to access the server.  (These are installed in the
`~/.kube` directory).  Since cluster certificates are typically self-signed, it
may take special configuration to get your http client to use root
certificate.

On some clusters, the apiserver does not require authentication; it may serve
on localhost, or be protected by a firewall.  There is not a standard
for this.  [Configuring Access to the API](/docs/admin/accessing-the-api)
describes how a cluster admin can configure this.  Such approaches may conflict
with future high-availability support.
-->
<p>上面的例子使用了 <code>--insecure</code> 参数，这使得它很容易受到 MITM 攻击。
当 kubectl 访问集群时，它使用存储的根证书和客户端证书来访问服务器
（它们安装在 <code>~/.kube</code> 目录中）。
由于集群证书通常是自签名的，因此可能需要特殊配置才能让你的 http 客户端使用根证书。</p>
<p>在一些集群中，apiserver 不需要身份验证；它可能只服务于 localhost，或者被防火墙保护，
这个没有一定的标准。
<a href="/zh/docs/concepts/security/controlling-access/">配置对 API 的访问</a>
描述了集群管理员如何进行配置。此类方法可能与未来的高可用性支持相冲突。</p>
<!--
## Programmatic access to the API

Kubernetes officially supports [Go](#go-client) and [Python](#python-client)
client libraries.

### Go client

* To get the library, run the following command: `go get k8s.io/client-go/<version number>/kubernetes`. See [https://github.com/kubernetes/client-go](https://github.com/kubernetes/client-go) to see which versions are supported.
* Write an application atop of the client-go clients. Note that client-go defines its own API objects, so if needed, please import API definitions from client-go rather than from the main repository, e.g., `import "k8s.io/client-go/1.4/pkg/api/v1"` is correct.

The Go client can use the same [kubeconfig file](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the apiserver. See this [example](https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go).

If the application is deployed as a Pod in the cluster, please refer to the [next section](#accessing-the-api-from-a-pod).
-->
<h2 id="以编程方式访问-api">以编程方式访问 API</h2>
<p>Kubernetes 官方提供对 <a href="#go-client">Go</a> 和 <a href="#python-client">Python</a> 的客户端库支持。</p>
<h3 id="go-客户端">Go 客户端</h3>
<ul>
<li>想要获得这个库，请运行命令：<code>go get k8s.io/client-go/&lt;version number&gt;/kubernetes</code>。
参阅 <a href="https://github.com/kubernetes/client-go">https://github.com/kubernetes/client-go</a>
来查看目前支持哪些版本。</li>
<li>基于这个 client-go 客户端库编写应用程序。
请注意，client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是从主存储库
导入 API 定义，例如，<code>import &quot;k8s.io/client-go/1.4/pkg/api/v1&quot;</code> 才是对的。</li>
</ul>
<p>Go 客户端可以像 kubectl CLI 一样使用相同的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
来定位和验证 apiserver。可参阅
<a href="https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go">示例</a>。</p>
<p>如果应用程序以 Pod 的形式部署在集群中，那么请参阅
<a href="#accessing-the-api-from-a-pod">下一章</a>。</p>
<!--
### Python client

To use [Python client](https://github.com/kubernetes-client/python), run the following command: `pip install kubernetes`. See [Python Client Library page](https://github.com/kubernetes-client/python) for more installation options.

The Python client can use the same [kubeconfig file](/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/)
as the kubectl CLI does to locate and authenticate to the apiserver. See this [example](https://github.com/kubernetes-client/python/tree/master/examples).

### Other languages

There are [client libraries](/docs/reference/using-api/client-libraries/) for accessing the API from other languages.
See documentation for other libraries for how they authenticate.
-->
<h3 id="python-客户端">Python 客户端</h3>
<p>如果想要使用 <a href="https://github.com/kubernetes-client/python">Python 客户端</a>，
请运行命令：<code>pip install kubernetes</code>。参阅
<a href="https://github.com/kubernetes-client/python">Python Client Library page</a>
以获得更详细的安装参数。</p>
<p>Python 客户端可以像 kubectl CLI 一样使用相同的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>
来定位和验证 apiserver，可参阅
<a href="https://github.com/kubernetes-client/python/tree/master/examples">示例</a>。</p>
<h3 id="其它语言">其它语言</h3>
<p>目前有多个<a href="/zh/docs/reference/using-api/client-libraries/">客户端库</a>
为其它语言提供访问 API 的方法。
参阅其它库的相关文档以获取他们是如何验证的。</p>
<!--
## Accessing the API from a Pod

When accessing the API from a pod, locating and authenticating
to the apiserver are somewhat different.

The recommended way to locate the apiserver within the pod is with
the `kubernetes.default.svc` DNS name, which resolves to a Service IP which in turn
will be routed to an apiserver.

The recommended way to authenticate to the apiserver is with a
[service account](/docs/tasks/configure-pod-container/configure-service-account/) credential. By kube-system, a pod
is associated with a service account, and a credential (token) for that
service account is placed into the filesystem tree of each container in that pod,
at `/var/run/secrets/kubernetes.io/serviceaccount/token`.
-->
<h3 id="accessing-the-api-from-a-pod">从 Pod 中访问 API  </h3>
<p>当你从 Pod 中访问 API 时，定位和验证 apiserver 会有些许不同。</p>
<p>在 Pod 中定位 apiserver 的推荐方式是通过 <code>kubernetes.default.svc</code>
这个 DNS 名称，该名称将会解析为服务 IP，然后服务 IP 将会路由到 apiserver。</p>
<p>向 apiserver 进行身份验证的推荐方法是使用
<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/">服务帐户</a> 凭据。
通过 kube-system，Pod 与服务帐户相关联，并且该服务帐户的凭证（token）
被放置在该 Pod 中每个容器的文件系统中，位于
<code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>。</p>
<!--
If available, a certificate bundle is placed into the filesystem tree of each
container at `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt`, and should be
used to verify the serving certificate of the apiserver.

Finally, the default namespace to be used for namespaced API operations is placed in a file
at `/var/run/secrets/kubernetes.io/serviceaccount/namespace` in each container.
-->
<p>如果可用，则将证书放入每个容器的文件系统中的
<code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code>，
并且应该用于验证 apiserver 的服务证书。</p>
<p>最后，名字空间作用域的 API 操作所使用的 default 名字空间将被放置在
每个容器的 <code>/var/run/secrets/kubernetes.io/serviceaccount/namespace</code>
文件中。</p>
<!--
From within a pod the recommended ways to connect to API are:

  - run `kubectl proxy` in a sidecar container in the pod, or as a background
    process within the container. This proxies the
    Kubernetes API to the localhost interface of the pod, so that other processes
    in any container of the pod can access it.
  - use the Go client library, and create a client using the `rest.InClusterConfig()` and `kubernetes.NewForConfig()` functions.
    They handle locating and authenticating to the apiserver. [example](https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go)

In each case, the credentials of the pod are used to communicate securely with the apiserver.
-->
<p>在 Pod 中，建议连接 API 的方法是：</p>
<ul>
<li>在 Pod 的边车容器中运行 <code>kubectl proxy</code>，或者以后台进程的形式运行。
这将把 Kubernetes API 代理到当前 Pod 的 localhost 接口，
所以 Pod 中的所有容器中的进程都能访问它。</li>
<li>使用 Go 客户端库，并使用 <code>rest.InClusterConfig()</code> 和
<code>kubernetes.NewForConfig()</code> 函数创建一个客户端。
他们处理 apiserver 的定位和身份验证。
<a href="https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go">示例</a></li>
</ul>
<p>在每种情况下，Pod 的凭证都是为了与 apiserver 安全地通信。</p>
<!--
## Accessing services running on the cluster

The previous section was about connecting the Kubernetes API server.  This section is about
connecting to other services running on Kubernetes cluster.  In Kubernetes, the
[nodes](/docs/admin/node), [pods](/docs/user-guide/pods) and [services](/docs/user-guide/services) all have
their own IPs.  In many cases, the node IPs, pod IPs, and some service IPs on a cluster will not be
routable, so they will not be reachable from a machine outside the cluster,
such as your desktop machine.
-->
<h2 id="accessing-services-running-on-the-cluster">访问集群中正在运行的服务 </h2>
<p>上一节介绍了如何连接 Kubernetes API 服务。本节介绍如何连接到 Kubernetes
集群上运行的其他服务。
在 Kubernetes 中，<a href="/zh/docs/concepts/architecture/nodes/">节点</a>、
<a href="/zh/docs/concepts/workloads/pods/">pods</a> 和
<a href="/zh/docs/concepts/services-networking/service/">服务</a> 都有自己的 IP。
在许多情况下，集群上的节点 IP、Pod IP 和某些服务 IP 将无法路由，
因此无法从集群外部的计算机（例如桌面计算机）访问它们。</p>
<!--
### Ways to connect

You have several options for connecting to nodes, pods and services from outside the cluster:

  - Access services through public IPs.
    - Use a service with type `NodePort` or `LoadBalancer` to make the service reachable outside
      the cluster.  See the [services](/docs/user-guide/services) and
      [kubectl expose](/docs/reference/generated/kubectl/kubectl-commands/#expose) documentation.
    - Depending on your cluster environment, this may only expose the service to your corporate network,
      or it may expose it to the internet.  Think about whether the service being exposed is secure.
      Does it do its own authentication?
    - Place pods behind services.  To access one specific pod from a set of replicas, such as for debugging,
      place a unique label on the pod and create a new service which selects this label.
    - In most cases, it should not be necessary for application developer to directly access
      nodes via their nodeIPs.
-->
<h3 id="ways-to-connect">连接的方法  </h3>
<p>有多种方式可以从集群外部连接节点、Pod 和服务：</p>
<ul>
<li>
<p>通过公共 IP 访问服务。</p>
<ul>
<li>类型为 <code>NodePort</code> 或 <code>LoadBalancer</code> 的服务，集群外部可以访问。
请参阅 <a href="/zh/docs/concepts/services-networking/service/">服务</a> 和
<a href="/docs/reference/generated/kubectl/kubectl-commands/#expose">kubectl expose</a> 文档。</li>
<li>取决于你的集群环境，该服务可能仅暴露给你的公司网络，或者也可能暴露给
整个互联网。
请考虑公开该服务是否安全。它是否进行自己的身份验证？</li>
<li>在服务后端放置 Pod。要从一组副本中访问一个特定的 Pod，例如进行调试，
请在 Pod 上设置一个唯一的标签，然后创建一个选择此标签的新服务。</li>
<li>在大多数情况下，应用程序开发人员不应该通过其 nodeIP 直接访问节点。</li>
</ul>
</li>
</ul>
<!--
  - Access services, nodes, or pods using the Proxy Verb.
    - Does apiserver authentication and authorization prior to accessing the remote service.
      Use this if the services are not secure enough to expose to the internet, or to gain
      access to ports on the node IP, or for debugging.
    - Proxies may cause problems for some web applications.
    - Only works for HTTP/HTTPS.
    - Described [here](#manually-constructing-apiserver-proxy-urls).
-->
<ul>
<li>使用 proxy 动词访问服务、节点或者 Pod。
<ul>
<li>在访问远程服务之前进行 apiserver 身份验证和授权。
如果服务不能够安全地暴露到互联网，或者服务不能获得节点 IP 端口的
访问权限，或者是为了调试，那么请使用此选项。</li>
<li>代理可能会给一些 web 应用带来问题。</li>
<li>只适用于 HTTP/HTTPS。</li>
<li>更多详细信息在<a href="#manually-constructing-apiserver-proxy-urls">这里</a>。</li>
</ul>
</li>
</ul>
<!--
  - Access from a node or pod in the cluster.
    - Run a pod, and then connect to a shell in it using [kubectl exec](/docs/reference/generated/kubectl/kubectl-commands/#exec).
      Connect to other nodes, pods, and services from that shell.
    - Some clusters may allow you to ssh to a node in the cluster.  From there you may be able to
      access cluster services.  This is a non-standard method, and will work on some clusters but
      not others.  Browsers and other tools may or may not be installed.  Cluster DNS may not work.
-->
<ul>
<li>
<p>从集群中的节点或者 Pod 中访问。</p>
<ul>
<li>运行一个 Pod，然后使用 <a href="/docs/reference/generated/kubectl/kubectl-commands/#exec">kubectl exec</a>
来连接 Pod 里的 Shell。
然后从 Shell 中连接其它的节点、Pod 和服务。</li>
<li>有些集群可能允许你通过 SSH 连接到节点，从那你可能可以访问集群的服务。
这是一个非正式的方式，可能可以运行在个别的集群上。
浏览器和其它一些工具可能没有被安装。集群的 DNS 可能无法使用。</li>
</ul>
</li>
</ul>
<!--
### Discovering builtin services

Typically, there are several services which are started on a cluster by kube-system. Get a list of these
with the `kubectl cluster-info` command:
-->
<h3 id="发现内建服务">发现内建服务</h3>
<p>通常来说，集群中会有 kube-system 创建的一些运行的服务。</p>
<p>通过 <code>kubectl cluster-info</code> 命令获得这些服务列表：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info
</code></pre></div><pre tabindex="0"><code>Kubernetes master is running at https://104.197.5.247
elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy
kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy
grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
</code></pre><!--
This shows the proxy-verb URL for accessing each service.
For example, this cluster has cluster-level logging enabled (using Elasticsearch), which can be reached
at `https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/` if suitable credentials are passed.  Logging can also be reached through a kubectl proxy, for example at:
`http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/`.
(See [Access Clusters Using the Kubernetes API](/docs/tasks/administer-cluster/access-cluster-api/) for how to pass credentials or use kubectl proxy.)
-->
<p>这展示了访问每个服务的 proxy-verb URL。
例如，如果集群启动了集群级别的日志（使用 Elasticsearch），并且传递合适的凭证，
那么可以通过
<code>https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>
进行访问。日志也能通过 kubectl 代理获取，例如：
<code>http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/</code>。
（参阅<a href="/zh/docs/tasks/administer-cluster/access-cluster-api/">使用 Kubernetes API 访问集群</a>
了解如何传递凭据，或者使用 kubectl proxy）</p>
<!--
#### Manually constructing apiserver proxy URLs

As mentioned above, you use the `kubectl cluster-info` command to retrieve the service's proxy URL. To create proxy URLs that include service endpoints, suffixes, and parameters, you append to the service's proxy URL:
`http://`*`kubernetes_master_address`*`/api/v1/namespaces/`*`namespace_name`*`/services/`*`service_name[:port_name]`*`/proxy`

If you haven't specified a name for your port, you don't have to specify *port_name* in the URL. You can also use the port number in place of the *port_name* for both named and unnamed ports.

By default, the API server proxies to your service using http. To use https, prefix the service name with `https:`:
`http://`*`kubernetes_master_address`*`/api/v1/namespaces/`*`namespace_name`*`/services/`*`https:service_name:[port_name]`*`/proxy`

The supported formats for the name segment of the URL are:

* `<service_name>` - proxies to the default or unnamed port using http
* `<service_name>:<port_name>` - proxies to the specified port name or port number using http
* `https:<service_name>:` - proxies to the default or unnamed port using https (note the trailing colon)
* `https:<service_name>:<port_name>` - proxies to the specified port name or port number using https
-->
<h4 id="manually-constructing-apiserver-proxy-urls">手动构建 apiserver 代理 URL</h4>
<p>如上所述，你可以使用 <code>kubectl cluster-info</code> 命令来获得服务的代理 URL。
要创建包含服务端点、后缀和参数的代理 URL，需添加到服务的代理 URL：
<code>http://</code><em><code>kubernetes_master_address</code></em><code>/api/v1/namespaces/</code><em><code>namespace_name</code></em><code>/services/</code><em><code>service_name[:port_name]</code></em><code>/proxy</code></p>
<p>如果尚未为端口指定名称，则不必在 URL 中指定 <em>port_name</em>。
对于已命名和未命名的端口，也可以使用端口号代替 <em>port_name</em>。</p>
<p>默认情况下，API server 使用 HTTP 代理你的服务。
要使用 HTTPS，请在服务名称前加上 <code>https:</code>：
<code>http://</code><em><code>kubernetes_master_address</code></em><code>/api/v1/namespaces/</code><em><code>namespace_name</code></em><code>/services/</code><em><code>https:service_name:[port_name]</code></em><code>/proxy</code></p>
<p>URL 名称段支持的格式为：</p>
<ul>
<li><code>&lt;service_name&gt;</code> - 使用 http 代理到默认或未命名的端口</li>
<li><code>&lt;service_name&gt;:&lt;port_name&gt;</code> - 使用 http 代理到指定的端口名称或端口号</li>
<li><code>https:&lt;service_name&gt;:</code> - 使用 https 代理到默认或未命名的端口（注意后面的冒号）</li>
<li><code>https:&lt;service_name&gt;:&lt;port_name&gt;</code> - 使用 https 代理到指定的端口名称或端口号</li>
</ul>
<!--
##### Examples

 * To access the Elasticsearch service endpoint `_search?q=user:kimchy`, you would use:   `http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy`
 * To access the Elasticsearch cluster health information `_cluster/health?pretty=true`, you would use:   `https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true`
-->
<h5 id="示例">示例</h5>
<ul>
<li>要访问 Elasticsearch  服务端点 <code>_search?q=user:kimchy</code>，你需要使用：
<code>http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy</code></li>
<li>要访问 Elasticsearch 集群健康信息 <code>_cluster/health?pretty=true</code>，你需要使用：
<code>https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">  {
    <span style="color:#008000;font-weight:bold">&#34;cluster_name&#34;</span> : <span style="color:#b44">&#34;kubernetes_logging&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;status&#34;</span> : <span style="color:#b44">&#34;yellow&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;timed_out&#34;</span> : <span style="color:#a2f;font-weight:bold">false</span>,
    <span style="color:#008000;font-weight:bold">&#34;number_of_nodes&#34;</span> : <span style="color:#666">1</span>,
    <span style="color:#008000;font-weight:bold">&#34;number_of_data_nodes&#34;</span> : <span style="color:#666">1</span>,
    <span style="color:#008000;font-weight:bold">&#34;active_primary_shards&#34;</span> : <span style="color:#666">5</span>,
    <span style="color:#008000;font-weight:bold">&#34;active_shards&#34;</span> : <span style="color:#666">5</span>,
    <span style="color:#008000;font-weight:bold">&#34;relocating_shards&#34;</span> : <span style="color:#666">0</span>,
    <span style="color:#008000;font-weight:bold">&#34;initializing_shards&#34;</span> : <span style="color:#666">0</span>,
    <span style="color:#008000;font-weight:bold">&#34;unassigned_shards&#34;</span> : <span style="color:#666">5</span>
  }
</code></pre></div><!--
### Using web browsers to access services running on the cluster

You may be able to put an apiserver proxy url into the address bar of a browser. However:

  - Web browsers cannot usually pass tokens, so you may need to use basic (password) auth.  Apiserver can be configured to accept basic auth,
    but your cluster may not be configured to accept basic auth.
  - Some web apps may not work, particularly those with client side javascript that construct urls in a
    way that is unaware of the proxy path prefix.
-->
<h3 id="使用-web-浏览器访问运行在集群上的服务">使用 web 浏览器访问运行在集群上的服务</h3>
<p>你可以在浏览器地址栏中输入 apiserver 代理 URL。但是：</p>
<ul>
<li>Web 浏览器通常不能传递令牌，因此你可能需要使用基本（密码）身份验证。
Apiserver 可以配置为接受基本身份验证，但你的集群可能未进行配置。</li>
<li>某些 Web 应用程序可能无法运行，尤其是那些使用客户端 javascript
以不知道代理路径前缀的方式构建 URL 的应用程序。</li>
</ul>
<!--
## Requesting redirects

The redirect capabilities have been deprecated and removed.  Please use a proxy (see below) instead.
-->
<h2 id="请求重定向">请求重定向</h2>
<p>重定向功能已弃用并被删除。请改用代理（见下文）。</p>
<!--
## So Many Proxies

There are several different proxies you may encounter when using Kubernetes:

1.  The [kubectl proxy](#directly-accessing-the-rest-api):

    - runs on a user's desktop or in a pod
    - proxies from a localhost address to the Kubernetes apiserver
    - client to proxy uses HTTP
    - proxy to apiserver uses HTTPS
    - locates apiserver
    - adds authentication headers

-->
<h2 id="多种代理">多种代理</h2>
<p>使用 Kubernetes 时可能会遇到几种不同的代理：</p>
<ol>
<li>
<p><a href="#directly-accessing-the-rest-api">kubectl 代理</a>：</p>
<ul>
<li>在用户的桌面或 Pod 中运行</li>
<li>代理从本地主机地址到 Kubernetes apiserver</li>
<li>客户端到代理将使用 HTTP</li>
<li>代理到 apiserver 使用 HTTPS</li>
<li>定位 apiserver</li>
<li>添加身份验证头部</li>
</ul>
</li>
</ol>
<!--
1.  The [apiserver proxy](#discovering-builtin-services):

    - is a bastion built into the apiserver
    - connects a user outside of the cluster to cluster IPs which otherwise might not be reachable
    - runs in the apiserver processes
    - client to proxy uses HTTPS (or http if apiserver so configured)
    - proxy to target may use HTTP or HTTPS as chosen by proxy using available information
    - can be used to reach a Node, Pod, or Service
    - does load balancing when used to reach a Service
-->
<ol start="2">
<li>
<p><a href="#discovering-builtin-services">apiserver 代理</a>：</p>
<ul>
<li>内置于 apiserver 中</li>
<li>将集群外部的用户连接到集群 IP，否则这些 IP 可能无法访问</li>
<li>运行在 apiserver 进程中</li>
<li>客户端代理使用 HTTPS（也可配置为 http）</li>
<li>代理将根据可用的信息决定使用 HTTP 或者 HTTPS 代理到目标</li>
<li>可用于访问节点、Pod 或服务</li>
<li>在访问服务时进行负载平衡</li>
</ul>
</li>
</ol>
<!--
1.  The [kube proxy](/docs/concepts/services-networking/service/#ips-and-vips):

    - runs on each node
    - proxies UDP and TCP
    - does not understand HTTP
    - provides load balancing
    - is only used to reach services
-->
<ol start="3">
<li>
<p><a href="/zh/docs/concepts/services-networking/service/#ips-and-vips">kube proxy</a>：</p>
<ul>
<li>运行在每个节点上</li>
<li>代理 UDP 和 TCP</li>
<li>不能代理 HTTP</li>
<li>提供负载均衡</li>
<li>只能用来访问服务</li>
</ul>
</li>
</ol>
<!--
1.  A Proxy/Load-balancer in front of apiserver(s):

    - existence and implementation varies from cluster to cluster (e.g. nginx)
    - sits between all clients and one or more apiservers
    - acts as load balancer if there are several apiservers.
-->
<ol start="4">
<li>
<p>位于 apiserver 之前的 Proxy/Load-balancer：</p>
<ul>
<li>存在和实现因集群而异（例如 nginx）</li>
<li>位于所有客户和一个或多个 apiserver 之间</li>
<li>如果有多个 apiserver，则充当负载均衡器</li>
</ul>
</li>
</ol>
<!--
1.  Cloud Load Balancers on external services:

    - are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)
    - are created automatically when the Kubernetes service has type `LoadBalancer`
    - use UDP/TCP only
    - implementation varies by cloud provider.

Kubernetes users will typically not need to worry about anything other than the first two types.  The cluster admin
will typically ensure that the latter types are setup correctly.
-->
<ol start="5">
<li>
<p>外部服务上的云负载均衡器：</p>
<ul>
<li>由一些云提供商提供（例如 AWS ELB，Google Cloud Load Balancer）</li>
<li>当 Kubernetes 服务类型为 <code>LoadBalancer</code> 时自动创建</li>
<li>只使用 UDP/TCP</li>
<li>具体实现因云提供商而异。</li>
</ul>
</li>
</ol>
<p>除了前两种类型之外，Kubernetes 用户通常不需要担心任何其他问题。
集群管理员通常会确保后者的正确配置。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-72d3dddbc0c166c9a364e753d2b31ff0">9.3 - 使用端口转发来访问集群中的应用</h1>
    
	<!--
title: Use Port Forwarding to Access Applications in a Cluster
content_type: task
weight: 40
-->
<!-- overview -->
<!--
This page shows how to use `kubectl port-forward` to connect to a MongoDB
server running in a Kubernetes cluster. This type of connection can be useful
for database debugging.
-->
<p>本文展示如何使用 <code>kubectl port-forward</code> 连接到在 Kubernetes 集群中
运行的 MongoDB 服务。这种类型的连接对数据库调试很有用。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!--
* Install [MongoDB Shell](https://www.mongodb.com/try/download/shell).
-->
<ul>
<li>安装 <a href="https://www.mongodb.com/try/download/shell">MongoDB Shell</a>。</li>
</ul>
<!-- steps -->
<!--
## Creating MongoDB deployment and service

1. Create a Deployment that runs MongoDB:
-->
<h2 id="创建-mongodb-deployment-和服务">创建 MongoDB deployment 和服务</h2>
<ol>
<li>
<p>创建一个运行 MongoDB 的 deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-deployment.yaml
</code></pre></div><!--
The output of a successful command verifies that the deployment was created:
-->
<p>查看输出是否成功，以验证是否成功创建 deployment：</p>
<pre tabindex="0"><code>deployment.apps/mongo created
</code></pre><!--
View the pod status to check that it is ready:
-->
<p>查看 pod 状态，检查其是否准备就绪：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!--
The output displays the pod created:
-->
<p>输出显示创建的 pod：</p>
<pre tabindex="0"><code>NAME                     READY   STATUS    RESTARTS   AGE
mongo-75f59d57f4-4nd6q   1/1     Running   0          2m4s
</code></pre><!--
View the Deployment's status:
-->
<p>查看 Deployment 状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployment
</code></pre></div><!--
The output displays that the Deployment was created:
-->
<p>输出显示创建的 Deployment：</p>
<pre tabindex="0"><code>NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mongo   1/1     1            1           2m21s
</code></pre><!--
The Deployment automatically manages a ReplicaSet.
View the ReplicaSet status using:
-->
<p>Deployment 自动管理 ReplicaSet。
查看 ReplicaSet 状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get replicaset
</code></pre></div><!--
The output displays that the ReplicaSet was created:
-->
<p>输出显示创建的 ReplicaSet：</p>
<pre tabindex="0"><code>NAME               DESIRED   CURRENT   READY   AGE
mongo-75f59d57f4   1         1         1       3m12s
</code></pre></li>
</ol>
<!--
2. Create a Service to expose MongoDB on the network:
-->
<ol start="2">
<li>
<p>创建一个在网络上公开的 MongoDB 服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-service.yaml
</code></pre></div><!--
The output of a successful command verifies that the Service was created:
-->
<p>查看输出是否成功，以验证是否成功创建 Service：</p>
<pre tabindex="0"><code>service/mongo created
</code></pre><!--
Check the Service created:
-->
<p>检查 Service 是否创建：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service mongo
</code></pre></div><!--   
The output displays the service created:
-->
<p>输出显示创建的 Service：</p>
<pre tabindex="0"><code>NAME    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
mongo   ClusterIP   10.96.41.183   &lt;none&gt;        27017/TCP   11s
</code></pre></li>
</ol>
<!--
3. Verify that the MongoDB server is running in the Pod, and listening on port 27017:
-->
<ol start="3">
<li>
<p>验证 MongoDB 服务是否运行在 Pod 中并且监听 27017 端口：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># Change mongo-75f59d57f4-4nd6q to the name of the Pod</span>
kubectl get pod mongo-75f59d57f4-4nd6q --template<span style="color:#666">=</span><span style="color:#b44">&#39;{{(index (index .spec.containers 0).ports 0).containerPort}}{{&#34;\n&#34;}}&#39;</span>
</code></pre></div><!--
The output displays the port for MongoDB in that Pod:
-->
<p>输出应该显示 Pod 中 MongoDB 的端口：</p>
<pre tabindex="0"><code>27017
</code></pre><!--
(this is the TCP port allocated to MongoDB on the internet).
-->
<p>（这是 Internet 分配给 MongoDB 的 TCP 端口）。</p>
</li>
</ol>
<!--
## Forward a local port to a port on the Pod

1.  `kubectl port-forward` allows using resource name, such as a pod name, to select a matching pod to port forward to.
-->
<h2 id="转发一个本地端口到-pod-端口">转发一个本地端口到 Pod 端口</h2>
<ol>
<li>
<p><code>kubectl port-forward</code> 允许使用资源名称
（例如 pod 名称）来选择匹配的 pod 来进行端口转发。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># Change mongo-75f59d57f4-4nd6q to the name of the Pod</span>
kubectl port-forward mongo-75f59d57f4-4nd6q 28015:27017
</code></pre></div><!--
which is the same as
-->
<p>这相当于</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017
</code></pre></div><!-- or -->
<p>或者</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl port-forward deployment/mongo 28015:27017
</code></pre></div><!-- or -->
<p>或者</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl port-forward replicaset/mongo-75f59d57f4 28015:27017
</code></pre></div><!-- or -->
<p>或者</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl port-forward service/mongo 28015:27017
</code></pre></div><!--
Any of the above commands works. The output is similar to this:
-->
<p>以上所有命令都应该有效。输出应该类似于：</p>
<pre tabindex="0"><code>Forwarding from 127.0.0.1:28015 -&gt; 27017
Forwarding from [::1]:28015 -&gt; 27017
</code></pre></li>
</ol>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>kubectl port-forward</code> does not return. To continue with the exercises, you will need to open another terminal.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>kubectl port-forward</code> 不会返回。你需要打开另一个终端来继续这个练习。</div>
</blockquote>
<!--
2.  Start the MongoDB command line interface:
-->
<ol start="2">
<li>启动 MongoDB 命令行接口：</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">mongosh --port <span style="color:#666">28015</span>
</code></pre></div><!--
3.  At the MongoDB command line prompt, enter the `ping` command:
-->
<ol start="3">
<li>
<p>在 MongoDB 命令行提示符下，输入 <code>ping</code> 命令：</p>
<pre tabindex="0"><code>db.runCommand( { ping: 1 } )
</code></pre><!--
A successful ping request returns:
-->
<p>成功的 ping 请求应该返回：</p>
<pre tabindex="0"><code>{ ok: 1 }
</code></pre></li>
</ol>
<!--
### Optionally let _kubectl_ choose the local port {#let-kubectl-choose-local-port}
-->
<h3 id="let-kubectl-choose-local-port">（可选操作）让 <em>kubectl</em> 来选择本地端口</h3>
<!--
If you don't need a specific local port, you can let `kubectl` choose and allocate 
the local port and thus relieve you from having to manage local port conflicts, with 
the slightly simpler syntax:
-->
<p>如果你不需要指定特定的本地端口，你可以让 <code>kubectl</code> 来选择和分配本地端口，
以便你不需要管理本地端口冲突。该命令使用稍微不同的语法：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl port-forward deployment/mongo :27017
</code></pre></div><!--
The `kubectl` tool finds a local port number that is not in use (avoiding low ports numbers,
because these might be used by other applications). The output is similar to:
-->
<p><code>kubectl</code> 工具会找到一个未被使用的本地端口号（避免使用低段位的端口号，因为他们可能会被其他应用程序使用）。
输出应该类似于：</p>
<pre tabindex="0"><code>Forwarding from 127.0.0.1:63753 -&gt; 27017
Forwarding from [::1]:63753 -&gt; 27017
</code></pre><!-- discussion -->
<!--
## Discussion

Connections made to local port 28015 are forwarded to port 27017 of the Pod that
is running the MongoDB server. With this connection in place, you can use your
local workstation to debug the database that is running in the Pod.
-->
<h2 id="discussion">讨论 </h2>
<p>与本地 28015 端口建立的连接将转发到运行 MongoDB 服务器的 Pod 的 27017 端口。
通过此连接，您可以使用本地工作站来调试在 Pod 中运行的数据库。</p>
<!--
`kubectl port-forward` is implemented for TCP ports only.
The support for UDP protocol is tracked in
[issue 47862](https://github.com/kubernetes/kubernetes/issues/47862).
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> <code>kubectl port-forward</code> 仅适用于 TCP 端口。
在 <a href="https://github.com/kubernetes/kubernetes/issues/47862">issue 47862</a>
中跟踪了对 UDP 协议的支持。</div>
</blockquote>

<h2 id="接下来">接下来</h2>
<!--
Learn more about [kubectl port-forward](/docs/reference/generated/kubectl/kubectl-commands/#port-forward).
-->
<p>进一步了解 <a href="/docs/reference/generated/kubectl/kubectl-commands/#port-forward">kubectl port-forward</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-312f29f850826b74618634cd877aa065">9.4 - 使用服务来访问集群中的应用</h1>
    
	<!--
title: Use a Service to Access an Application in a Cluster
content_type: tutorial
weight: 60
-->
<!-- overview -->
<!--
This page shows how to create a Kubernetes Service object that external
clients can use to access an application running in a cluster. The Service
provides load balancing for an application that has two running instances.
-->
<p>本文展示如何创建一个 Kubernetes 服务对象，能让外部客户端访问在集群中运行的应用。
该服务为一个应用的两个运行实例提供负载均衡。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<h2 id="教程目标">教程目标</h2>
<!--
* Run two instances of a Hello World application.
* Create a Service object that exposes a node port.
* Use the Service object to access the running application.
-->
<ul>
<li>运行 Hello World 应用的两个实例。</li>
<li>创建一个服务对象来暴露 node port。</li>
<li>使用服务对象来访问正在运行的应用。</li>
</ul>
<!-- lessoncontent -->
<!--
## Creating a service for an application running in two pods

Here is the configuration file for the application Deployment:
-->
<h2 id="为运行在两个-pod-中的应用创建一个服务">为运行在两个 pod 中的应用创建一个服务</h2>
<p>这是应用程序部署的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/access/hello-application.yaml" download="service/access/hello-application.yaml"><code>service/access/hello-application.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-access-hello-application-yaml')" title="Copy service/access/hello-application.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-access-hello-application-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello-world<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>load-balancer-example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">run</span>:<span style="color:#bbb"> </span>load-balancer-example<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello-world<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/google-samples/node-hello:1.0<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
1. Run a Hello World application in your cluster:
   Create the application Deployment using the file above:
   ```shell
   kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml
   ```
   The preceding command creates a
   [Deployment](/docs/concepts/workloads/controllers/deployment/)
   object and an associated
   [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/)
   object. The ReplicaSet has two
   [Pods](/docs/concepts/workloads/pods/pod/),
   each of which runs the Hello World application.
-->
<ol>
<li>
<p>在你的集群中运行一个 Hello World 应用：
使用上面的文件创建应用程序 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml
</code></pre></div><p>上面的命令创建一个 <a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a> 对象
和一个关联的 <a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> 对象。
这个 ReplicaSet 有两个 <a href="/zh/docs/concepts/workloads/pods/">Pod</a>，
每个 Pod 都运行着 Hello World 应用。</p>
</li>
</ol>
<!--
1. Display information about the Deployment:
-->
<ol start="2">
<li>
<p>展示 Deployment 的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get deployments hello-world
kubectl describe deployments hello-world
</code></pre></div></li>
</ol>
<!--
1. Display information about your ReplicaSet objects:
-->
<ol start="3">
<li>
<p>展示你的 ReplicaSet 对象信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get replicasets
kubectl describe replicasets
</code></pre></div></li>
</ol>
<!--
1. Create a Service object that exposes the deployment:
-->
<ol start="4">
<li>
<p>创建一个服务对象来暴露 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment hello-world --type<span style="color:#666">=</span>NodePort --name<span style="color:#666">=</span>example-service
</code></pre></div></li>
</ol>
<!--
1. Display information about the Service:
-->
<ol start="5">
<li>
<p>展示 Service 信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe services example-service
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Name:                   example-service
Namespace:              default
Labels:                 <span style="color:#b8860b">run</span><span style="color:#666">=</span>load-balancer-example
Annotations:            &lt;none&gt;
Selector:               <span style="color:#b8860b">run</span><span style="color:#666">=</span>load-balancer-example
Type:                   NodePort
IP:                     10.32.0.16
Port:                   &lt;unset&gt; 8080/TCP
TargetPort:             8080/TCP
NodePort:               &lt;unset&gt; 31496/TCP
Endpoints:              10.200.1.4:8080,10.200.2.5:8080
Session Affinity:       None
Events:                 &lt;none&gt;
</code></pre></div><!--
Make a note of the NodePort value for the service. For example,
in the preceding output, the NodePort value is 31496.
-->
<p>注意服务中的 NodePort 值。例如在上面的输出中，NodePort 是 31496。</p>
</li>
</ol>
<!--
1. List the pods that are running the Hello World application:
-->
<ol start="7">
<li>
<p>列出运行 Hello World 应用的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --selector<span style="color:#666">=</span><span style="color:#b44">&#34;run=load-balancer-example&#34;</span> --output<span style="color:#666">=</span>wide
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                           READY   STATUS    ...  IP           NODE
hello-world-2895499144-bsbk5   1/1     Running   ...  10.200.1.4   worker1
hello-world-2895499144-m1pwt   1/1     Running   ...  10.200.2.5   worker2
</code></pre></div></li>
</ol>
<!--
1. Get the public IP address of one of your nodes that is running
   a Hello World pod. How you get this address depends on how you set
   up your cluster. For example, if you are using Minikube, you can
   see the node address by running `kubectl cluster-info`. If you are
   using Google Compute Engine instances, you can use the
   `gcloud compute instances list` command to see the public addresses of your
   nodes.

1. On your chosen node, create a firewall rule that allows TCP traffic
   on your node port. For example, if your Service has a NodePort value of
   31568, create a firewall rule that allows TCP traffic on port 31568. Different
   cloud providers offer different ways of configuring firewall rules.

1. Use the node address and node port to access the Hello World application:
-->
<ol start="8">
<li>
<p>获取运行 Hello World 的 pod 的其中一个节点的公共 IP 地址。如何获得此地址取决于你设置集群的方式。
例如，如果你使用的是 Minikube，则可以通过运行 <code>kubectl cluster-info</code> 来查看节点地址。
如果你使用的是 Google Compute Engine 实例，则可以使用 <code>gcloud compute instances list</code> 命令查看节点的公共地址。</p>
</li>
<li>
<p>在你选择的节点上，创建一个防火墙规则以开放节点端口上的 TCP 流量。
例如，如果你的服务的 NodePort 值为 31568，请创建一个防火墙规则以允许 31568 端口上的 TCP 流量。
不同的云提供商提供了不同方法来配置防火墙规则。</p>
</li>
<li>
<p>使用节点地址和 node port 来访问 Hello World 应用：</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl http://&lt;public-node-ip&gt;:&lt;node-port&gt;
</code></pre></div>   <!--
   where `<public-node-ip>` is the public IP address of your node,
   and `<node-port>` is the NodePort value for your service. The
   response to a successful request is a hello message:
   -->
<p>这里的 <code>&lt;public-node-ip&gt;</code> 是你节点的公共 IP 地址，<code>&lt;node-port&gt;</code> 是你服务的 NodePort 值。
对于请求成功的响应是一个 hello 消息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Hello Kubernetes!
</code></pre></div><!--
## Using a service configuration file

As an alternative to using `kubectl expose`, you can use a
[service configuration file](/docs/concepts/services-networking/service/)
to create a Service.
-->
<h2 id="使用服务配置文件">使用服务配置文件</h2>
<p>作为 <code>kubectl expose</code> 的替代方法，你可以使用
<a href="/zh/docs/concepts/services-networking/service/">服务配置文件</a> 来创建服务。</p>
<h2 id="清理现场">清理现场</h2>
<!--
To delete the Service, enter this command:
-->
<p>想要删除服务，输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete services example-service
</code></pre></div><!--
To delete the Deployment, the ReplicaSet, and the Pods that are running
the Hello World application, enter this command:
-->
<p>想要删除运行 Hello World 应用的 Deployment、ReplicaSet 和 Pod，输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment hello-world
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
Learn more about
[connecting applications with services](/docs/concepts/services-networking/connect-applications-service/).
-->
<ul>
<li>进一步了解<a href="/zh/docs/concepts/services-networking/connect-applications-service/">通过服务连接应用</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f3dac629bea950fc026d920306f09fb4">9.5 - 使用 Service 把前端连接到后端</h1>
    
	<!--
title: Connect a Frontend to a Backend Using Services
content_type: tutorial
weight: 70
-->
<!-- overview -->
<!--
This task shows how to create a _frontend_ and a _backend_ microservice. The backend 
microservice is a hello greeter. The frontend exposes the backend using nginx and a 
Kubernetes <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务（Service）'>服务（Service）</a> object.
-->
<p>本任务会描述如何创建前端（Frontend）微服务和后端（Backend）微服务。后端微服务是一个 hello 欢迎程序。
前端通过 nginx 和一个 Kubernetes <a class='glossary-tooltip' title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/services-networking/service/' target='_blank' aria-label='服务'>服务</a>
暴露后端所提供的服务。</p>
<h2 id="教程目标">教程目标</h2>
<!--
* Create and run a sample `hello` backend microservice using a
  <a class='glossary-tooltip' title='Deployment 是管理应用副本的 API 对象。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/deployment/' target='_blank' aria-label='Deployment'>Deployment</a> object.
* Use a Service object to send traffic to the backend microservice's multiple replicas.
* Create and run a `nginx` frontend microservice, also using a Deployment object.
* Configure the frontend microservice to send traffic to the backend microservice.
* Use a Service object of `type=LoadBalancer` to expose the frontend microservice
  outside the cluster.
-->
<ul>
<li>使用部署对象（Deployment object）创建并运行一个 <code>hello</code> 后端微服务</li>
<li>使用一个 Service 对象将请求流量发送到后端微服务的多个副本</li>
<li>同样使用一个 Deployment 对象创建并运行一个 <code>nginx</code> 前端微服务</li>
<li>配置前端微服务将请求流量发送到后端微服务</li>
<li>使用 <code>type=LoadBalancer</code> 的 Service 对象将全段微服务暴露到集群外部</li>
</ul>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
This task uses
[Services with external load balancers](/docs/tasks/access-application-cluster/create-external-load-balancer/), which
require a supported environment. If your environment does not support this, you can use a Service of type
[NodePort](/docs/concepts/services-networking/service/#nodeport) instead.
-->
<p>本任务使用<a href="/zh/docs/tasks/access-application-cluster/create-external-load-balancer/">外部负载均衡服务</a>，
所以需要对应的可支持此功能的环境。如果你的环境不能支持，你可以使用
<a href="/zh/docs/concepts/services-networking/service/#nodeport">NodePort</a>
类型的服务代替。</p>
<!-- lessoncontent -->
<!--
## Creating the backend using a Deployment

The backend is a simple hello greeter microservice. Here is the configuration
file for the backend Deployment:
-->
<h3 id="使用部署对象-deployment-创建后端">使用部署对象（Deployment）创建后端</h3>
<p>后端是一个简单的 hello 欢迎微服务应用。这是后端应用的 Deployment 配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/access/backend-deployment.yaml" download="service/access/backend-deployment.yaml"><code>service/access/backend-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-access-backend-deployment-yaml')" title="Copy service/access/backend-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-access-backend-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;gcr.io/google-samples/hello-go-gke:1.0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span></code></pre></div>
    </div>
</div>


<!-- 
Create the backend Deployment:
-->
<p>创建后端 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/access/backend-deployment.yaml
</code></pre></div><!--
View information about the backend Deployment:
-->
<p>查看后端的 Deployment 信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe deployment backend
</code></pre></div><!--
The output is similar to this:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code>Name:                           backend
Namespace:                      default
CreationTimestamp:              Mon, 24 Oct 2016 14:21:02 -0700
Labels:                         app=hello
                                tier=backend
                                track=stable
Annotations:                    deployment.kubernetes.io/revision=1
Selector:                       app=hello,tier=backend,track=stable
Replicas:                       3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:                   RollingUpdate
MinReadySeconds:                0
RollingUpdateStrategy:          1 max unavailable, 1 max surge
Pod Template:
  Labels:       app=hello
                tier=backend
                track=stable
  Containers:
   hello:
    Image:              &quot;gcr.io/google-samples/hello-go-gke:1.0&quot;
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
OldReplicaSets:                 &lt;none&gt;
NewReplicaSet:                  hello-3621623197 (3/3 replicas created)
Events:
...
</code></pre><!--
## Creating the `hello` Service object

The key to sending requests from a frontend to a backend is the backend
Service. A Service creates a persistent IP address and DNS name entry
so that the backend microservice can always be reached. A Service uses
<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='selectors'>selectors</a> to find
the Pods that it routes traffic to.

First, explore the Service configuration file:
-->
<h3 id="创建-hello-service-对象">创建 <code>hello</code> Service 对象</h3>
<p>将请求从前端发送到到后端的关键是后端 Service。Service 创建一个固定 IP 和 DNS 解析名入口，
使得后端微服务总是可达。Service 使用
<a class='glossary-tooltip' title='选择算符允许用户通过标签对一组资源对象进行筛选过滤。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/overview/working-with-objects/labels/' target='_blank' aria-label='选择算符'>选择算符</a>
来寻找目标 Pod。</p>
<p>首先，浏览 Service 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/access/backend-service.yaml" download="service/access/backend-service.yaml"><code>service/access/backend-service.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-access-backend-service-yaml')" title="Copy service/access/backend-service.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-access-backend-service-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>backend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span>http<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Service named `hello` routes
traffic to Pods that have the labels `app: hello` and `tier: backend`.
-->
<p>配置文件中，你可以看到名为 <code>hello</code> 的 Service 将流量路由到包含 <code>app: hello</code>
和 <code>tier: backend</code> 标签的 Pod。</p>
<!--
Create the backend Service:
-->
<p>创建后端 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/access/backend-service.yaml
</code></pre></div><!--
At this point, you have a `backend` Deployment running three replicas of your `hello`
application, and you have a Service that can route traffic to them. However, this
service is neither available nor resolvable outside the cluster.
-->
<p>此时，你已经有了一个运行着 <code>hello</code> 应用的三个副本的 <code>backend</code> Deployment，你也有了
一个 Service 用于路由网络流量。不过，这个服务在集群外部无法访问也无法解析。</p>
<!--
## Creating the frontend

Now that you have your backend running, you can create a frontend that is accessible 
outside the cluster, and connects to the backend by proxying requests to it.

The frontend sends requests to the backend worker Pods by using the DNS name
given to the backend Service. The DNS name is `hello`, which is the value
of the `name` field in the `examples/service/access/backend-service.yaml` 
configuration file.

The Pods in the frontend Deployment run an nginx image that is configured
to proxy requests to the hello backend Service. Here is the nginx configuration file:
-->
<h3 id="创建前端应用">创建前端应用</h3>
<p>现在你已经有了运行中的后端应用，你可以创建一个可在集群外部访问的前端，并通过代理
前端的请求连接到后端。</p>
<p>前端使用被赋予后端 Service 的 DNS 名称将请求发送到后端工作 Pods。这一 DNS
名称为 <code>hello</code>，也就是 <code>examples/service/access/backend-service.yaml</code> 配置
文件中 <code>name</code> 字段的取值。</p>
<p>前端 Deployment 中的 Pods 运行一个 nginx 镜像，这个已经配置好的镜像会将请求转发
给后端的 hello Service。下面是  nginx 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/access/frontend-nginx.conf" download="service/access/frontend-nginx.conf"><code>service/access/frontend-nginx.conf</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-access-frontend-nginx-conf')" title="Copy service/access/frontend-nginx.conf to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-access-frontend-nginx-conf">
    <pre tabindex="0"><code class="language-conf" data-lang="conf"># The identifier Backend is internal to nginx, and used to name this specific upstream
upstream Backend {
    # hello is the internal DNS name used by the backend Service inside Kubernetes
    server hello;
}

server {
    listen 80;

    location / {
        # The following statement will proxy traffic to the upstream named Backend
        proxy_pass http://Backend;
    }
}
</code></pre>
    </div>
</div>


<!--
Similar to the backend, the frontend has a Deployment and a Service. An important
difference to notice between the backend and frontend services, is that the
configuration for the frontend Service has `type: LoadBalancer`, which means that
the Service uses a load balancer provisioned by your cloud provider and will be
accessible from outside the cluster.
-->
<p>与后端类似，前端用包含一个 Deployment 和一个 Service。后端与前端服务之间的一个
重要区别是前端 Service 的配置文件包含了 <code>type: LoadBalancer</code>，也就是说，Service
会使用你的云服务商的默认负载均衡设备，从而实现从集群外访问的目的。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/access/frontend-service.yaml" download="service/access/frontend-service.yaml"><code>service/access/frontend-service.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-access-frontend-service-yaml')" title="Copy service/access/frontend-service.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-access-frontend-service-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;TCP&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span></code></pre></div>
    </div>
</div>




 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/access/frontend-deployment.yaml" download="service/access/frontend-deployment.yaml"><code>service/access/frontend-deployment.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-access-frontend-deployment-yaml')" title="Copy service/access/frontend-deployment.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-access-frontend-deployment-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hello<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">track</span>:<span style="color:#bbb"> </span>stable<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;gcr.io/google-samples/hello-frontend:1.0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">lifecycle</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">preStop</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">exec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/usr/sbin/nginx&#34;</span>,<span style="color:#b44">&#34;-s&#34;</span>,<span style="color:#b44">&#34;quit&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span></code></pre></div>
    </div>
</div>


<!--
Create the frontend Deployment and Service:
-->
<p>创建前端 Deployment 和 Service：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/access/frontend-deployment.yaml
kubectl apply -f https://k8s.io/examples/service/access/frontend-service.yaml
</code></pre></div><!--
The output verifies that both resources were created:
-->
<p>通过输出确认两个资源都已经被创建：</p>
<pre tabindex="0"><code>deployment.apps/frontend created
service/frontend created
</code></pre><!--
The nginx configuration is baked into the
[container image](/examples/service/access/Dockerfile). A better way to do this would
be to use a
[ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/),
so that you can change the configuration more easily.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 这个 nginx 配置文件是被打包在
<a href="/examples/service/access/Dockerfile">容器镜像</a> 里的。
更好的方法是使用
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a>，
这样的话你可以更轻易地更改配置。</div>
</blockquote>
<!--
## Interact with the frontend Service

Once you've created a Service of type LoadBalancer, you can use this
command to find the external IP:
-->
<h3 id="interact-with-the-frontend-service">与前端 Service 交互  </h3>
<p>一旦你创建了 LoadBalancer 类型的 Service，你可以使用这条命令查看外部 IP：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service frontend
</code></pre></div><!--
This displays the configuration for the `frontend` Service and watches for
changes. Initially, the external IP is listed as `<pending>`:
-->
<p>外部 IP 字段的生成可能需要一些时间。如果是这种情况，外部 IP 会显示为 <code>&lt;pending&gt;</code>。</p>
<pre tabindex="0"><code>NAME       CLUSTER-IP      EXTERNAL-IP   PORT(S)  AGE
frontend   10.51.252.116   &lt;pending&gt;     80/TCP   10s
</code></pre><!--
As soon as an external IP is provisioned, however, the configuration updates
to include the new IP under the `EXTERNAL-IP` heading:
-->
<p>当外部 IP 地址被分配可用时，配置会更新，在 <code>EXTERNAL-IP</code> 头部下显示新的 IP：</p>
<pre tabindex="0"><code>NAME       CLUSTER-IP      EXTERNAL-IP        PORT(S)  AGE
frontend   10.51.252.116   XXX.XXX.XXX.XXX    80/TCP   1m
</code></pre><!--
That IP can now be used to interact with the `frontend` service from outside the
cluster.
-->
<p>这一新的 IP 地址就可以用来从集群外与 <code>frontend</code> 服务交互了。</p>
<!--
## Send traffic through the frontend

The frontend and backend are now connected. You can hit the endpoint
by using the curl command on the external IP of your frontend Service.
-->
<h3 id="通过前端发送流量">通过前端发送流量</h3>
<p>前端和后端已经完成连接了。你可以使用 curl 命令通过你的前端 Service 的外部
IP 访问服务端点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl http://<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">EXTERNAL_IP</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#080;font-style:italic"># 将 EXTERNAL_P 替换为你之前看到的外部 IP</span>
</code></pre></div><!--
The output shows the message generated by the backend:
-->
<p>输出显示后端生成的消息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{<span style="color:#008000;font-weight:bold">&#34;message&#34;</span>:<span style="color:#b44">&#34;Hello&#34;</span>}
</code></pre></div><h2 id="清理现场">清理现场</h2>
<!--
To delete the Services, enter this command:
-->
<p>要删除服务，输入下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete services frontend backend
</code></pre></div><!--
To delete the Deployments, the ReplicaSets and the Pods that are running the backend and frontend applications, enter this command:
-->
<p>要删除在前端和后端应用中运行的 Deployment、ReplicaSet 和 Pod，输入下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete deployment frontend backend
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* Learn more about [Services](/docs/concepts/services-networking/service/)
* Learn more about [ConfigMaps](/docs/tasks/configure-pod-container/configure-pod-configmap/)
* Learn more about [DNS for Service and Pods](/docs/concepts/services-networking/dns-pod-service/)
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/services-networking/service/">Service</a></li>
<li>进一步了解 <a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap</a></li>
<li>进一步了解 <a href="/zh/docs/concepts/services-networking/dns-pod-service/">Service 和 Pods 的 DNS</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-21cd8f87563675fb0278d3694ba9ecb0">9.6 - 创建外部负载均衡器</h1>
    
	<!--
title: Create an External Load Balancer
content_type: task
weight: 80
-->
<!-- overview -->
<!--
This page shows how to create an External Load Balancer.
-->
<p>本文展示如何创建一个外部负载均衡器。</p>
<!-- 
This feature is only available for cloud providers or environments which support external load balancers. 
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 此功能仅适用于支持外部负载均衡器的云提供商或环境。</div>
</blockquote>
<!--
When creating a service, you have the option of automatically creating a
cloud network load balancer. This provides an externally-accessible IP address
that sends traffic to the correct port on your cluster nodes
_provided your cluster runs in a supported environment and is configured with
the correct cloud load balancer provider package_.
-->
<p>创建服务时，你可以选择自动创建云网络负载均衡器。这提供了一个外部可访问的 IP 地址，
可将流量分配到集群节点上的正确端口上
（ <em>假设集群在支持的环境中运行，并配置了正确的云负载平衡器提供商包</em>）。</p>
<!--
For information on provisioning and using an Ingress resource that can give
services externally-reachable URLs, load balance the traffic, terminate SSL etc.,
please check the [Ingress](/docs/concepts/services-networking/ingress/)
documentation.
-->
<p>有关如何配置和使用 Ingress 资源为服务提供外部可访问的 URL、负载均衡流量、终止 SSL 等功能，
请查看 <a href="/zh/docs/concepts/services-networking/ingress/">Ingress</a> 文档。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!-- steps -->
<!--
## Configuration file

To create an external load balancer, add the following line to your
[service configuration file](/docs/concepts/services-networking/service/#loadbalancer):
-->
<h2 id="配置文件">配置文件</h2>
<p>要创建外部负载均衡器，请将以下内容添加到
<a href="/zh/docs/concepts/services-networking/service/#loadbalancer">服务配置文件</a>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></code></pre></div><!--
Your configuration file might look like:
-->
<p>你的配置文件可能会如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8765</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></code></pre></div><!--
## Using kubectl

You can alternatively create the service with the `kubectl expose` command and
its `--type=LoadBalancer` flag:
-->
<h2 id="使用-kubectl">使用 kubectl</h2>
<p>你也可以使用 <code>kubectl expose</code> 命令及其 <code>--type=LoadBalancer</code> 参数创建服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl expose rc example --port<span style="color:#666">=</span><span style="color:#666">8765</span> --target-port<span style="color:#666">=</span><span style="color:#666">9376</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>        --name<span style="color:#666">=</span>example-service --type<span style="color:#666">=</span>LoadBalancer
</code></pre></div><!--
This command creates a new service using the same selectors as the referenced
resource (in the case of the example above, a replication controller named
`example`).

For more information, including optional flags, refer to the
[`kubectl expose` reference](/docs/reference/generated/kubectl/kubectl-commands/#expose).
-->
<p>此命令通过使用与引用资源（在上面的示例的情况下，名为 <code>example</code> 的 replication controller）相同的选择器来创建一个新的服务。</p>
<p>更多信息（包括更多的可选参数），请参阅
<a href="/docs/reference/generated/kubectl/kubectl-commands/#expose"><code>kubectl expose</code> 指南</a>。</p>
<!--
## Finding your IP address

You can find the IP address created for your service by getting the service
information through `kubectl`:
-->
<h2 id="找到你的-ip-地址">找到你的 IP 地址</h2>
<p>你可以通过 <code>kubectl</code> 获取服务信息，找到为你的服务创建的 IP 地址：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe services example-service
</code></pre></div><!--
which should produce output like this:
-->
<p>这将获得如下输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">    Name:                   example-service
    Namespace:              default
    Labels:                 &lt;none&gt;
    Annotations:            &lt;none&gt;
    Selector:               <span style="color:#b8860b">app</span><span style="color:#666">=</span>example
    Type:                   LoadBalancer
    IP:                     10.67.252.103
    LoadBalancer Ingress:   192.0.2.89
    Port:                   &lt;unnamed&gt; 80/TCP
    NodePort:               &lt;unnamed&gt; 32445/TCP
    Endpoints:              10.64.0.4:80,10.64.1.5:80,10.64.2.4:80
    Session Affinity:       None
    Events:                 &lt;none&gt;
</code></pre></div><!--
The IP address is listed next to `LoadBalancer Ingress`.
-->
<p>IP 地址列在 <code>LoadBalancer Ingress</code> 旁边。</p>
<!--
If you are running your service on Minikube, you can find the assigned IP address and port with:
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>如果你在 Minikube 上运行服务，你可以通过以下命令找到分配的 IP 地址和端口：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">minikube service example-service --url
</code></pre></div></div>
</blockquote>
<!--
## Preserving the client source IP

Due to the implementation of this feature, the source IP seen in the target
container is *not the original source IP* of the client. To enable
preservation of the client IP, the following fields can be configured in the
service spec (supported in GCE/Google Kubernetes Engine environments):
-->
<h2 id="保留客户端源-ip">保留客户端源 IP</h2>
<p>由于此功能的实现，目标容器中看到的源 IP 将 <em>不是客户端的原始源 IP</em>。
要启用保留客户端 IP，可以在服务的 spec 中配置以下字段（支持 GCE/Google Kubernetes Engine  环境）：</p>
<!--
* `service.spec.externalTrafficPolicy` - denotes if this Service desires to route
external traffic to node-local or cluster-wide endpoints. There are two available
options: Cluster (default) and Local. Cluster obscures the client source
IP and may cause a second hop to another node, but should have good overall
load-spreading. Local preserves the client source IP and avoids a second hop
for LoadBalancer and NodePort type services, but risks potentially imbalanced
traffic spreading.
-->
<ul>
<li><code>service.spec.externalTrafficPolicy</code> - 表示此服务是否希望将外部流量路由到节点本地或集群范围的端点。
有两个可用选项：Cluster（默认）和 Local。
Cluster 隐藏了客户端源 IP，可能导致第二跳到另一个节点，但具有良好的整体负载分布。
Local 保留客户端源 IP 并避免 LoadBalancer 和 NodePort 类型服务的第二跳，
但存在潜在的不均衡流量传播风险。</li>
</ul>
<!--
* `service.spec.healthCheckNodePort` - specifies the health check nodePort
(numeric port number) for the service. If `healthCheckNodePort` isn't specified,
the service controller allocates a port from your cluster's NodePort range. You
can configure that range by setting an API server command line option,
`--service-node-port-range`. It will use the
user-specified `healthCheckNodePort` value if specified by the client. It only has an
effect when `type` is set to LoadBalancer and `externalTrafficPolicy` is set
to Local.
-->
<ul>
<li><code>service.spec.healthCheckNodePort</code> - 指定服务的 healthcheck nodePort（数字端口号）。
如果未指定 <code>healthCheckNodePort</code>，服务控制器从集群的 NodePort 范围内分配一个端口。
你可以通过设置 API 服务器的命令行选项 <code>--service-node-port-range</code> 来配置上述范围。
它将会使用用户指定的 <code>healthCheckNodePort</code> 值（如果被客户端指定）。
仅当 <code>type</code>  设置为 LoadBalancer 并且 <code>externalTrafficPolicy</code> 设置为 Local 时才生效。</li>
</ul>
<!--
Setting `externalTrafficPolicy` to Local in the Service configuration file
activates this feature.
-->
<p>可以通过在服务的配置文件中将 <code>externalTrafficPolicy</code> 设置为 Local 来激活此功能。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8765</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">externalTrafficPolicy</span>:<span style="color:#bbb"> </span>Local<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span></code></pre></div><!--
## Garbage Collecting Load Balancers

In usual case, the correlating load balancer resources in cloud provider should
be cleaned up soon after a LoadBalancer type Service is deleted. But it is known
that there are various corner cases where cloud resources are orphaned after the
associated Service is deleted. Finalizer Protection for Service LoadBalancers was
introduced to prevent this from happening. By using finalizers, a Service resource
will never be deleted until the correlating load balancer resources are also deleted.
-->
<h2 id="回收负载均衡器">回收负载均衡器</h2>
<p>在通常情况下，应在删除 LoadBalancer 类型服务后立即清除云提供商中的相关负载均衡器资源。
但是，众所周知，在删除关联的服务后，云资源被孤立的情况很多。
引入了针对服务负载均衡器的终结器保护，以防止这种情况发生。
通过使用终结器，在删除相关的负载均衡器资源之前，也不会删除服务资源。</p>
<!--
Specifically, if a Service has `type` LoadBalancer, the service controller will attach
a finalizer named `service.kubernetes.io/load-balancer-cleanup`.
The finalizer will only be removed after the load balancer resource is cleaned up.
This prevents dangling load balancer resources even in corner cases such as the
service controller crashing.
-->
<p>具体来说，如果服务具有 <code>type</code> LoadBalancer，则服务控制器将附加一个名为
<code>service.kubernetes.io/load-balancer-cleanup</code> 的终结器。
仅在清除负载均衡器资源后才能删除终结器。
即使在诸如服务控制器崩溃之类的极端情况下，这也可以防止负载均衡器资源悬空。</p>
<!--
## External Load Balancer Providers

It is important to note that the datapath for this functionality is provided by a load balancer external to the Kubernetes cluster.
-->
<h2 id="外部负载均衡器提供商">外部负载均衡器提供商</h2>
<p>请务必注意，此功能的数据路径由 Kubernetes 集群外部的负载均衡器提供。</p>
<!--
When the Service `type` is set to LoadBalancer, Kubernetes provides functionality equivalent to `type` equals ClusterIP to pods
within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the Kubernetes
pods. The Kubernetes service controller automates the creation of the external load balancer, health checks (if needed),
firewall rules (if needed) and retrieves the external IP allocated by the cloud provider and populates it in the service
object.
-->
<p>当服务 <code>type</code> 设置为 LoadBalancer 时，Kubernetes 向集群中的 Pod 提供的功能等同于
<code>type</code> 等于  ClusterIP，并通过使用 Kubernetes pod 的条目对负载均衡器（从外部到 Kubernetes）
进行编程来扩展它。
Kubernetes 服务控制器自动创建外部负载均衡器、健康检查（如果需要）、防火墙规则（如果需要），
并获取云提供商分配的外部 IP 并将其填充到服务对象中。</p>
<!--
## Caveats and Limitations when preserving source IPs

GCE/AWS load balancers do not provide weights for their target pools. This was not an issue with the old LB
kube-proxy rules which would correctly balance across all endpoints.
-->
<h2 id="保留源-ip-时的注意事项和限制">保留源 IP 时的注意事项和限制</h2>
<p>GCE/AWS 负载均衡器不为其目标池提供权重。
对于旧的 LB kube-proxy 规则来说，这不是一个问题，它可以在所有端点之间正确平衡。</p>
<!--
With the new functionality, the external traffic is not equally load balanced across pods, but rather
equally balanced at the node level (because GCE/AWS and other external LB implementations do not have the ability
for specifying the weight per node, they balance equally across all target nodes, disregarding the number of
pods on each node).
-->
<p>使用新功能，外部流量不会在 pod 之间平均负载，而是在节点级别平均负载
（因为 GCE/AWS 和其他外部 LB 实现无法指定每个节点的权重，
因此它们的平衡跨所有目标节点，并忽略每个节点上的 Pod 数量）。</p>
<!--
We can, however, state that for NumServicePods << NumNodes or NumServicePods >> NumNodes, a fairly close-to-equal
distribution will be seen, even without weights.
-->
<p>但是，我们可以声明，对于 <code>NumServicePods &lt;&lt; NumNodes</code> 或 <code>NumServicePods &gt;&gt; NumNodes</code> 时，
即使没有权重，也会看到接近相等的分布。</p>
<!--
Once the external load balancers provide weights, this functionality can be added to the LB programming path.
*Future Work: No support for weights is provided for the 1.4 release, but may be added at a future date*

Internal pod to pod traffic should behave similar to ClusterIP services, with equal probability across all pods.
-->
<p>一旦外部负载平衡器提供权重，就可以将此功能添加到 LB 编程路径中。
<em>未来工作：1.4 版本不提供权重支持，但可能会在将来版本中添加</em></p>
<p>内部 Pod 到 Pod 的流量应该与 ClusterIP 服务类似，所有 Pod 的概率相同。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-48e8f306f919c5b81265e265a2b76ab4">9.7 - 列出集群中所有运行容器的镜像</h1>
    
	<!--
title: List All Container Images Running in a Cluster
content_type: task
weight: 100
-->
<!-- overview -->
<!--
This page shows how to use kubectl to list all of the Container images
for Pods running in a cluster.
-->
<p>本文展示如何使用 kubectl 来列出集群中所有运行 Pod 的容器的镜像</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
In this exercise you will use kubectl to fetch all of the Pods
running in a cluster, and format the output to pull out the list
of Containers for each.
-->
<p>在本练习中，你将使用 kubectl 来获取集群中运行的所有 Pod，并格式化输出来提取每个 Pod 中的容器列表。</p>
<!--
## List all Containers in all namespaces

- Fetch all Pods in all namespaces using `kubectl get pods --all-namespaces`
- Format the output to include only the list of Container image names
  using `-o jsonpath={.items[*].spec.containers[*].image}`.  This will recursively parse out the
  `image` field from the returned json.
  - See the [jsonpath reference](/docs/user-guide/jsonpath/)
    for further information on how to use jsonpath.
- Format the output using standard tools: `tr`, `sort`, `uniq`
  - Use `tr` to replace spaces with newlines
  - Use `sort` to sort the results
  - Use `uniq` to aggregate image counts
-->
<h2 id="列出所有命名空间下的所有容器">列出所有命名空间下的所有容器</h2>
<ul>
<li>使用 <code>kubectl get pods --all-namespaces</code> 获取所有命名空间下的所有 Pod</li>
<li>使用 <code>-o jsonpath={.items[*].spec.containers[*].image}</code> 来格式化输出，以仅包含容器镜像名称。
这将以递归方式从返回的 json 中解析出 <code>image</code> 字段。
<ul>
<li>参阅 <a href="/zh/docs/reference/kubectl/jsonpath/">jsonpath 说明</a>
获取更多关于如何使用 jsonpath 的信息。</li>
</ul>
</li>
<li>使用标准化工具来格式化输出：<code>tr</code>, <code>sort</code>, <code>uniq</code>
<ul>
<li>使用 <code>tr</code> 以用换行符替换空格</li>
<li>使用 <code>sort</code> 来对结果进行排序</li>
<li>使用 <code>uniq</code> 来聚合镜像计数</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --all-namespaces -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#34;{.items[*].spec.containers[*].image}&#34;</span> |<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>tr -s <span style="color:#b44">&#39;[[:space:]]&#39;</span> <span style="color:#b44">&#39;\n&#39;</span> |<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>sort |<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>uniq -c
</code></pre></div><!--
The above command will recursively return all fields named `image`
for all items returned.

As an alternative, it is possible to use the absolute path to the image
field within the Pod.  This ensures the correct field is retrieved
even when the field name is repeated,
e.g. many fields are called `name` within a given item:
-->
<p>上面的命令将递归获取所有返回项目的名为 <code>image</code> 的字段。</p>
<p>作为替代方案，可以使用 Pod 的镜像字段的绝对路径。这确保即使字段名称重复的情况下也能检索到正确的字段，例如，特定项目中的许多字段都称为 <code>name</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --all-namespaces -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#34;{.items[*].spec.containers[*].image}&#34;</span>
</code></pre></div><!--
The jsonpath is interpreted as follows:

- `.items[*]`: for each returned value
- `.spec`: get the spec
- `.containers[*]`: for each container
- `.image`: get the image
-->
<p>jsonpath 解释如下：</p>
<ul>
<li><code>.items[*]</code>: 对于每个返回的值</li>
<li><code>.spec</code>: 获取 spec</li>
<li><code>.containers[*]</code>: 对于每个容器</li>
<li><code>.image</code>: 获取镜像</li>
</ul>
<!--
When fetching a single Pod by name, e.g. `kubectl get pod nginx`,
the `.items[*]` portion of the path should be omitted because a single
Pod is returned instead of a list of items.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 按名字获取单个 Pod 时，例如 <code>kubectl get pod nginx</code>，路径的 <code>.items[*]</code> 部分应该省略，
因为返回的是一个 Pod 而不是一个项目列表。</div>
</blockquote>
<!--
## List Containers by Pod

The formatting can be controlled further by using the `range` operation to
iterate over elements individually.
-->
<h2 id="列出-pod-中的容器">列出 Pod 中的容器</h2>
<p>可以使用 <code>range</code> 操作进一步控制格式化，以单独操作每个元素。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --all-namespaces -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{range .items[*]}{&#34;\n&#34;}{.metadata.name}{&#34;:\t&#34;}{range .spec.containers[*]}{.image}{&#34;, &#34;}{end}{end}&#39;</span> |<span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>sort
</code></pre></div><!--
## List Containers filtering by Pod label

To target only Pods matching a specific label, use the -l flag.  The
following matches only Pods with labels matching `app=nginx`.
-->
<h2 id="列出以标签过滤后的-pod-的所有容器">列出以标签过滤后的 Pod 的所有容器</h2>
<p>要获取匹配特定标签的 Pod，请使用 -l 参数。以下匹配仅与标签 <code>app=nginx</code> 相符的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --all-namespaces -o<span style="color:#666">=</span><span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#34;{.items[*].spec.containers[*].image}&#34;</span> -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>nginx
</code></pre></div><!--
## List Containers filtering by Pod namespace

To target only pods in a specific namespace, use the namespace flag. The
following matches only Pods in the `kube-system` namespace.
-->
<h2 id="列出以命名空间过滤后的-pod-的所有容器">列出以命名空间过滤后的 Pod 的所有容器</h2>
<p>要获取匹配特定命名空间的 Pod，请使用 namespace 参数。以下仅匹配 <code>kube-system</code> 命名空间下的 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace kube-system -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#34;{.items[*].spec.containers[*].image}&#34;</span>
</code></pre></div><!--
## List Containers using a go-template instead of jsonpath

As an alternative to jsonpath, Kubectl supports using [go-templates](https://golang.org/pkg/text/template/)
for formatting the output:
-->
<h2 id="使用-go-template-代替-jsonpath-来获取容器">使用 go-template 代替 jsonpath 来获取容器</h2>
<p>作为 jsonpath 的替代，Kubectl 支持使用 <a href="https://golang.org/pkg/text/template/">go-templates</a> 来格式化输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --all-namespaces -o go-template --template<span style="color:#666">=</span><span style="color:#b44">&#34;{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}&#34;</span>
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
### Reference

* [Jsonpath](/docs/reference/kubectl/jsonpath/) reference guide
* [Go template](https://golang.org/pkg/text/template/) reference guide
-->
<h3 id="参考">参考</h3>
<ul>
<li><a href="/zh/docs/reference/kubectl/jsonpath/">Jsonpath</a> 参考指南</li>
<li><a href="https://golang.org/pkg/text/template/">Go template</a> 参考指南</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1839d8468a083839ed1cc8d18fe1142e">9.8 - 在 Minikube 环境中使用 NGINX Ingress 控制器配置 Ingress</h1>
    
	<!--
title: Set up Ingress on Minikube with the NGINX Ingress Controller
content_type: task
weight: 100
-->
<!-- overview -->
<!--
An [Ingress](/docs/concepts/services-networking/ingress/) is an API object that defines rules which allow external access
to services in a cluster. An [Ingress controller](/docs/concepts/services-networking/ingress-controllers/) fulfills the rules set in the Ingress.

This page shows you how to set up a simple Ingress which routes requests to Service web or web2 depending on the HTTP URI.
-->
<p><a href="/zh/docs/concepts/services-networking/ingress/">Ingress</a>是一种 API 对象，其中定义了一些规则使得集群中的
服务可以从集群外访问。
<a href="/zh/docs/concepts/services-networking/ingress-controllers/">Ingress 控制器</a>
负责满足 Ingress 中所设置的规则。</p>
<p>本节为你展示如何配置一个简单的 Ingress，根据 HTTP URI 将服务请求路由到
服务 <code>web</code> 或 <code>web2</code>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Create a Minikube cluster

1. Click **Launch Terminal**
-->
<h2 id="创建一个-minikube-集群">创建一个 Minikube  集群</h2>
<ol>
<li>
<p>点击 <strong>Launch Terminal</strong></p>
<script defer src="https://katacoda.com/embed.js"></script>
<button class="button" onclick="window.katacoda.init(); ">Launch Terminal</button>

</li>
</ol>
<!--
1. (Optional) If you installed Minikube locally, run the following command:
-->
<ol start="2">
<li>
<p>（可选操作）如果你在本地安装了 Minikube，运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start
</code></pre></div></li>
</ol>
<!--
## Enable the Ingress controller

1. To enable the NGINX Ingress controller, run the following command:
-->
<h2 id="启用-ingress-控制器">启用 Ingress 控制器</h2>
<ol>
<li>
<p>为了启用 NGINIX Ingress 控制器，可以运行下面的命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube addons <span style="color:#a2f">enable</span> ingress
</code></pre></div></li>
</ol>
<!--
1. Verify that the NGINX Ingress controller is running
-->
<ol start="2">
<li>
<p>检查验证 NGINX Ingress 控制器处于运行状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -n kube-system
</code></pre></div><!-- This can take up to a minute. -->
<blockquote class="note callout">
  <div><strong>说明：</strong> 这一操作可能需要近一分钟时间。</div>
</blockquote>
<p>输出：</p>
<pre tabindex="0"><code>NAME                                        READY     STATUS    RESTARTS   AGE
default-http-backend-59868b7dd6-xb8tq       1/1       Running   0          1m
kube-addon-manager-minikube                 1/1       Running   0          3m
kube-dns-6dcb57bcc8-n4xd4                   3/3       Running   0          2m
kubernetes-dashboard-5498ccf677-b8p5h       1/1       Running   0          2m
nginx-ingress-controller-5984b97644-rnkrg   1/1       Running   0          1m
storage-provisioner                         1/1       Running   0          2m
</code></pre></li>
</ol>
<!--
## Deploy a hello, world app

1. Create a Deployment using the following command:
-->
<h2 id="部署一个-hello-world-应用">部署一个 Hello World 应用</h2>
<ol>
<li>
<p>使用下面的命令创建一个 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment web --image<span style="color:#666">=</span>gcr.io/google-samples/hello-app:1.0
</code></pre></div><!--Output:-->
<p>输出：</p>
<pre tabindex="0"><code>deployment.apps/web created
</code></pre></li>
</ol>
<!--
1. Expose the Deployment:
-->
<ol start="2">
<li>
<p>将 Deployment 暴露出来：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment web --type<span style="color:#666">=</span>NodePort --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div><!-- Output: -->
<p>输出：</p>
<pre tabindex="0"><code>service/web exposed
</code></pre></li>
</ol>
<!--
1. Verify the Service is created and is available on a node port:
-->
<ol start="3">
<li>
<p>验证 Service 已经创建，并且可能从节点端口访问：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service web
</code></pre></div><!-- Output: -->
<p>输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT<span style="color:#666">(</span>S<span style="color:#666">)</span>          AGE
web       NodePort   10.104.133.249   &lt;none&gt;        8080:31637/TCP   12m
</code></pre></div></li>
</ol>
<!--
1. Visit the service via NodePort:
-->
<ol start="4">
<li>
<p>使用节点端口信息访问服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube service web --url
</code></pre></div><!-- Output: -->
<p>输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">http://172.17.0.15:31637
</code></pre></div><!--
Katacoda environment only: at the top of the terminal panel, click the plus sign, and then click **Select port to view on Host 1**. Enter the NodePort, in this case `31637`, and then click **Display Port**.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果使用的是 Katacoda 环境，在终端面板顶端，请点击加号标志。
然后点击 <strong>Select port to view on Host 1</strong>。
输入节点和端口号（这里是<code>31637</code>），之后点击 <strong>Display Port</strong>。</div>
</blockquote>
<!-- Output: -->
<p>输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre></div><!--
You can now access the sample app via the Minikube IP address and NodePort. The next step lets you access
the app using the Ingress resource.
-->
<p>你现在应该可以通过 Minikube 的 IP 地址和节点端口来访问示例应用了。
下一步是让自己能够通过 Ingress 资源来访问应用。</p>
</li>
</ol>
<!--
## Create an Ingress resource

The following file is an Ingress resource that sends traffic to your Service via hello-world.info.

1. Create `example-ingress.yaml` from the following file:
-->
<h2 id="创建一个-ingress-资源">创建一个 Ingress 资源</h2>
<p>下面是一个 Ingress 资源的配置文件，负责通过 <code>hello-world.info</code> 将服务请求
转发到你的服务。</p>
<ol>
<li>
<p>根据下面的 YAML 创建文件 <code>example-ingress.yaml</code>：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/example-ingress.yaml" download="service/networking/example-ingress.yaml"><code>service/networking/example-ingress.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-example-ingress-yaml')" title="Copy service/networking/example-ingress.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-example-ingress-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>networking.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Ingress<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-ingress<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nginx.ingress.kubernetes.io/rewrite-target</span>:<span style="color:#bbb"> </span>/$1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>hello-world.info<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">http</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">paths</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span>- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span></code></pre></div>
    </div>
</div>


</li>
</ol>
<!--
1. Create the Ingress resource by running the following command:
-->
<ol start="2">
<li>
<p>通过运行下面的命令创建 Ingress 资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml
</code></pre></div><!-- Output: -->
<p>输出：</p>
<pre tabindex="0"><code>ingress.networking.k8s.io/example-ingress created
</code></pre></li>
</ol>
<!--
1. Verify the IP address is set:
-->
<ol start="3">
<li>
<p>验证 IP 地址已被设置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ingress
</code></pre></div><!-- This can take a couple of minutes. -->
<blockquote class="note callout">
  <div><strong>说明：</strong> 此操作可能需要几分钟时间。</div>
</blockquote>
<pre tabindex="0"><code>NAME              CLASS    HOSTS              ADDRESS        PORTS   AGE
example-ingress   &lt;none&gt;   hello-world.info   172.17.0.15    80      38s
</code></pre></li>
</ol>
<!--
1. Add the following line to the bottom of the `/etc/hosts` file.
-->
<ol start="4">
<li>
<p>在 <code>/etc/hosts</code> 文件的末尾添加以下内容：</p>
<!--
If you are running Minikube locally, use `minikube ip` to get the external IP. The IP address displayed within the ingress list will be the internal IP.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果你在本地运行 Minikube 环境，需要使用 <code>minikube ip</code> 获得外部 IP 地址。
Ingress 列表中显示的 IP 地址会是内部 IP 地址。</div>
</blockquote>
<pre tabindex="0"><code>172.17.0.15 hello-world.info
</code></pre><!-- This sends requests from hello-world.info to Minikube. -->
<p>此设置使得来自 <code>hello-world.info</code> 的请求被发送到 Minikube。</p>
</li>
</ol>
<!--
1. Verify that the Ingress controller is directing traffic:
-->
<ol start="5">
<li>
<p>验证 Ingress 控制器能够转发请求流量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl hello-world.info
</code></pre></div><!-- Output: -->
<p>输出：</p>
<pre tabindex="0"><code>Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre><!--
If you are running Minikube locally, you can visit hello-world.info from your browser.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果你在使用本地 Minikube 环境，你可以从浏览器中访问 hello-world.info。</div>
</blockquote>
</li>
</ol>
<!--
## Create Second Deployment

1. Create a v2 Deployment using the following command:
-->
<h2 id="创建第二个-deployment">创建第二个 Deployment</h2>
<ol>
<li>
<p>使用下面的命令创建 v2 的 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment web2 --image<span style="color:#666">=</span>gcr.io/google-samples/hello-app:2.0
</code></pre></div><!-- Output: -->
<p>输出：</p>
<pre tabindex="0"><code>deployment.apps/web2 created
</code></pre></li>
</ol>
<!--
1. Expose the Deployment:
-->
<ol start="2">
<li>
<p>将 Deployment 暴露出来：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment web2 --port<span style="color:#666">=</span><span style="color:#666">8080</span> --type<span style="color:#666">=</span>NodePort
</code></pre></div><!-- Output:  -->
<p>输出：</p>
<pre tabindex="0"><code>service/web2 exposed
</code></pre></li>
</ol>
<!--
## Edit Ingress

1. Edit the existing `example-ingress.yaml` and add the following lines:
-->
<h2 id="编辑-ingress">编辑 Ingress</h2>
<ol>
<li>
<p>编辑现有的 <code>example-ingress.yaml</code>，添加以下行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">- <span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/v2<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">pathType</span>:<span style="color:#bbb"> </span>Prefix<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">backend</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>web2<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">number</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
1. Apply the changes:
-->
<ol start="2">
<li>
<p>应用所作变更：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f example-ingress.yaml
</code></pre></div><!-- Output: -->
<p>输出：</p>
<pre tabindex="0"><code>ingress.networking/example-ingress configured
</code></pre></li>
</ol>
<!--
## Test Your Ingress

1. Access the 1st version of the Hello World app.
-->
<h2 id="测试你的-ingress">测试你的 Ingress</h2>
<ol>
<li>
<p>访问 HelloWorld 应用的第一个版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl hello-world.info
</code></pre></div><!-- Output: -->
<p>输出：</p>
<pre tabindex="0"><code>Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564
</code></pre></li>
</ol>
<!--
1. Access the 2nd version of the Hello World app.
-->
<ol start="2">
<li>
<p>访问 HelloWorld 应用的第二个版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl hello-world.info/v2
</code></pre></div><!-- Output: -->
<p>输出：</p>
<pre tabindex="0"><code>Hello, world!
Version: 2.0.0
Hostname: web2-75cd47646f-t8cjk
</code></pre><!--
If you are running Minikube locally, you can visit hello-world.info and hello-world.info/v2 from your browser
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果你在本地运行 Minikube 环境，你可以使用浏览器来访问
hello-world.info 和 hello-world.info/v2。</div>
</blockquote>
</li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
* Read more about [Ingress](/docs/concepts/services-networking/ingress/)
* Read more about [Ingress Controllers](/docs/concepts/services-networking/ingress-controllers/)
* Read more about [Services](/docs/concepts/services-networking/service/)
-->
<ul>
<li>进一步了解 <a href="/zh/docs/concepts/services-networking/ingress/">Ingress</a>。</li>
<li>进一步了解 <a href="/zh/docs/concepts/services-networking/ingress-controllers/">Ingress 控制器</a></li>
<li>进一步了解<a href="/zh/docs/concepts/services-networking/service/">服务</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-322786b38586b210fab68f785259c5f6">9.9 - 为集群配置 DNS</h1>
    
	<!--
---
title: Configure DNS for a Cluster
weight: 120
content_type: concept
---
-->
<!-- overview -->
<!--
Kubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm.
-->
<p>Kubernetes 提供 DNS 集群插件，大多数支持的环境默认情况下都会启用。
在 Kubernetes 1.11 及其以后版本中，推荐使用 CoreDNS，
kubeadm 默认会安装 CoreDNS。</p>
<!-- body -->
<!--
For more information on how to configure CoreDNS for a Kubernetes cluster, see the [Customizing DNS Service](/docs/tasks/administer-cluster/dns-custom-nameservers/). An example demonstrating how to use Kubernetes DNS with kube-dns, see the [Kubernetes DNS sample plugin](https://github.com/kubernetes/examples/tree/master/staging/cluster-dns)
-->
<p>要了解关于如何为 Kubernetes 集群配置 CoreDNS 的更多信息，参阅
<a href="/zh/docs/tasks/administer-cluster/dns-custom-nameservers/">定制 DNS 服务</a>。
关于如何利用 kube-dns 配置 kubernetes DNS 的演示例子，参阅
<a href="https://github.com/kubernetes/examples/tree/master/staging/cluster-dns">Kubernetes DNS 插件示例</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7c319a9981586e5fbcfa21b392720650">9.10 - 同 Pod 内的容器使用共享卷通信</h1>
    
	<!--
title: Communicate Between Containers in the Same Pod Using a Shared Volume
content_type: task
weight: 110
-->
<!-- overview -->
<!--
This page shows how to use a Volume to communicate between two Containers running
in the same Pod. See also how to allow processes to communicate by
[sharing process namespace](/docs/tasks/configure-pod-container/share-process-namespace/)
between containers.
-->
<p>本文旨在说明如何让一个 Pod 内的两个容器使用一个卷（Volume）进行通信。
参阅如何让两个进程跨容器通过
<a href="/zh/docs/tasks/configure-pod-container/share-process-namespace/">共享进程名字空间</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
##  Creating a Pod that runs two Containers

In this exercise, you create a Pod that runs two Containers. The two containers
share a Volume that they can use to communicate. Here is the configuration file
for the Pod:
-->
<h2 id="创建一个包含两个容器的-pod">创建一个包含两个容器的 Pod</h2>
<p>在这个练习中，你会创建一个包含两个容器的 Pod。两个容器共享一个卷用于他们之间的通信。
Pod 的配置文件如下：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/pods/two-container-pod.yaml" download="pods/two-container-pod.yaml"><code>pods/two-container-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('pods-two-container-pod-yaml')" title="Copy pods/two-container-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="pods-two-container-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>two-containers<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>debian-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/pod-data<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;echo Hello from the debian container &gt; /pod-data/index.html&#34;</span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In the configuration file, you can see that the Pod has a Volume named
`shared-data`.

The first container listed in the configuration file runs an nginx server. The
mount path for the shared Volume is `/usr/share/nginx/html`.
The second container is based on the debian image, and has a mount path of
`/pod-data`. The second container runs the following command and then terminates.
-->
<p>在配置文件中，你可以看到 Pod 有一个共享卷，名为 <code>shared-data</code>。</p>
<p>配置文件中的第一个容器运行了一个 nginx 服务器。共享卷的挂载路径是 <code>/usr/share/nginx/html</code>。
第二个容器是基于 debian 镜像的，有一个 <code>/pod-data</code> 的挂载路径。第二个容器运行了下面的命令然后终止。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">echo</span> Hello from the debian container &gt; /pod-data/index.html
</code></pre></div><!--
Notice that the second container writes the `index.html` file in the root
directory of the nginx server.

Create the Pod and the two Containers:
-->
<p>注意，第二个容器在 nginx 服务器的根目录下写了 <code>index.html</code> 文件。</p>
<p>创建一个包含两个容器的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/pods/two-container-pod.yaml
</code></pre></div><!--
View information about the Pod and the Containers:
-->
<p>查看 Pod 和容器的信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod two-containers --output<span style="color:#666">=</span>yaml
</code></pre></div><!--
Here is a portion of the output:
-->
<p>这是输出的一部分：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>two-containers<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containerStatuses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">containerID</span>:<span style="color:#bbb"> </span>docker://c1d8abd1 ...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">lastState</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminated</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>debian-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">containerID</span>:<span style="color:#bbb"> </span>docker://96c1ff2c5bb ...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">state</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">running</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></code></pre></div><!--
You can see that the debian Container has terminated, and the nginx Container
is still running.

Get a shell to nginx Container:
-->
<p>你可以看到 debian 容器已经被终止了，而 nginx 服务器依然在运行。</p>
<p>进入 nginx 容器的 shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it two-containers -c nginx-container -- /bin/bash
</code></pre></div><!--
In your shell, verify that nginx is running:
-->
<p>在 shell 中，确认 nginx 还在运行。</p>
<pre tabindex="0"><code>root@two-containers:/# ps aux
</code></pre><!--
The output is similar to this:
-->
<p>输出类似于这样：</p>
<pre tabindex="0"><code>USER       PID  ...  STAT START   TIME COMMAND
root         1  ...  Ss   21:12   0:00 nginx: master process nginx -g daemon off;
</code></pre><!--
Recall that the debian Container created the `index.html` file in the nginx root
directory. Use `curl` to send a GET request to the nginx server:
-->
<p>回忆一下，debian 容器在 nginx 的根目录下创建了 <code>index.html</code> 文件。
使用 <code>curl</code> 向 nginx 服务器发送一个 GET 请求：</p>
<pre tabindex="0"><code>root@two-containers:/# curl localhost
</code></pre><p>输出表示 nginx 提供了 debian 容器写的页面：</p>
<pre tabindex="0"><code>Hello from the debian container
</code></pre><!-- discussion -->
<!--
## Discussion

The primary reason that Pods can have multiple containers is to support
helper applications that assist a primary application. Typical examples of
helper applications are data pullers, data pushers, and proxies.
Helper and primary applications often need to communicate with each other.
Typically this is done through a shared filesystem, as shown in this exercise,
or through the loopback network interface, localhost. An example of this pattern is a
web server along with a helper program that polls a Git repository for new updates.
-->
<h2 id="讨论">讨论</h2>
<p>Pod 能有多个容器的主要原因是为了支持辅助应用（helper applications），以协助主应用（primary application）。
辅助应用的典型例子是数据抽取，数据推送和代理。辅助应用和主应用经常需要相互通信。
就如这个练习所示，通信通常是通过共享文件系统完成的，或者，也通过回环网络接口 localhost 完成。
举个网络接口的例子，web 服务器带有一个协助程序用于拉取 Git 仓库的更新。</p>
<!--
The Volume in this exercise provides a way for Containers to communicate during
the life of the Pod. If the Pod is deleted and recreated, any data stored in
the shared Volume is lost.
-->
<p>在本练习中的卷为 Pod 生命周期中的容器相互通信提供了一种方法。如果 Pod 被删除或者重建了，
任何共享卷中的数据都会丢失。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn more about [patterns for composite containers](https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns).
* Learn about [composite containers for modular architecture](https://www.slideshare.net/Docker/slideshare-burns).
* See [Configuring a Pod to Use a Volume for Storage](/docs/tasks/configure-pod-container/configure-volume-storage/).
* See [Configure a Pod to share process namespace between containers in a Pod](/docs/tasks/configure-pod-container/share-process-namespace/)
* See [Volume](/docs/reference/generated/kubernetes-api/v1.22/#volume-v1-core).
* See [Pod](/docs/reference/generated/kubernetes-api/v1.22/#pod-v1-core).
-->
<ul>
<li>进一步了解<a href="https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns.html">复合容器的模式</a></li>
<li>学习<a href="https://www.slideshare.net/Docker/slideshare-burns">模块化架构中的复合容器</a></li>
<li>参见<a href="/zh/docs/tasks/configure-pod-container/configure-volume-storage/">配置 Pod 使用卷来存储数据</a></li>
<li>参考<a href="/zh/docs/tasks/configure-pod-container/share-process-namespace/">在 Pod 中的容器之间共享进程命名空间</a></li>
<li>参考 <a href="/docs/reference/generated/kubernetes-api/v1.22/#volume-v1-core">Volume</a></li>
<li>参考 <a href="/docs/reference/generated/kubernetes-api/v1.22/#pod-v1-core">Pod</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5a233e14205d77fe1294917d2da6f876">9.11 - 配置对多集群的访问</h1>
    
	<!--
title: Configure Access to Multiple Clusters
content_type: task
weight: 30
card:
  name: tasks
  weight: 40
-->
<!-- overview -->
<!--
This page shows how to configure access to multiple clusters by using
configuration files. After your clusters, users, and contexts are defined in
one or more configuration files, you can quickly switch between clusters by using the
`kubectl config use-context` command.
-->
<p>本文展示如何使用配置文件来配置对多个集群的访问。 在将集群、用户和上下文定义在一个或多个配置文件中之后，用户可以使用 <code>kubectl config use-context</code> 命令快速地在集群之间进行切换。</p>
<!--
A file that is used to configure access to a cluster is sometimes called
a *kubeconfig file*. This is a generic way of referring to configuration files.
It does not mean that there is a file named `kubeconfig`.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 用于配置集群访问的文件有时被称为 <em>kubeconfig 文件</em>。
这是一种引用配置文件的通用方式，并不意味着存在一个名为 <code>kubeconfig</code> 的文件。</div>
</blockquote>
<!--
<blockquote class="warning callout">
  <div><strong>警告：</strong> Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure.
If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.</div>
</blockquote>

-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 只使用来源可靠的 kubeconfig 文件。使用特制的 kubeconfig 文件可能会导致恶意代码执行或文件暴露。
如果必须使用不受信任的 kubeconfig 文件，请首先像检查 shell 脚本一样仔细检查它。</div>
</blockquote>

<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
To check that <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes API 服务器进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a> is installed,
run `kubectl version --client`. The kubectl version should be
[within one minor version](/releases/version-skew-policy/#kubectl) of your
cluster's API server.
-->
<p>要检查 <a class='glossary-tooltip' title='kubectl 是用来和 Kubernetes API 服务器进行通信的命令行工具。' data-toggle='tooltip' data-placement='top' href='/docs/user-guide/kubectl-overview/' target='_blank' aria-label='kubectl'>kubectl</a> 是否安装，
执行 <code>kubectl version --client</code> 命令。
kubectl 的版本应该与集群的 API 服务器
<a href="/zh/releases/version-skew-policy/#kubectl">使用同一次版本号</a>。</p>
<!-- steps -->
<!--
## Define clusters, users, and contexts

Suppose you have two clusters, one for development work and one for scratch work.
In the `development` cluster, your frontend developers work in a namespace called `frontend`,
and your storage developers work in a namespace called `storage`. In your `scratch` cluster,
developers work in the default namespace, or they create auxiliary namespaces as they
see fit. Access to the development cluster requires authentication by certificate. Access
to the scratch cluster requires authentication by username and password.

Create a directory named `config-exercise`. In your
`config-exercise` directory, create a file named `config-demo` with this content:
-->
<h2 id="定义集群-用户和上下文">定义集群、用户和上下文</h2>
<p>假设用户有两个集群，一个用于正式开发工作，一个用于其它临时用途（scratch）。
在 <code>development</code> 集群中，前端开发者在名为 <code>frontend</code> 的名字空间下工作，
存储开发者在名为 <code>storage</code> 的名字空间下工作。 在 <code>scratch</code> 集群中，
开发人员可能在默认名字空间下工作，也可能视情况创建附加的名字空间。
访问开发集群需要通过证书进行认证。
访问其它临时用途的集群需要通过用户名和密码进行认证。</p>
<p>创建名为 <code>config-exercise</code> 的目录。 在
<code>config-exercise</code> 目录中，创建名为 <code>config-demo</code> 的文件，其内容为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>scratch<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-storage<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>exp-scratch<span style="color:#bbb">
</span></code></pre></div><!--
A configuration file describes clusters, users, and contexts. Your `config-demo` file
has the framework to describe two clusters, two users, and three contexts.

Go to your `config-exercise` directory. Enter these commands to add cluster details to
your configuration file:
-->
<p>配置文件描述了集群、用户名和上下文。<code>config-demo</code> 文件中含有描述两个集群、
两个用户和三个上下文的框架。</p>
<p>进入 <code>config-exercise</code> 目录。输入以下命令，将群集详细信息添加到配置文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-cluster development --server<span style="color:#666">=</span>https://1.2.3.4 --certificate-authority<span style="color:#666">=</span>fake-ca-file
kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-cluster scratch --server<span style="color:#666">=</span>https://5.6.7.8 --insecure-skip-tls-verify
</code></pre></div><!--
Add user details to your configuration file:
-->
<p>将用户详细信息添加到配置文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-credentials developer --client-certificate<span style="color:#666">=</span>fake-cert-file --client-key<span style="color:#666">=</span>fake-key-seefile
kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-credentials experimenter --username<span style="color:#666">=</span>exp --password<span style="color:#666">=</span>some-password
</code></pre></div><!--
- To delete a user you can run `kubectl --kubeconfig=config-demo config unset users.<name>`
- To remove a cluster, you can run `kubectl --kubeconfig=config-demo config unset clusters.<name>`
- To remove a context, you can run `kubectl --kubeconfig=config-demo config unset contexts.<name>`
-->
<p>注意：</p>
<ul>
<li>要删除用户，可以运行 <code>kubectl --kubeconfig=config-demo config unset users.&lt;name&gt;</code></li>
<li>要删除集群，可以运行 <code>kubectl --kubeconfig=config-demo config unset clusters.&lt;name&gt;</code></li>
<li>要删除上下文，可以运行 <code>kubectl --kubeconfig=config-demo config unset contexts.&lt;name&gt;</code></li>
</ul>
<!--
Add context details to your configuration file:
-->
<p>将上下文详细信息添加到配置文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-context dev-frontend --cluster<span style="color:#666">=</span>development --namespace<span style="color:#666">=</span>frontend --user<span style="color:#666">=</span>developer
kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-context dev-storage --cluster<span style="color:#666">=</span>development --namespace<span style="color:#666">=</span>storage --user<span style="color:#666">=</span>developer
kubectl config --kubeconfig<span style="color:#666">=</span>config-demo set-context exp-scratch --cluster<span style="color:#666">=</span>scratch --namespace<span style="color:#666">=</span>default --user<span style="color:#666">=</span>experimenter
</code></pre></div><!--
Open your `config-demo` file to see the added details. As an alternative to opening the
`config-demo` file, you can use the `config view` command.
-->
<p>打开 <code>config-demo</code> 文件查看添加的详细信息。 也可以使用 <code>config view</code>
命令进行查看：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view
</code></pre></div><!--
The output shows the two clusters, two users, and three contexts:
-->
<p>输出展示了两个集群、两个用户和三个上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">certificate-authority</span>:<span style="color:#bbb"> </span>fake-ca-file<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>https://1.2.3.4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">insecure-skip-tls-verify</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>https://5.6.7.8<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>scratch<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>storage<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-storage<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>scratch<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>exp-scratch<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">current-context</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-certificate</span>:<span style="color:#bbb"> </span>fake-cert-file<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-key</span>:<span style="color:#bbb"> </span>fake-key-file<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>some-password<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>exp<span style="color:#bbb">
</span></code></pre></div><!--
The `fake-ca-file`, `fake-cert-file` and `fake-key-file` above are the placeholders
for the pathnames of the certificate files. You need to change these to the actual pathnames
of certificate files in your environment.

Sometimes you may want to use Base64-encoded data embedded here instead of separate
certificate files; in that case you need to add the suffix `-data` to the keys, for example,
`certificate-authority-data`, `client-certificate-data`, `client-key-data`.
-->
<p>其中的 <code>fake-ca-file</code>、<code>fake-cert-file</code> 和 <code>fake-key-file</code> 是证书文件路径名的占位符。
你需要更改这些值，使之对应你的环境中证书文件的实际路径名。</p>
<p>有时你可能希望在这里使用 BASE64 编码的数据而不是一个个独立的证书文件。
如果是这样，你需要在键名上添加 <code>-data</code> 后缀。例如，
<code>certificate-authority-data</code>、<code>client-certificate-data</code> 和 <code>client-key-data</code>。</p>
<!--
Each context is a triple (cluster, user, namespace). For example, the
`dev-frontend` context says, "Use the credentials of the `developer`
user to access the `frontend` namespace of the `development` cluster".

Set the current context:
-->
<p>每个上下文包含三部分（集群、用户和名字空间），例如，
<code>dev-frontend</code> 上下文表明：使用 <code>developer</code> 用户的凭证来访问 <code>development</code> 集群的
<code>frontend</code> 名字空间。</p>
<p>设置当前上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo use-context dev-frontend
</code></pre></div><!--
Now whenever you enter a `kubectl` command, the action will apply to the cluster,
and namespace listed in the `dev-frontend` context. And the command will use
the credentials of the user listed in the `dev-frontend` context.

To see only the configuration information associated with
the current context, use the `--minify` flag.
-->
<p>现在当输入 <code>kubectl</code> 命令时，相应动作会应用于 <code>dev-frontend</code> 上下文中所列的集群和名字空间，
同时，命令会使用 <code>dev-frontend</code> 上下文中所列用户的凭证。</p>
<p>使用 <code>--minify</code> 参数，来查看与当前上下文相关联的配置信息。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view --minify
</code></pre></div><!--
The output shows configuration information associated with the `dev-frontend` context:
-->
<p>输出结果展示了 <code>dev-frontend</code> 上下文相关的配置信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">clusters</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">certificate-authority</span>:<span style="color:#bbb"> </span>fake-ca-file<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">server</span>:<span style="color:#bbb"> </span>https://1.2.3.4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">current-context</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-certificate</span>:<span style="color:#bbb"> </span>fake-cert-file<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">client-key</span>:<span style="color:#bbb"> </span>fake-key-file<span style="color:#bbb">
</span></code></pre></div><!--
Now suppose you want to work for a while in the scratch cluster.

Change the current context to `exp-scratch`:
-->
<p>现在假设用户希望在其它临时用途集群中工作一段时间。</p>
<p>将当前上下文更改为 <code>exp-scratch</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo use-context exp-scratch
</code></pre></div><!--
Now any `kubectl` command you give will apply to the default namespace of
the `scratch` cluster. And the command will use the credentials of the user
listed in the `exp-scratch` context.

View configuration associated with the new current context, `exp-scratch`.
-->
<p>现在你发出的所有 <code>kubectl</code> 命令都将应用于 <code>scratch</code> 集群的默认名字空间。
同时，命令会使用 <code>exp-scratch</code> 上下文中所列用户的凭证。</p>
<p>查看更新后的当前上下文 <code>exp-scratch</code> 相关的配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view --minify
</code></pre></div><!--
Finally, suppose you want to work for a while in the `storage` namespace of the
`development` cluster.

Change the current context to `dev-storage`:
-->
<p>最后，假设用户希望在 <code>development</code> 集群中的 <code>storage</code> 名字空间下工作一段时间。</p>
<p>将当前上下文更改为 <code>dev-storage</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo use-context dev-storage
</code></pre></div><!--
View configuration associated with the new current context, `dev-storage`.
-->
<p>查看更新后的当前上下文 <code>dev-storage</code> 相关的配置：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config --kubeconfig<span style="color:#666">=</span>config-demo view --minify
</code></pre></div><!--
## Create a second configuration file

In your `config-exercise` directory, create a file named `config-demo-2` with this content:
-->
<h2 id="创建第二个配置文件">创建第二个配置文件</h2>
<p>在 <code>config-exercise</code> 目录中，创建名为 <code>config-demo-2</code> 的文件，其中包含以下内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Config<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">preferences</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>ramp<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-ramp-up<span style="color:#bbb">
</span></code></pre></div><!--
The preceding configuration file defines a new context named `dev-ramp-up`.
-->
<p>上述配置文件定义了一个新的上下文，名为 <code>dev-ramp-up</code>。</p>
<!--
## Set the KUBECONFIG environment variable

See whether you have an environment variable named `KUBECONFIG`. If so, save the
current value of your `KUBECONFIG` environment variable, so you can restore it later.
For example:
-->
<h2 id="设置-kubeconfig-环境变量">设置 KUBECONFIG 环境变量</h2>
<p>查看是否有名为 <code>KUBECONFIG</code> 的环境变量。
如有，保存 <code>KUBECONFIG</code> 环境变量当前的值，以便稍后恢复。
例如：</p>
<h3 id="linux">Linux</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG_SAVED</span><span style="color:#666">=</span><span style="color:#b8860b">$KUBECONFIG</span>
</code></pre></div><h3 id="windows-powershell">Windows PowerShell</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#b8860b">$Env:KUBECONFIG_SAVED</span>=<span style="color:#b8860b">$ENV:KUBECONFIG</span>
</code></pre></div><!--
 The `KUBECONFIG` environment variable is a list of paths to configuration files. The list is
colon-delimited for Linux and Mac, and semicolon-delimited for Windows. If you have
a `KUBECONFIG` environment variable, familiarize yourself with the configuration files
in the list.

Temporarily append two paths to your `KUBECONFIG` environment variable. For example:
-->
<p><code>KUBECONFIG</code> 环境变量是配置文件路径的列表，该列表在 Linux 和 Mac 中以冒号分隔，
在 Windows 中以分号分隔。
如果有 <code>KUBECONFIG</code> 环境变量，请熟悉列表中的配置文件。</p>
<p>临时添加两条路径到 <code>KUBECONFIG</code> 环境变量中。 例如：</p>
<h3 id="linux-1">Linux</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span>  <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b8860b">$KUBECONFIG</span>:config-demo:config-demo-2
</code></pre></div><h3 id="windows-powershell-1">Windows PowerShell</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#b8860b">$Env:KUBECONFIG</span>=(<span style="color:#b44">&#34;config-demo;config-demo-2&#34;</span>)
</code></pre></div><!--
In your `config-exercise` directory, enter this command:
-->
<p>在 <code>config-exercise</code> 目录中输入以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><!--
The output shows merged information from all the files listed in your `KUBECONFIG`
environment variable. In particular, notice that the merged information has the
`dev-ramp-up` context from the `config-demo-2` file and the three contexts from
the `config-demo` file:
-->
<p>输出展示了 <code>KUBECONFIG</code> 环境变量中所列举的所有文件合并后的信息。
特别地，注意合并信息中包含来自 <code>config-demo-2</code> 文件的 <code>dev-ramp-up</code> 上下文和来自
<code>config-demo</code> 文件的三个上下文：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">contexts</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-frontend<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>ramp<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-ramp-up<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>development<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>storage<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>developer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dev-storage<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">context</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cluster</span>:<span style="color:#bbb"> </span>scratch<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">user</span>:<span style="color:#bbb"> </span>experimenter<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>exp-scratch<span style="color:#bbb">
</span></code></pre></div><!--
For more information about how kubeconfig files are merged, see
[Organizing Cluster Access Using kubeconfig Files](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
-->
<p>关于 kubeconfig 文件如何合并的更多信息，请参考
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">使用 kubeconfig 文件组织集群访问</a></p>
<!--
## Explore the $HOME/.kube directory

If you already have a cluster, and you can use `kubectl` to interact with
the cluster, then you probably have a file named `config` in the `$HOME/.kube`
directory.

Go to `$HOME/.kube`, and see what files are there. Typically, there is a file named
`config`. There might also be other configuration files in this directory. Briefly
familiarize yourself with the contents of these files.
-->
<h2 id="探索-home-kube-目录">探索 $HOME/.kube 目录</h2>
<p>如果用户已经拥有一个集群，可以使用 <code>kubectl</code> 与集群进行交互，
那么很可能在 <code>$HOME/.kube</code> 目录下有一个名为 <code>config</code> 的文件。</p>
<p>进入 <code>$HOME/.kube</code> 目录，看看那里有什么文件。通常会有一个名为
<code>config</code> 的文件，目录中可能还有其他配置文件。请简单地熟悉这些文件的内容。</p>
<!--
## Append $HOME/.kube/config to your KUBECONFIG environment variable

If you have a `$HOME/.kube/config` file, and it's not already listed in your
`KUBECONFIG` environment variable, append it to your `KUBECONFIG` environment variable now.
For example:
-->
<h2 id="将-home-kube-config-追加到-kubeconfig-环境变量中">将 $HOME/.kube/config 追加到 KUBECONFIG 环境变量中</h2>
<p>如果有 <code>$HOME/.kube/config</code> 文件，并且还未列在 <code>KUBECONFIG</code> 环境变量中，
那么现在将它追加到 <code>KUBECONFIG</code> 环境变量中。
例如：</p>
<h3 id="linux-2">Linux</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b8860b">$KUBECONFIG</span>:<span style="color:#b8860b">$HOME</span>/.kube/config
</code></pre></div><h3 id="windows-powershell-2">Windows Powershell</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#b8860b">$Env:KUBECONFIG</span>=<span style="color:#b44">&#34;$Env:KUBECONFIG;$HOME\.kube\config&#34;</span>
</code></pre></div><!--
View configuration information merged from all the files that are now listed
in your `KUBECONFIG` environment variable. In your config-exercise directory, enter:
-->
<p>在配置练习目录中输入以下命令，查看当前 <code>KUBECONFIG</code> 环境变量中列举的所有文件合并后的配置信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl config view
</code></pre></div><!--
## Clean up

Return your `KUBECONFIG` environment variable to its original value. For example:
-->
<h2 id="清理">清理</h2>
<p>将 <code>KUBECONFIG</code> 环境变量还原为原始值。 例如：</p>
<h3 id="linux-3">Linux</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span><span style="color:#b8860b">$KUBECONFIG_SAVED</span>
</code></pre></div><h3 id="windows-powershell-3">Windows PowerShell</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell"><span style="color:#b8860b">$Env:KUBECONFIG</span>=<span style="color:#b8860b">$ENV:KUBECONFIG_SAVED</span>
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* [Organizing Cluster Access Using kubeconfig Files](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
* [kubectl config](/docs/reference/generated/kubectl/kubectl-commands#config)
-->
<ul>
<li><a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">使用 kubeconfig 文件组织集群访问</a></li>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands#config">kubectl config</a></li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f6a755efe831d24956501e4bcd49ff96">10 - 监控、日志和排错</h1>
    <div class="lead">设置监视和日志记录以对集群进行故障排除或调试容器化应用。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-5e4a82f171ec2c11da7360a67efb4abf">10.1 - 使用 crictl 对 Kubernetes 节点进行调试</h1>
    
	<!--
reviewers:
- Random-Liu
- feiskyer
- mrunalp
title: Debugging Kubernetes nodes with crictl
content_type: task
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.11 [stable]</code>
</div>

<!--
`crictl` is a command-line interface for CRI-compatible container runtimes.
You can use it to inspect and debug container runtimes and applications on a
Kubernetes node. `crictl` and its source are hosted in the
[cri-tools](https://github.com/kubernetes-sigs/cri-tools) repository.
-->
<p><code>crictl</code> 是 CRI 兼容的容器运行时命令行接口。
你可以使用它来检查和调试 Kubernetes 节点上的容器运行时和应用程序。
<code>crictl</code> 和它的源代码在
<a href="https://github.com/kubernetes-sigs/cri-tools">cri-tools</a> 代码库。</p>
<h2 id="准备开始">准备开始</h2>
<!--
`crictl` requires a Linux operating system with a CRI runtime.
-->
<p><code>crictl</code> 需要带有 CRI 运行时的 Linux 操作系统。</p>
<!-- steps -->
<!--
## Installing crictl

You can download a compressed archive `crictl` from the cri-tools [release
page](https://github.com/kubernetes-sigs/cri-tools/releases), for several
different architectures. Download the version that corresponds to your version
of Kubernetes. Extract it and move it to a location on your system path, such as
`/usr/local/bin/`.
-->
<h2 id="安装-crictl">安装 crictl</h2>
<p>你可以从 cri-tools <a href="https://github.com/kubernetes-sigs/cri-tools/releases">发布页面</a>
下载一个压缩的 <code>crictl</code> 归档文件，用于几种不同的架构。
下载与你的 kubernetes 版本相对应的版本。
提取它并将其移动到系统路径上的某个位置，例如<code>/usr/local/bin/</code>。</p>
<!--
## General usage

The `crictl` command has several subcommands and runtime flags. Use
`crictl help` or `crictl <subcommand> help` for more details.
-->
<h2 id="一般用法">一般用法</h2>
<p><code>crictl</code> 命令有几个子命令和运行时参数。
有关详细信息，请使用 <code>crictl help</code> 或 <code>crictl &lt;subcommand&gt; help</code> 获取帮助信息。</p>
<!--
`crictl` connects to `unix:///var/run/dockershim.sock` by default. For other
runtimes, you can set the endpoint in multiple different ways:
-->
<p><code>crictl</code> 默认连接到 <code>unix:///var/run/dockershim.sock</code>。
对于其他的运行时，你可以用多种不同的方法设置端点：</p>
<!--
- By setting flags `--runtime-endpoint` and `--image-endpoint`
- By setting environment variables `CONTAINER_RUNTIME_ENDPOINT` and `IMAGE_SERVICE_ENDPOINT`
- By setting the endpoint in the config file `--config=/etc/crictl.yaml`
-->
<ul>
<li>通过设置参数 <code>--runtime-endpoint</code> 和 <code>--image-endpoint</code></li>
<li>通过设置环境变量 <code>CONTAINER_RUNTIME_ENDPOINT</code> 和 <code>IMAGE_SERVICE_ENDPOINT</code></li>
<li>通过在配置文件中设置端点 <code>--config=/etc/crictl.yaml</code></li>
</ul>
<!--
You can also specify timeout values when connecting to the server and enable or
disable debugging, by specifying `timeout` or `debug` values in the configuration
file or using the `--timeout` and `--debug` command-line flags.
-->
<p>你还可以在连接到服务器并启用或禁用调试时指定超时值，方法是在配置文件中指定
<code>timeout</code> 或 <code>debug</code> 值，或者使用 <code>--timeout</code> 和 <code>--debug</code> 命令行参数。</p>
<!--
To view or edit the current configuration, view or edit the contents of
`/etc/crictl.yaml`.
-->
<p>要查看或编辑当前配置，请查看或编辑 <code>/etc/crictl.yaml</code> 的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat /etc/crictl.yaml
</code></pre></div><pre tabindex="0"><code>runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
timeout: 10
debug: true
</code></pre><!--
## Example crictl commands

The following examples show some `crictl` commands and example output.
-->
<h2 id="crictl-命令示例">crictl 命令示例</h2>
<blockquote class="warning callout">
  <div><strong>警告：</strong> <!--
If you use `crictl` to create pod sandboxes or containers on a running
Kubernetes cluster, the Kubelet will eventually delete them. `crictl` is not a
general purpose workflow tool, but a tool that is useful for debugging.
-->
<p>如果使用 <code>crictl</code> 在正在运行的 Kubernetes 集群上创建 Pod 沙盒或容器，
kubelet 最终将删除它们。
<code>crictl</code> 不是一个通用的工作流工具，而是一个对调试有用的工具。</div>
</blockquote>

<!--
### List pods

List all pods:
-->
<h3 id="打印-pod-清单">打印 Pod 清单</h3>
<p>打印所有 Pod 的清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl pods
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">POD ID              CREATED              STATE               NAME                         NAMESPACE           ATTEMPT
926f1b5a1d33a       About a minute ago   Ready               sh-84d7dcf559-4r2gq          default             0
4dccb216c4adb       About a minute ago   Ready               nginx-65899c769f-wv2gp       default             0
a86316e96fa89       17 hours ago         Ready               kube-proxy-gblk4             kube-system         0
919630b8f81f1       17 hours ago         Ready               nvidia-device-plugin-zgbbv   kube-system         0
</code></pre><!--
List pods by name:
-->
<p>根据名称打印 Pod 清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl pods --name nginx-65899c769f-wv2gp
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">POD ID              CREATED             STATE               NAME                     NAMESPACE           ATTEMPT
4dccb216c4adb       2 minutes ago       Ready               nginx-65899c769f-wv2gp   default             0
</code></pre><!--
List pods by label:
-->
<p>根据标签打印 Pod 清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl pods --label <span style="color:#b8860b">run</span><span style="color:#666">=</span>nginx
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">POD ID              CREATED             STATE               NAME                     NAMESPACE           ATTEMPT
4dccb216c4adb       2 minutes ago       Ready               nginx-65899c769f-wv2gp   default             0
</code></pre><!--
### List images

List all images:
-->
<h3 id="打印镜像清单">打印镜像清单</h3>
<p>打印所有镜像清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl images
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">IMAGE                                     TAG                 IMAGE ID            SIZE
busybox                                   latest              8c811b4aec35f       1.15MB
k8s-gcrio.azureedge.net/hyperkube-amd64   v1.10.3             e179bbfe5d238       665MB
k8s-gcrio.azureedge.net/pause-amd64       3.1                 da86e6ba6ca19       742kB
nginx                                     latest              cd5239a0906a6       109MB
</code></pre><!--
List images by repository:
-->
<p>根据仓库打印镜像清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl images nginx
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">IMAGE               TAG                 IMAGE ID            SIZE
nginx               latest              cd5239a0906a6       109MB
</code></pre><!--
Only list image IDs:
-->
<p>只打印镜像 ID：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl images -q
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">sha256:8c811b4aec35f259572d0f79207bc0678df4c736eeec50bc9fec37ed936a472a
sha256:e179bbfe5d238de6069f3b03fccbecc3fb4f2019af741bfff1233c4d7b2970c5
sha256:da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e
sha256:cd5239a0906a6ccf0562354852fae04bc5b52d72a2aff9a871ddb6bd57553569
</code></pre><!--
### List containers

List all containers:
-->
<h3 id="打印容器清单">打印容器清单</h3>
<p>打印所有容器清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl ps -a
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">CONTAINER ID        IMAGE                                                                                                             CREATED             STATE               NAME                       ATTEMPT
1f73f2d81bf98       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   7 minutes ago       Running             sh                         1
9c5951df22c78       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   8 minutes ago       Exited              sh                         0
87d3992f84f74       nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f                                     8 minutes ago       Running             nginx                      0
1941fb4da154f       k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a   18 hours ago        Running             kube-proxy                 0
</code></pre><!--
List running containers:
-->
<p>打印正在运行的容器清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl ps
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">CONTAINER ID        IMAGE                                                                                                             CREATED             STATE               NAME                       ATTEMPT
1f73f2d81bf98       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   6 minutes ago       Running             sh                         1
87d3992f84f74       nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f                                     7 minutes ago       Running             nginx                      0
1941fb4da154f       k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a   17 hours ago        Running             kube-proxy                 0
</code></pre><!--
### Execute a command in a running container
-->
<h3 id="在正在运行的容器上执行命令">在正在运行的容器上执行命令</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl <span style="color:#a2f">exec</span> -i -t 1f73f2d81bf98 ls
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">bin   dev   etc   home  proc  root  sys   tmp   usr   var
</code></pre><!--
### Get a container's logs

Get all container logs:
-->
<h3 id="获取容器日志">获取容器日志</h3>
<p>获取容器的所有日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl logs 87d3992f84f74
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">10.240.0.96 - - [06/Jun/2018:02:45:49 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.47.0&quot; &quot;-&quot;
10.240.0.96 - - [06/Jun/2018:02:45:50 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.47.0&quot; &quot;-&quot;
10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.47.0&quot; &quot;-&quot;
</code></pre><!--
Get only the latest `N` lines of logs:
-->
<p>获取最近的 <code>N</code> 行日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl logs --tail<span style="color:#666">=</span><span style="color:#666">1</span> 87d3992f84f74
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.47.0&quot; &quot;-&quot;
</code></pre><!--
### Run a pod sandbox

Using `crictl` to run a pod sandbox is useful for debugging container runtimes.
On a running Kubernetes cluster, the sandbox will eventually be stopped and
deleted by the Kubelet.
-->
<h3 id="运行-pod-沙盒">运行 Pod 沙盒</h3>
<p>用 <code>crictl</code> 运行 Pod 沙盒对容器运行时排错很有帮助。
在运行的 Kubernetes 集群中，沙盒会随机地被 kubelet 停止和删除。</p>
<ol>
<li>
<!--Create a JSON file like the following:-->
<p>编写下面的 JSON 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;nginx-sandbox&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;default&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;attempt&#34;</span>: <span style="color:#666">1</span>,
        <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;hdishd83djaidwnduwk28bcsb&#34;</span>
    },
    <span style="color:#008000;font-weight:bold">&#34;logDirectory&#34;</span>: <span style="color:#b44">&#34;/tmp&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;linux&#34;</span>: {
    }
}
</code></pre></div></li>
<li>
<!--Use the `crictl runp` command to apply the JSON and run the sandbox.-->
<p>使用 <code>crictl runp</code> 命令应用 JSON 文件并运行沙盒。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl runp pod-config.json
</code></pre></div><!--The ID of the sandbox is returned.-->
<p>返回了沙盒的 ID。</p>
</li>
</ol>
<!--
### Create a container

Using `crictl` to create a container is useful for debugging container runtimes.
On a running Kubernetes cluster, the sandbox will eventually be stopped and
deleted by the Kubelet.
-->
<h3 id="创建容器">创建容器</h3>
<p>用 <code>crictl</code> 创建容器对容器运行时排错很有帮助。
在运行的 Kubernetes 集群中，沙盒会随机的被 kubelet 停止和删除。</p>
<ol>
<li>
<!--Pull a busybox image-->
<p>拉取 busybox 镜像</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">crictl pull busybox
Image is up to date <span style="color:#a2f;font-weight:bold">for</span> busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
</code></pre></div></li>
<li>
<!--Create configs for the pod and the container:-->
<p>创建 Pod 和容器的配置：</p>
<!--**Pod config**:-->
<p><strong>Pod 配置</strong>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">{<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">&#34;metadata&#34;: </span>{<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">&#34;name&#34;: </span><span style="color:#b44">&#34;nginx-sandbox&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">&#34;namespace&#34;: </span><span style="color:#b44">&#34;default&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">&#34;attempt&#34;: </span><span style="color:#666">1</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">&#34;uid&#34;: </span><span style="color:#b44">&#34;hdishd83djaidwnduwk28bcsb&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>},<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">&#34;log_directory&#34;: </span><span style="color:#b44">&#34;/tmp&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">&#34;linux&#34;: </span>{<span style="color:#bbb">
</span><span style="color:#bbb">    </span>}<span style="color:#bbb">
</span><span style="color:#bbb"></span>}<span style="color:#bbb">
</span></code></pre></div><!--**Container config**:-->
<p><strong>容器配置</strong>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">{<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">&#34;metadata&#34;: </span>{<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">&#34;name&#34;: </span><span style="color:#b44">&#34;busybox&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>},<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#b44">&#34;image&#34;</span>:{<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">&#34;image&#34;: </span><span style="color:#b44">&#34;busybox&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>},<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">&#34;command&#34;: </span>[<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#b44">&#34;top&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>],<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#b44">&#34;log_path&#34;</span>:<span style="color:#b44">&#34;busybox.log&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">&#34;linux&#34;: </span>{<span style="color:#bbb">
</span><span style="color:#bbb">  </span>}<span style="color:#bbb">
</span><span style="color:#bbb"></span>}<span style="color:#bbb">
</span></code></pre></div></li>
<li>
<!--Create the container, passing the ID of the previously-created pod, the
container config file, and the pod config file. The ID of the container is
returned.-->
<p>创建容器，传递先前创建的 Pod 的 ID、容器配置文件和 Pod 配置文件。返回容器的 ID。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">crictl create f84dd361f8dc51518ed291fbadd6db537b0496536c1d2d6c05ff943ce8c9a54f container-config.json pod-config.json
</code></pre></div></li>
<li>
<!--List all containers and verify that the newly-created container has its
state set to `Created`.-->
<p>查询所有容器并确认新创建的容器状态为 <code>Created</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">crictl ps -a
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">CONTAINER ID        IMAGE               CREATED             STATE               NAME                ATTEMPT
3e025dd50a72d       busybox             32 seconds ago      Created             busybox             0
</code></pre></li>
</ol>
<!--
### Start a container

To start a container, pass its ID to `crictl start`:
-->
<h3 id="启动容器">启动容器</h3>
<p>要启动容器，要将容器 ID 传给 <code>crictl start</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl start 3e025dd50a72d956c4f14881fbb5b1080c9275674e95fb67f965f6478a957d60
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">3e025dd50a72d956c4f14881fbb5b1080c9275674e95fb67f965f6478a957d60
</code></pre><!--
Check the container has its state set to `Running`.
-->
<p>确认容器的状态为 <code>Running</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">crictl ps
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">CONTAINER ID        IMAGE               CREATED              STATE               NAME                ATTEMPT
3e025dd50a72d       busybox             About a minute ago   Running             busybox             0
</code></pre><!-- discussion -->
<!--
See [kubernetes-sigs/cri-tools](https://github.com/kubernetes-sigs/cri-tools)
for more information.
-->
<p>更多信息请参考 <a href="https://github.com/kubernetes-sigs/cri-tools">kubernetes-sigs/cri-tools</a>。</p>
<!--
## Mapping from docker cli to crictl
-->
<h2 id="docker-cli-和-crictl-的映射">Docker CLI 和 crictl 的映射</h2>
<!--
The exact versions for below mapping table are for docker cli v1.40 and crictl v1.19.0. Please note that the list is not exhaustive. For example, it doesn't include experimental commands of docker cli.
-->
<p>以下的映射表格只适用于 Docker CLI v1.40 和 crictl v1.19.0 版本。
请注意该表格并不详尽。例如，其中不包含 Docker CLI 的实验性命令。</p>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> The output format of CRICTL is similar to Docker CLI, despite some missing columns for some CLI. Make sure to check output for the specific command if your script output parsing.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 尽管有些命令的输出缺少了一些数据列，CRICTL 的输出格式与 Docker CLI 是类似的。
如果你的脚本程序需要解析命令的输出，请确认检查该特定命令的输出。</div>
</blockquote>
<!--
### Retrieve Debugging Information






<p>--&gt;</p>
<h3 id="获取调试信息">获取调试信息</h3>
<!--
docker cli | crictl | Description | Unsupported Features
-- | -- | -- | --
`attach` | `attach` | Attach to a running container | `--detach-keys`, `--sig-proxy`
`exec` | `exec` | Run a command in a running container | `--privileged`, `--user`, `--detach-keys`
`images` | `images` | List images |  
`info` | `info` | Display system-wide information |  
`inspect` | `inspect`, `inspecti` | Return low-level information on a container, image or task |  
`logs` | `logs` | Fetch the logs of a container | `--details`
`ps` | `ps` | List containers |  
`stats` | `stats` | Display a live stream of container(s) resource usage statistics | Column: NET/BLOCK I/O, PIDs
`version` | `version` | Show the runtime (Docker, ContainerD, or others) version information |
-->
<table><caption style="display: none;">mapping from docker cli to crictl - retrieve debugging information</caption>
<thead>
<tr>
<th>docker cli</th>
<th>crictl</th>
<th>描述</th>
<th>不支持的功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>attach</code></td>
<td><code>attach</code></td>
<td>连接到一个运行中的容器</td>
<td><code>--detach-keys</code>, <code>--sig-proxy</code></td>
</tr>
<tr>
<td><code>exec</code></td>
<td><code>exec</code></td>
<td>在运行中的容器里运行一个命令</td>
<td><code>--privileged</code>, <code>--user</code>, <code>--detach-keys</code></td>
</tr>
<tr>
<td><code>images</code></td>
<td><code>images</code></td>
<td>列举镜像</td>
<td> </td>
</tr>
<tr>
<td><code>info</code></td>
<td><code>info</code></td>
<td>显示系统级的信息</td>
<td> </td>
</tr>
<tr>
<td><code>inspect</code></td>
<td><code>inspect</code>, <code>inspecti</code></td>
<td>返回容器、镜像或者任务的详细信息</td>
<td> </td>
</tr>
<tr>
<td><code>logs</code></td>
<td><code>logs</code></td>
<td>获取容器的日志</td>
<td><code>--details</code></td>
</tr>
<tr>
<td><code>ps</code></td>
<td><code>ps</code></td>
<td>列举容器</td>
<td> </td>
</tr>
<tr>
<td><code>stats</code></td>
<td><code>stats</code></td>
<td>实时显示容器的资源使用统计信息</td>
<td>列：NET/BLOCK I/O, PIDs</td>
</tr>
<tr>
<td><code>version</code></td>
<td><code>version</code></td>
<td>显示运行时（Docker、ContainerD、或者其他) 的版本信息</td>
<td> </td>
</tr>
</tbody>
</table>


<!--
### Perform Changes






<p>--&gt;</p>
<h3 id="进行改动">进行改动</h3>
<!--
docker cli | crictl | Description | Unsupported Features
-- | -- | -- | --
`create` | `create` | Create a new container |  
`kill` | `stop` (timeout = 0) | Kill one or more running container | `--signal`
`pull` | `pull` | Pull an image or a repository from a registry | `--all-tags`, `--disable-content-trust`
`rm` | `rm` | Remove one or more containers |  
`rmi` | `rmi` | Remove one or more images |  
`run` | `run` | Run a command in a new container |  
`start` | `start` | Start one or more stopped containers | `--detach-keys`
`stop` | `stop` | Stop one or more running containers |  
`update` | `update` | Update configuration of one or more containers | `--restart`, `--blkio-weight` and some other resource limit not supported by CRI.
-->
<table><caption style="display: none;">mapping from docker cli to crictl - perform changes</caption>
<thead>
<tr>
<th>docker cli</th>
<th>crictl</th>
<th>描述</th>
<th>不支持的功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>create</code></td>
<td><code>create</code></td>
<td>创建一个新的容器</td>
<td> </td>
</tr>
<tr>
<td><code>kill</code></td>
<td><code>stop</code> (timeout=0)</td>
<td>杀死一个或多个正在运行的容器</td>
<td><code>--signal</code></td>
</tr>
<tr>
<td><code>pull</code></td>
<td><code>pull</code></td>
<td>从镜像仓库拉取镜像或者代码仓库</td>
<td><code>--all-tags</code>, <code>--disable-content-trust</code></td>
</tr>
<tr>
<td><code>rm</code></td>
<td><code>rm</code></td>
<td>移除一个或多个容器</td>
<td> </td>
</tr>
<tr>
<td><code>rmi</code></td>
<td><code>rmi</code></td>
<td>移除一个或多个镜像</td>
<td> </td>
</tr>
<tr>
<td><code>run</code></td>
<td><code>run</code></td>
<td>在新容器里运行一个命令</td>
<td> </td>
</tr>
<tr>
<td><code>start</code></td>
<td><code>start</code></td>
<td>启动一个或多个停止的容器</td>
<td><code>--detach-keys</code></td>
</tr>
<tr>
<td><code>stop</code></td>
<td><code>stop</code></td>
<td>停止一个或多个正运行的容器</td>
<td> </td>
</tr>
<tr>
<td><code>update</code></td>
<td><code>update</code></td>
<td>更新一个或多个容器的配置</td>
<td>CRI 不支持 <code>--restart</code>、<code>--blkio-weight</code> 以及一些其他的资源限制选项。</td>
</tr>
</tbody>
</table>


<!--
### Supported only in crictl






<p>--&gt;</p>
<h3 id="仅-crictl-支持">仅 crictl 支持</h3>
<!--
crictl | Description
-- | --
`imagefsinfo` | Return image filesystem info
`inspectp` | Display the status of one or more pods
`port-forward` | Forward local port to a pod
`pods` | List pods
`runp` | Run a new pod
`rmp` | Remove one or more pods
`stopp` | Stop one or more running pods
-->
<table><caption style="display: none;">mapping from docker cli to crictl - supported only in crictl</caption>
<thead>
<tr>
<th>crictl</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>imagefsinfo</code></td>
<td>返回镜像的文件系统信息</td>
</tr>
<tr>
<td><code>inspectp</code></td>
<td>显示一个或多个 Pod 的状态</td>
</tr>
<tr>
<td><code>port-forward</code></td>
<td>转发本地端口到 Pod</td>
</tr>
<tr>
<td><code>pods</code></td>
<td>列举 Pod</td>
</tr>
<tr>
<td><code>runp</code></td>
<td>运行一个新的 Pod</td>
</tr>
<tr>
<td><code>rmp</code></td>
<td>移除一个或多个 Pod</td>
</tr>
<tr>
<td><code>stopp</code></td>
<td>停止一个或多个正运行的 Pod</td>
</tr>
</tbody>
</table>


</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d25a16285195bd17d9055b1eb7bc605c">10.2 - 在本地开发和调试服务</h1>
    
	<!--
title: Developing and debugging services locally
content_type: task
-->
<!-- overview -->
<!--
Kubernetes applications usually consist of multiple, separate services, each running in its own container. Developing and debugging these services on a remote Kubernetes cluster can be cumbersome, requiring you to [get a shell on a running container](/docs/tasks/debug-application-cluster/get-shell-running-container/) and running your tools inside the remote shell.
-->
<p>Kubernetes 应用程序通常由多个独立的服务组成，每个服务都在自己的容器中运行。
在远端的 Kubernetes 集群上开发和调试这些服务可能很麻烦，需要
<a href="/zh/docs/tasks/debug-application-cluster/get-shell-running-container/">在运行的容器上打开 Shell</a>，
然后在远端 Shell 中运行你所需的工具。</p>
<!--
`telepresence` is a tool to ease the process of developing and debugging services locally, while proxying the service to a remote Kubernetes cluster. Using `telepresence` allows you to use custom tools, such as a debugger and IDE, for a local service and provides the service full access to ConfigMap, secrets, and the services running on the remote cluster.
-->
<p><code>telepresence</code> 是一种工具，用于在本地轻松开发和调试服务，同时将服务代理到远程 Kubernetes 集群。
使用 <code>telepresence</code> 可以为本地服务使用自定义工具（如调试器和 IDE），
并提供对 Configmap、Secret 和远程集群上运行的服务的完全访问。</p>
<!--
This document describes using `telepresence` to develop and debug services running on a remote cluster locally.
-->
<p>本文档描述如何在本地使用 <code>telepresence</code> 开发和调试远程集群上运行的服务。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* Kubernetes cluster is installed
* `kubectl` is configured to communicate with the cluster
* [Telepresence](https://www.telepresence.io/reference/install) is installed
-->
<ul>
<li>Kubernetes 集群安装完毕</li>
<li>配置好 <code>kubectl</code> 与集群交互</li>
<li><a href="https://www.telepresence.io/reference/install">Telepresence</a> 安装完毕</li>
</ul>
<!-- steps -->
<!--
## Getting a shell on a remote cluster

Open a terminal and run `telepresence` with no arguments to get a `telepresence` shell. This shell runs locally, giving you full access to your local filesystem.
-->
<p>打开终端，不带参数运行 <code>telepresence</code>，以打开 <code>telepresence</code> Shell。
这个 Shell 在本地运行，使你可以完全访问本地文件系统。</p>
<!--
The `telepresence` shell can be used in a variety of ways. For example, write a shell script on your laptop, and run it directly from the shell in real time. You can do this on a remote shell as well, but you might not be able to use your preferred code editor, and the script is deleted when the container is terminated.

Enter `exit` to quit and close the shell.
-->
<p><code>telepresence</code> Shell 的使用方式多种多样。
例如，在你的笔记本电脑上写一个 Shell 脚本，然后直接在 Shell 中实时运行它。
你也可以在远端 Shell 上执行此操作，但这样可能无法使用首选的代码编辑器，并且在容器终止时脚本将被删除。</p>
<!--
## Developing or debugging an existing service

When developing an application on Kubernetes, you typically program or debug a single service. The service might require access to other services for testing and debugging. One option is to use the continuous deployment pipeline, but even the fastest deployment pipeline introduces a delay in the program or debug cycle.
-->
<h2 id="开发和调试现有的服务">开发和调试现有的服务</h2>
<p>在 Kubernetes 上开发应用程序时，通常对单个服务进行编程或调试。
服务可能需要访问其他服务以进行测试和调试。
一种选择是使用连续部署流水线，但即使最快的部署流水线也会在程序或调试周期中引入延迟。</p>
<!--
Use the `--swap-deployment` option to swap an existing deployment with the Telepresence proxy. Swapping allows you to run a service locally and connect to the remote Kubernetes cluster. The services in the remote cluster can now access the locally running instance.

To run telepresence with `--swap-deployment`, enter:
-->
<p>使用 <code>--swap-deployment</code> 选项将现有部署与 Telepresence 代理交换。
交换允许你在本地运行服务并能够连接到远端的 Kubernetes 集群。
远端集群中的服务现在就可以访问本地运行的实例。</p>
<p>要运行 telepresence 并带有 <code>--swap-deployment</code> 选项，请输入：</p>
<p><code>telepresence --swap-deployment $DEPLOYMENT_NAME</code></p>
<!--
where $DEPLOYMENT_NAME is the name of your existing deployment.

Running this command spawns a shell. In the shell, start your service. You can then make edits to the source code locally, save, and see the changes take effect immediately. You can also run your service in a debugger, or any other local development tool.
-->
<p>这里的 <code>$DEPLOYMENT_NAME</code> 是你现有的部署名称。</p>
<p>运行此命令将生成 Shell。在该 Shell 中，启动你的服务。
然后，你就可以在本地对源代码进行编辑、保存并能看到更改立即生效。
你还可以在调试器或任何其他本地开发工具中运行服务。</p>
<h2 id="接下来">接下来</h2>
<!--
If you're interested in a hands-on tutorial, check out [this tutorial](https://cloud.google.com/community/tutorials/developing-services-with-k8s) that walks through locally developing the Guestbook application on Google Kubernetes Engine.
-->
<p>如果你对实践教程感兴趣，请查看<a href="https://cloud.google.com/community/tutorials/developing-services-with-k8s">本教程</a>，其中介绍了在 Google Kubernetes Engine 上本地开发 Guestbook 应用程序。</p>
<!--
Telepresence has [numerous proxying options](https://www.telepresence.io/reference/methods), depending on your situation.

For further reading, visit the [Telepresence website](https://www.telepresence.io).
-->
<p>Telepresence 有<a href="https://www.telepresence.io/reference/methods">多种代理选项</a>，以满足你的各种情况。</p>
<p>要了解更多信息，请访问 <a href="https://www.telepresence.io">Telepresence 网站</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cbd33a50cc4779f855318a0dd00d7b06">10.3 - 审计</h1>
    
	<!--
reviewers:
- soltysh
- sttts
- ericchiang
content_type: concept
title: Auditing
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code>
</div>

<!--
Kubernetes _auditing_ provides a security-relevant, chronological set of records documenting
the sequence of actions in a cluster. The cluster audits the activities generated by users,
by applications that use the Kubernetes API, and by the control plane itself.

Auditing allows cluster administrators to answer the following questions:
-->
<p>Kubernetes <em>审计（Auditing）</em> 功能提供了与安全相关的、按时间顺序排列的记录集，
记录每个用户、使用 Kubernetes API 的应用以及控制面自身引发的活动。</p>
<p>审计功能使得集群管理员能够回答以下问题：</p>
<!--
 - what happened?
 - when did it happen?
 - who initiated it?
 - on what did it happen?
 - where was it observed?
 - from where was it initiated?
 - to where was it going?
-->
<ul>
<li>发生了什么？</li>
<li>什么时候发生的？</li>
<li>谁触发的？</li>
<li>活动发生在哪个（些）对象上？</li>
<li>在哪观察到的？</li>
<li>它从哪触发的？</li>
<li>活动的后续处理行为是什么？</li>
</ul>
<!-- body -->
<!--
Audit records begin their lifecycle inside the
[kube-apiserver](/docs/reference/command-line-tools-reference/kube-apiserver/)
component. Each request on each stage
of its execution generates an audit event, which is then pre-processed according to
a certain policy and written to a backend. The policy determines what's recorded
and the backends persist the records. The current backend implementations
include logs files and webhooks.
-->
<p>审计记录最初产生于
<a href="/zh/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a>
内部。每个请求在不同执行阶段都会生成审计事件；这些审计事件会根据特定策略
被预处理并写入后端。策略确定要记录的内容和用来存储记录的后端。
当前的后端支持日志文件和 webhook。</p>
<!--
Each request can be recorded with an associated _stage_. The defined stages are:

- `RequestReceived` - The stage for events generated as soon as the audit
  handler receives the request, and before it is delegated down the handler
  chain.
- `ResponseStarted` - Once the response headers are sent, but before the
  response body is sent. This stage is only generated for long-running requests
  (e.g. watch).
- `ResponseComplete` - The response body has been completed and no more bytes
  will be sent.
- `Panic` - Events generated when a panic occurred.
-->
<p>每个请求都可被记录其相关的 <em>阶段（stage）</em>。已定义的阶段有：</p>
<ul>
<li><code>RequestReceived</code> - 此阶段对应审计处理器接收到请求后，并且在委托给
其余处理器之前生成的事件。</li>
<li><code>ResponseStarted</code> - 在响应消息的头部发送后，响应消息体发送前生成的事件。
只有长时间运行的请求（例如 watch）才会生成这个阶段。</li>
<li><code>ResponseComplete</code> - 当响应消息体完成并且没有更多数据需要传输的时候。</li>
<li><code>Panic</code> - 当 panic 发生时生成。</li>
</ul>
<!-- 
The configuration of an
[Audit Event configuration](/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Event)
is different from the
[Event](/docs/reference/generated/kubernetes-api/v1.22/#event-v1-core)
API object.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <a href="/zh/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Event">审计事件配置</a>
的配置与 <a href="/zh/docs/reference/generated/kubernetes-api/v1.22/#event-v1-core">Event</a>
API 对象不同。</div>
</blockquote>
<!--
The audit logging feature increases the memory consumption of the API server
because some context required for auditing is stored for each request.
Additionally, memory consumption depends on the audit logging configuration.
-->
<p>审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。
此外，内存消耗取决于审计日志记录的配置。</p>
<!--
## Audit Policy

Audit policy defines rules about what events should be recorded and what data
they should include. The audit policy object structure is defined in the
[`audit.k8s.io` API group](/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy).
When an event is processed, it's
compared against the list of rules in order. The first matching rule sets the
_audit level_ of the event. The defined audit levels are:
-->
<h2 id="audit-policy">审计策略 </h2>
<p>审计政策定义了关于应记录哪些事件以及应包含哪些数据的规则。
审计策略对象结构定义在
<a href="/zh/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy"><code>audit.k8s.io</code> API 组</a>
处理事件时，将按顺序与规则列表进行比较。第一个匹配规则设置事件的
<em>审计级别（Audit Level）</em>。已定义的审计级别有：</p>
<!--
- `None` - don't log events that match this rule.
- `Metadata` - log request metadata (requesting user, timestamp, resource,
  verb, etc.) but not request or response body.
- `Request` - log event metadata and request body but not response body.
  This does not apply for non-resource requests.
- `RequestResponse` - log event metadata, request and response bodies.
  This does not apply for non-resource requests.
-->
<ul>
<li><code>None</code> - 符合这条规则的日志将不会记录。</li>
<li><code>Metadata</code> - 记录请求的元数据（请求的用户、时间戳、资源、动词等等），
但是不记录请求或者响应的消息体。</li>
<li><code>Request</code> - 记录事件的元数据和请求的消息体，但是不记录响应的消息体。
这不适用于非资源类型的请求。</li>
<li><code>RequestResponse</code> - 记录事件的元数据，请求和响应的消息体。这不适用于非资源类型的请求。</li>
</ul>
<!--
You can pass a file with the policy to `kube-apiserver`
using the `--audit-policy-file` flag. If the flag is omitted, no events are logged.
Note that the `rules` field __must__ be provided in the audit policy file.
A policy with no (0) rules is treated as illegal.

Below is an example audit policy file:
-->
<p>你可以使用 <code>--audit-policy-file</code> 标志将包含策略的文件传递给 <code>kube-apiserver</code>。
如果不设置该标志，则不记录事件。
注意 <code>rules</code> 字段 <strong>必须</strong> 在审计策略文件中提供。没有（0）规则的策略将被视为非法配置。</p>
<p>以下是一个审计策略文件的示例：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/audit/audit-policy.yaml" download="audit/audit-policy.yaml"><code>audit/audit-policy.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('audit-audit-policy-yaml')" title="Copy audit/audit-policy.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="audit-audit-policy-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>audit.k8s.io/v1<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># This is required.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Policy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># Don&#39;t generate audit events for all requests in RequestReceived stage.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">omitStages</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#b44">&#34;RequestReceived&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Log pod changes at RequestResponse level</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>RequestResponse<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Resource &#34;pods&#34; doesn&#39;t match requests to any subresource of pods,</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># which is consistent with the RBAC policy.</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;pods&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Log &#34;pods/log&#34;, &#34;pods/status&#34; at Metadata level</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>Metadata<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;pods/log&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;pods/status&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Don&#39;t log requests to a configmap called &#34;controller-leader&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;configmaps&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resourceNames</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;controller-leader&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Don&#39;t log watch requests by the &#34;system:kube-proxy&#34; on endpoints or services</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">users</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;system:kube-proxy&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;watch&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># core API group</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;endpoints&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;services&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Don&#39;t log authenticated requests to certain non-resource URL paths.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">userGroups</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;system:authenticated&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">nonResourceURLs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;/api*&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Wildcard matching.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;/version&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Log the request body of configmap changes in kube-system.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>Request<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># core API group</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;configmaps&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># This rule only applies to resources in the &#34;kube-system&#34; namespace.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># The empty string &#34;&#34; can be used to select non-namespaced resources.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespaces</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;kube-system&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Log configmap and secret changes in all other namespaces at the Metadata level.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>Metadata<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># core API group</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;secrets&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;configmaps&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Log all other resources in core and extensions at the Request level.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>Request<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># core API group</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;extensions&#34;</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Version of group should NOT be included.</span><span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># A catch-all rule to log all other requests at the Metadata level.</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>Metadata<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Long-running requests like watches that fall under this rule will not</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># generate an audit event in RequestReceived.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">omitStages</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;RequestReceived&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
You can use a minimal audit policy file to log all requests at the `Metadata` level:
-->
<p>你可以使用最低限度的审计策略文件在 <code>Metadata</code> 级别记录所有请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 在 Metadata 级别为所有请求生成日志</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>audit.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Policy<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">level</span>:<span style="color:#bbb"> </span>Metadata<span style="color:#bbb">
</span></code></pre></div><!--
If you're crafting your own audit profile, you can use the audit profile for Google Container-Optimized OS as a starting point. You can check the
[configure-helper.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh)
script, which generates the audit policy file. You can see most of the audit policy file by looking directly at the script.

You can also refer to the [`Policy` configuration reference](/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy)
for details about the fields defined.
-->
<p>如果你在打磨自己的审计配置文件，你可以使用为 Google Container-Optimized OS
设计的审计配置作为出发点。你可以参考
<a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh">configure-helper.sh</a>
脚本，该脚本能够生成审计策略文件。你可以直接在脚本中看到审计策略的绝大部份内容。</p>
<p>你也可以参考 <a href="/zh/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy"><code>Policy</code> 配置参考</a>
以获取有关已定义字段的详细信息。</p>
<!--
## Audit backends

Audit backends persist audit events to an external storage.
Out of the box, the kube-apiserver provides two backends:

- Log backend, which writes events into the filesystem
- Webhook backend, which sends events to an external HTTP API

In all cases, audit events follow a structure defined by the Kubernetes API in the
[`audit.k8s.io` API group](/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Event).
-->
<h2 id="audit-backends">审计后端  </h2>
<p>审计后端实现将审计事件导出到外部存储。<code>Kube-apiserver</code> 默认提供两个后端：</p>
<ul>
<li>Log 后端，将事件写入到文件系统</li>
<li>Webhook 后端，将事件发送到外部 HTTP API</li>
</ul>
<p>在这所有情况下，审计事件均遵循 Kubernetes API 在
<a href="/zh/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Event"><code>audit.k8s.io</code> API 组</a>
中定义的结构。</p>
<!--
In case of patches, request body is a JSON array with patch operations, not a JSON object
with an appropriate Kubernetes API object. For example, the following request body is a valid patch
request to `/apis/batch/v1/namespaces/some-namespace/jobs/some-job-name`.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>对于 patch 请求，请求的消息体需要是设定 patch 操作的 JSON 所构成的一个串，
而不是一个完整的 Kubernetes API 对象 JSON 串。
例如，以下的示例是一个合法的 patch 请求消息体，该请求对应
<code>/apis/batch/v1/namespaces/some-namespace/jobs/some-job-name</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">[
  {
    <span style="color:#008000;font-weight:bold">&#34;op&#34;</span>: <span style="color:#b44">&#34;replace&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;path&#34;</span>: <span style="color:#b44">&#34;/spec/parallelism&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;value&#34;</span>: <span style="color:#666">0</span>
  },
  {
    <span style="color:#008000;font-weight:bold">&#34;op&#34;</span>: <span style="color:#b44">&#34;remove&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;path&#34;</span>: <span style="color:#b44">&#34;/spec/template/spec/containers/0/terminationMessagePolicy&#34;</span>
  }
]
</code></pre></div></div>
</blockquote>
<!--
### Log backend

The log backend writes audit events to a file in [JSONlines](https://jsonlines.org/) format.
You can configure the log audit backend using the following `kube-apiserver` flags:

Log backend writes audit events to a file in JSON format. You can configure
log audit backend using the following [kube-apiserver][kube-apiserver] flags:
-->
<h3 id="log-后端">Log 后端</h3>
<p>Log 后端将审计事件写入 <a href="https://jsonlines.org/">JSONlines</a>  格式的文件。
你可以使用以下 <code>kube-apiserver</code> 标志配置 Log 审计后端：</p>
<!--
- `--audit-log-path` specifies the log file path that log backend uses to write
  audit events. Not specifying this flag disables log backend. `-` means standard out
- `--audit-log-maxage` defined the maximum number of days to retain old audit log files
- `--audit-log-maxbackup` defines the maximum number of audit log files to retain
- `--audit-log-maxsize` defines the maximum size in megabytes of the audit log file before it gets rotated
-->
<ul>
<li><code>--audit-log-path</code> 指定用来写入审计事件的日志文件路径。不指定此标志会禁用日志后端。<code>-</code> 意味着标准化</li>
<li><code>--audit-log-maxage</code> 定义保留旧审计日志文件的最大天数</li>
<li><code>--audit-log-maxbackup</code> 定义要保留的审计日志文件的最大数量</li>
<li><code>--audit-log-maxsize</code> 定义审计日志文件的最大大小（兆字节）</li>
</ul>
<!--
If your cluster's control plane runs the kube-apiserver as a Pod, remember to mount the `hostPath`
to the location of the policy file and log file, so that audit records are persisted. For example:
-->
<p>如果你的集群控制面以 Pod 的形式运行 kube-apiserver，记得要通过 <code>hostPath</code>
卷来访问策略文件和日志文件所在的目录，这样审计记录才会持久保存下来。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">  --audit-policy-file<span style="color:#666">=</span>/etc/kubernetes/audit-policy.yaml
  --audit-log-path<span style="color:#666">=</span>/var/log/audit.log
</code></pre></div><p>接下来挂载数据卷：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/kubernetes/audit-policy.yaml<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>audit<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log/audit.log<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>audit-log<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span></code></pre></div><!-- 
and finally configure the `hostPath`:
-->
<p>最后配置 <code>hostPath</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>audit<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/etc/kubernetes/audit-policy.yaml<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>File<span style="color:#bbb">
</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>audit-log<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log/audit.log<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>FileOrCreate<span style="color:#bbb">
</span></code></pre></div><!--
### Webhook backend

The webhook audit backend sends audit events to a remote web API, which is assumed to
be a form of the Kubernetes API, including means of authentication. You can configure
a webhook audit backend using the following kube-apiserver flags:
-->
<h3 id="webhook-backend">Webhook 后端  </h3>
<p>Webhook 后端将审计事件发送到远程 Web API，该远程 API 应该暴露与 <code>kube-apiserver</code>
形式相同的 API，包括其身份认证机制。你可以使用如下 kube-apiserver 标志来配置
Webhook 审计后端：</p>
<!--
- `--audit-webhook-config-file` specifies the path to a file with a webhook
  configuration. The webhook configuration is effectively a specialized
  [kubeconfig](/docs/tasks/access-application-cluster/configure-access-multiple-clusters).
- `--audit-webhook-initial-backoff` specifies the amount of time to wait after the first failed
  request before retrying. Subsequent requests are retried with exponential backoff.

The webhook config file uses the kubeconfig format to specify the remote address of
the service and credentials used to connect to it.
-->
<ul>
<li><code>--audit-webhook-config-file</code> 设置 Webhook 配置文件的路径。Webhook 配置文件实际上是一个
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig 文件</a>。</li>
<li><code>--audit-webhook-initial-backoff</code> 指定在第一次失败后重发请求等待的时间。随后的请求将以指数退避重试。</li>
</ul>
<p>Webhook 配置文件使用 kubeconfig 格式指定服务的远程地址和用于连接它的凭据。</p>
<!--
## Event batching {#batching}

Both log and webhook backends support batching. Using webhook as an example, here's the list of
available flags. To get the same flag for log backend, replace `webhook` with `log` in the flag
name. By default, batching is enabled in `webhook` and disabled in `log`. Similarly, by default
throttling is enabled in `webhook` and disabled in `log`.
-->
<h2 id="batching">事件批处理 </h2>
<p>日志和 Webhook 后端都支持批处理。以 Webhook 为例，以下是可用参数列表。要获取日志
后端的同样参数，请在参数名称中将 <code>webhook</code> 替换为 <code>log</code>。
默认情况下，在 <code>webhook</code> 中批处理是被启用的，在 <code>log</code> 中批处理是被禁用的。
同样，默认情况下，在 <code>webhook</code> 中启用带宽限制，在 <code>log</code> 中禁用带宽限制。</p>
<!--
  - `--audit-webhook-mode` defines the buffering strategy. One of the following:
  - `batch` - buffer events and asynchronously process them in batches. This is the default.
  - `blocking` - block API server responses on processing each individual event.
  - `blocking-strict` - Same as blocking, but when there is a failure during audit logging at the
     RequestReceived stage, the whole request to the kube-apiserver fails.
-->
<ul>
<li><code>--audit-webhook-mode</code> 定义缓存策略，可选值如下：
<ul>
<li><code>batch</code> - 以批处理缓存事件和异步的过程。这是默认值。</li>
<li><code>blocking</code> - 在 API 服务器处理每个单独事件时，阻塞其响应。</li>
<li><code>blocking-strict</code> - 与 <code>blocking</code> 相同，不过当审计日志在 RequestReceived 阶段
失败时，整个 API 服务请求会失效。</li>
</ul>
</li>
</ul>
<!--
The following flags are used only in the `batch` mode.

- `--audit-webhook-batch-buffer-size` defines the number of events to buffer before batching.
  If the rate of incoming events overflows the buffer, events are dropped.
- `--audit-webhook-batch-max-size` defines the maximum number of events in one batch.
- `--audit-webhook-batch-max-wait` defines the maximum amount of time to wait before unconditionally
  batching events in the queue.
- `--audit-webhook-batch-throttle-qps` defines the maximum average number of batches generated
  per second.
- `--audit-webhook-batch-throttle-burst` defines the maximum number of batches generated at the same
  moment if the allowed QPS was underutilized previously.
-->
<p>以下参数仅用于 <code>batch</code> 模式。</p>
<ul>
<li><code>--audit-webhook-batch-buffer-size</code> 定义 batch 之前要缓存的事件数。
如果传入事件的速率溢出缓存区，则会丢弃事件。</li>
<li><code>--audit-webhook-batch-max-size</code> 定义一个 batch 中的最大事件数。</li>
<li><code>--audit-webhook-batch-max-wait</code> 无条件 batch 队列中的事件前等待的最大事件。</li>
<li><code>--audit-webhook-batch-throttle-qps</code> 每秒生成的最大批次数。</li>
<li><code>--audit-webhook-batch-throttle-burst</code> 在达到允许的 QPS 前，同一时刻允许存在的最大 batch 生成数。</li>
</ul>
<!--
## Parameter tuning

Parameters should be set to accommodate the load on the API server.

For example, if kube-apiserver receives 100 requests each second, and each request is audited only
on `ResponseStarted` and `ResponseComplete` stages, you should account for ≅200 audit
events being generated each second. Assuming that there are up to 100 events in a batch,
you should set throttling level at least 2 queries per second. Assuming that the backend can take up to
5 seconds to write events, you should set the buffer size to hold up to 5 seconds of events;
that is: 10 batches, or 1000 events.
-->
<h2 id="parameter-tuning">参数调整  </h2>
<p>需要设置参数以适应 API 服务器上的负载。</p>
<p>例如，如果 kube-apiserver 每秒收到 100 个请求，并且每个请求仅在 <code>ResponseStarted</code>
和 <code>ResponseComplete</code> 阶段进行审计，则应该考虑每秒生成约 200 个审计事件。
假设批处理中最多有 100 个事件，则应将限制级别设置为每秒至少 2 个查询。
假设后端最多需要 5 秒钟来写入事件，你应该设置缓冲区大小以容纳最多 5 秒的事件，
即 10 个 batch，即 1000 个事件。</p>
<!--
In most cases however, the default parameters should be sufficient and you don't have to worry about
setting them manually. You can look at the following Prometheus metrics exposed by kube-apiserver
and in the logs to monitor the state of the auditing subsystem.

- `apiserver_audit_event_total` metric contains the total number of audit events exported.
- `apiserver_audit_error_total` metric contains the total number of events dropped due to an error
  during exporting.
-->
<p>但是，在大多数情况下，默认参数应该足够了，你不必手动设置它们。
你可以查看 kube-apiserver 公开的以下 Prometheus 指标，并在日志中监控审计子系统的状态。</p>
<ul>
<li><code>apiserver_audit_event_total</code> 包含所有暴露的审计事件数量的指标。</li>
<li><code>apiserver_audit_error_total</code> 在暴露时由于发生错误而被丢弃的事件的数量。</li>
</ul>
<!--
### Log entry truncation {#truncate}

Both log and webhook backends support limiting the size of events that are logged.
As an example, the following is the list of flags available for the log backend:
-->
<h3 id="truncate">日志条目截断  </h3>
<p>日志后端和 Webhook 后端都支持限制所输出的事件的尺寸。
例如，下面是可以为日志后端配置的标志列表：</p>
<!--
- `audit-log-truncate-enabled` whether event and batch truncating is enabled.
- `audit-log-truncate-max-batch-size` maximum size in bytes of the batch sent to the underlying backend.
- `audit-log-truncate-max-event-size` maximum size in bytes of the audit event sent to the underlying backend.
-->
<ul>
<li><code>audit-log-truncate-enabled</code>：是否弃用事件和批次的截断处理。</li>
<li><code>audit-log-truncate-max-batch-size</code>：向下层后端发送的各批次的最大尺寸字节数。</li>
<li><code>audit-log-truncate-max-event-size</code>：向下层后端发送的审计事件的最大尺寸字节数。</li>
</ul>
<!--
By default truncate is disabled in both `webhook` and `log`, a cluster administrator should set
`audit-log-truncate-enabled` or `audit-webhook-truncate-enabled` to enable the feature.
-->
<p>默认情况下，截断操作在 <code>webhook</code> 和 <code>log</code> 后端都是被禁用的，集群管理员需要设置
<code>audit-log-truncate-enabled</code> 或 <code>audit-webhook-truncate-enabled</code> 标志来启用此操作。</p>
<h2 id="接下来">接下来</h2>
<!--
* Learn about [Mutating webhook auditing annotations](/docs/reference/access-authn-authz/extensible-admission-controllers/#mutating-webhook-auditing-annotations).
-->
<ul>
<li>了解 <a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#mutating-webhook-auditing-annotations">Mutating webhook 审计注解</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3556c4dbd027b9e90a5b3d72649003fb">10.4 - 应用故障排查</h1>
    
	<!--
title: Troubleshoot Applications
content_type: concept
-->
<!-- overview -->
<!--
This guide is to help users debug applications that are deployed into Kubernetes and not behaving correctly.
This is *not* a guide for people who want to debug their cluster.  For that you should check out
[this guide](/docs/admin/cluster-troubleshooting).
-->
<p>本指南帮助用户调试那些部署到 Kubernetes 上后没有正常运行的应用。
本指南 <em>并非</em> 指导用户如何调试集群。
如果想调试集群的话，请参阅<a href="/zh/docs/tasks/debug-application-cluster/debug-cluster/">这里</a>。</p>
<!-- body -->
<!--
## Diagnosing the problem

The first step in troubleshooting is triage.  What is the problem?  Is it your Pods, your Replication Controller or
your Service?

   * [Debugging Pods](#debugging-pods)
   * [Debugging Replication Controllers](#debugging-replication-controllers)
   * [Debugging Services](#debugging-services)
-->
<h2 id="diagnosing-the-problem">诊断问题  </h2>
<p>故障排查的第一步是先给问题分类。问题是什么？是关于 Pods、Replication Controller 还是 Service？</p>
<ul>
<li><a href="#debugging-pods">调试 Pods</a></li>
<li><a href="#debugging-replication-controllers">调试副本控制器</a></li>
<li><a href="#debugging-services">调试服务</a></li>
</ul>
<!--
### Debugging Pods

The first step in debugging a Pod is taking a look at it.  Check the current state of the Pod and recent events with the following command:
-->
<h3 id="debugging-pods">调试 Pods  </h3>
<p>调试 Pod 的第一步是查看 Pod 信息。用如下命令查看 Pod 的当前状态和最近的事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pods <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">POD_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><!--
Look at the state of the containers in the pod.  Are they all `Running`?  Have there been recent restarts?

Continue debugging depending on the state of the pods.
-->
<p>查看一下 Pod 中的容器所处的状态。这些容器的状态都是 <code>Running</code> 吗？最近有没有重启过？</p>
<p>后面的调试都是要依靠 Pod 的状态的。</p>
<!--
#### My pod stays pending

If a Pod is stuck in `Pending` it means that it can not be scheduled onto a node.  Generally this is because
there are insufficient resources of one type or another that prevent scheduling.  Look at the output of the
`kubectl describe ...` command above.  There should be messages from the scheduler about why it can not schedule
your pod.  Reasons include:
-->
<h4 id="pod-停滞在-pending-状态">Pod 停滞在 Pending 状态</h4>
<p>如果一个 Pod 停滞在 <code>Pending</code> 状态，表示 Pod 没有被调度到节点上。通常这是因为
某种类型的资源不足导致无法调度。
查看上面的 <code>kubectl describe ...</code> 命令的输出，其中应该显示了为什么没被调度的原因。
常见原因如下：</p>
<!--
* **You don't have enough resources**:  You may have exhausted the supply of CPU or Memory in your cluster, in this case
you need to delete Pods, adjust resource requests, or add new nodes to your cluster. See [Compute Resources document](/docs/user-guide/compute-resources/#my-pods-are-pending-with-event-message-failedscheduling) for more information.

* **You are using `hostPort`**:  When you bind a Pod to a `hostPort` there are a limited number of places that pod can be
scheduled.  In most cases, `hostPort` is unnecessary, try using a Service object to expose your Pod.  If you do require
`hostPort` then you can only schedule as many Pods as there are nodes in your Kubernetes cluster.
-->
<ul>
<li>
<p><strong>资源不足</strong>:
你可能耗尽了集群上所有的 CPU 或内存。此时，你需要删除 Pod、调整资源请求或者为集群添加节点。
更多信息请参阅<a href="/zh/docs/concepts/configuration/manage-resources-containers/">计算资源文档</a></p>
</li>
<li>
<p><strong>使用了 <code>hostPort</code></strong>:
如果绑定 Pod 到 <code>hostPort</code>，那么能够运行该 Pod 的节点就有限了。
多数情况下，<code>hostPort</code> 是非必要的，而应该采用 Service 对象来暴露 Pod。
如果确实需要使用 <code>hostPort</code>，那么集群中节点的个数就是所能创建的 Pod
的数量上限。</p>
</li>
</ul>
<!--
#### My pod stays waiting

If a Pod is stuck in the `Waiting` state, then it has been scheduled to a worker node, but it can't run on that machine.
Again, the information from `kubectl describe ...` should be informative.  The most common cause of `Waiting` pods is a failure to pull the image.  There are three things to check:

* Make sure that you have the name of the image correct.
* Have you pushed the image to the repository?
* Run a manual `docker pull <image>` on your machine to see if the image can be pulled.
-->
<h4 id="pod-停滞在-waiting-状态">Pod 停滞在 Waiting 状态</h4>
<p>如果 Pod 停滞在 <code>Waiting</code> 状态，则表示 Pod 已经被调度到某工作节点，但是无法在该节点上运行。
同样，<code>kubectl describe ...</code> 命令的输出可能很有用。
<code>Waiting</code> 状态的最常见原因是拉取镜像失败。要检查的有三个方面：</p>
<ul>
<li>确保镜像名字拼写正确</li>
<li>确保镜像已被推送到镜像仓库</li>
<li>用手动命令 <code>docker pull &lt;镜像&gt;</code> 试试看镜像是否可拉取</li>
</ul>
<!--
#### My pod is crashing or otherwise unhealthy

Once your pod has been scheduled, the methods described in [Debug Running Pods](
/docs/tasks/debug-application-cluster/debug-running-pod/) are available for debugging.
-->
<h4 id="pod-处于-crashing-或别的不健康状态">Pod 处于 Crashing 或别的不健康状态</h4>
<p>一旦 Pod 被调度，就可以采用
<a href="/zh/docs/tasks/debug-application-cluster/debug-running-pod/">调试运行中的 Pod</a>
中的方法来进一步调试。</p>
<!--
#### My pod is running but not doing what I told it to do

If your pod is not behaving as you expected, it may be that there was an error in your
pod description (e.g. `mypod.yaml` file on your local machine), and that the error
was silently ignored when you created the pod.  Often a section of the pod description
is nested incorrectly, or a key name is typed incorrectly, and so the key is ignored.
For example, if you misspelled `command` as `commnd` then the pod will be created but
will not use the command line you intended it to use.
-->
<h4 id="pod-处于-running-态但是没有正常工作">Pod 处于 Running 态但是没有正常工作</h4>
<p>如果 Pod 行为不符合预期，很可能 Pod 描述（例如你本地机器上的 <code>mypod.yaml</code>）中有问题，
并且该错误在创建 Pod 时被忽略掉，没有报错。
通常，Pod 的定义中节区嵌套关系错误、字段名字拼错的情况都会引起对应内容被忽略掉。
例如，如果你误将 <code>command</code> 写成 <code>commnd</code>，Pod 虽然可以创建，但它不会执行
你期望它执行的命令行。</p>
<!--
The first thing to do is to delete your pod and try creating it again with the `--validate` option.
For example, run `kubectl apply --validate -f mypod.yaml`.
If you misspelled `command` as `commnd` then will give an error like this:
-->
<p>可以做的第一件事是删除你的 Pod，并尝试带有 <code>--validate</code> 选项重新创建。
例如，运行 <code>kubectl apply --validate -f mypod.yaml</code>。
如果 <code>command</code>  被误拼成 <code>commnd</code>，你将会看到下面的错误信息：</p>
<pre tabindex="0"><code>I0805 10:43:25.129850   46757 schema.go:126] unknown field: commnd
I0805 10:43:25.129973   46757 schema.go:129] this may be a false alarm, see https://github.com/kubernetes/kubernetes/issues/6842
pods/mypod
</code></pre><!-- TODO: Now that #11914 is merged, this advice may need to be updated -->
<!--
The next thing to check is whether the pod on the apiserver
matches the pod you meant to create (e.g. in a yaml file on your local machine).
For example, run `kubectl get pods/mypod -o yaml > mypod-on-apiserver.yaml` and then
manually compare the original pod description, `mypod.yaml` with the one you got
back from apiserver, `mypod-on-apiserver.yaml`.  There will typically be some
lines on the "apiserver" version that are not on the original version.  This is
expected.  However, if there are lines on the original that are not on the apiserver
version, then this may indicate a problem with your pod spec.
-->
<p>接下来就要检查的是 API 服务器上的 Pod 与你所期望创建的是否匹配
（例如，你原本使用本机上的一个 YAML 文件来创建 Pod）。
例如，运行 <code>kubectl get pods/mypod -o yaml &gt; mypod-on-apiserver.yaml</code>，之后
手动比较 <code>mypod.yaml</code> 与从 API 服务器取回的 Pod 描述。
从 API 服务器处获得的 YAML 通常包含一些创建 Pod 所用的 YAML 中不存在的行，这是正常的。
不过，如果如果源文件中有些行在 API 服务器版本中不存在，则意味着
Pod 规约是有问题的。</p>
<!--
### Debugging Replication Controllers

Replication controllers are fairly straightforward.  They can either create Pods or they can't.  If they can't
create pods, then please refer to the [instructions above](#debugging-pods) to debug your pods.

You can also use `kubectl describe rc ${CONTROLLER_NAME}` to introspect events related to the replication
controller.
-->
<h3 id="debugging-replication-controllers">调试副本控制器 </h3>
<p>副本控制器相对比较简单直接。它们要么能创建 Pod，要么不能。
如果不能创建 Pod，请参阅<a href="#debugging-pods">上述说明</a>调试 Pod。</p>
<p>你也可以使用 <code>kubectl describe rc ${CONTROLLER_NAME}</code> 命令来检视副本控制器相关的事件。</p>
<!--
### Debugging Services

Services provide load balancing across a set of pods.  There are several common problems that can make Services
not work properly.  The following instructions should help debug Service problems.

First, verify that there are endpoints for the service. For every Service object, the apiserver makes an `endpoints` resource available.

You can view this resource with:
-->
<h3 id="debugging-services">调试服务  </h3>
<p>服务支持在多个 Pod 间负载均衡。
有一些常见的问题可以造成服务无法正常工作。
以下说明将有助于调试服务的问题。</p>
<p>首先，验证服务是否有端点。对于每一个 Service 对象，API 服务器为其提供
对应的 <code>endpoints</code> 资源。</p>
<p>通过如下命令可以查看 endpoints 资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get endpoints <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">SERVICE_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><!--
Make sure that the endpoints match up with the number of pods that you expect to be members of your service.
For example, if your Service is for an nginx container with 3 replicas, you would expect to see three different
IP addresses in the Service's endpoints.
-->
<p>确保 Endpoints 与服务成员 Pod 个数一致。
例如，如果你的 Service 用来运行 3 个副本的 nginx 容器，你应该会在服务的 Endpoints
中看到 3 个不同的 IP 地址。</p>
<!--
#### My service is missing endpoints

If you are missing endpoints, try listing pods using the labels that Service uses.  Imagine that you have
a Service where the labels are:
-->
<h4 id="服务缺少-endpoints">服务缺少 Endpoints</h4>
<p>如果没有 Endpoints，请尝试使用 Service 所使用的标签列出 Pod。
假定你的服务包含如下标签选择算符：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">     </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb">
</span></code></pre></div><!--
You can use:
```shell
kubectl get pods --selector=name=nginx,type=frontend
```

to list pods that match this selector.  Verify that the list matches the Pods that you expect to provide your Service.
-->
<p>你可以使用如下命令列出与选择算符相匹配的 Pod，并验证这些 Pod 是否归属于创建的服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --selector<span style="color:#666">=</span><span style="color:#b8860b">name</span><span style="color:#666">=</span>nginx,type<span style="color:#666">=</span>frontend
</code></pre></div><!--
Verify that the pod's `containerPort` matches up with the Service's `targetPort`
-->
<p>验证 Pod 的 <code>containerPort</code> 与服务的 <code>targetPort</code> 是否匹配。</p>
<!--
#### Network traffic is not forwarded

Please see [debugging service](/docs/tasks/debug-application-cluster/debug-service/) for more information.
-->
<h4 id="网络流量未被转发">网络流量未被转发</h4>
<p>请参阅<a href="/zh/docs/tasks/debug-application-cluster/debug-service/">调试 service</a> 了解更多信息。</p>
<h2 id="接下来">接下来</h2>
<!--
If none of the above solves your problem, follow the instructions in [Debugging Service document](/docs/user-guide/debugging-services) to make sure that your `Service` is running, has `Endpoints`, and your `Pods` are actually serving; you have DNS working, iptables rules installed, and kube-proxy does not seem to be misbehaving.

You may also visit [troubleshooting document](/docs/troubleshooting/) for more information.
-->
<p>如果上述方法都不能解决你的问题，请按照
<a href="/zh/docs/tasks/debug-application-cluster/debug-service/">调试服务文档</a>中的介绍，
确保你的 <code>Service</code> 处于 Running 态，有 <code>Endpoints</code> 被创建，<code>Pod</code> 真的在提供服务；
DNS 服务已配置并正常工作，iptables 规则也以安装并且 <code>kube-proxy</code> 也没有异常行为。</p>
<p>你也可以访问<a href="/zh/docs/tasks/debug-application-cluster/troubleshooting/">故障排查文档</a>来获取更多信息。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-731bb8b338c16aebfb9590ba2bd3fdd1">10.5 - 应用自测与调试</h1>
    
	<!-- overview -->
<!--
Once your application is running, you'll inevitably need to debug problems with it.
Earlier we described how you can use `kubectl get pods` to retrieve simple status information about
your pods. But there are a number of ways to get even more information about your application.
-->
<p>运行应用时，不可避免的需要定位问题。
前面我们介绍了如何使用 <code>kubectl get pods</code> 来查询 pod 的简单信息。
除此之外，还有一系列的方法来获取应用的更详细信息。</p>
<!-- body -->
<!--
## Using `kubectl describe pod` to fetch details about pods
-->
<h2 id="使用-kubectl-describe-pod-命令获取-pod-详情">使用 <code>kubectl describe pod</code> 命令获取 Pod 详情</h2>
<!--
For this example we'll use a Deployment to create two pods, similar to the earlier example.
-->
<p>与之前的例子类似，我们使用一个 Deployment 来创建两个 Pod。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/nginx-with-request.yaml" download="application/nginx-with-request.yaml"><code>application/nginx-with-request.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-nginx-with-request-yaml')" title="Copy application/nginx-with-request.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-nginx-with-request-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">2</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;128Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;500m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create deployment by running following command:
-->
<p>使用如下命令创建 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/application/nginx-with-request.yaml
</code></pre></div><pre tabindex="0"><code>deployment.apps/nginx-deployment created
</code></pre><!--
Check pod status by following command:
-->
<p>使用如下命令查看 Pod 状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1006230814-6winp   1/1       Running   0          11s
nginx-deployment-1006230814-fmgu3   1/1       Running   0          11s
</code></pre><!--
We can retrieve a lot more information about each of these pods using `kubectl describe pod`. For example:
-->
<p>我们可以使用 <code>kubectl describe pod</code> 命令来查询每个 Pod 的更多信息，比如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod nginx-deployment-1006230814-6winp
</code></pre></div><pre tabindex="0"><code>Name:		nginx-deployment-1006230814-6winp
Namespace:	default
Node:		kubernetes-node-wul5/10.240.0.9
Start Time:	Thu, 24 Mar 2016 01:39:49 +0000
Labels:		app=nginx,pod-template-hash=1006230814
Annotations:    kubernetes.io/created-by={&quot;kind&quot;:&quot;SerializedReference&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;reference&quot;:{&quot;kind&quot;:&quot;ReplicaSet&quot;,&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;nginx-deployment-1956810328&quot;,&quot;uid&quot;:&quot;14e607e7-8ba1-11e7-b5cb-fa16&quot; ...
Status:		Running
IP:		10.244.0.6
Controllers:	ReplicaSet/nginx-deployment-1006230814
Containers:
  nginx:
    Container ID:	docker://90315cc9f513c724e9957a4788d3e625a078de84750f244a40f97ae355eb1149
    Image:		nginx
    Image ID:		docker://6f62f48c4e55d700cf3eb1b5e33fa051802986b77b874cc351cce539e5163707
    Port:		80/TCP
    QoS Tier:
      cpu:	Guaranteed
      memory:	Guaranteed
    Limits:
      cpu:	500m
      memory:	128Mi
    Requests:
      memory:		128Mi
      cpu:		500m
    State:		Running
      Started:		Thu, 24 Mar 2016 01:39:51 +0000
    Ready:		True
    Restart Count:	0
    Environment:        &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5kdvl (ro)
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-4bcbi:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-4bcbi
    Optional:   false
QoS Class:      Guaranteed
Node-Selectors: &lt;none&gt;
Tolerations:    &lt;none&gt;
Events:
  FirstSeen	LastSeen	Count	From					SubobjectPath		Type		Reason		Message
  ---------	--------	-----	----					-------------		--------	------		-------
  54s		54s		1	{default-scheduler }						Normal		Scheduled	Successfully assigned nginx-deployment-1006230814-6winp to kubernetes-node-wul5
  54s		54s		1	{kubelet kubernetes-node-wul5}	spec.containers{nginx}	Normal		Pulling		pulling image &quot;nginx&quot;
  53s		53s		1	{kubelet kubernetes-node-wul5}	spec.containers{nginx}	Normal		Pulled		Successfully pulled image &quot;nginx&quot;
  53s		53s		1	{kubelet kubernetes-node-wul5}	spec.containers{nginx}	Normal		Created		Created container with docker id 90315cc9f513
  53s		53s		1	{kubelet kubernetes-node-wul5}	spec.containers{nginx}	Normal		Started		Started container with docker id 90315cc9f513
</code></pre><!--
Here you can see configuration information about the container(s) and Pod (labels, resource requirements, etc.), 
as well as status information about the container(s) and Pod (state, readiness, restart count, events, etc.).
-->
<p>这里可以看到容器和 Pod 的标签、资源需求等配置信息，还可以看到状态、就绪态、
重启次数、事件等状态信息。</p>
<!--
The container state is one of Waiting, Running, or Terminated. 
Depending on the state, additional information will be provided -- here you can see that for a container in Running state, the system tells you when the container started.
-->
<p>容器状态是 Waiting、Running 和 Terminated 之一。
根据状态的不同，还有对应的额外的信息 —— 在这里你可以看到，
对于处于运行状态的容器，系统会告诉你容器的启动时间。</p>
<!--
Ready tells you whether the container passed its last readiness probe. 
(In this case, the container does not have a readiness probe configured; the container is assumed to be ready if no readiness probe is configured.)
-->
<p>Ready 指示是否通过了最后一个就绪态探测。
(在本例中，容器没有配置就绪态探测；如果没有配置就绪态探测，则假定容器已经就绪。)</p>
<!--
Restart Count tells you how many times the container has been restarted; 
this information can be useful for detecting crash loops in containers that are configured with a restart policy of 'always.'
-->
<p>Restart Count 告诉你容器已重启的次数；
这些信息对于定位配置了 “Always” 重启策略的容器持续崩溃问题非常有用。</p>
<!--
Currently the only Condition associated with a Pod is the binary Ready condition, 
which indicates that the pod is able to service requests and should be added to the load balancing pools of all matching services.
-->
<p>目前，唯一与 Pod 有关的状态是 Ready 状况，该状况表明 Pod 能够为请求提供服务，
并且应该添加到相应服务的负载均衡池中。</p>
<!--
Lastly, you see a log of recent events related to your Pod. 
The system compresses multiple identical events by indicating the first and last time it was seen and the number of times it was seen. 
"From" indicates the component that is logging the event, 
"SubobjectPath" tells you which object (e.g. container within the pod) is being referred to, 
and "Reason" and "Message" tell you what happened.
-->
<p>最后，你还可以看到与 Pod 相关的近期事件。
系统通过指示第一次和最后一次看到事件以及看到该事件的次数来压缩多个相同的事件。
“From” 标明记录事件的组件，
“SubobjectPath” 告诉你引用了哪个对象（例如 Pod 中的容器），
“Reason” 和 “Message” 告诉你发生了什么。</p>
<!--
## Example: debugging Pending Pods

A common scenario that you can detect using events is when you've created a Pod that won't fit on any node. 
For example, the Pod might request more resources than are free on any node, 
or it might specify a label selector that doesn't match any nodes. 
Let's say we created the previous Deployment with 5 replicas (instead of 2) and requesting 600 millicores instead of 500, 
on a four-node cluster where each (virtual) machine has 1 CPU. 
In that case one of the Pods will not be able to schedule. 
(Note that because of the cluster addon pods such as fluentd, skydns, etc., that run on each node, if we requested 1000 millicores then none of the Pods would be able to schedule.)
-->
<h2 id="例子-调试-pending-状态的-pod">例子: 调试 Pending 状态的 Pod</h2>
<p>可以使用事件来调试的一个常见的场景是，你创建 Pod 无法被调度到任何节点。
比如，Pod 请求的资源比较多，没有任何一个节点能够满足，或者它指定了一个标签，没有节点可匹配。
假定我们创建之前的 Deployment 时指定副本数是 5（不再是 2），并且请求 600 毫核（不再是 500），
对于一个 4 个节点的集群，若每个节点只有 1 个 CPU，这时至少有一个 Pod 不能被调度。
（需要注意的是，其他集群插件 Pod，比如 fluentd、skydns 等等会在每个节点上运行，
如果我们需求 1000 毫核，将不会有 Pod 会被调度。）</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><pre tabindex="0"><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1006230814-6winp   1/1       Running   0          7m
nginx-deployment-1006230814-fmgu3   1/1       Running   0          7m
nginx-deployment-1370807587-6ekbw   1/1       Running   0          1m
nginx-deployment-1370807587-fg172   0/1       Pending   0          1m
nginx-deployment-1370807587-fz9sd   0/1       Pending   0          1m
</code></pre><!--
To find out why the nginx-deployment-1370807587-fz9sd pod is not running, we can use `kubectl describe pod` on the pending Pod and look at its events:
-->
<p>为了查找 Pod nginx-deployment-1370807587-fz9sd 没有运行的原因，我们可以使用
<code>kubectl describe pod</code> 命令描述 Pod，查看其事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod nginx-deployment-1370807587-fz9sd
</code></pre></div><pre tabindex="0"><code>  Name:		nginx-deployment-1370807587-fz9sd
  Namespace:	default
  Node:		/
  Labels:		app=nginx,pod-template-hash=1370807587
  Status:		Pending
  IP:
  Controllers:	ReplicaSet/nginx-deployment-1370807587
  Containers:
    nginx:
      Image:	nginx
      Port:	80/TCP
      QoS Tier:
        memory:	Guaranteed
        cpu:	Guaranteed
      Limits:
        cpu:	1
        memory:	128Mi
      Requests:
        cpu:	1
        memory:	128Mi
      Environment Variables:
  Volumes:
    default-token-4bcbi:
      Type:	Secret (a volume populated by a Secret)
      SecretName:	default-token-4bcbi
  Events:
    FirstSeen	LastSeen	Count	From			        SubobjectPath	Type		Reason			    Message
    ---------	--------	-----	----			        -------------	--------	------			    -------
    1m		    48s		    7	    {default-scheduler }			        Warning		FailedScheduling	pod (nginx-deployment-1370807587-fz9sd) failed to fit in any node
  fit failure on node (kubernetes-node-6ta5): Node didn't have enough resource: CPU, requested: 1000, used: 1420, capacity: 2000
  fit failure on node (kubernetes-node-wul5): Node didn't have enough resource: CPU, requested: 1000, used: 1100, capacity: 2000
</code></pre><!--
Here you can see the event generated by the scheduler saying that the Pod failed to schedule for reason `FailedScheduling` (and possibly others).  
The message tells us that there were not enough resources for the Pod on any of the nodes.
-->
<p>这里你可以看到由调度器记录的事件，它表明了 Pod 不能被调度的原因是 <code>FailedScheduling</code>（也可能是其他值）。
其 message 部分表明没有任何节点拥有足够多的资源。</p>
<!--
To correct this situation, you can use `kubectl scale` to update your Deployment to specify four or fewer replicas. (Or you could leave the one Pod pending, which is harmless.)
-->
<p>要纠正这种情况，可以使用 <code>kubectl scale</code> 更新 Deployment，以指定 4 个或更少的副本。
(或者你可以让 Pod 继续保持这个状态，这是无害的。)</p>
<!--
Events such as the ones you saw at the end of `kubectl describe pod` are persisted in etcd and 
provide high-level information on what is happening in the cluster. 
To list all events you can use
-->
<p>你在 <code>kubectl describe pod</code> 结尾处看到的事件都保存在 etcd 中，
并提供关于集群中正在发生的事情的高级信息。
如果需要列出所有事件，可使用命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events
</code></pre></div><!--
but you have to remember that events are namespaced. 
This means that if you're interested in events for some namespaced object 
(e.g. what happened with Pods in namespace `my-namespace`) you need to explicitly provide a namespace to the command:
-->
<p>但是，需要注意的是，事件是区分名字空间的。
如果你对某些名字空间域的对象（比如 <code>my-namespace</code> 名字下的 Pod）的事件感兴趣,
你需要显式地在命令行中指定名字空间：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events --namespace<span style="color:#666">=</span>my-namespace
</code></pre></div><!--
To see events from all namespaces, you can use the `--all-namespaces` argument.
-->
<p>查看所有 namespace 的事件，可使用 <code>--all-namespaces</code> 参数。</p>
<!--
In addition to `kubectl describe pod`, another way to get extra information about a pod (beyond what is provided by `kubectl get pod`) is 
to pass the `-o yaml` output format flag to `kubectl get pod`. 
This will give you, in YAML format, even more information than `kubectl describe pod`--essentially all of the information the system has about the Pod. 
Here you will see things like annotations (which are key-value metadata without the label restrictions, that is used internally by Kubernetes system components), 
restart policy, ports, and volumes.
-->
<p>除了 <code>kubectl describe pod</code> 以外，另一种获取 Pod 额外信息（除了 <code>kubectl get pod</code>）的方法
是给 <code>kubectl get pod</code> 增加 <code>-o yaml</code> 输出格式参数。
该命令将以 YAML 格式为你提供比 <code>kubectl describe pod</code> 更多的信息 —— 实际上是系统拥有的关于 Pod 的所有信息。
在这里，你将看到注解（没有标签限制的键值元数据，由 Kubernetes 系统组件在内部使用）、
重启策略、端口和卷等。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod nginx-deployment-1006230814-6winp -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/created-by</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">      </span><span style="color:#bbb">      </span>{<span style="color:#b44">&#34;kind&#34;</span>:<span style="color:#b44">&#34;SerializedReference&#34;</span>,<span style="color:#b44">&#34;apiVersion&#34;</span>:<span style="color:#b44">&#34;v1&#34;</span>,<span style="color:#b44">&#34;reference&#34;</span>:{<span style="color:#b44">&#34;kind&#34;</span>:<span style="color:#b44">&#34;ReplicaSet&#34;</span>,<span style="color:#b44">&#34;namespace&#34;</span>:<span style="color:#b44">&#34;default&#34;</span>,<span style="color:#b44">&#34;name&#34;</span>:<span style="color:#b44">&#34;nginx-deployment-1006230814&#34;</span>,<span style="color:#b44">&#34;uid&#34;</span>:<span style="color:#b44">&#34;4c84c175-f161-11e5-9a78-42010af00005&#34;</span>,<span style="color:#b44">&#34;apiVersion&#34;</span>:<span style="color:#b44">&#34;extensions&#34;</span>,<span style="color:#b44">&#34;resourceVersion&#34;</span>:<span style="color:#b44">&#34;133434&#34;</span>}}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2016-03-24T01:39:50Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">generateName</span>:<span style="color:#bbb"> </span>nginx-deployment-1006230814-<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pod-template-hash</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1006230814&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx-deployment-1006230814-6winp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;133447&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>4c879808-f161-11e5-9a78-42010af00005<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imagePullPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>128Mi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>500m<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>128Mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">terminationMessagePath</span>:<span style="color:#bbb"> </span>/dev/termination-log<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/run/secrets/kubernetes.io/serviceaccount<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-token-4bcbi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dnsPolicy</span>:<span style="color:#bbb"> </span>ClusterFirst<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeName</span>:<span style="color:#bbb"> </span>kubernetes-node-wul5<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Always<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceAccount</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default-token-4bcbi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">secret</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">secretName</span>:<span style="color:#bbb"> </span>default-token-4bcbi<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conditions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">lastProbeTime</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">lastTransitionTime</span>:<span style="color:#bbb"> </span>2016-03-24T01:39:51Z<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;True&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Ready<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containerStatuses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">containerID</span>:<span style="color:#bbb"> </span>docker://90315cc9f513c724e9957a4788d3e625a078de84750f244a40f97ae355eb1149<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">imageID</span>:<span style="color:#bbb"> </span>docker://6f62f48c4e55d700cf3eb1b5e33fa051802986b77b874cc351cce539e5163707<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">lastState</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ready</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">restartCount</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">state</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">running</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">startedAt</span>:<span style="color:#bbb"> </span>2016-03-24T01:39:51Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.240.0.9</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">phase</span>:<span style="color:#bbb"> </span>Running<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.244.0.6</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">startTime</span>:<span style="color:#bbb"> </span>2016-03-24T01:39:49Z<span style="color:#bbb">
</span></code></pre></div><!--
## Example: debugging a down/unreachable node

Sometimes when debugging it can be useful to look at the status of a node 
-- for example, because you've noticed strange behavior of a Pod that's running on the node, 
or to find out why a Pod won't schedule onto the node. 
As with Pods, you can use `kubectl describe node` and `kubectl get node -o yaml` to 
retrieve detailed information about nodes. 
For example, here's what you'll see if a node is down 
(disconnected from the network, or kubelet dies and won't restart, etc.). 
Notice the events that show the node is NotReady, and 
also notice that the pods are no longer running 
(they are evicted after five minutes of NotReady status).
-->
<h2 id="示例-调试宕机或无法联系的节点">示例：调试宕机或无法联系的节点</h2>
<p>有时候，在调试时，查看节点的状态是很有用的 —— 例如，因为你已经注意到节点上运行的 Pod 的奇怪行为，
或者想了解为什么 Pod 不会调度到节点上。
与 Pod 一样，你可以使用 <code>kubectl describe node</code> 和 <code>kubectl get node -o yaml</code> 来查询节点的详细信息。
例如，如果某个节点宕机（与网络断开连接，或者 kubelet 挂掉无法重新启动等等），你将看到以下情况。
请注意显示节点未就绪的事件，也请注意 Pod 不再运行(它们在5分钟未就绪状态后被驱逐)。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><pre tabindex="0"><code>NAME                     STATUS       ROLES     AGE     VERSION
kubernetes-node-861h     NotReady     &lt;none&gt;    1h      v1.13.0
kubernetes-node-bols     Ready        &lt;none&gt;    1h      v1.13.0
kubernetes-node-st6x     Ready        &lt;none&gt;    1h      v1.13.0
kubernetes-node-unaj     Ready        &lt;none&gt;    1h      v1.13.0
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe node kubernetes-node-861h
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Name:			kubernetes-node-861h
Role
Labels:		 kubernetes.io/arch=amd64
           kubernetes.io/os=linux
           kubernetes.io/hostname=kubernetes-node-861h
Annotations:        node.alpha.kubernetes.io/ttl=0
                    volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             &lt;none&gt;
CreationTimestamp:	Mon, 04 Sep 2017 17:13:23 +0800
Phase:
Conditions:
  Type		Status		LastHeartbeatTime			LastTransitionTime			Reason					Message
  ----    ------    -----------------     ------------------      ------          -------
  OutOfDisk             Unknown         Fri, 08 Sep 2017 16:04:28 +0800         Fri, 08 Sep 2017 16:20:58 +0800         NodeStatusUnknown       Kubelet stopped posting node status.
  MemoryPressure        Unknown         Fri, 08 Sep 2017 16:04:28 +0800         Fri, 08 Sep 2017 16:20:58 +0800         NodeStatusUnknown       Kubelet stopped posting node status.
  DiskPressure          Unknown         Fri, 08 Sep 2017 16:04:28 +0800         Fri, 08 Sep 2017 16:20:58 +0800         NodeStatusUnknown       Kubelet stopped posting node status.
  Ready                 Unknown         Fri, 08 Sep 2017 16:04:28 +0800         Fri, 08 Sep 2017 16:20:58 +0800         NodeStatusUnknown       Kubelet stopped posting node status.
Addresses:	10.240.115.55,104.197.0.26
Capacity:
 cpu:           2
 hugePages:     0
 memory:        4046788Ki
 pods:          110
Allocatable:
 cpu:           1500m
 hugePages:     0
 memory:        1479263Ki
 pods:          110
System Info:
 Machine ID:                    8e025a21a4254e11b028584d9d8b12c4
 System UUID:                   349075D1-D169-4F25-9F2A-E886850C47E3
 Boot ID:                       5cd18b37-c5bd-4658-94e0-e436d3f110e0
 Kernel Version:                4.4.0-31-generic
 OS Image:                      Debian GNU/Linux 8 (jessie)
 Operating System:              linux
 Architecture:                  amd64
 Container Runtime Version:     docker://1.12.5
 Kubelet Version:               v1.6.9+a3d1dfa6f4335
 Kube-Proxy Version:            v1.6.9+a3d1dfa6f4335
ExternalID:                     15233045891481496305
Non-terminated Pods:            (9 in total)
  Namespace                     Name                                            CPU Requests    CPU Limits      Memory Requests Memory Limits
  ---------                     ----                                            ------------    ----------      --------------- -------------
......
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits      Memory Requests         Memory Limits
  ------------  ----------      ---------------         -------------
  900m (60%)    2200m (146%)    1009286400 (66%)        5681286400 (375%)
Events:         &lt;none&gt;
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get node kubernetes-node-861h -o yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Node<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span>2015-07-10T21:32:29Z<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/hostname</span>:<span style="color:#bbb"> </span>kubernetes-node-861h<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kubernetes-node-861h<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;757&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span>/api/v1/nodes/kubernetes-node-861h<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>2a69374e-274b-11e5-a234-42010af0d969<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">externalID</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;15233045891481496305&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">podCIDR</span>:<span style="color:#bbb"> </span><span style="color:#666">10.244.0.0</span>/24<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">providerID</span>:<span style="color:#bbb"> </span>gce://striped-torus-760/us-central1-b/kubernetes-node-861h<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">addresses</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">address</span>:<span style="color:#bbb"> </span><span style="color:#666">10.240.115.55</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>InternalIP<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">address</span>:<span style="color:#bbb"> </span><span style="color:#666">104.197.0.26</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>ExternalIP<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>3800808Ki<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pods</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conditions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">lastHeartbeatTime</span>:<span style="color:#bbb"> </span>2015-07-10T21:34:32Z<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">lastTransitionTime</span>:<span style="color:#bbb"> </span>2015-07-10T21:35:15Z<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">reason</span>:<span style="color:#bbb"> </span>Kubelet stopped posting node status.<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span>Unknown<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Ready<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeInfo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">bootID</span>:<span style="color:#bbb"> </span><span style="color:#666">4e316776</span>-b40d-4f78-a4ea-ab0d73390897<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">containerRuntimeVersion</span>:<span style="color:#bbb"> </span>docker://Unknown<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kernelVersion</span>:<span style="color:#bbb"> </span><span style="color:#666">3.16.0-0.</span>bpo.4-amd64<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubeProxyVersion</span>:<span style="color:#bbb"> </span>v0.21.1-185-gffc5a86098dc01<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubeletVersion</span>:<span style="color:#bbb"> </span>v0.21.1-185-gffc5a86098dc01<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">machineID</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">osImage</span>:<span style="color:#bbb"> </span>Debian GNU/Linux 7 (wheezy)<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">systemUUID</span>:<span style="color:#bbb"> </span>ABE5F6B4-D44B-108B-C46A-24CCE16C8B6E<span style="color:#bbb">
</span></code></pre></div><h2 id="接下来">接下来</h2>
<!--
Learn about additional debugging tools, including:
-->
<p>了解更多的调试工具：</p>
<!--
* [Logging](/docs/concepts/cluster-administration/logging/)
* [Monitoring](/docs/tasks/debug-application-cluster/resource-usage-monitoring/)
* [Getting into containers via `exec`](/docs/tasks/debug-application-cluster/get-shell-running-container/)
* [Connecting to containers via proxies](/docs/tasks/extend-kubernetes/http-proxy-access-api/)
* [Connecting to containers via port forwarding](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)
* [Inspect Kubernetes node with crictl](/docs/tasks/debug-application-cluster/crictl/)
-->
<ul>
<li><a href="/zh/docs/concepts/cluster-administration/logging/">日志</a></li>
<li><a href="/zh/docs/tasks/debug-application-cluster/resource-usage-monitoring/">监控</a></li>
<li><a href="/zh/docs/tasks/debug-application-cluster/get-shell-running-container/">使用 <code>exec</code> 进入容器</a></li>
<li><a href="/zh/docs/tasks/extend-kubernetes/http-proxy-access-api/">使用代理连接容器</a></li>
<li><a href="/zh/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">使用端口转发连接容器</a></li>
<li><a href="/zh/docs/tasks/debug-application-cluster/crictl/">使用 crictl 检查节点</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-434e0133d71583a27478b10fc1d3d105">10.6 - 故障诊断</h1>
    
	<!--
reviewers:
- brendandburns
- davidopp
content_type: concept
title: Troubleshooting
-->
<!-- overview -->
<!--
Sometimes things go wrong. This guide is aimed at making them right. It has
two sections:
-->
<p>有时候事情会出错。本指南旨在解决这些问题。它包含两个部分：</p>
<!--
   * [Troubleshooting your application](/docs/tasks/debug-application-cluster/debug-application/) - Useful for users who are deploying code into Kubernetes and wondering why it is not working.
   * [Troubleshooting your cluster](/docs/tasks/debug-application-cluster/debug-cluster/) - Useful for cluster administrators and people whose Kubernetes cluster is unhappy.
-->
<ul>
<li><a href="/zh/docs/tasks/debug-application-cluster/debug-application/">应用排错</a> -
针对部署代码到 Kubernetes 并想知道代码为什么不能正常运行的用户。</li>
<li><a href="/zh/docs/tasks/debug-application-cluster/debug-cluster/">集群排错</a> -
针对集群管理员以及 Kubernetes 集群表现异常的用户。</li>
</ul>
<!--
You should also check the known issues for the [release](https://github.com/kubernetes/kubernetes/releases)
you're using.
-->
<p>你也应该查看所用<a href="https://github.com/kubernetes/kubernetes/releases">发行版本</a>的已知问题。</p>
<!-- body -->
<!--
## Getting help

If your problem isn't answered by any of the guides above, there are variety of
ways for you to get help from the Kubernetes team.
-->
<h2 id="getting-help">获取帮助 </h2>
<p>如果你的问题在上述指南中没有得到答案，你还有另外几种方式从 Kubernetes 团队获得帮助。</p>
<!--
### Questions

The documentation on this site has been structured to provide answers to a wide
range of questions. [Concepts](/docs/concepts/) explain the Kubernetes
architecture and how each component works, while [Setup](/docs/setup/) provides
practical instructions for getting started. [Tasks](/docs/tasks/) show how to
accomplish commonly used tasks, and [Tutorials](/docs/tutorials/) are more
comprehensive walkthroughs of real-world, industry-specific, or end-to-end
development scenarios. The [Reference](/docs/reference/) section provides
detailed documentation on the [Kubernetes API](/docs/reference/generated/kubernetes-api/v1.22/)
and command-line interfaces (CLIs), such as [`kubectl`](/docs/user-guide/kubectl-overview/).
-->
<h3 id="questions">问题 </h3>
<p>本网站上的文档针对回答各类问题进行了结构化组织和分类。
<a href="/zh/docs/concepts/">概念</a>部分解释 Kubernetes 体系结构以及每个组件的工作方式，
<a href="/zh/docs/setup/">安装</a>部分提供了安装的实用说明。
<a href="/zh/docs/tasks/">任务</a>部分展示了如何完成常用任务，
<a href="/zh/docs/tutorials/">教程</a>部分则提供对现实世界、特定行业或端到端开发场景的更全面的演练。
<a href="/zh/docs/reference/">参考</a>部分提供了详细的
<a href="/docs/reference/generated/kubernetes-api/v1.22/">Kubernetes API</a> 文档
和命令行 (CLI) 接口的文档，例如<a href="/zh/docs/reference/kubectl/overview/"><code>kubectl</code></a>。</p>
<!--
## Help! My question isn't covered!  I need help now!
-->
<h2 id="求救-我的问题还没有解决-我现在需要帮助">求救！我的问题还没有解决！我现在需要帮助！</h2>
<!--
### Stack Overflow

Someone else from the community may have already asked a similar question or may
be able to help with your problem. The Kubernetes team will also monitor
[posts tagged Kubernetes](https://stackoverflow.com/questions/tagged/kubernetes).
If there aren't any existing questions that help, please
[ask a new one](https://stackoverflow.com/questions/ask?tags=kubernetes)!
-->
<h3 id="stack-overflow">Stack Overflow   </h3>
<p>社区中的其他人可能已经问过和你类似的问题，也可能能够帮助解决你的问题。
Kubernetes 团队还会监视<a href="https://stackoverflow.com/questions/tagged/kubernetes">带有 Kubernetes 标签的帖子</a>。
如果现有的问题对你没有帮助，请<a href="https://stackoverflow.com/questions/ask?tags=kubernetes">问一个新问题</a>!</p>
<!--
### Slack

Many people from the Kubernetes community hang out on Kubernetes Slack in the `#kubernetes-users` channel.
Slack requires registration; you can [request an invitation](https://slack.kubernetes.io),
and registration is open to everyone). Feel free to come and ask any and all questions.
Once registered, access the [Kubernetes organisation in Slack](https://kubernetes.slack.com)
via your web browser or via Slack's own dedicated app.
-->
<h3 id="slack">Slack</h3>
<p>Kubernetes 社区中有很多人在 <code>#kubernetes-users</code> 这一 Slack 频道聚集。
Slack 需要注册；你可以<a href="https://slack.kubernetes.io">请求一份邀请</a>，
并且注册是对所有人开放的。欢迎你随时来问任何问题。
一旦注册了，就可以访问通过 Web 浏览器或者 Slack 专用的应用访问
<a href="https://kubernetes.slack.com">Slack 上的 Kubernetes 组织</a>。</p>
<!--
Once you are registered, browse the growing list of channels for various subjects of
interest. For example, people new to Kubernetes may also want to join the
[`#kubernetes-novice`](https://kubernetes.slack.com/messages/kubernetes-novice) channel. As another example, developers should join the
[`#kubernetes-dev`](https://kubernetes.slack.com/messages/kubernetes-dev) channel.
-->
<p>一旦你完成了注册，就可以浏览各种感兴趣主题的频道列表（一直在增长）。
例如，Kubernetes 新人可能还想加入
<a href="https://kubernetes.slack.com/messages/kubernetes-novice"><code>#kubernetes-novice</code></a>
频道。又比如，开发人员应该加入
<a href="https://kubernetes.slack.com/messages/kubernetes-dev"><code>#kubernetes-dev</code></a>
频道。</p>
<!--
There are also many country specific/local language channels. Feel free to join
these channels for localized support and info:
-->
<p>还有许多国家/地区语言频道。请随时加入这些频道以获得本地化支持和信息：</p>





<!--
Country | Channels
:---------|:------------
China | [`#cn-users`](https://kubernetes.slack.com/messages/cn-users), [`#cn-events`](https://kubernetes.slack.com/messages/cn-events)
Finland | [`#fi-users`](https://kubernetes.slack.com/messages/fi-users)
France | [`#fr-users`](https://kubernetes.slack.com/messages/fr-users), [`#fr-events`](https://kubernetes.slack.com/messages/fr-events)
Germany | [`#de-users`](https://kubernetes.slack.com/messages/de-users), [`#de-events`](https://kubernetes.slack.com/messages/de-events)
India | [`#in-users`](https://kubernetes.slack.com/messages/in-users), [`#in-events`](https://kubernetes.slack.com/messages/in-events)
Italy | [`#it-users`](https://kubernetes.slack.com/messages/it-users), [`#it-events`](https://kubernetes.slack.com/messages/it-events)
Japan | [`#jp-users`](https://kubernetes.slack.com/messages/jp-users), [`#jp-events`](https://kubernetes.slack.com/messages/jp-events)
Korea | [`#kr-users`](https://kubernetes.slack.com/messages/kr-users)
Netherlands | [`#nl-users`](https://kubernetes.slack.com/messages/nl-users)
Norway | [`#norw-users`](https://kubernetes.slack.com/messages/norw-users)
Poland | [`#pl-users`](https://kubernetes.slack.com/messages/pl-users)
Russia | [`#ru-users`](https://kubernetes.slack.com/messages/ru-users)
Spain | [`#es-users`](https://kubernetes.slack.com/messages/es-users)
Sweden | [`#se-users`](https://kubernetes.slack.com/messages/se-users)
Turkey | [`#tr-users`](https://kubernetes.slack.com/messages/tr-users), [`#tr-events`](https://kubernetes.slack.com/messages/tr-events)
-->
<table><caption style="display: none;">Country / language specific Slack channels</caption>
<thead>
<tr>
<th style="text-align:left">国家</th>
<th style="text-align:left">频道</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">中国</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/cn-users"><code>#cn-users</code></a>, <a href="https://kubernetes.slack.com/messages/cn-events"><code>#cn-events</code></a></td>
</tr>
<tr>
<td style="text-align:left">芬兰</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/fi-users"><code>#fi-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">法国</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/fr-users"><code>#fr-users</code></a>, <a href="https://kubernetes.slack.com/messages/fr-events"><code>#fr-events</code></a></td>
</tr>
<tr>
<td style="text-align:left">德国</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/de-users"><code>#de-users</code></a>, <a href="https://kubernetes.slack.com/messages/de-events"><code>#de-events</code></a></td>
</tr>
<tr>
<td style="text-align:left">印度</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/in-users"><code>#in-users</code></a>, <a href="https://kubernetes.slack.com/messages/in-events"><code>#in-events</code></a></td>
</tr>
<tr>
<td style="text-align:left">意大利</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/it-users"><code>#it-users</code></a>, <a href="https://kubernetes.slack.com/messages/it-events"><code>#it-events</code></a></td>
</tr>
<tr>
<td style="text-align:left">日本</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/jp-users"><code>#jp-users</code></a>, <a href="https://kubernetes.slack.com/messages/jp-events"><code>#jp-events</code></a></td>
</tr>
<tr>
<td style="text-align:left">韩国</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/kr-users"><code>#kr-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">荷兰</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/nl-users"><code>#nl-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">挪威</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/norw-users"><code>#norw-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">波兰</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/pl-users"><code>#pl-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">俄罗斯</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/ru-users"><code>#ru-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">西班牙</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/es-users"><code>#es-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">瑞典</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/se-users"><code>#se-users</code></a></td>
</tr>
<tr>
<td style="text-align:left">土耳其</td>
<td style="text-align:left"><a href="https://kubernetes.slack.com/messages/tr-users"><code>#tr-users</code></a>, <a href="https://kubernetes.slack.com/messages/tr-events"><code>#tr-events</code></a></td>
</tr>
</tbody>
</table>

<!--
### Forum

You're welcome to join the official Kubernetes Forum: [discuss.kubernetes.io](https://discuss.kubernetes.io).
-->
<h3 id="forum">论坛 </h3>
<p>欢迎你加入 Kubernetes 官方论坛
<a href="https://discuss.kubernetes.io">discuss.kubernetes.io</a>。</p>
<!--
### Bugs and Feature requests

If you have what looks like a bug, or you would like to make a feature request,
please use the [Github issue tracking system](https://github.com/kubernetes/kubernetes/issues).
-->
<h3 id="bugs-and-feature-requests">Bugs 和功能请求  </h3>
<p>如果你发现一个看起来像 Bug 的问题，或者你想提出一个功能请求，请使用
<a href="https://github.com/kubernetes/kubernetes/issues">Github 问题跟踪系统</a>。</p>
<!--
Before you file an issue, please search existing issues to see if your issue is
already covered.

If filing a bug, please include detailed information about how to reproduce the
problem, such as:
-->
<p>在提交问题之前，请搜索现有问题列表以查看是否其中已涵盖你的问题。</p>
<p>如果提交 Bug，请提供如何重现问题的详细信息，例如：</p>
<!--
* Kubernetes version: `kubectl version`
* Cloud provider, OS distro, network configuration, and Docker version
* Steps to reproduce the problem
-->
<ul>
<li>Kubernetes 版本：<code>kubectl version</code></li>
<li>云平台、OS 发行版、网络配置和 Docker 版本</li>
<li>重现问题的步骤</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ef360b1f8e65236251826db478cfcab3">10.7 - 确定 Pod 失败的原因</h1>
    
	<!--
title: Determine the Reason for Pod Failure
content_type: task
-->
<!-- overview -->
<!--
This page shows how to write and read a Container
termination message.
-->
<p>本文介绍如何编写和读取容器的终止消息。</p>
<!--
Termination messages provide a way for containers to write information about
fatal events to a location where it can be easily retrieved and surfaced by
tools like dashboards and monitoring software. In most cases, information that
you put in a termination message should also be written to the general
[Kubernetes logs](/docs/concepts/cluster-administration/logging/).
-->
<p>终止消息为容器提供了一种方法，可以将有关致命事件的信息写入某个位置，
在该位置可以通过仪表板和监控软件等工具轻松检索和显示致命事件。
在大多数情况下，您放入终止消息中的信息也应该写入
<a href="/zh/docs/concepts/cluster-administration/logging/">常规 Kubernetes 日志</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Writing and reading a termination message

In this exercise, you create a Pod that runs one container.
The configuration file specifies a command that runs when
the container starts.
-->
<h2 id="读写终止消息">读写终止消息</h2>
<p>在本练习中，您将创建运行一个容器的 Pod。
配置文件指定在容器启动时要运行的命令。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/debug/termination.yaml" download="debug/termination.yaml"><code>debug/termination.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('debug-termination-yaml')" title="Copy debug/termination.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="debug-termination-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>termination-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>termination-demo-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/bin/sh&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;-c&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span>]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<ol>
<li>
<!--Create a Pod based on the YAML configuration file:-->基于 YAML 配置文件创建 Pod：
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/debug/termination.yaml
</code></pre></div><!--In the YAML file, in the `command` and `args` fields, you can see that the
container sleeps for 10 seconds and then writes "Sleep expired" to
the `/dev/termination-log` file. After the container writes
the "Sleep expired" message, it terminates.-->
<p>YAML 文件中，在 <code>command</code> 和 <code>args</code> 字段，你可以看到容器休眠 10 秒然后将 &quot;Sleep expired&quot;
写入 <code>/dev/termination-log</code> 文件。
容器写完 &quot;Sleep expired&quot; 消息后就终止了。</p>
</li>
<li>
<!--Display information about the Pod:-->显示 Pod 的信息：
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod termination-demo
</code></pre></div><!--Repeat the preceding command until the Pod is no longer running.-->
<p>重复前面的命令直到 Pod 不再运行。</p>
</li>
<li>
<!--Display detailed information about the Pod:-->
<p>显示 Pod 的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod --output<span style="color:#666">=</span>yaml
</code></pre></div><!--The output includes the "Sleep expired" message:-->输出结果包含 "Sleep expired" 消息：
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">lastState</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminated</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">containerID</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">exitCode</span>:<span style="color:#bbb"> </span><span style="color:#666">0</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">finishedAt</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">          </span><span style="color:#bbb">          </span>Sleep expired<span style="color:#bbb">
</span><span style="color:#bbb">        </span>...<span style="color:#bbb">
</span></code></pre></div></li>
<li>
<!--Use a Go template to filter the output so that it includes only the termination message:-->
<p>使用 Go 模板过滤输出结果，使其只含有终止消息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod termination-demo -o go-template<span style="color:#666">=</span><span style="color:#b44">&#34;{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}&#34;</span>
</code></pre></div></li>
</ol>
<!--
## Customizing the termination message

Kubernetes retrieves termination messages from the termination message file
specified in the `terminationMessagePath` field of a Container, which as a default
value of `/dev/termination-log`. By customizing this field, you can tell Kubernetes
to use a different file. Kubernetes use the contents from the specified file to
populate the Container's status message on both success and failure.
-->
<h2 id="定制终止消息">定制终止消息</h2>
<p>Kubernetes 从容器的 <code>terminationMessagePath</code> 字段中指定的终止消息文件中检索终止消息，
默认值为 <code>/dev/termination-log</code>。
通过定制这个字段，您可以告诉 Kubernetes 使用不同的文件。
Kubernetes 使用指定文件中的内容在成功和失败时填充容器的状态消息。</p>
<!--
In the following example, the container writes termination messages to
`/tmp/my-log` for Kubernetes to retrieve:
-->
<p>在下例中，容器将终止消息写入 <code>/tmp/my-log</code> 给 Kubernetes 来接收：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>msg-path-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>msg-path-demo-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>debian<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">terminationMessagePath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/tmp/my-log&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
Moreover, users can set the `terminationMessagePolicy` field of a Container for
further customization. This field defaults to "`File`" which means the termination
messages are retrieved only from the termination message file. By setting the
`terminationMessagePolicy` to "`FallbackToLogsOnError`", you can tell Kubernetes
to use the last chunk of container log output if the termination message file
is empty and the container exited with an error. The log output is limited to
2048 bytes or 80 lines, whichever is smaller.
-->
<p>此外，用户可以设置容器的 <code>terminationMessagePolicy</code> 字段，以便进一步自定义。
此字段默认为 &quot;<code>File</code>&quot;，这意味着仅从终止消息文件中检索终止消息。
通过将 <code>terminationMessagePolicy</code> 设置为 &quot;<code>FallbackToLogsOnError</code>&quot;，你就可以告诉 Kubernetes，在容器因错误退出时，如果终止消息文件为空，则使用容器日志输出的最后一块作为终止消息。
日志输出限制为 2048 字节或 80 行，以较小者为准。</p>
<h2 id="接下来">接下来</h2>
<!--
* See the `terminationMessagePath` field in
  [Container](/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core).
* Learn about [retrieving logs](/docs/concepts/cluster-administration/logging/).
* Learn about [Go templates](https://golang.org/pkg/text/template/).
-->
<ul>
<li>参考 <a href="/docs/reference/generated/kubernetes-api/v1.22/#container-v1-core">Container</a>
资源的 <code>terminationMessagePath</code> 字段。</li>
<li>了解<a href="/zh/docs/concepts/cluster-administration/logging/">接收日志</a>。</li>
<li>了解 <a href="https://golang.org/pkg/text/template/">Go 模版</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bc729eafe3688124d3a6f1110bd5a89c">10.8 - 节点健康监测</h1>
    
	<!-- 
title: Monitor Node Health
content_type: task
reviewers:
- Random-Liu
- dchen1107
-->
<!-- overview -->
<!-- 
*Node Problem Detector* is a daemon for monitoring and reporting about a node's health.
You can run Node Problem Detector as a `DaemonSet` or as a standalone daemon.
Node Problem Detector collects information about node problems from various daemons
and reports these conditions to the API server as [NodeCondition](/docs/concepts/architecture/nodes/#condition)
and [Event](/docs/reference/generated/kubernetes-api/v1.22/#event-v1-core). 

To learn how to install and use Node Problem Detector, see
[Node Problem Detector project documentation](https://github.com/kubernetes/node-problem-detector).
-->
<p><em>节点问题检测器（Node Problem Detector）</em> 是一个守护程序，用于监视和报告节点的健康状况。
你可以将节点问题探测器以 <code>DaemonSet</code> 或独立守护程序运行。
节点问题检测器从各种守护进程收集节点问题，并以
<a href="/zh/docs/concepts/architecture/nodes/#condition">NodeCondition</a> 和
<a href="/docs/reference/generated/kubernetes-api/v1.22/#event-v1-core">Event</a>
的形式报告给 API 服务器。</p>
<p>要了解如何安装和使用节点问题检测器，请参阅
<a href="https://github.com/kubernetes/node-problem-detector">节点问题探测器项目文档</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!-- 
## Limitations 

* Node Problem Detector only supports file based kernel log.
  Log tools such as `journald` are not supported.

* Node Problem Detector uses the kernel log format for reporting kernel issues.
  To learn how to extend the kernel log format, see [Add support for another log format](#support-other-log-format).
-->
<h2 id="limitations">局限性 </h2>
<ul>
<li>节点问题检测器只支持基于文件类型的内核日志。
它不支持像 journald 这样的命令行日志工具。</li>
<li>节点问题检测器使用内核日志格式来报告内核问题。
要了解如何扩展内核日志格式，请参阅<a href="#support-other-log-format">添加对另一个日志格式的支持</a>。</li>
</ul>
<!-- 
## Enabling Node Problem Detector

Some cloud providers enable Node Problem Detector as an <a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='Addon'>Addon</a>.
You can also enable Node Problem Detector with `kubectl` or by creating an Addon pod.
-->
<h2 id="启用节点问题检测器">启用节点问题检测器</h2>
<p>一些云供应商将节点问题检测器以<a class='glossary-tooltip' title='扩展 Kubernetes 功能的资源。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/cluster-administration/addons/' target='_blank' aria-label='插件'>插件</a>形式启用。
你还可以使用 <code>kubectl</code> 或创建插件 Pod 来启用节点问题探测器。</p>
<!-- 
## Using kubectl to enable Node Problem Detector {#using-kubectl}

`kubectl` provides the most flexible management of Node Problem Detector.
You can overwrite the default configuration to fit it into your environment or
to detect customized node problems. For example:
-->
<h2 id="using-kubectl">使用 kubectl 启用节点问题检测器</h2>
<p><code>kubectl</code> 提供了节点问题探测器最灵活的管理。
你可以覆盖默认配置使其适合你的环境或检测自定义节点问题。例如：</p>
<!-- 
1. Create a Node Problem Detector configuration similar to `node-problem-detector.yaml`:

   

 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/debug/node-problem-detector.yaml" download="debug/node-problem-detector.yaml"><code>debug/node-problem-detector.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('debug-node-problem-detector-yaml')" title="Copy debug/node-problem-detector.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="debug-node-problem-detector-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector-v0.1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector  <span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/node-problem-detector:v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">privileged</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log/</code></pre></div>
    </div>
</div>



   <blockquote class="note callout">
  <div><strong>说明：</strong> You should verify that the system log directory is right for your operating system distribution.</div>
</blockquote>

1. Start node problem detector with `kubectl`:

   ```shell
   kubectl apply -f https://k8s.io/examples/debug/node-problem-detector.yaml
   ```
-->
<ol>
<li>
<p>创建类似于 <code>node-strought-detector.yaml</code> 的节点问题检测器配置：


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/debug/node-problem-detector.yaml" download="debug/node-problem-detector.yaml"><code>debug/node-problem-detector.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('debug-node-problem-detector-yaml')" title="Copy debug/node-problem-detector.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="debug-node-problem-detector-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector-v0.1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector  <span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/node-problem-detector:v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">privileged</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log/</code></pre></div>
    </div>
</div>

</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> 你应该检查系统日志目录是否适用于操作系统发行版本。</div>
</blockquote>
</li>
<li>
<p>使用 <code>kubectl</code> 启动节点问题检测器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/debug/node-problem-detector.yaml
</code></pre></div></li>
</ol>
<!-- 
### Using an Addon pod to enable Node Problem Detector {#using-addon-pod}

If you are using a custom cluster bootstrap solution and don't need
to overwrite the default configuration, you can leverage the Addon pod to
further automate the deployment.

Create `node-problem-detector.yaml`, and save the configuration in the Addon pod's
directory `/etc/kubernetes/addons/node-problem-detector` on a control plane node.
-->
<h3 id="using-addon-pod">使用插件 pod 启用节点问题检测器</h3>
<p>如果你使用的是自定义集群引导解决方案，不需要覆盖默认配置，
可以利用插件 Pod 进一步自动化部署。</p>
<p>创建 <code>node-strick-detector.yaml</code>，并在控制平面节点上保存配置到插件 Pod 的目录
<code>/etc/kubernetes/addons/node-problem-detector</code>。</p>
<!-- 
## Overwrite the Configuration 

The [default configuration](https://github.com/kubernetes/node-problem-detector/tree/v0.1/config)
is embedded when building the Docker image of Node Problem Detector.
-->
<h2 id="覆盖配置文件">覆盖配置文件</h2>
<p>构建节点问题检测器的 docker 镜像时，会嵌入
<a href="https://github.com/kubernetes/node-problem-detector/tree/v0.1/config">默认配置</a>。</p>
<!-- 
However, you can use a [`ConfigMap`](/docs/tasks/configure-pod-container/configure-pod-configmap/)
to overwrite the configuration:
-->
<p>不过，你可以像下面这样使用 <a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap/"><code>ConfigMap</code></a>
将其覆盖：</p>
<!-- 
1. Change the configuration files in `config/`
1. Create the `ConfigMap` `node-problem-detector-config`:

   ```shell
   kubectl create configmap node-problem-detector-config --from-file=config/
   ```

1. Change the `node-problem-detector.yaml` to use the `ConfigMap`:

   

 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/debug/node-problem-detector-configmap.yaml" download="debug/node-problem-detector-configmap.yaml"><code>debug/node-problem-detector-configmap.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('debug-node-problem-detector-configmap-yaml')" title="Copy debug/node-problem-detector-configmap.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="debug-node-problem-detector-configmap-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector-v0.1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector  <span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/node-problem-detector:v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">privileged</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Overwrite the config/ directory with ConfigMap volume</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/config<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log/<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Define ConfigMap volume</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector-config</code></pre></div>
    </div>
</div>



1. Recreate the Node Problem Detector with the new configuration file:

   ```shell
   # If you have a node-problem-detector running, delete before recreating
   kubectl delete -f https://k8s.io/examples/debug/node-problem-detector.yaml
   kubectl apply -f https://k8s.io/examples/debug/node-problem-detector-configmap.yaml
   ```
 -->
<ol>
<li>
<p>更改 <code>config/</code> 中的配置文件</p>
</li>
<li>
<p>创建 <code>ConfigMap</code> <code>node-strick-detector-config</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create configmap node-problem-detector-config --from-file<span style="color:#666">=</span>config/
</code></pre></div></li>
<li>
<p>更改 <code>node-problem-detector.yaml</code> 以使用 ConfigMap:</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/debug/node-problem-detector-configmap.yaml" download="debug/node-problem-detector-configmap.yaml"><code>debug/node-problem-detector-configmap.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('debug-node-problem-detector-configmap-yaml')" title="Copy debug/node-problem-detector-configmap.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="debug-node-problem-detector-configmap-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector-v0.1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector  <span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/node-problem-detector:v0.1<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">privileged</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;200m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;100Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20m&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20Mi&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/log<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Overwrite the config/ directory with ConfigMap volume</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/config<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log/<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>config<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># Define ConfigMap volume</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">configMap</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>node-problem-detector-config</code></pre></div>
    </div>
</div>


</li>
<li>
<p>使用新的配置文件重新创建节点问题检测器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 如果你正在运行节点问题检测器，请先删除，然后再重新创建</span>
kubectl delete -f https://k8s.io/examples/debug/node-problem-detector.yaml
kubectl apply -f https://k8s.io/examples/debug/node-problem-detector-configmap.yaml
</code></pre></div></li>
</ol>
<!--  
<blockquote class="note callout">
  <div><strong>说明：</strong> This approach only applies to a Node Problem Detector started with <code>kubectl</code>.</div>
</blockquote>

Overwriting a configuration is not supported if a Node Problem Detector runs as a cluster Addon.
The Addon manager does not support `ConfigMap`.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 此方法仅适用于通过 <code>kubectl</code> 启动的节点问题检测器。</div>
</blockquote>
<p>如果节点问题检测器作为集群插件运行，则不支持覆盖配置。
插件管理器不支持 <code>ConfigMap</code>。</p>
<!-- 
## Kernel Monitor 

*Kernel Monitor* is a system log monitor daemon supported in the Node Problem Detector.
Kernel monitor watches the kernel log and detects known kernel issues following predefined rules.
-->
<h2 id="内核监视器">内核监视器</h2>
<p><em>内核监视器（Kernel Monitor）</em> 是节点问题检测器中支持的系统日志监视器守护进程。
内核监视器观察内核日志并根据预定义规则检测已知的内核问题。</p>
<!-- 
The Kernel Monitor matches kernel issues according to a set of predefined rule list in
[`config/kernel-monitor.json`](https://github.com/kubernetes/node-problem-detector/blob/v0.1/config/kernel-monitor.json). The rule list is extensible. You can expand the rule list by overwriting the
configuration.
-->
<p>内核监视器根据 <a href="https://github.com/kubernetes/node-problem-detector/blob/v0.1/config/kernel-monitor.json"><code>config/kernel-monitor.json</code></a>
中的一组预定义规则列表匹配内核问题。
规则列表是可扩展的，你始终可以通过覆盖配置来扩展它。</p>
<!-- 
### Add new NodeConditions 

To support a new `NodeCondition`, create a condition definition within the `conditions` field in
`config/kernel-monitor.json`, for example:
```
-->
<h3 id="添加新的-nodecondition">添加新的 NodeCondition</h3>
<p>要支持新的 <code>NodeCondition</code>，请在 <code>config/kernel-monitor.json</code> 中的
<code>conditions</code> 字段中创建一个条件定义：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;NodeConditionType&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;reason&#34;</span>: <span style="color:#b44">&#34;CamelCaseDefaultNodeConditionReason&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;message&#34;</span>: <span style="color:#b44">&#34;arbitrary default node condition message&#34;</span>
}
</code></pre></div><!-- 
### Detect new problems 

To detect new problems, you can extend the `rules` field in `config/kernel-monitor.json`
with a new rule definition:
-->
<h3 id="检测新的问题">检测新的问题</h3>
<p>你可以使用新的规则描述来扩展 <code>config/kernel-monitor.json</code> 中的 <code>rules</code> 字段以检测新问题：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;temporary/permanent&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;condition&#34;</span>: <span style="color:#b44">&#34;NodeConditionOfPermanentIssue&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;reason&#34;</span>: <span style="color:#b44">&#34;CamelCaseShortReason&#34;</span>,
  <span style="color:#008000;font-weight:bold">&#34;message&#34;</span>: <span style="color:#b44">&#34;regexp matching the issue in the kernel log&#34;</span>
}
</code></pre></div><!-- 
### Configure path for the kernel log device {#kernel-log-device-path}

Check your kernel log path location in your operating system (OS) distribution.
The Linux kernel [log device](https://www.kernel.org/doc/Documentation/ABI/testing/dev-kmsg) is usually presented as `/dev/kmsg`. However, the log path location varies by OS distribution.
The `log` field in `config/kernel-monitor.json` represents the log path inside the container.
You can configure the `log` field to match the device path as seen by the Node Problem Detector.
-->
<h3 id="kernel-log-device-path">配置内核日志设备的路径</h3>
<p>检查你的操作系统（OS）发行版本中的内核日志路径位置。
Linux 内核<a href="https://www.kernel.org/doc/documentation/abi/testing/dev-kmsg">日志设备</a>
通常呈现为 <code>/dev/kmsg</code>。
但是，日志路径位置因 OS 发行版本而异。
<code>config/kernel-monitor.json</code> 中的 <code>log</code> 字段表示容器内的日志路径。
你可以配置 <code>log</code> 字段以匹配节点问题检测器所示的设备路径。</p>
<!-- 
### Add support for another log format {#support-other-log-format}

Kernel monitor uses the
[`Translator`](https://github.com/kubernetes/node-problem-detector/blob/v0.1/pkg/kernelmonitor/translator/translator.go) plugin to translate the internal data structure of the kernel log.
You can implement a new translator for a new log format.
-->
<h3 id="support-other-log-format">添加对其它日志格式的支持 </h3>
<p>内核监视器使用
<a href="https://github.com/kubernetes/node-problem-detector/blob/v0.1/pkg/kernelmonitor/translator.go"><code>Translator</code></a>
插件转换内核日志的内部数据结构。
你可以为新的日志格式实现新的转换器。</p>
<!-- discussion -->
<!-- 
## Recommendations and restrictions

It is recommended to run the Node Problem Detector in your cluster to monitor node health.
When running the Node Problem Detector, you can expect extra resource overhead on each node.
Usually this is fine, because:

* The kernel log grows relatively slowly.
* A resource limit is set for the Node Problem Detector.
* Even under high load, the resource usage is acceptable. For more information, see the Node Problem Detector
[benchmark result](https://github.com/kubernetes/node-problem-detector/issues/2#issuecomment-220255629).
-->
<h2 id="建议和限制">建议和限制</h2>
<p>建议在集群中运行节点问题检测器以监控节点运行状况。
运行节点问题检测器时，你可以预期每个节点上的额外资源开销。
通常这是可接受的，因为：</p>
<ul>
<li>内核日志增长相对缓慢。</li>
<li>已经为节点问题检测器设置了资源限制。</li>
<li>即使在高负载下，资源使用也是可接受的。有关更多信息，请参阅节点问题检测器
<a href="https://github.com/kubernetes/node-problem-detector/issues/2.suecomment-220255629">基准结果</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9713ac27b6d9e3034033200d968221f2">10.9 - 获取正在运行容器的 Shell</h1>
    
	<!--
---
reviewers:
- caesarxuchao
- mikedanese
title: Get a Shell to a Running Container
content_type: task
---
-->
<!-- overview -->
<!--
This page shows how to use `kubectl exec` to get a shell to a
running Container.
-->
<p>本文介绍怎样使用 <code>kubectl exec</code> 命令获取正在运行容器的 Shell。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Getting a shell to a Container
-->
<h2 id="获取容器的-shell">获取容器的 Shell</h2>
<!--
In this exercise, you create a Pod that has one Container. The Container
runs the nginx image. Here is the configuration file for the Pod:
-->
<p>在本练习中，你将创建包含一个容器的 Pod。容器运行 nginx 镜像。下面是 Pod 的配置文件：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/application/shell-demo.yaml" download="application/shell-demo.yaml"><code>application/shell-demo.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('application-shell-demo-yaml')" title="Copy application/shell-demo.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="application-shell-demo-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>shell-demo<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>shared-data<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/usr/share/nginx/html<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">dnsPolicy</span>:<span style="color:#bbb"> </span>Default<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Create the Pod:
-->
<p>创建 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/application/shell-demo.yaml
</code></pre></div><!--
Verify that the Container is running:
-->
<p>检查容器是否运行正常：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod shell-demo
</code></pre></div><!--
Get a shell to the running Container:
-->
<p>获取正在运行容器的 Shell：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it shell-demo -- /bin/bash
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <!--
The double dash symbol "--" is used to separate the arguments you want to pass to the command from the kubectl arguments.
-->
<p>双破折号 &quot;--&quot; 用于将要传递给命令的参数与 kubectl 的参数分开。</div>
</blockquote>
<!--
In your shell, list the root directory:
-->
<p>在 shell 中，打印根目录：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@shell-demo:/# ls /
</code></pre></div><!--
In your shell, experiment with other commands. Here are
some examples:
-->
<p>在 shell 中，实验其他命令。下面是一些示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@shell-demo:/# ls /
root@shell-demo:/# cat /proc/mounts
root@shell-demo:/# cat /proc/1/maps
root@shell-demo:/# apt-get update
root@shell-demo:/# apt-get install -y tcpdump
root@shell-demo:/# tcpdump
root@shell-demo:/# apt-get install -y lsof
root@shell-demo:/# lsof
root@shell-demo:/# apt-get install -y procps
root@shell-demo:/# ps aux
root@shell-demo:/# ps aux | grep nginx
</code></pre></div><!--
## Writing the root page for nginx
-->
<h2 id="编写-nginx-的-根页面">编写 nginx 的 根页面</h2>
<!--
Look again at the configuration file for your Pod. The Pod
has an `emptyDir` volume, and the Container mounts the volume
at `/usr/share/nginx/html`.
-->
<p>在看一下 Pod 的配置文件。该 Pod 有个 <code>emptyDir</code> 卷，容器将该卷挂载到了 <code>/usr/share/nginx/html</code>。</p>
<!--
In your shell, create an `index.html` file in the `/usr/share/nginx/html`
directory:
-->
<p>在 shell 中，在 <code>/usr/share/nginx/html</code> 目录创建一个 <code>index.html</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@shell-demo:/# <span style="color:#a2f">echo</span> Hello shell demo &gt; /usr/share/nginx/html/index.html
</code></pre></div><!--
In your shell, send a GET request to the nginx server:
-->
<p>在 shell 中，向 nginx 服务器发送 GET 请求：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">root@shell-demo:/# apt-get update
root@shell-demo:/# apt-get install curl
root@shell-demo:/# curl localhost
</code></pre></div><!--
The output shows the text that you wrote to the `index.html` file:
-->
<p>输出结果显示了你在 <code>index.html</code> 中写入的文本。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Hello shell demo
</code></pre></div><!--
When you are finished with your shell, enter `exit`.
-->
<p>当用完 shell 后，输入 <code>exit</code> 退出。</p>
<!--
## Running individual commands in a Container
-->
<h2 id="在容器中运行单个命令">在容器中运行单个命令</h2>
<!--
In an ordinary command window, not your shell, list the environment
variables in the running Container:
-->
<p>在普通的命令窗口（而不是 shell）中，打印环境运行容器中的变量：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> shell-demo env
</code></pre></div><!--
Experiment running other commands. Here are some examples:
-->
<p>实验运行其他命令。下面是一些示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> shell-demo ps aux
kubectl <span style="color:#a2f">exec</span> shell-demo ls /
kubectl <span style="color:#a2f">exec</span> shell-demo cat /proc/1/mounts
</code></pre></div><!-- discussion -->
<!--
## Opening a shell when a Pod has more than one Container
-->
<h2 id="当-pod-包含多个容器时打开-shell">当 Pod 包含多个容器时打开 shell</h2>
<!--
If a Pod has more than one Container, use `--container` or `-c` to
specify a Container in the `kubectl exec` command. For example,
suppose you have a Pod named my-pod, and the Pod has two containers
named main-app and helper-app. The following command would open a
shell to the main-app Container.
-->
<p>如果 Pod 有多个容器，<code>--container</code> 或者 <code>-c</code> 可以在 <code>kubectl exec</code> 命令中指定容器。
例如，您有个名为 my-pod 的容器，该 Pod 有两个容器分别为 main-app 和 healper-app。
下面的命令将会打开一个 shell 访问 main-app 容器。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it my-pod --container main-app -- /bin/bash
</code></pre></div><h2 id="接下来">接下来</h2>
<ul>
<li><a href="/docs/reference/generated/kubectl/kubectl-commands/#exec">kubectl exec</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-06bb252f25983de12f635c806d180d30">10.10 - 调试 Init 容器</h1>
    
	<!--
reviewers:
- bprashanth
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
- smarterclayton
title: Debug Init Containers
content_type: task
-->
<!-- overview -->
<!--
This page shows how to investigate problems related to the execution of
Init Containers. The example command lines below refer to the Pod as
`<pod-name>` and the Init Containers as `<init-container-1>` and
`<init-container-2>`.
-->
<p>此页显示如何核查与 Init 容器执行相关的问题。
下面的示例命令行将 Pod 称为 <code>&lt;pod-name&gt;</code>，而 Init 容器称为 <code>&lt;init-container-1&gt;</code> 和
<code>&lt;init-container-2&gt;</code>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
* You should be familiar with the basics of
  [Init Containers](/docs/concepts/workloads/pods/init-containers/).
* You should have [Configured an Init Container](/docs/tasks/configure-pod-container/configure-pod-initialization/#creating-a-pod-that-has-an-init-container/).
-->
<ul>
<li>你应该熟悉 <a href="/zh/docs/concepts/workloads/pods/init-containers/">Init 容器</a>的基础知识。</li>
<li>你应该已经<a href="/zh/docs/tasks/configure-pod-container/configure-pod-initialization/#creating-a-pod-that-has-an-init-container/">配置好一个 Init 容器</a>。</li>
</ul>
<!-- steps -->
<!--
## Checking the status of Init Containers

Display the status of your pod:
-->
<h2 id="检查-init-容器的状态">检查 Init 容器的状态</h2>
<p>显示你的 Pod 的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod &lt;pod-name&gt;
</code></pre></div><!--
For example, a status of `Init:1/2` indicates that one of two Init Containers
has completed successfully:
-->
<p>例如，状态 <code>Init:1/2</code> 表明两个 Init 容器中的一个已经成功完成：</p>
<pre tabindex="0"><code>NAME         READY     STATUS     RESTARTS   AGE
&lt;pod-name&gt;   0/1       Init:1/2   0          7s
</code></pre><!--
See [Understanding Pod status](#understanding-pod-status) for more examples of
status values and their meanings.
-->
<p>更多状态值及其含义请参考<a href="#understanding-pod-status">理解 Pod 的状态</a>。</p>
<!--
## Getting details about Init Containers

View more detailed information about Init Container execution:
-->
<h2 id="getting-details-about-init-containers">获取 Init 容器详情  </h2>
<p>查看 Init 容器运行的更多详情：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod &lt;pod-name&gt;
</code></pre></div><!--
For example, a Pod with two Init Containers might show the following:
-->
<p>例如，对于包含两个 Init 容器的 Pod 可能显示如下信息：</p>
<pre tabindex="0"><code>Init Containers:
  &lt;init-container-1&gt;:
    Container ID:    ...
    ...
    State:           Terminated
      Reason:        Completed
      Exit Code:     0
      Started:       ...
      Finished:      ...
    Ready:           True
    Restart Count:   0
    ...
  &lt;init-container-2&gt;:
    Container ID:    ...
    ...
    State:           Waiting
      Reason:        CrashLoopBackOff
    Last State:      Terminated
      Reason:        Error
      Exit Code:     1
      Started:       ...
      Finished:      ...
    Ready:           False
    Restart Count:   3
    ...
</code></pre><!--
You can also access the Init Container statuses programmatically by reading the
`status.initContainerStatuses` field on the Pod Spec:
-->
<p>你还可以通过编程方式读取 Pod Spec 上的 <code>status.initContainerStatuses</code> 字段，了解 Init 容器的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod nginx --template <span style="color:#b44">&#39;{{.status.initContainerStatuses}}&#39;</span>
</code></pre></div><!--
This command will return the same information as above in raw JSON.
-->
<p>此命令将返回与原始 JSON 中相同的信息.</p>
<!--
## Accessing logs from Init Containers

Pass the Init Container name along with the Pod name
to access its logs.
-->
<h2 id="accessing-logs-from-init-containers">通过 Init 容器访问日志  </h2>
<p>与 Pod 名称一起传递 Init 容器名称，以访问容器的日志。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs &lt;pod-name&gt; -c &lt;init-container-2&gt;
</code></pre></div><!--
Init Containers that run a shell script print
commands as they're executed. For example, you can do this in Bash by running
`set -x` at the beginning of the script.
-->
<p>运行 Shell 脚本的 Init 容器在执行 Shell 脚本时输出命令本身。
例如，你可以在 Bash 中通过在脚本的开头运行 <code>set -x</code> 来实现。</p>
<!-- discussion -->
<!--
## Understanding Pod status

A Pod status beginning with `Init:` summarizes the status of Init Container
execution. The table below describes some example status values that you might
see while debugging Init Containers.
-->
<h2 id="understanding-pod-status">理解 Pod 的状态  </h2>
<p>以 <code>Init:</code> 开头的 Pod 状态汇总了 Init 容器执行的状态。
下表介绍调试 Init 容器时可能看到的一些状态值示例。</p>
<!--
Status | Meaning
------ | -------
`Init:N/M` | The Pod has `M` Init Containers, and `N` have completed so far.
`Init:Error` | An Init Container has failed to execute.
`Init:CrashLoopBackOff` | An Init Container has failed repeatedly.
`Pending` | The Pod has not yet begun executing Init Containers.
`PodInitializing` or `Running` | The Pod has already finished executing Init Containers.
-->
<table>
<thead>
<tr>
<th>状态</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Init:N/M</code></td>
<td>Pod 包含 <code>M</code> 个 Init 容器，其中 <code>N</code> 个已经运行完成。</td>
</tr>
<tr>
<td><code>Init:Error</code></td>
<td>Init 容器已执行失败。</td>
</tr>
<tr>
<td><code>Init:CrashLoopBackOff</code></td>
<td>Init 容器执行总是失败。</td>
</tr>
<tr>
<td><code>Pending</code></td>
<td>Pod 还没有开始执行 Init 容器。</td>
</tr>
<tr>
<td><code>PodInitializing</code> or <code>Running</code></td>
<td>Pod 已经完成执行 Init 容器。</td>
</tr>
</tbody>
</table>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-858517cd46a1b5a1fd2e650edd785cea">10.11 - 调试 Pods 和 ReplicationControllers</h1>
    
	<!-- 
reviewers:
- bprashanth
title: Debug Pods and ReplicationControllers
content_type: task
-->
<!-- overview -->
<!-- 
This page shows how to debug Pods and ReplicationControllers. 
-->
<p>此页面展示如何调试 Pod 和 ReplicationController。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- 
* You should be familiar with the basics of
  [Pods](/docs/concepts/workloads/pods/) and [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/). 
-->
<ul>
<li>你应该先熟悉 <a href="/zh/docs/concepts/workloads/pods/">Pods</a> 和
<a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/">Pod 生命周期</a> 的基础概念。</li>
</ul>
<!-- steps -->
<!-- 
## Debugging Pods 

The first step in debugging a pod is taking a look at it. Check the current
state of the pod and recent events with the following command: 
-->
<h2 id="debugging-pods">调试 Pod </h2>
<p>调试一个 pod 的第一步是观察它。使用下面的命令检查 Pod 的当前状态和最近事件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pods <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">POD_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><!-- 
Look at the state of the containers in the pod. Are they all `Running`?  Have
there been recent restarts?

Continue debugging depending on the state of the pods. 
-->
<p>看看 Pod 中的容器的状态。它们都是 <code>Running</code> 吗？最近有重启吗？</p>
<p>根据 Pod 的状态继续调试。</p>
<!-- 
### My pod stays pending 
-->
<!-- 
If a pod is stuck in `Pending` it means that it can not be scheduled onto a
node. Generally this is because there are insufficient resources of one type or
another that prevent scheduling. Look at the output of the `kubectl describe
...` command above. There should be messages from the scheduler about why it
can not schedule your pod. Reasons include: 
-->
<h3 id="我的-pod-停滞在-pending-状态">我的 Pod 停滞在 Pending 状态</h3>
<p>如果 Pod 被卡在 <code>Pending</code> 状态，就意味着它不能调度在某个节点上。一般来说，这是因为某种类型的资源不足而
导致无法调度。 查看上面的命令 <code>kubectl describe ...</code> 的输出。调度器的消息中应该会包含无法调度 Pod 的原因。
原因包括：</p>
<!-- 
#### Insufficient resources 

You may have exhausted the supply of CPU or Memory in your cluster. In this
case you can try several things:

* Add more nodes to the cluster.

* [Terminate unneeded pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)
  to make room for pending pods.

* Check that the pod is not larger than your nodes. For example, if all
  nodes have a capacity of `cpu:1`, then a pod with a request of `cpu: 1.1`
  will never be scheduled.

    You can check node capacities with the `kubectl get nodes -o <format>`
    command. Here are some example command lines that extract the necessary
    information: 
-->
<h4 id="资源不足">资源不足</h4>
<p>你可能已经耗尽了集群中供应的 CPU 或内存。在这个情况下你可以尝试几件事情：</p>
<ul>
<li>
<p>向集群中添加节点。</p>
</li>
<li>
<p><a href="/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">终止不需要的 Pod</a>
为 Pending 状态的 Pod 提供空间。</p>
</li>
<li>
<p>检查该 Pod 是否不大于你的节点。例如，如果全部节点具有 <code>cpu:1</code> 容量，那么具有
请求为 <code>cpu: 1.1</code> 的 Pod 永远不会被调度。</p>
<p>你可以使用 <code>kubectl get nodes -o &lt;format&gt;</code> 命令来检查节点容量。
下面是一些能够提取必要信息的命令示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes -o yaml | egrep <span style="color:#b44">&#39;\sname:|cpu:|memory:&#39;</span>
kubectl get nodes -o json | jq <span style="color:#b44">&#39;.items[] | {name: .metadata.name, cap: .status.capacity}&#39;</span>
</code></pre></div></li>
</ul>
<!-- 
  The [resource quota](/docs/concepts/policy/resource-quotas/)
  feature can be configured to limit the total amount of
  resources that can be consumed. If used in conjunction with namespaces, it can
  prevent one team from hogging all the resources. 
-->
<p>可以考虑配置<a href="/zh/docs/concepts/policy/resource-quotas/">资源配额</a> 来限制可耗用的资源总量。
如果与命名空间一起使用，它可以防止一个团队吞噬所有的资源。</p>
<!-- 
#### Using hostPort 

When you bind a pod to a `hostPort` there are a limited number of places that
the pod can be scheduled. In most cases, `hostPort` is unnecessary; try using a
service object to expose your pod. If you do require `hostPort` then you can
only schedule as many pods as there are nodes in your container cluster. 
-->
<h4 id="使用hostport">使用hostPort</h4>
<p>当你将一个 Pod 绑定到某 <code>hostPort</code> 时，这个 Pod 能被调度的位置数量有限。
在大多数情况下，<code>hostPort</code> 是不必要的; 尝试使用服务对象来暴露你的 Pod。
如果你需要 <code>hostPort</code>，那么你可以调度的 Pod 数量不能超过集群的节点个数。</p>
<!-- 
### My pod stays waiting 

If a pod is stuck in the `Waiting` state, then it has been scheduled to a
worker node, but it can't run on that machine. Again, the information from
`kubectl describe ...` should be informative. The most common cause of
`Waiting` pods is a failure to pull the image. There are three things to check: 

* Make sure that you have the name of the image correct.
* Have you pushed the image to the repository?
* Run a manual `docker pull <image>` on your machine to see if the image can be
  pulled.
-->
<h3 id="我的-pod-一直在-waiting">我的 Pod 一直在 Waiting</h3>
<p>如果 Pod 一直停滞在 <code>Waiting</code> 状态，那么它已被调度在某个工作节点，但它不能在该机器上运行。
再次，来自 <code>kubectl describe ...</code> 的内容应该是可以是很有用的。
最常见的原因 <code>Waiting</code> 的 Pod 是无法拉取镜像。有三件事要检查：</p>
<ul>
<li>确保你的镜像的名称正确。</li>
<li>你是否将镜像推送到存储库？</li>
<li>在你的机器上手动运行 <code>docker pull &lt;image&gt;</code>，看看是否可以拉取镜像。</li>
</ul>
<!-- 
### My pod is crashing or otherwise unhealthy 

Once your pod has been scheduled, the methods described in [Debug Running Pods](
/docs/tasks/debug-application-cluster/debug-running-pod/) are available for debugging.
-->
<h3 id="我的-pod-一直-crashing-或者其他不健康状态">我的 Pod 一直 Crashing 或者其他不健康状态</h3>
<p>一旦 Pod 已经被调度，就可以依据
<a href="/zh/docs/tasks/debug-application-cluster/debug-running-pod/">调试运行中的 Pod</a>
展开进一步的调试工作。</p>
<!-- 
## Debugging ReplicationControllers 

ReplicationControllers are fairly straightforward. They can either create pods
or they can't. If they can't create pods, then please refer to the
[instructions above](#debugging-pods) to debug your pods. 
-->
<h2 id="调试-replication-controller">调试 Replication Controller</h2>
<p>Replication Controller 相当简单。它们或者能或者不能创建 Pod。如果它们无法创建 Pod，
请参考<a href="#debugging_pods">上面的说明</a> 来调试你的 Pod。</p>
<!-- 
You can also use `kubectl describe rc ${CONTROLLER_NAME}` to inspect events
related to the replication controller. 
-->
<p>你也可以使用 <code>kubectl describe rc ${CONTROLLER_NAME}</code> 来检查和副本控制器有关的事件。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f79645981e310858111bd5673614cab6">10.12 - 调试 Service</h1>
    
	<!--
reviewers:
- thockin
- bowei
content_type: concept
title: Debug Services
-->
<!-- overview -->
<!--
An issue that comes up rather frequently for new installations of Kubernetes is
that a Service is not working properly.  You've run your Pods through a
Deployment (or other workload controller) and created a Service, but you
get no response when you try to access it.  This document will hopefully help
you to figure out what's going wrong.
-->
<p>对于新安装的 Kubernetes，经常出现的问题是 Service 无法正常运行。 你已经通过
Deployment（或其他工作负载控制器）运行了 Pod，并创建 Service ，但是
当你尝试访问它时，没有任何响应。此文档有望对你有所帮助并找出问题所在。</p>
<!-- body -->
<!--
## Running commands in a Pod

For many steps here you will want to see what a Pod running in the cluster
sees.  The simplest way to do this is to run an interactive busybox Pod:
-->
<h2 id="在-pod-中运行命令">在 Pod 中运行命令</h2>
<p>对于这里的许多步骤，你可能希望知道运行在集群中的 Pod 看起来是什么样的。
最简单的方法是运行一个交互式的 busybox Pod：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox sh
</code></pre><!--
<blockquote class="note callout">
  <div><strong>说明：</strong> If you don't see a command prompt, try pressing enter.</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果没有看到命令提示符，请按回车。</div>
</blockquote>
<!--
If you already have a running Pod that you prefer to use, you can run a
command in it using:
-->
<p>如果你已经有了你想使用的正在运行的 Pod，则可以运行以下命令去进入：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> &lt;POD-NAME&gt; -c &lt;CONTAINER-NAME&gt; -- &lt;COMMAND&gt;
</code></pre></div><!--
## Setup

For the purposes of this walk-through, let's run some Pods.  Since you're
probably debugging your own Service you can substitute your own details, or you
can follow along and get a second data point.
-->
<h2 id="setup">设置 </h2>
<p>为了完成本次实践的任务，我们先运行几个 Pod。
由于你可能正在调试自己的 Service，所以，你可以使用自己的信息进行替换，
或者你也可以跟着教程并开始下面的步骤来获得第二个数据点。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl  create deployment hostnames --image<span style="color:#666">=</span>k8s.gcr.io/serve_hostname 
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/hostnames created
</code></pre><!--
`kubectl` commands will print the type and name of the resource created or mutated, which can then be used in subsequent commands.

Let's scale the deployment to 3 replicas.
-->
<p><code>kubectl</code> 命令将打印创建或变更的资源的类型和名称，它们可以在后续命令中使用。
让我们将这个 deployment 的副本数扩至 3。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale deployment hostnames --replicas<span style="color:#666">=</span><span style="color:#666">3</span>
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">deployment.apps/hostnames scaled
</code></pre><!--
Note that this is the same as if you had the Deployment with the following YAML:
-->
<p>请注意这与你使用以下 YAML 方式启动 Deployment 类似：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hostnames<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hostnames<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hostnames<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hostnames<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hostnames<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/serve_hostname<span style="color:#bbb">
</span></code></pre></div><!--
The label "app" is automatically set by `kubectl create deployment` to the name of the Deployment.

You can confirm your Pods are running:
-->
<p>&quot;app&quot; 标签是 <code>kubectl create deployment</code> 根据 Deployment 名称自动设置的。</p>
<p>确认你的 Pods 是运行状态:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME                        READY     STATUS    RESTARTS   AGE
hostnames-632524106-bbpiw   1/1       Running   0          2m
hostnames-632524106-ly40y   1/1       Running   0          2m
hostnames-632524106-tlaok   1/1       Running   0          2m
</code></pre><!--
You can also confirm that your Pods are serving.  You can get the list of
Pod IP addresses and test them directly.
-->
<p>你还可以确认你的 Pod 是否正在提供服务。你可以获取 Pod IP 地址列表并直接对其进行测试。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>hostnames <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    -o go-template<span style="color:#666">=</span><span style="color:#b44">&#39;{{range .items}}{{.status.podIP}}{{&#34;\n&#34;}}{{end}}&#39;</span>
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">10.244.0.5
10.244.0.6
10.244.0.7
</code></pre><!--
The example container used for this walk-through serves its own hostname
via HTTP on port 9376, but if you are debugging your own app, you'll want to
use whatever port number your Pods are listening on.

From within a pod:
-->
<p>用于本教程的示例容器通过 HTTP 在端口 9376 上提供其自己的主机名，
但是如果要调试自己的应用程序，则需要使用你的 Pod 正在侦听的端口号。</p>
<p>在 Pod 内运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; <span style="color:#a2f;font-weight:bold">do</span>
    wget -qO- <span style="color:#b8860b">$ep</span>
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
This should produce something like:
-->
<p>输出类似这样：</p>
<pre tabindex="0"><code>hostnames-632524106-bbpiw
hostnames-632524106-ly40y
hostnames-632524106-tlaok
</code></pre><!--
If you are not getting the responses you expect at this point, your Pods
might not be healthy or might not be listening on the port you think they are.
You might find `kubectl logs` to be useful for seeing what is happening, or
perhaps you need to `kubectl exec` directly into your Pods and debug from
there.

Assuming everything has gone to plan so far, you can start to investigate why
your Service doesn't work.
-->
<p>如果此时你没有收到期望的响应，则你的 Pod 状态可能不健康，或者可能没有在你认为正确的端口上进行监听。
你可能会发现 <code>kubectl logs</code> 命令对于查看正在发生的事情很有用，
或者你可能需要通过<code>kubectl exec</code> 直接进入 Pod 中并从那里进行调试。</p>
<p>假设到目前为止一切都已按计划进行，那么你可以开始调查为何你的 Service 无法正常工作。</p>
<!--
## Does the Service exist?

The astute reader will have noticed that you did not actually create a Service
yet - that is intentional.  This is a step that sometimes gets forgotten, and
is the first thing to check.

What would happen if you tried to access a non-existent Service?  If
you have another Pod that consumes this Service by name you would get
something like:
-->
<h2 id="service-是否存在">Service 是否存在？</h2>
<p>细心的读者会注意到我们实际上尚未创建 Service -这是有意而为之。 这一步有时会被遗忘，这是首先要检查的步骤。</p>
<p>那么，如果我尝试访问不存在的 Service 会怎样？ 假设你有另一个 Pod 通过名称匹配到 Service ，你将得到类似结果：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget -O- hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Resolving hostnames (hostnames)... failed: Name or service not known.
wget: unable to resolve host address 'hostnames'
</code></pre><!--
The first thing to check is whether that Service actually exists:
-->
<p>首先要检查的是该 Service 是否真实存在：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">No resources found.
Error from server (NotFound): services &quot;hostnames&quot; not found
</code></pre><!--
Let's create the Service.  As before, this is for the walk-through - you can
use your own Service's details here.
-->
<p>让我们创建 Service。 和以前一样，在这次实践中 - 你可以在此处使用自己的 Service 的内容。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl expose deployment hostnames --port<span style="color:#666">=</span><span style="color:#666">80</span> --target-port<span style="color:#666">=</span><span style="color:#666">9376</span>
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">service/hostnames exposed
</code></pre><!--
And read it back:
-->
<p>重新运行查询命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
hostnames   ClusterIP   10.0.1.175   &lt;none&gt;        80/TCP    5s
</code></pre><!--
Now you know that the Service exists.

As before, this is the same as if you had started the `Service` with YAML:
-->
<p>现在你知道了 Service 确实存在。</p>
<p>同前，此步骤效果与通过 YAML 方式启动 'Service' 一样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hostnames<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>hostnames<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span></code></pre></div><!--
In order to highlight the full range of configuration, the Service you created
here uses a different port number than the Pods.  For many real-world
Services, these values might be the same.
-->
<p>为了突出配置范围的完整性，你在此处创建的 Service 使用的端口号与 Pods 不同。
对于许多真实的 Service，这些值可以是相同的。</p>
<!--
## Does the Service work by DNS name?

One of the most common ways that clients consume a Service is through a DNS
name.

From a Pod in the same Namespace:
-->
<h2 id="service-是否可通过-dns-名字访问">Service 是否可通过 DNS 名字访问？</h2>
<p>通常客户端通过 DNS 名称来匹配到 Service。</p>
<p>从相同命名空间下的 Pod 中运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">nslookup hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      hostnames
Address 1: 10.0.1.175 hostnames.default.svc.cluster.local
</code></pre><!--
If this fails, perhaps your Pod and Service are in different
Namespaces, try a namespace-qualified name (again, from within a Pod):
-->
<p>如果失败，那么你的 Pod 和 Service 可能位于不同的命名空间中，
请尝试使用限定命名空间的名称（同样在 Pod 内运行）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">nslookup hostnames.default
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      hostnames.default
Address 1: 10.0.1.175 hostnames.default.svc.cluster.local
</code></pre><!--
If this works, you'll need to adjust your app to use a cross-namespace name, or
run your app and Service in the same Namespace.  If this still fails, try a
fully-qualified name:
-->
<p>如果成功，那么需要调整你的应用，使用跨命名空间的名称去访问它，
或者在相同的命名空间中运行应用和 Service。如果仍然失败，请尝试一个完全限定的名称：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">nslookup hostnames.default.svc.cluster.local
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      hostnames.default.svc.cluster.local
Address 1: 10.0.1.175 hostnames.default.svc.cluster.local
</code></pre><!--
Note the suffix here: "default.svc.cluster.local".  The "default" is the
Namespace you're operating in.  The "svc" denotes that this is a Service.
The "cluster.local" is your cluster domain, which COULD be different in your
own cluster.

You can also try this from a `Node` in the cluster:

<blockquote class="note callout">
  <div><strong>说明：</strong> 10.0.0.10 is the cluster's DNS Service IP, yours might be different.</div>
</blockquote>
-->
<p>注意这里的后缀：&quot;default.svc.cluster.local&quot;。&quot;default&quot; 是我们正在操作的命名空间。
&quot;svc&quot; 表示这是一个 Service。&quot;cluster.local&quot; 是你的集群域，在你自己的集群中可能会有所不同。</p>
<p>你也可以在集群中的节点上尝试此操作：</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> 10.0.0.10 是集群的 DNS 服务 IP，你的可能有所不同。</div>
</blockquote>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">nslookup hostnames.default.svc.cluster.local 10.0.0.10
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Server:         10.0.0.10
Address:        10.0.0.10#53

Name:   hostnames.default.svc.cluster.local
Address: 10.0.1.175
</code></pre><!--
If you are able to do a fully-qualified name lookup but not a relative one, you
need to check that your `/etc/resolv.conf` file in your Pod is correct.  From
within a Pod:
-->
<p>如果你能够使用完全限定的名称查找，但不能使用相对名称，则需要检查你 Pod 中的
<code>/etc/resolv.conf</code> 文件是否正确。在 Pod 中运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat /etc/resolv.conf
</code></pre></div><!--
You should see something like:
-->
<p>你应该可以看到类似这样的输出：</p>
<pre tabindex="0"><code>nameserver 10.0.0.10
search default.svc.cluster.local svc.cluster.local cluster.local example.com
options ndots:5
</code></pre><!--
The `nameserver` line must indicate your cluster's DNS `Service`.  This is
passed into `kubelet` with the `--cluster-dns` flag.

The `search` line must include an appropriate suffix for you to find the
Service name.  In this case it is looking for Services in the local
Namespace ("default.svc.cluster.local"), Services in all Namespaces
("svc.cluster.local"), and lastly for names in the cluster ("cluster.local").
Depending on your own install you might have additional records after that (up
to 6 total).  The cluster suffix is passed into `kubelet` with the
`--cluster-domain` flag.  Throughout this document, the cluster suffix is
assumed to be "cluster.local".  Your own clusters might be configured
differently, in which case you should change that in all of the previous
commands.

The `options` line must set `ndots` high enough that your DNS client library
considers search paths at all.  Kubernetes sets this to 5 by default, which is
high enough to cover all of the DNS names it generates.
-->
<p><code>nameserver</code> 行必须指示你的集群的 DNS Service，
它是通过 <code>--cluster-dns</code> 标志传递到 kubelet 的。</p>
<p><code>search</code> 行必须包含一个适当的后缀，以便查找 Service 名称。
在本例中，它查找本地命名空间（<code>default.svc.cluster.local</code>）中的服务和
所有命名空间（<code>svc.cluster.local</code>）中的服务，最后在集群（<code>cluster.local</code>）中查找
服务的名称。根据你自己的安装情况，可能会有额外的记录（最多 6 条）。
集群后缀是通过 <code>--cluster-domain</code> 标志传递给 <code>kubelet</code> 的。
本文中，我们假定后缀是 “cluster.local”。
你的集群配置可能不同，这种情况下，你应该在上面的所有命令中更改它。</p>
<p><code>options</code> 行必须设置足够高的 <code>ndots</code>，以便 DNS 客户端库考虑搜索路径。
在默认情况下，Kubernetes 将这个值设置为 5，这个值足够高，足以覆盖它生成的所有 DNS 名称。</p>
<!--
### Does any Service work by DNS name? {#does-any-service-exist-in-dns}

If the above still fails, DNS lookups are not working for your Service.  You
can take a step back and see what else is not working.  The Kubernetes master
Service should always work.  From within a Pod:
-->
<h3 id="does-any-service-exist-in-dns">是否存在 Service 能通过 DNS 名称访问？</h3>
<p>如果上面的方式仍然失败，DNS 查找不到你需要的 Service ，你可以后退一步，
看看还有什么其它东西没有正常工作。
Kubernetes 主 Service 应该一直是工作的。在 Pod 中运行如下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">nslookup kubernetes.default
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local
</code></pre><!--
If this fails, please see the [kube-proxy](#is-the-kube-proxy-working) section
of this document, or even go back to the top of this document and start over,
but instead of debugging your own Service, debug the DNS Service.
-->
<p>如果失败，你可能需要转到本文的 <a href="#is-the-kube-proxy-working">kube-proxy</a> 节，
或者甚至回到文档的顶部重新开始，但不是调试你自己的 Service ，而是调试 DNS Service。</p>
<!--
## Does the Service work by IP?

Assuming you have confirmed that DNS works, the next thing to test is whether your
Service works by its IP address.  From a Pod in your cluster, access the
Service's IP (from `kubectl get` above).
-->
<h3 id="service-能够通过-ip-访问么">Service 能够通过 IP 访问么？</h3>
<p>假设你已经确认 DNS 工作正常，那么接下来要测试的是你的 Service 能否通过它的 IP 正常访问。
从集群中的一个 Pod，尝试访问 Service 的 IP（从上面的 <code>kubectl get</code> 命令获取）。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#a2f;font-weight:bold">$(</span>seq <span style="color:#666">1</span> 3<span style="color:#a2f;font-weight:bold">)</span>; <span style="color:#a2f;font-weight:bold">do</span> 
    wget -qO- 10.0.1.175:80
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
This should produce something like:
-->
<p>输出应该类似这样：</p>
<pre tabindex="0"><code>hostnames-632524106-bbpiw
hostnames-632524106-ly40y
hostnames-632524106-tlaok
</code></pre><!--
If your Service is working, you should get correct responses.  If not, there
are a number of things that could be going wrong.  Read on.
-->
<p>如果 Service 状态是正常的，你应该得到正确的响应。如果没有，有很多可能出错的地方，请继续阅读。</p>
<!--
## Is the Service defined correctly?

It might sound silly, but you should really double and triple check that your
`Service` is correct and matches your `Pod`'s port.  Read back your `Service`
and verify it:
-->
<h2 id="service-的配置是否正确">Service 的配置是否正确？</h2>
<p>这听起来可能很愚蠢，但你应该两次甚至三次检查你的 Service 配置是否正确，并且与你的 Pod 匹配。
查看你的 Service 配置并验证它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get service hostnames -o json
</code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#008000;font-weight:bold">&#34;kind&#34;</span>: <span style="color:#b44">&#34;Service&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;apiVersion&#34;</span>: <span style="color:#b44">&#34;v1&#34;</span>,
    <span style="color:#008000;font-weight:bold">&#34;metadata&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;hostnames&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;namespace&#34;</span>: <span style="color:#b44">&#34;default&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;uid&#34;</span>: <span style="color:#b44">&#34;428c8b6c-24bc-11e5-936d-42010af0a9bc&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;resourceVersion&#34;</span>: <span style="color:#b44">&#34;347189&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;creationTimestamp&#34;</span>: <span style="color:#b44">&#34;2015-07-07T15:24:29Z&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;labels&#34;</span>: {
            <span style="color:#008000;font-weight:bold">&#34;app&#34;</span>: <span style="color:#b44">&#34;hostnames&#34;</span>
        }
    },
    <span style="color:#008000;font-weight:bold">&#34;spec&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;ports&#34;</span>: [
            {
                <span style="color:#008000;font-weight:bold">&#34;name&#34;</span>: <span style="color:#b44">&#34;default&#34;</span>,
                <span style="color:#008000;font-weight:bold">&#34;protocol&#34;</span>: <span style="color:#b44">&#34;TCP&#34;</span>,
                <span style="color:#008000;font-weight:bold">&#34;port&#34;</span>: <span style="color:#666">80</span>,
                <span style="color:#008000;font-weight:bold">&#34;targetPort&#34;</span>: <span style="color:#666">9376</span>,
                <span style="color:#008000;font-weight:bold">&#34;nodePort&#34;</span>: <span style="color:#666">0</span>
            }
        ],
        <span style="color:#008000;font-weight:bold">&#34;selector&#34;</span>: {
            <span style="color:#008000;font-weight:bold">&#34;app&#34;</span>: <span style="color:#b44">&#34;hostnames&#34;</span>
        },
        <span style="color:#008000;font-weight:bold">&#34;clusterIP&#34;</span>: <span style="color:#b44">&#34;10.0.1.175&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;type&#34;</span>: <span style="color:#b44">&#34;ClusterIP&#34;</span>,
        <span style="color:#008000;font-weight:bold">&#34;sessionAffinity&#34;</span>: <span style="color:#b44">&#34;None&#34;</span>
    },
    <span style="color:#008000;font-weight:bold">&#34;status&#34;</span>: {
        <span style="color:#008000;font-weight:bold">&#34;loadBalancer&#34;</span>: {}
    }
}
</code></pre></div><!--
* Is the Service port you are trying to access listed in `spec.ports[]`?
* Is the `targetPort` correct for your Pods (some Pods use a different port than the Service)?
* If you meant to use a numeric port, is it a number (9376) or a string "9376"?
* If you meant to use a named port, do your Pods expose a port with the same name?
* Is the port's `protocol` correct for your Pods?
-->
<ul>
<li>你想要访问的 Service 端口是否在 <code>spec.ports[]</code> 中列出？</li>
<li><code>targetPort</code> 对你的 Pod 来说正确吗（许多 Pod 使用与 Service 不同的端口）？</li>
<li>如果你想使用数值型端口，那么它的类型是一个数值（9376）还是字符串 “9376”？</li>
<li>如果你想使用名称型端口，那么你的 Pod 是否暴露了一个同名端口？</li>
<li>端口的 <code>protocol</code> 和 Pod 的是否对应？</li>
</ul>
<!--
## Does the Service have any Endpoints?

If you got this far, you have confirmed that your Service is correctly
defined and is resolved by DNS.  Now let's check that the Pods you ran are
actually being selected by the Service.

Earlier you saw that the Pods were running.  You can re-check that:
-->
<h2 id="service-有-endpoints-吗">Service 有 Endpoints 吗？</h2>
<p>如果你已经走到了这一步，你已经确认你的 Service 被正确定义，并能通过 DNS 解析。
现在，让我们检查一下，你运行的 Pod 确实是被 Service 选中的。</p>
<p>早些时候，我们已经看到 Pod 是运行状态。我们可以再检查一下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME                        READY     STATUS    RESTARTS   AGE
hostnames-632524106-bbpiw   1/1       Running   0          1h
hostnames-632524106-ly40y   1/1       Running   0          1h
hostnames-632524106-tlaok   1/1       Running   0          1h
</code></pre><!--
The `-l app=hostnames` argument is a label selector configured on the Service.

The "AGE" column says that these Pods are about an hour old, which implies that
they are running fine and not crashing.

The "RESTARTS" column says that these pods are not crashing frequently or being
restarted.  Frequent restarts could lead to intermittent connectivity issues.
If the restart count is high, read more about how to [debug pods](/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#debugging-pods).

Inside the Kubernetes system is a control loop which evaluates the selector of
every Service and saves the results into a corresponding Endpoints object.
-->
<p><code>-l app=hostnames</code> 参数是在 Service 上配置的标签选择器。</p>
<p>&quot;AGE&quot; 列表明这些 Pod 已经启动一个小时了，这意味着它们运行良好，而未崩溃。</p>
<p>&quot;RESTARTS&quot; 列表明 Pod 没有经常崩溃或重启。经常性崩溃可能导致间歇性连接问题。
如果重启次数过大，通过<a href="/zh/docs/tasks/debug-application-cluster/debug-application/#debugging-pods">调试 pod</a>
了解相关技术。</p>
<p>在 Kubernetes 系统中有一个控制回路，它评估每个 Service 的选择算符，并将结果保存到 Endpoints 对象中。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get endpoints hostnames
</code></pre></div><pre tabindex="0"><code>NAME        ENDPOINTS
hostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
</code></pre><!--
This confirms that the endpoints controller has found the correct Pods for
your Service.  If the `ENDPOINTS` column is `<none>`, you should check that
the `spec.selector` field of your Service actually selects for
`metadata.labels` values on your Pods.  A common mistake is to have a typo or
other error, such as the Service selecting for `app=hostnames`, but the
Deployment specifying `run=hostnames`, as in versions previous to 1.18, where
the `kubectl run` command could have been also used to create a Deployment.
-->
<p>这证实 Endpoints 控制器已经为你的 Service 找到了正确的 Pods。
如果 <code>ENDPOINTS</code> 列的值为 <code>&lt;none&gt;</code>，则应检查 Service 的 <code>spec.selector</code> 字段，
以及你实际想选择的 Pod 的 <code>metadata.labels</code> 的值。
常见的错误是输入错误或其他错误，例如 Service 想选择 <code>app=hostnames</code>，但是
Deployment 指定的是 <code>run=hostnames</code>。在 1.18之前的版本中 <code>kubectl run</code>
也可以被用来创建 Deployment。</p>
<!--
## Are the Pods working?

At this point, you know that your Service exists and has selected your Pods.
At the beginning of this walk-through, you verified the Pods themselves.
Let's check again that the Pods are actually working - you can bypass the
Service mechanism and go straight to the Pods, as listed by the Endpoints
above.

<blockquote class="note callout">
  <div><strong>说明：</strong> These commands use the Pod port (9376), rather than the Service port (80).</div>
</blockquote>

From within a Pod:
-->
<h2 id="pod-正常工作吗">Pod 正常工作吗？</h2>
<p>至此，你知道你的 Service 已存在，并且已匹配到你的Pod。在本实验的开始，你已经检查了 Pod 本身。
让我们再次检查 Pod 是否确实在工作 - 你可以绕过 Service 机制并直接转到 Pod，如上面的 Endpoint 所示。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> 这些命令使用的是 Pod 端口（9376），而不是 Service 端口（80）。</div>
</blockquote>
<p>在 Pod 中运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; <span style="color:#a2f;font-weight:bold">do</span>
    wget -qO- <span style="color:#b8860b">$ep</span>
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
This should produce something like:
-->
<p>输出应该类似这样：</p>
<pre tabindex="0"><code>hostnames-632524106-bbpiw
hostnames-632524106-ly40y
hostnames-632524106-tlaok
</code></pre><!--
You expect each Pod in the Endpoints list to return its own hostname.  If
this is not what happens (or whatever the correct behavior is for your own
Pods), you should investigate what's happening there.
-->
<p>你希望 Endpoint 列表中的每个 Pod 都返回自己的主机名。
如果情况并非如此（或你自己的 Pod 的正确行为是什么），你应调查发生了什么事情。</p>
<!--
## Is the kube-proxy working?

If you get here, your Service is running, has Endpoints, and your Pods
are actually serving.  At this point, the whole Service proxy mechanism is
suspect.  Let's confirm it, piece by piece.

The default implementation of Services, and the one used on most clusters, is
kube-proxy.  This is a program that runs on every node and configures one of a
small set of mechanisms for providing the Service abstraction.  If your
cluster does not use kube-proxy, the following sections will not apply, and you
will have to investigate whatever implementation of Services you are using.
-->
<h2 id="kube-proxy-正常工作吗">kube-proxy 正常工作吗？</h2>
<p>如果你到达这里，则说明你的 Service 正在运行，拥有 Endpoints，Pod 真正在提供服务。
此时，整个 Service 代理机制是可疑的。让我们一步一步地确认它没问题。</p>
<p>Service 的默认实现（在大多数集群上应用的）是 kube-proxy。
这是一个在每个节点上运行的程序，负责配置用于提供 Service 抽象的机制之一。
如果你的集群不使用 kube-proxy，则以下各节将不适用，你将必须检查你正在使用的 Service 的实现方式。</p>
<!--
## Is the kube-proxy working?

Confirm that `kube-proxy` is running on your Nodes.  Running directly on a
Node, you should get something like the below:
-->
<h3 id="kube-proxy-正常运行吗">kube-proxy 正常运行吗？</h3>
<p>确认 <code>kube-proxy</code> 正在节点上运行。 在节点上直接运行，你将会得到类似以下的输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ps auxw | grep kube-proxy
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">root  4194  0.4  0.1 101864 17696 ?    Sl Jul04  25:43 /usr/local/bin/kube-proxy --master=https://kubernetes-master --kubeconfig=/var/lib/kube-proxy/kubeconfig --v=2
</code></pre><!--
Next, confirm that it is not failing something obvious, like contacting the
master.  To do this, you'll have to look at the logs.  Accessing the logs
depends on your Node OS.  On some OSes it is a file, such as
/var/log/kube-proxy.log, while other OSes use `journalctl` to access logs.  You
should see something like:
-->
<p>下一步，确认它并没有出现明显的失败，比如连接主节点失败。要做到这一点，你必须查看日志。
访问日志的方式取决于你节点的操作系统。
在某些操作系统上日志是一个文件，如 /var/log/messages kube-proxy.log，
而其他操作系统使用 <code>journalctl</code> 访问日志。你应该看到输出类似于：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">I1027 22:14:53.995134    5063 server.go:200] Running in resource-only container &quot;/kube-proxy&quot;
I1027 22:14:53.998163    5063 server.go:247] Using iptables Proxier.
I1027 22:14:53.999055    5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.
I1027 22:14:54.038140    5063 proxier.go:352] Setting endpoints for &quot;kube-system/kube-dns:dns-tcp&quot; to [10.244.1.3:53]
I1027 22:14:54.038164    5063 proxier.go:352] Setting endpoints for &quot;kube-system/kube-dns:dns&quot; to [10.244.1.3:53]
I1027 22:14:54.038209    5063 proxier.go:352] Setting endpoints for &quot;default/kubernetes:https&quot; to [10.240.0.2:443]
I1027 22:14:54.038238    5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master
I1027 22:14:54.040048    5063 proxier.go:294] Adding new service &quot;default/kubernetes:https&quot; at 10.0.0.1:443/TCP
I1027 22:14:54.040154    5063 proxier.go:294] Adding new service &quot;kube-system/kube-dns:dns&quot; at 10.0.0.10:53/UDP
I1027 22:14:54.040223    5063 proxier.go:294] Adding new service &quot;kube-system/kube-dns:dns-tcp&quot; at 10.0.0.10:53/TCP
</code></pre><!--
If you see error messages about not being able to contact the master, you
should double-check your Node configuration and installation steps.

One of the possible reasons that `kube-proxy` cannot run correctly is that the
required `conntrack` binary cannot be found. This may happen on some Linux
systems, depending on how you are installing the cluster, for example, you are
installing Kubernetes from scratch. If this is the case, you need to manually
install the `conntrack` package (e.g. `sudo apt install conntrack` on Ubuntu)
and then retry.
-->
<p>如果你看到有关无法连接主节点的错误消息，则应再次检查节点配置和安装步骤。</p>
<p><code>kube-proxy</code> 无法正确运行的可能原因之一是找不到所需的 <code>conntrack</code> 二进制文件。
在一些 Linux 系统上，这也是可能发生的，这取决于你如何安装集群，
例如，你是手动开始一步步安装 Kubernetes。如果是这样的话，你需要手动安装
<code>conntrack</code> 包（例如，在 Ubuntu 上使用 <code>sudo apt install conntrack</code>），然后重试。</p>
<!--
Kube-proxy can run in one of a few modes.  In the log listed above, the
line `Using iptables Proxier` indicates that kube-proxy is running in
"iptables" mode.  The most common other mode is "ipvs".  The older "userspace"
mode has largely been replaced by these.

-->
<p>Kube-proxy 可以以若干模式之一运行。在上述日志中，<code>Using iptables Proxier</code>
行表示 kube-proxy 在 &quot;iptables&quot; 模式下运行。
最常见的另一种模式是 &quot;ipvs&quot;。先前的 &quot;userspace&quot; 模式已经被这些所代替。</p>
<!--
#### Iptables mode

In "iptables" mode, you should see something like the following on a Node:
-->
<h4 id="iptables-模式">Iptables 模式</h4>
<p>在 &quot;iptables&quot; 模式中, 你应该可以在节点上看到如下输出:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">iptables-save | grep hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.244.3.6:9376
-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.244.1.7:9376
-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.244.2.3:9376
-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment &quot;default/hostnames: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -j KUBE-SEP-57KPRZ3JQVENLNBR
</code></pre><!--
For each port of each Service, there should be 1 rule in `KUBE-SERVICES` and
one `KUBE-SVC-<hash>` chain.  For each Pod endpoint, there should be a small
number of rules in that `KUBE-SVC-<hash>` and one `KUBE-SEP-<hash>` chain with
a small number of rules in it.  The exact rules will vary based on your exact
config (including node-ports and load-balancers).
-->
<p>对于每个 Service 的每个端口，应有 1 条 <code>KUBE-SERVICES</code> 规则、一个 <code>KUBE-SVC-&lt;hash&gt;</code> 链。
对于每个 Pod 末端，在那个 <code>KUBE-SVC-&lt;hash&gt;</code> 链中应该有一些规则与之对应，还应该
有一个 <code>KUBE-SEP-&lt;hash&gt;</code> 链与之对应，其中包含为数不多的几条规则。
实际的规则数量可能会根据你实际的配置（包括 NodePort 和 LoadBalancer 服务）有所不同。</p>
<!--
#### IPVS mode

In "ipvs" mode, you should see something like the following on a Node:
-->
<h4 id="ipvs-模式">IPVS 模式</h4>
<p>在 &quot;ipvs&quot; 模式中, 你应该在节点下看到如下输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ipvsadm -ln
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
...
TCP  10.0.1.175:80 rr
  -&gt; 10.244.0.5:9376               Masq    1      0          0
  -&gt; 10.244.0.6:9376               Masq    1      0          0
  -&gt; 10.244.0.7:9376               Masq    1      0          0
...
</code></pre><!--
For each port of each Service, plus any NodePorts, external IPs, and
load-balancer IPs, kube-proxy will create a virtual server.  For each Pod
endpoint, it will create corresponding real servers. In this example, service
hostnames(`10.0.1.175:80`) has 3 endpoints(`10.244.0.5:9376`,
`10.244.0.6:9376`, `10.244.0.7:9376`).
-->
<p>对于每个 Service 的每个端口，还有 NodePort，External IP 和 LoadBalancer 类型服务
的 IP，kube-proxy 将创建一个虚拟服务器。
对于每个 Pod 末端，它将创建相应的真实服务器。
在此示例中，服务主机名（<code>10.0.1.175:80</code>）拥有 3 个末端（<code>10.244.0.5:9376</code>、
<code>10.244.0.6:9376</code> 和 <code>10.244.0.7:9376</code>）。</p>
<!--
#### Userspace mode

In rare cases, you may be using "userspace" mode.  From your Node:
-->
<h4 id="userspace-模式">Userspace 模式</h4>
<p>在极少数情况下，你可能会用到 &quot;userspace&quot; 模式。在你的节点上运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">iptables-save | grep hostnames
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">-A KUBE-PORTALS-CONTAINER -d 10.0.1.175/32 -p tcp -m comment --comment &quot;default/hostnames:default&quot; -m tcp --dport 80 -j REDIRECT --to-ports 48577
-A KUBE-PORTALS-HOST -d 10.0.1.175/32 -p tcp -m comment --comment &quot;default/hostnames:default&quot; -m tcp --dport 80 -j DNAT --to-destination 10.240.115.247:48577
</code></pre><!--
There should be 2 rules for each port of your Service (only one in this
example) - a "KUBE-PORTALS-CONTAINER" and a "KUBE-PORTALS-HOST".

Almost nobody should be using the "userspace" mode any more, so you won't spend
more time on it here.
-->
<p>对于 Service （本例中只有一个）的每个端口，应当有 2 条规则：
一条 &quot;KUBE-PORTALS-CONTAINER&quot; 和一条 &quot;KUBE-PORTALS-HOST&quot; 规则。</p>
<p>几乎没有人应该再使用 &quot;userspace&quot; 模式，因此你在这里不会花更多的时间。</p>
<!--
### Is kube-proxy proxying?

Assuming you do see one the above cases, try again to access your Service by
IP from one of your Nodes:
-->
<h3 id="kube-proxy-是否在运行">kube-proxy 是否在运行?</h3>
<p>假设你确实遇到上述情况之一，请重试从节点上通过 IP 访问你的 Service ：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl 10.0.1.175:80
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">hostnames-632524106-bbpiw
</code></pre><!--
If this fails and you are using the userspace proxy, you can try accessing the
proxy directly.  If you are using the iptables proxy, skip this section.

Look back at the `iptables-save` output above, and extract the
port number that `kube-proxy` is using for your Service.  In the above
examples it is "48577".  Now connect to that:
-->
<p>如果失败，并且你正在使用用户空间代理，则可以尝试直接访问代理。
如果你使用的是 iptables 代理，请跳过本节。</p>
<p>回顾上面的 <code>iptables-save</code> 输出，并提取 <code>kube-proxy</code> 为你的 Service 所使用的端口号。
在上面的例子中，端口号是 “48577”。现在试着连接它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl localhost:48577
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">hostnames-632524106-tlaok
</code></pre><!--
If this still fails, look at the `kube-proxy` logs for specific lines like:
-->
<p>如果这步操作仍然失败，请查看 <code>kube-proxy</code> 日志中的特定行，如：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">Setting endpoints for default/hostnames:default to [10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376]
</code></pre><!--
If you don't see those, try restarting `kube-proxy` with the `-v` flag set to 4, and
then look at the logs again.
-->
<p>如果你没有看到这些，请尝试将 <code>-V</code> 标志设置为 4 并重新启动 <code>kube-proxy</code>，然后再查看日志。</p>
<!--
### Edge case: A Pod fails to reach itself via the Service IP {#a-pod-fails-to-reach-itself-via-the-service-ip}

This might sound unlikely, but it does happen and it is supposed to work.

This can happen when the network is not properly configured for "hairpin"
traffic, usually when `kube-proxy` is running in `iptables` mode and Pods
are connected with bridge network. The `Kubelet` exposes a `hairpin-mode`
[flag](/docs/reference/command-line-tools-reference/kubelet/) that allows endpoints of a Service to loadbalance
back to themselves if they try to access their own Service VIP. The
`hairpin-mode` flag must either be set to `hairpin-veth` or
`promiscuous-bridge`.

-->
<h3 id="a-pod-fails-to-reach-itself-via-the-service-ip">边缘案例: Pod 无法通过 Service IP 连接到它本身      </h3>
<p>这听起来似乎不太可能，但是确实可能发生，并且应该可行。</p>
<p>如果网络没有为“发夹模式（Hairpin）”流量生成正确配置，
通常当 <code>kube-proxy</code> 以 <code>iptables</code> 模式运行，并且 Pod 与桥接网络连接时，就会发生这种情况。
<code>kubelet</code> 提供了 <code>hairpin-mode</code>
<a href="/zh/docs/reference/command-line-tools-reference/kubelet/">标志</a>。
如果 Service 的末端尝试访问自己的 Service VIP，则该端点可以把流量负载均衡回来到它们自身。
<code>hairpin-mode</code> 标志必须被设置为 <code>hairpin-veth</code> 或者 <code>promiscuous-bridge</code>。</p>
<!--
The common steps to trouble shoot this are as follows:

* Confirm `hairpin-mode` is set to `hairpin-veth` or `promiscuous-bridge`.
You should see something like the below. `hairpin-mode` is set to
`promiscuous-bridge` in the following example.
-->
<p>诊断此类问题的常见步骤如下：</p>
<ul>
<li>
<p>确认 <code>hairpin-mode</code> 被设置为 <code>hairpin-veth</code> 或 <code>promiscuous-bridge</code>。
你应该可以看到下面这样。本例中 <code>hairpin-mode</code> 被设置为 <code>promiscuous-bridge</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ps auxw | grep kubelet
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">root      3392  1.1  0.8 186804 65208 ?        Sl   00:51  11:11 /usr/local/bin/kubelet --enable-debugging-handlers=true --config=/etc/kubernetes/manifests --allow-privileged=True --v=4 --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --configure-cbr0=true --cgroup-root=/ --system-cgroups=/system --hairpin-mode=promiscuous-bridge --runtime-cgroups=/docker-daemon --kubelet-cgroups=/kubelet --babysit-daemons=true --max-pods=110 --serialize-image-pulls=false --outofdisk-transition-frequency=0
</code></pre></li>
</ul>
<!--
* Confirm the effective `hairpin-mode`. To do this, you'll have to look at
kubelet log. Accessing the logs depends on your Node OS. On some OSes it
is a file, such as /var/log/kubelet.log, while other OSes use `journalctl`
to access logs. Please be noted that the effective hairpin mode may not
match `--hairpin-mode` flag due to compatibility. Check if there is any log
lines with key word `hairpin` in kubelet.log. There should be log lines
indicating the effective hairpin mode, like something below.
-->
<ul>
<li>
<p>确认有效的 <code>hairpin-mode</code>。要做到这一点，你必须查看 kubelet 日志。
访问日志取决于节点的操作系统。在一些操作系统上，它是一个文件，如 /var/log/kubelet.log，
而其他操作系统则使用 <code>journalctl</code> 访问日志。请注意，由于兼容性，
有效的 <code>hairpin-mode</code> 可能不匹配 <code>--hairpin-mode</code> 标志。在 kubelet.log
中检查是否有带有关键字 <code>hairpin</code> 的日志行。应该有日志行指示有效的
<code>hairpin-mode</code>，就像下面这样。</p>
<pre tabindex="0"><code class="language-none" data-lang="none">I0629 00:51:43.648698    3252 kubelet.go:380] Hairpin mode set to &quot;promiscuous-bridge&quot;
</code></pre></li>
</ul>
<!--
* If the effective hairpin mode is `hairpin-veth`, ensure the `Kubelet` has
the permission to operate in `/sys` on node. If everything works properly,
you should see something like:
-->
<ul>
<li>
<p>如果有效的发夹模式是 <code>hairpin-veth</code>, 要保证 <code>Kubelet</code> 有操作节点上 <code>/sys</code> 的权限。
如果一切正常，你将会看到如下输出:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> intf in /sys/devices/virtual/net/cbr0/brif/*; <span style="color:#a2f;font-weight:bold">do</span> cat <span style="color:#b8860b">$intf</span>/hairpin_mode; <span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">1
1
1
1
</code></pre></li>
</ul>
<!--
* If the effective hairpin mode is `promiscuous-bridge`, ensure `Kubelet`
has the permission to manipulate linux bridge on node. If `cbr0` bridge is
used and configured properly, you should see:
-->
<ul>
<li>
<p>如果有效的发卡模式是 <code>promiscuous-bridge</code>, 要保证 <code>Kubelet</code> 有操作节点上
Linux 网桥的权限。如果 <code>cbr0</code> 桥正在被使用且被正确设置，你将会看到如下输出:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ifconfig cbr0 |grep PROMISC
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">UP BROADCAST RUNNING PROMISC MULTICAST  MTU:1460  Metric:1
</code></pre></li>
</ul>
<!--
* Seek help if none of above works out.
-->
<ul>
<li>如果以上步骤都不能解决问题，请寻求帮助。</li>
</ul>
<!--
## Seek help

If you get this far, something very strange is happening.  Your Service is
running, has Endpoints, and your Pods are actually serving.  You have DNS
working, and `kube-proxy` does not seem to be misbehaving.  And yet your
Service is not working.  Please let us know what is going on, so we can help
investigate!

Contact us on
[Slack](/docs/tasks/debug-application-cluster/troubleshooting/#slack) or
[Forum](https://discuss.kubernetes.io) or
[GitHub](https://github.com/kubernetes/kubernetes).
-->
<h2 id="寻求帮助">寻求帮助</h2>
<p>如果你走到这一步，那么就真的是奇怪的事情发生了。你的 Service 正在运行，有 Endpoints 存在，
你的 Pods 也确实在提供服务。你的 DNS 正常，<code>iptables</code> 规则已经安装，<code>kube-proxy</code> 看起来也正常。
然而 Service 还是没有正常工作。这种情况下，请告诉我们，以便我们可以帮助调查！</p>
<p>通过
<a href="/zh/docs/tasks/debug-application-cluster/troubleshooting/#slack">Slack</a> 或者
<a href="https://discuss.kubernetes.io">Forum</a> 或者
<a href="https://github.com/kubernetes/kubernetes">GitHub</a>
联系我们。</p>
<h2 id="接下来">接下来</h2>
<!--
Visit [troubleshooting document](/docs/tasks/debug-application-cluster/troubleshooting/)
for more information.
-->
<p>访问<a href="/zh/docs/tasks/debug-application-cluster/troubleshooting/">故障排查文档</a> 获取更多信息。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a070b1250ee142402d492b505a56ca83">10.13 - 调试StatefulSet</h1>
    
	<!-- overview -->
<!--
This task shows you how to debug a StatefulSet.
-->
<p>此任务展示如何调试 StatefulSet。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster.
* You should have a StatefulSet running that you want to investigate.
-->
<ul>
<li>你需要有一个 Kubernetes 集群，已配置好的 kubectl 命令行工具与你的集群进行通信。</li>
<li>你应该有一个运行中的 StatefulSet，以便用于调试。</li>
</ul>
<!-- steps -->
<!--
## Debugging a StatefulSet

In order to list all the pods which belong to a StatefulSet, which have a label `app=myapp` set on them,
you can use the following:
-->
<h2 id="debuggin-a-statefulset">调试 StatefulSet  </h2>
<p>StatefulSet 在创建 Pod 时为其设置了 <code>app=myapp</code> 标签，列出仅属于某 StatefulSet
的所有 Pod 时，可以使用以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>myapp
</code></pre></div><!--
If you find that any Pods listed are in `Unknown` or `Terminating` state for an extended period of time,
refer to the [Deleting StatefulSet Pods](/docs/tasks/run-application/delete-stateful-set/) task for
instructions on how to deal with them.
You can debug individual Pods in a StatefulSet using the
[Debugging Pods](/docs/tasks/debug-application-cluster/debug-pod-replication-controller/) guide.
-->
<p>如果你发现列出的任何 Pod 长时间处于 <code>Unknown</code> 或 <code>Terminating</code> 状态，请参阅
<a href="/zh/docs/tasks/run-application/delete-stateful-set/">删除 StatefulSet Pods</a>
了解如何处理它们的说明。
你可以参考<a href="/zh/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">调试 Pods</a>
来调试 StatefulSet 中的各个 Pod。</p>
<h2 id="接下来">接下来</h2>
<!--
Learn more about [debugging an init-container](/docs/tasks/debug-application-cluster/debug-init-containers/).
-->
<ul>
<li>进一步了解如何<a href="/zh/docs/tasks/debug-application-cluster/debug-init-containers/">调试 Init 容器</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c0ec963f381296ca26b839cdf0a6f242">10.14 - 调试运行中的 Pod</h1>
    
	<!-- overview -->
<!--
This page explains how to debug Pods running (or crashing) on a Node.
-->
<p>本页解释如何在节点上调试运行中（或崩溃）的 Pod。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* Your <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> should already be
  scheduled and running. If your Pod is not yet running, start with [Troubleshoot
  Applications](/docs/tasks/debug-application-cluster/debug-application/).
* For some of the advanced debugging steps you need to know on which Node the
  Pod is running and have shell access to run commands on that Node. You don't
  need that access to run the standard debug steps that use `kubectl`.
-->
<ul>
<li>
<p>你的 <a class='glossary-tooltip' title='Pod 表示您的集群上一组正在运行的容器。' data-toggle='tooltip' data-placement='top' href='/docs/concepts/workloads/pods/pod-overview/' target='_blank' aria-label='Pod'>Pod</a> 应该已经被调度并正在运行中，
如果你的 Pod 还没有运行，请参阅
<a href="/zh/docs/tasks/debug-application-cluster/debug-application/">应用问题排查</a>。</p>
</li>
<li>
<p>对于一些高级调试步骤，你应该知道 Pod 具体运行在哪个节点上，在该节点上有权限去运行一些命令。
你不需要任何访问权限就可以使用 <code>kubectl</code> 去运行一些标准调试步骤。</p>
</li>
</ul>
<!-- steps -->
<!--
## Examining pod logs {#examine-pod-logs}

First, look at the logs of the affected container:

```shell
kubectl logs ${POD_NAME} ${CONTAINER_NAME}
```

If your container has previously crashed, you can access the previous container's crash log with:

```shell
kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME}
```
-->
<h2 id="examine-pod-logs">检查 Pod 的日志</h2>
<p>首先，查看受到影响的容器的日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">POD_NAME</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONTAINER_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><p>如果你的容器之前崩溃过，你可以通过下面命令访问之前容器的崩溃日志：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs --previous <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">POD_NAME</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONTAINER_NAME</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><!--
## Debugging with container exec {#container-exec}

```shell
kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} ... ${ARGN}
```

As an example, to look at the logs from a running Cassandra pod, you might run

```shell
kubectl exec cassandra -- cat /var/log/cassandra/system.log
```

You can run a shell that's connected to your terminal using the `-i` and `-t`
arguments to `kubectl exec`, for example:

```shell
kubectl exec -it cassandra -- sh
```

For more details, see [Get a Shell to a Running Container](
/docs/tasks/debug-application-cluster/get-shell-running-container/).
-->
<h2 id="container-exec">使用容器 exec 进行调试</h2>
<p>如果 <a class='glossary-tooltip' title='镜像是保存的容器实例，它打包了应用运行所需的一组软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-image' target='_blank' aria-label='容器镜像'>容器镜像</a> 包含调试程序，
比如从 Linux 和 Windows 操作系统基础镜像构建的镜像，你可以使用 <code>kubectl exec</code> 命令
在特定的容器中运行一些命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">POD_NAME</span><span style="color:#b68;font-weight:bold">}</span> -c <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CONTAINER_NAME</span><span style="color:#b68;font-weight:bold">}</span> -- <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">CMD</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ARG1</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ARG2</span><span style="color:#b68;font-weight:bold">}</span> ... <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">ARGN</span><span style="color:#b68;font-weight:bold">}</span>
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> <code>-c ${CONTAINER_NAME}</code> 是可选择的。如果Pod中仅包含一个容器，就可以忽略它。</div>
</blockquote>
<p>例如，要查看正在运行的 Cassandra pod中的日志，可以运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> cassandra -- cat /var/log/cassandra/system.log
</code></pre></div><p>你可以在 <code>kubectl exec</code> 命令后面加上 <code>-i</code> 和 <code>-t</code> 来运行一个连接到你的终端的 Shell，比如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it cassandra -- sh
</code></pre></div><p>若要了解更多内容，可查看<a href="/zh/docs/tasks/debug-application-cluster/get-shell-running-container/">获取正在运行容器的 Shell</a>。</p>
<!--
## Debugging with an ephemeral debug container {#ephemeral-container}






<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>


<a class='glossary-tooltip' title='您可以在 Pod 中临时运行的一种容器类型' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/pods/ephemeral-containers/' target='_blank' aria-label='Ephemeral containers'>Ephemeral containers</a>
are useful for interactive troubleshooting when `kubectl exec` is insufficient
because a container has crashed or a container image doesn't include debugging
utilities, such as with [distroless images](
https://github.com/GoogleContainerTools/distroless).
-->
<h2 id="ephemeral-container">使用临时调试容器来进行调试</h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code>
</div>

<p>当由于容器崩溃或容器镜像不包含调试程序（例如<a href="https://github.com/GoogleContainerTools/distroless">无发行版镜像</a>等）
而导致 <code>kubectl exec</code> 无法运行时，<a class='glossary-tooltip' title='您可以在 Pod 中临时运行的一种容器类型' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/pods/ephemeral-containers/' target='_blank' aria-label='临时容器'>临时容器</a>对于排除交互式故障很有用。</p>
<!--
### Example debugging using ephemeral containers {#ephemeral-container-example}

The examples in this section require the `EphemeralContainers` [feature gate](
/docs/reference/command-line-tools-reference/feature-gates/) enabled in your
cluster and `kubectl` version v1.22 or later.

You can use the `kubectl debug` command to add ephemeral containers to a
running Pod. First, create a pod for the example:

```shell
kubectl run ephemeral-demo --image=k8s.gcr.io/pause:3.1 --restart=Never
```

This section use the `pause` container image in examples because it does not
contain debugging utilities, but this method works with all container
images.
-->
<h2 id="ephemeral-container-example">使用临时容器来调试的例子</h2>
<blockquote class="note callout">
  <div><strong>说明：</strong> 本示例需要你的集群已经开启 <code>EphemeralContainers</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>，
<code>kubectl</code> 版本为 v1.22 或者更高。</div>
</blockquote>
<p>你可以使用 <code>kubectl debug</code> 命令来给正在运行中的 Pod 增加一个临时容器。
首先，像示例一样创建一个 pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run ephemeral-demo --image<span style="color:#666">=</span>k8s.gcr.io/pause:3.1 --restart<span style="color:#666">=</span>Never
</code></pre></div><blockquote class="note callout">
  <div><strong>说明：</strong> 本节示例中使用 <code>pause</code> 容器镜像，因为它不包含调试程序，但是这个方法适用于所有容器镜像。</div>
</blockquote>
<!--
If you attempt to use `kubectl exec` to create a shell you will see an error
because there is no shell in this container image.

```shell
kubectl exec -it ephemeral-demo -- sh
```

```
OCI runtime exec failed: exec failed: container_linux.go:346: starting container process caused "exec: \"sh\": executable file not found in $PATH": unknown
```

You can instead add a debugging container using `kubectl debug`. If you
specify the `-i`/`--interactive` argument, `kubectl` will automatically attach
to the console of the Ephemeral Container.

```shell
kubectl debug -it ephemeral-demo --image=busybox --target=ephemeral-demo
```

```
Defaulting debug container name to debugger-8xzrl.
If you don't see a command prompt, try pressing enter.
/ #
```
-->
<p>如果你尝试使用 <code>kubectl exec</code> 来创建一个 shell，你将会看到一个错误，因为这个容器镜像中没有 shell。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it ephemeral-demo -- sh
</code></pre></div><pre tabindex="0"><code>OCI runtime exec failed: exec failed: container_linux.go:346: starting container process caused &quot;exec: \&quot;sh\&quot;: executable file not found in $PATH&quot;: unknown
</code></pre><p>你可以改为使用 <code>kubectl debug</code> 添加调试容器。
如果你指定 <code>-i</code> 或者 <code>--interactive</code> 参数，<code>kubectl</code> 将自动挂接到临时容器的控制台。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl debug -it ephemeral-demo --image<span style="color:#666">=</span>busybox --target<span style="color:#666">=</span>ephemeral-demo
</code></pre></div><pre tabindex="0"><code>Defaulting debug container name to debugger-8xzrl.
If you don't see a command prompt, try pressing enter.
/ #
</code></pre><!--
This command adds a new busybox container and attaches to it. The `--target`
parameter targets the process namespace of another container. It's necessary
here because `kubectl run` does not enable [process namespace sharing](
/docs/tasks/configure-pod-container/share-process-namespace/) in the pod it
creates.

The `--target` parameter must be supported by the <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='Container Runtime'>Container Runtime</a>. When not supported,
the Ephemeral Container may not be started, or it may be started with an
isolated process namespace so that `ps` does not reveal processes in other containers.

You can view the state of the newly created ephemeral container using `kubectl describe`:
-->
<p>此命令添加一个新的 busybox 容器并将其挂接到该容器。<code>--target</code> 参数指定另一个容器的进程命名空间。
这是必需的，因为 <code>kubectl run</code> 不能在它创建的pod中启用
<a href="/zh/docs/tasks/configure-pod-container/share-process-namespace/">共享进程命名空间</a>。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <a class='glossary-tooltip' title='容器运行时是负责运行容器的软件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/setup/production-environment/container-runtimes' target='_blank' aria-label='容器运行时'>容器运行时</a>必须支持<code>--target</code>参数。
如果不支持，则临时容器可能不会启动，或者可能使用隔离的进程命名空间启动，
以便 <code>ps</code> 不显示其他容器内的进程。</div>
</blockquote>
<p>你可以使用 <code>kubectl describe</code> 查看新创建的临时容器的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod ephemeral-demo
</code></pre></div><pre tabindex="0"><code>...
Ephemeral Containers:
  debugger-8xzrl:
    Container ID:   docker://b888f9adfd15bd5739fefaa39e1df4dd3c617b9902082b1cfdc29c4028ffb2eb
    Image:          busybox
    Image ID:       docker-pullable://busybox@sha256:1828edd60c5efd34b2bf5dd3282ec0cc04d47b2ff9caa0b6d4f07a21d1c08084
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Wed, 12 Feb 2020 14:25:42 +0100
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:         &lt;none&gt;
...
</code></pre><!--
Use `kubectl delete` to remove the Pod when you're finished:
-->
<p>使用 <code>kubectl delete</code> 来移除已经结束掉的 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod ephemeral-demo
</code></pre></div><!--
## Debugging using a copy of the Pod
-->
<h2 id="通过-pod-副本调试">通过 Pod 副本调试</h2>
<!--
Sometimes Pod configuration options make it difficult to troubleshoot in certain
situations. For example, you can't run `kubectl exec` to troubleshoot your
container if your container image does not include a shell or if your application
crashes on startup. In these situations you can use `kubectl debug` to create a
copy of the Pod with configuration values changed to aid debugging.
-->
<p>有些时候 Pod 的配置参数使得在某些情况下很难执行故障排查。
例如，在容器镜像中不包含 shell 或者你的应用程序在启动时崩溃的情况下，
就不能通过运行 <code>kubectl exec</code> 来排查容器故障。
在这些情况下，你可以使用 <code>kubectl debug</code> 来创建 Pod 的副本，通过更改配置帮助调试。</p>
<!--
### Copying a Pod while adding a new container
-->
<h3 id="在添加新的容器时创建-pod-副本">在添加新的容器时创建 Pod 副本</h3>
<!--
Adding a new container can be useful when your application is running but not
behaving as you expect and you'd like to add additional troubleshooting
utilities to the Pod.
-->
<p>当应用程序正在运行但其表现不符合预期时，你会希望在 Pod 中添加额外的调试工具，
这时添加新容器是很有用的。</p>
<!--
For example, maybe your application's container images are built on `busybox`
but you need debugging utilities not included in `busybox`. You can simulate
this scenario using `kubectl run`:
-->
<p>例如，应用的容器镜像是建立在 <code>busybox</code> 的基础上，
但是你需要 <code>busybox</code> 中并不包含的调试工具。
你可以使用 <code>kubectl run</code> 模拟这个场景:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run myapp --image<span style="color:#666">=</span>busybox --restart<span style="color:#666">=</span>Never -- sleep 1d
</code></pre></div><!--
Run this command to create a copy of `myapp` named `myapp-debug` that adds a
new Ubuntu container for debugging:
-->
<p>通过运行以下命令，建立 <code>myapp</code> 的一个名为 <code>myapp-debug</code> 的副本，
新增了一个用于调试的 Ubuntu 容器，</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl debug myapp -it --image<span style="color:#666">=</span>ubuntu --share-processes --copy-to<span style="color:#666">=</span>myapp-debug
</code></pre></div><pre tabindex="0"><code>Defaulting debug container name to debugger-w7xmf.
If you don't see a command prompt, try pressing enter.
root@myapp-debug:/#
</code></pre><!--
<blockquote class="note callout">
  <div><strong>说明：</strong> <ul>
<li><code>kubectl debug</code> automatically generates a container name if you don't choose
one using the <code>--container</code> flag.</li>
<li>The <code>-i</code> flag causes <code>kubectl debug</code> to attach to the new container by
default.  You can prevent this by specifying <code>--attach=false</code>. If your session
becomes disconnected you can reattach using <code>kubectl attach</code>.</li>
<li>The <code>--share-processes</code> allows the containers in this Pod to see processes
from the other containers in the Pod. For more information about how this
works, see <a href="/docs/tasks/configure-pod-container/share-process-namespace/">Share Process Namespace between Containers in a Pod</a>.</li>
</ul>
</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <ul>
<li>如果你没有使用 <code>--container</code> 指定新的容器名，<code>kubectl debug</code> 会自动生成的。</li>
<li>默认情况下，<code>-i</code> 标志使 <code>kubectl debug</code> 附加到新容器上。
你可以通过指定 <code>--attach=false</code> 来防止这种情况。
如果你的会话断开连接，你可以使用 <code>kubectl attach</code> 重新连接。</li>
<li><code>--share-processes</code> 允许在此 Pod 中的其他容器中查看该容器的进程。
参阅<a href="/zh/docs/tasks/configure-pod-container/share-process-namespace/">在 Pod 中的容器之间共享进程命名空间</a>
获取更多信息。</li>
</ul>
</div>
</blockquote>
<!--
Don't forget to clean up the debugging Pod when you're finished with it:
-->
<p>不要忘了清理调试 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod myapp myapp-debug
</code></pre></div><!--
### Copying a Pod while changing its command
-->
<h3 id="在改变-pod-命令时创建-pod-副本">在改变 Pod 命令时创建 Pod 副本</h3>
<!--
Sometimes it's useful to change the command for a container, for example to
add a debugging flag or because the application is crashing.
-->
<p>有时更改容器的命令很有用，例如添加调试标志或因为应用崩溃。</p>
<!--
To simulate a crashing application, use `kubectl run` to create a container
that immediately exits:
-->
<p>为了模拟应用崩溃的场景，使用 <code>kubectl run</code> 命令创建一个立即退出的容器：</p>
<pre tabindex="0"><code>kubectl run --image=busybox myapp -- false
</code></pre><!--
You can see using `kubectl describe pod myapp` that this container is crashing:
-->
<p>使用 <code>kubectl describe pod myapp</code> 命令，你可以看到容器崩溃了：</p>
<pre tabindex="0"><code>Containers:
  myapp:
    Image:         busybox
    ...
    Args:
      false
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
</code></pre><!--
You can use `kubectl debug` to create a copy of this Pod with the command
changed to an interactive shell:
-->
<p>你可以使用 <code>kubectl debug</code> 命令创建该 Pod 的一个副本，
在该副本中命令改变为交互式 shell：</p>
<pre tabindex="0"><code>kubectl debug myapp -it --copy-to=myapp-debug --container=myapp -- sh
</code></pre><pre tabindex="0"><code>If you don't see a command prompt, try pressing enter.
/ #
</code></pre><!--
Now you have an interactive shell that you can use to perform tasks like
checking filesystem paths or running the container command manually.
-->
<p>现在你有了一个可以执行类似检查文件系统路径或者手动运行容器命令的交互式 shell。</p>
<!--
<blockquote class="note callout">
  <div><strong>说明：</strong> <ul>
<li>To change the command of a specific container you must
specify its name using <code>--container</code> or <code>kubectl debug</code> will instead
create a new container to run the command you specified.</li>
<li>The <code>-i</code> flag causes <code>kubectl debug</code> to attach to the container by default.
You can prevent this by specifying <code>--attach=false</code>. If your session becomes
disconnected you can reattach using <code>kubectl attach</code>.</li>
</ul>
</div>
</blockquote>
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <ul>
<li>要更改指定容器的命令，你必须用 <code>--container</code> 命令指定容器的名字，
否则 <code>kubectl debug</code> 将建立一个新的容器运行你指定的命令。</li>
<li>默认情况下，标志 <code>-i</code> 使 <code>kubectl debug</code> 附加到容器。
你可通过指定 <code>--attach=false</code> 来防止这种情况。
如果你的断开连接，可以使用 <code>kubectl attach</code> 重新连接。</li>
</ul>
</div>
</blockquote>
<!--
Don't forget to clean up the debugging Pod when you're finished with it:
-->
<p>不要忘了清理调试 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod myapp myapp-debug
</code></pre></div><!--
### Copying a Pod while changing container images

In some situations you may want to change a misbehaving Pod from its normal
production container images to an image containing a debugging build or
additional utilities.

As an example, create a Pod using `kubectl run`:
-->
<h3 id="在更改容器镜像时创建-pod-副本">在更改容器镜像时创建 Pod 副本</h3>
<p>在某些情况下，你可能想从正常生产容器镜像中
把行为异常的 Pod 改变为包含调试版本或者附加应用的镜像。</p>
<p>下面的例子，用 <code>kubectl run</code>创建一个 Pod：</p>
<pre tabindex="0"><code>kubectl run myapp --image=busybox --restart=Never -- sleep 1d
</code></pre><!--
Now use `kubectl debug` to make a copy and change its container image
to `ubuntu`:
-->
<p>现在可以使用 <code>kubectl debug</code>  创建一个副本
并改变容器镜像为 <code>ubuntu</code>：</p>
<pre tabindex="0"><code>kubectl debug myapp --copy-to=myapp-debug --set-image=*=ubuntu
</code></pre><!--
The syntax of `--set-image` uses the same `container_name=image` syntax as
`kubectl set image`. `*=ubuntu` means change the image of all containers
to `ubuntu`.

Don't forget to clean up the debugging Pod when you're finished with it:
-->
<p><code>--set-image</code> 与 <code>container_name=image</code> 使用相同的 <code>kubectl set image</code> 语法。
<code>*=ubuntu</code> 表示把所有容器的镜像改为 <code>ubuntu</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete pod myapp myapp-debug
</code></pre></div><!--
## Debugging via a shell on the node {#node-shell-session}

If none of these approaches work, you can find the host machine that the pod is
running on and SSH into that host, but this should generally not be necessary
given tools in the Kubernetes API. Therefore, if you find yourself needing to
ssh into a machine, please file a feature request on GitHub describing your use
case and why these tools are insufficient.
-->
<h2 id="node-shell-session">在节点上通过 shell 来调试</h2>
<p>如果这些方法都不起作用，你可以找到运行 Pod 的主机并通过 SSH 进入该主机，
但是如果使用 Kubernetes API 中的工具，则通常不需要这样做。
因此，如果你发现自己需要使用 ssh 进入主机，请在GitHub 上提交功能请求，
以描述你的用例以及这些工具不足的原因。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-96b25d30e732385047272b84d3c4188f">10.15 - 资源指标管道</h1>
    
	<!--
reviewers:
- fgrzadkowski
- piosz
title: Resource metrics pipeline
content_type: concept
-->
<!-- overview -->
<!--
Resource usage metrics, such as container CPU and memory usage,
are available in Kubernetes through the Metrics API. These metrics can be accessed either directly
by the user with the `kubectl top` command, or by a controller in the cluster, for example
Horizontal Pod Autoscaler, to make decisions.
-->
<p>资源使用指标，例如容器 CPU 和内存使用率，可通过 Metrics API 在 Kubernetes 中获得。
这些指标可以直接被用户访问，比如使用 <code>kubectl top</code> 命令行，或者被集群中的控制器
（例如 Horizontal Pod Autoscalers) 使用来做决策。</p>
<!-- body -->
<!--
## The Metrics API

Through the Metrics API, you can get the amount of resource currently used
by a given node or a given pod. This API doesn't store the metric values,
so it's not possible, for example, to get the amount of resources used by a
given node 10 minutes ago.
-->
<h2 id="the-metrics-api">Metrics API  </h2>
<p>通过 Metrics API，你可以获得指定节点或 Pod 当前使用的资源量。
此 API 不存储指标值，因此想要获取某个指定节点 10 分钟前的
资源使用量是不可能的。</p>
<!--
The API is no different from any other API:
-->
<p>此 API 与其他 API 没有区别：</p>
<!--
- it is discoverable through the same endpoint as the other Kubernetes APIs under the path: `/apis/metrics.k8s.io/`
- it offers the same security, scalability, and reliability guarantees
-->
<ul>
<li>此 API 和其它 Kubernetes API 一起位于同一端点（endpoint）之下且可发现，
路径为 <code>/apis/metrics.k8s.io/</code></li>
<li>它具有相同的安全性、可扩展性和可靠性保证</li>
</ul>
<!--
The API is defined in [k8s.io/metrics](https://github.com/kubernetes/metrics/blob/master/pkg/apis/metrics/v1beta1/types.go)
repository. You can find more information about the API there.
-->
<p>Metrics API 在 <a href="https://github.com/kubernetes/metrics/blob/master/pkg/apis/metrics/v1beta1/types.go">k8s.io/metrics</a>
仓库中定义。你可以在那里找到有关 Metrics API 的更多信息。</p>
<!--
The API requires metrics server to be deployed in the cluster. Otherwise it will be not available.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> Metrics API 需要在集群中部署 Metrics Server。否则它将不可用。</div>
</blockquote>
<!--
## Measuring Resource Usage

### CPU

CPU is reported as the average usage, in
[CPU cores](/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu),
over a period of time. This value is derived by taking a rate over a cumulative CPU counter
provided by the kernel (in both Linux and Windows kernels).
The kubelet chooses the window for the rate calculation.
-->
<h2 id="measuring-resource-usage">度量资源用量  </h2>
<h3 id="cpu">CPU</h3>
<p>CPU 用量按其一段时间内的平均值统计，单位为
<a href="/zh/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu">CPU 核</a>。
此度量值通过在内核（包括 Linux 和 Windows）提供的累积 CPU 计数器乘以一个系数得到。
<code>kubelet</code> 组件负责选择计算系数所使用的窗口大小。</p>
<!--
### Memory

Memory is reported as the working set, in bytes, at the instant the metric was collected.
In an ideal world, the "working set" is the amount of memory in-use that cannot be freed under memory pressure.
However, calculation of the working set varies by host OS, and generally makes heavy use of heuristics to produce an estimate.
It includes all anonymous (non-file-backed) memory since Kubernetes does not support swap.
The metric typically also includes some cached (file-backed) memory, because the host OS cannot always reclaim such pages.
-->
<h3 id="memory">内存 </h3>
<p>内存用量按工作集（Working Set）的大小字节数统计，其数值为收集度量值的那一刻的内存用量。
如果一切都很理想化，“工作集” 是任务在使用的内存总量，该内存是不可以在内存压力较大
的情况下被释放的。
不过，具体的工作集计算方式取决于宿主 OS，有很大不同，且通常都大量使用启发式
规则来给出一个估计值。
其中包含所有匿名内存使用（没有后台文件提供存储者），因为 Kubernetes 不支持交换分区。
度量值通常包含一些高速缓存（有后台文件提供存储）内存，因为宿主操作系统并不是总能
回收这些页面。</p>
<!--
## Metrics Server

[Metrics Server](https://github.com/kubernetes-sigs/metrics-server) is a cluster-wide aggregator of resource usage data.
By default, it is deployed in clusters created by `kube-up.sh` script
as a Deployment object. If you use a different Kubernetes setup mechanism you can deploy it using the provided
[deployment components.yaml](https://github.com/kubernetes-sigs/metrics-server/releases) file.
-->
<h2 id="metrics-server">Metrics 服务器   </h2>
<p><a href="https://github.com/kubernetes-sigs/metrics-server">Metrics 服务器</a>
是集群范围资源用量数据的聚合器。
默认情况下，在由 <code>kube-up.sh</code> 脚本创建的集群中会以 Deployment 的形式被部署。
如果你使用其他 Kubernetes 安装方法，则可以使用提供的
<a href="https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy">部署组件 components.yaml</a>
来部署。</p>
<!--
Metric server collects metrics from the Summary API, exposed by
[Kubelet](/docs/reference/command-line-tools-reference/kubelet/) on each node, and is registered with the main API server via
[Kubernetes aggregator](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
-->
<p>Metric 服务器从每个节点上的 <a href="/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
公开的 Summary API 中采集指标信息。
该 API 通过
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Kubernetes 聚合器</a>
注册到主 API 服务器上。</p>
<!--
Learn more about the metrics server in
[the design doc](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/metrics-server.md).
-->
<p>在<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/metrics-server.md">设计文档</a>
中可以了解到有关 Metrics 服务器的更多信息。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9e6e1b706f11386fe2c4b4ffda1409e4">10.16 - 资源监控工具</h1>
    
	<!--
reviewers:
- mikedanese
content_type: concept
title: Tools for Monitoring Resources
-->
<!-- overview -->
<!--
To scale an application and provide a reliable service, you need to
understand how the application behaves when it is deployed. You can examine
application performance in a Kubernetes cluster by examining the containers,
[pods](/docs/concepts/workloads/pods/),
[services](/docs/concepts/services-networking/service/), and
the characteristics of the overall cluster. Kubernetes provides detailed
information about an application's resource usage at each of these levels.
This information allows you to evaluate your application's performance and
where bottlenecks can be removed to improve overall performance.
-->
<p>要扩展应用程序并提供可靠的服务，你需要了解应用程序在部署时的行为。
你可以通过检测容器检查 Kubernetes 集群中的应用程序性能，
<a href="/zh/docs/concepts/workloads/pods">Pods</a>,
<a href="/zh/docs/concepts/services-networking/service/">服务</a>
和整个集群的特征。
Kubernetes 在每个级别上提供有关应用程序资源使用情况的详细信息。
此信息使你可以评估应用程序的性能，以及在何处可以消除瓶颈以提高整体性能。</p>
<!-- body -->
<!--
In Kubernetes, application monitoring does not depend on a single monitoring solution.
On new clusters, you can use [resource metrics](#resource-metrics-pipeline) or
[full metrics](#full-metrics-pipeline) pipelines to collect monitoring statistics.
-->
<p>在 Kubernetes 中，应用程序监控不依赖单个监控解决方案。
在新集群上，你可以使用<a href="#resource-metrics-pipeline">资源度量</a>或
<a href="#full-metrics-pipeline">完整度量</a>管道来收集监视统计信息。</p>
<!--
## Resource metrics pipeline

The resource metrics pipeline provides a limited set of metrics related to
cluster components such as the
[Horizontal Pod Autoscaler](/docs/tasks/run-application/horizontal-pod-autoscale)
controller, as well as the `kubectl top` utility.
These  metrics are collected by the lightweight, short-term, in-memory 
[metrics-server](https://github.com/kubernetes-sigs/metrics-server) and
 are exposed via the `metrics.k8s.io` API. 
-->
<h2 id="resource-metrics-pipeline">资源度量管道 </h2>
<p>资源指标管道提供了一组与集群组件，例如
<a href="/zh/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a>
控制器以及 <code>kubectl top</code> 实用程序相关的有限度量。
这些指标是由轻量级的、短期、内存存储的
<a href="https://github.com/kubernetes-sigs/metrics-server">metrics-server</a> 收集的，
通过 <code>metrics.k8s.io</code> 公开。</p>
<!--
metrics-server discovers all nodes on the cluster and 
queries each node's 
[kubelet](/docs/reference/command-line-tools-reference/kubelet/) for CPU and 
memory usage. The kubelet acts as a bridge between the Kubernetes master and 
the nodes, managing the pods and containers running on a machine. The kubelet 
translates each pod into its constituent containers and fetches individual 
container usage statistics from the container runtime through the container 
runtime interface. The kubelet fetches this information from the integrated 
cAdvisor for the legacy Docker integration.  It then exposes the aggregated pod 
resource usage statistics through the metrics-server Resource Metrics API.
This API is served at `/metrics/resource/v1beta1` on the kubelet's authenticated and 
read-only ports. 
-->
<p>度量服务器发现集群中的所有节点，并且查询每个节点的
<a href="/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
以获取 CPU 和内存使用情况。
Kubelet 充当 Kubernetes 主节点与节点之间的桥梁，管理机器上运行的 Pod 和容器。
kubelet 将每个 Pod 转换为其组成的容器，并在容器运行时通过容器运行时接口
获取各个容器使用情况统计信息。
kubelet 从集成的 cAdvisor 获取此信息，以进行旧式 Docker 集成。
然后，它通过 metrics-server Resource Metrics API 公开聚合的 pod 资源使用情况统计信息。
该 API 在 kubelet 的经过身份验证和只读的端口上的 <code>/metrics/resource/v1beta1</code> 中提供。</p>
<!--
## Full metrics pipeline

A full metrics pipeline gives you access to richer metrics. Kubernetes can
respond to these metrics by  automatically scaling or adapting the cluster
based on its current state, using mechanisms such as the Horizontal Pod
Autoscaler. The monitoring pipeline fetches metrics from the kubelet and
then exposes them to Kubernetes via an adapter by implementing either the
`custom.metrics.k8s.io` or `external.metrics.k8s.io` API. 
-->
<h2 id="full-metrics-pipeline">完整度量管道 </h2>
<p>一个完整度量管道可以让你访问更丰富的度量。
Kubernetes 还可以根据集群的当前状态，使用 Pod 水平自动扩缩器等机制，
通过自动调用扩展或调整集群来响应这些度量。
监控管道从 kubelet 获取度量值，然后通过适配器将它们公开给 Kubernetes，
方法是实现 <code>custom.metrics.k8s.io</code> 或 <code>external.metrics.k8s.io</code> API。</p>
<!--
[Prometheus](https://prometheus.io), a CNCF project, can natively monitor Kubernetes, nodes, and Prometheus itself.
Full metrics pipeline projects that are not part of the CNCF are outside the scope of Kubernetes documentation.  
-->
<p><a href="https://prometheus.io">Prometheus</a> 是一个 CNCF 项目，可以原生监控 Kubernetes、
节点和 Prometheus 本身。
完整度量管道项目不属于 CNCF 的一部分，不在 Kubernetes 文档的范围之内。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-47290c80fb8b00accec6729f3da49734">10.17 - 集群故障排查</h1>
    
	<!--
reviewers:
- davidopp
title: Troubleshoot Clusters
content_type: concept
-->
<!-- overview -->
<!--
This doc is about cluster troubleshooting; we assume you have already ruled out your application as the root cause of the
problem you are experiencing. See
the [application troubleshooting guide](/docs/tasks/debug-application-cluster/debug-application) for tips on application debugging.
You may also visit [troubleshooting document](/docs/tasks/debug-application-cluster/troubleshooting/) for more information.
-->
<p>本篇文档是介绍集群故障排查的；我们假设对于你碰到的问题，你已经排除了是由应用程序造成的。
对于应用的调试，请参阅
<a href="/zh/docs/tasks/debug-application-cluster/debug-application/">应用故障排查指南</a>。
你也可以访问<a href="/zh/docs/tasks/debug-application-cluster/troubleshooting/">故障排查</a>
来获取更多的信息。</p>
<!-- body -->
<!--
## Listing your cluster

The first thing to debug in your cluster is if your nodes are all registered correctly.

Run
-->
<h2 id="列举集群节点">列举集群节点</h2>
<p>调试的第一步是查看所有的节点是否都已正确注册。</p>
<p>运行</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes
</code></pre></div><!--
And verify that all of the nodes you expect to see are present and that they are all in the `Ready` state.

To get detailed information about the overall health of your cluster, you can run:
-->
<p>验证你所希望看见的所有节点都能够显示出来，并且都处于 <code>Ready</code> 状态。</p>
<p>为了了解你的集群的总体健康状况详情，你可以运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info dump
</code></pre></div><!--
## Looking at logs

For now, digging deeper into the cluster requires logging into the relevant machines.  Here are the locations
of the relevant log files.  (note that on systemd-based systems, you may need to use `journalctl` instead)
-->
<h2 id="查看日志">查看日志</h2>
<p>到这里，挖掘出集群更深层的信息就需要登录到相关的机器上。下面是相关日志文件所在的位置。
（注意，对于基于 systemd 的系统，你可能需要使用<code>journalctl</code>）。</p>
<!--
### Master

   * `/var/log/kube-apiserver.log` - API Server, responsible for serving the API
   * `/var/log/kube-scheduler.log` - Scheduler, responsible for making scheduling decisions
   * `/var/log/kube-controller-manager.log` - Controller that manages replication controllers
-->
<h3 id="主控节点">主控节点</h3>
<ul>
<li><code>/var/log/kube-apiserver.log</code> - API 服务器, 提供API服务</li>
<li><code>/var/log/kube-scheduler.log</code> - 调度器, 负责产生调度决策</li>
<li><code>/var/log/kube-controller-manager.log</code> - 管理副本控制器的控制器</li>
</ul>
<!--
### Worker Nodes

* `/var/log/kubelet.log` - Kubelet, responsible for running containers on the node
* `/var/log/kube-proxy.log` - Kube Proxy, responsible for service load balancing
-->
<h3 id="工作节点">工作节点</h3>
<ul>
<li><code>/var/log/kubelet.log</code> - <code>kubelet</code>，负责在节点运行容器</li>
<li><code>/var/log/kube-proxy.log</code> - <code>kube-proxy</code>, 负责服务的负载均衡</li>
</ul>
<!--
## A general overview of cluster failure modes

This is an incomplete list of things that could go wrong, and how to adjust your cluster setup to mitigate the problems.
-->
<h2 id="集群故障模式的一般性概述">集群故障模式的一般性概述</h2>
<p>下面是一个不完整的列表，列举了一些可能的出错场景，以及通过调整集群配置来解决相关问题的方法。</p>
<!--
### Root causes:

  - VM(s) shutdown
  - Network partition within cluster, or between cluster and users
  - Crashes in Kubernetes software
  - Data loss or unavailability of persistent storage (e.g. GCE PD or AWS EBS volume)
  - Operator error, for example misconfigured Kubernetes software or application software
-->
<h3 id="根本原因">根本原因</h3>
<ul>
<li>VM(s) 关机</li>
<li>集群之间，或者集群和用户之间网络分裂</li>
<li>Kubernetes 软件本身崩溃</li>
<li>数据丢失或者持久化存储不可用（如：GCE PD 或 AWS EBS 卷）</li>
<li>操作错误，如：Kubernetes 或者应用程序配置错误</li>
</ul>
<!--
### Specific scenarios:

  - Apiserver VM shutdown or apiserver crashing
    - Results
      - unable to stop, update, or start new pods, services, replication controller
      - existing pods and services should continue to work normally, unless they depend on the Kubernetes API
  - Apiserver backing storage lost
    - Results
      - apiserver should fail to come up
      - kubelets will not be able to reach it but will continue to run the same pods and provide the same service proxying
      - manual recovery or recreation of apiserver state necessary before apiserver is restarted
-->
<h3 id="具体情况">具体情况:</h3>
<ul>
<li>API 服务器所在的 VM 关机或者 API 服务器崩溃
<ul>
<li>结果
<ul>
<li>不能停止、更新或者启动新的 Pod、服务或副本控制器</li>
<li>现有的 Pod 和服务在不依赖 Kubernetes API 的情况下应该能继续正常工作</li>
</ul>
</li>
</ul>
</li>
<li>API 服务器的后端存储丢失
<ul>
<li>结果
<ul>
<li>API 服务器应该不能启动</li>
<li>kubelet 将不能访问 API 服务器，但是能够继续运行之前的 Pod 和提供相同的服务代理</li>
<li>在 API 服务器重启之前，需要手动恢复或者重建 API 服务器的状态</li>
</ul>
</li>
</ul>
</li>
</ul>
<!--
  - Supporting services (node controller, replication controller manager, scheduler, etc) VM shutdown or crashes
    - currently those are colocated with the apiserver, and their unavailability has similar consequences as apiserver
    - in future, these will be replicated as well and may not be co-located
    - they do not have their own persistent state
  - Individual node (VM or physical machine) shuts down
    - Results
      - pods on that Node stop running
  - Network partition
    - Results
      - partition A thinks the nodes in partition B are down; partition B thinks the apiserver is down. (Assuming the master VM ends up in partition A.)
-->
<ul>
<li>Kubernetes 服务组件（节点控制器、副本控制器管理器、调度器等）所在的 VM 关机或者崩溃
<ul>
<li>当前，这些控制器是和 API 服务器在一起运行的，它们不可用的现象是与 API 服务器类似的</li>
<li>将来，这些控制器也会复制为多份，并且可能不在运行于同一节点上</li>
<li>它们没有自己的持久状态</li>
</ul>
</li>
<li>单个节点（VM 或者物理机）关机
<ul>
<li>结果
<ul>
<li>此节点上的所有 Pod 都停止运行</li>
</ul>
</li>
</ul>
</li>
<li>网络分裂
<ul>
<li>结果
<ul>
<li>分区 A 认为分区 B 中所有的节点都已宕机；分区 B 认为 API 服务器宕机
（假定主控节点所在的 VM 位于分区 A 内)。</li>
</ul>
</li>
</ul>
</li>
</ul>
<!--
  - Kubelet software fault
    - Results
      - crashing kubelet cannot start new pods on the node
      - kubelet might delete the pods or not
      - node marked unhealthy
      - replication controllers start new pods elsewhere
  - Cluster operator error
    - Results
      - loss of pods, services, etc
      - lost of apiserver backing store
      - users unable to read API
      - etc.
-->
<ul>
<li>kubelet 软件故障
<ul>
<li>结果
<ul>
<li>崩溃的 kubelet 就不能在其所在的节点上启动新的 Pod</li>
<li>kubelet 可能删掉 Pod 或者不删</li>
<li>节点被标识为非健康态</li>
<li>副本控制器会在其它的节点上启动新的 Pod</li>
</ul>
</li>
</ul>
</li>
<li>集群操作错误
<ul>
<li>结果
<ul>
<li>丢失 Pod 或服务等等</li>
<li>丢失 API 服务器的后端存储</li>
<li>用户无法读取API</li>
<li>等等</li>
</ul>
</li>
</ul>
</li>
</ul>
<!--
### Mitigations:

- Action: Use IaaS provider's automatic VM restarting feature for IaaS VMs
  - Mitigates: Apiserver VM shutdown or apiserver crashing
  - Mitigates: Supporting services VM shutdown or crashes

- Action: Use IaaS providers reliable storage (e.g. GCE PD or AWS EBS volume) for VMs with apiserver+etcd
  - Mitigates: Apiserver backing storage lost

- Action: Use [high-availability](/docs/setup/production-environment/tools/kubeadm/high-availability/) configuration
  - Mitigates: Control plane node shutdown or control plane components (scheduler, API server, controller-manager) crashing
    - Will tolerate one or more simultaneous node or component failures
  - Mitigates: API server backing storage (i.e., etcd's data directory) lost
    - Assumes HA (highly-available) etcd configuration
-->
<h3 id="缓解措施">缓解措施：</h3>
<ul>
<li>
<p>措施：对于 IaaS 上的 VMs，使用 IaaS 的自动 VM 重启功能</p>
<ul>
<li>缓解：API 服务器 VM 关机或 API 服务器崩溃</li>
<li>缓解：Kubernetes 服务组件所在的 VM 关机或崩溃</li>
</ul>
</li>
<li>
<p>措施: 对于运行 API 服务器和 etcd 的 VM，使用 IaaS 提供的可靠的存储（例如 GCE PD 或者 AWS EBS 卷）</p>
<ul>
<li>缓解：API 服务器后端存储的丢失</li>
</ul>
</li>
<li>
<p>措施：使用<a href="/zh/docs/setup/production-environment/tools/kubeadm/high-availability/">高可用性</a>的配置</p>
<ul>
<li>缓解：主控节点 VM 关机或者主控节点组件（调度器、API 服务器、控制器管理器）崩馈
<ul>
<li>将容许一个或多个节点或组件同时出现故障</li>
</ul>
</li>
<li>缓解：API 服务器后端存储（例如 etcd 的数据目录）丢失
<ul>
<li>假定你使用了高可用的 etcd 配置</li>
</ul>
</li>
</ul>
</li>
</ul>
<!--
- Action: Snapshot apiserver PDs/EBS-volumes periodically
  - Mitigates: Apiserver backing storage lost
  - Mitigates: Some cases of operator error
  - Mitigates: Some cases of Kubernetes software fault

- Action: use replication controller and services in front of pods
  - Mitigates: Node shutdown
  - Mitigates: Kubelet software fault

- Action: applications (containers) designed to tolerate unexpected restarts
  - Mitigates: Node shutdown
  - Mitigates: Kubelet software fault
-->
<ul>
<li>
<p>措施：定期对 API 服务器的 PDs/EBS 卷执行快照操作</p>
<ul>
<li>缓解：API 服务器后端存储丢失</li>
<li>缓解：一些操作错误的场景</li>
<li>缓解：一些 Kubernetes 软件本身故障的场景</li>
</ul>
</li>
<li>
<p>措施：在 Pod 的前面使用副本控制器或服务</p>
<ul>
<li>缓解：节点关机</li>
<li>缓解：kubelet 软件故障</li>
</ul>
</li>
<li>
<p>措施：应用（容器）设计成容许异常重启</p>
<ul>
<li>缓解：节点关机</li>
<li>缓解：kubelet 软件故障</li>
</ul>
</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-11a6d16334428909c99e7208ab8fa5e9">11 - 扩展 Kubernetes</h1>
    <div class="lead">了解针对工作环境需要来调整 Kubernetes 集群的进阶方法。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-8f1f7f0d3a1cc21537506bd4f9103a29">11.1 - 使用自定义资源</h1>
    
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-dc64883f1fd119402b112d3ff6733452">11.1.1 - 使用 CustomResourceDefinition 扩展 Kubernetes API</h1>
    
	<!--
title: Extend the Kubernetes API with CustomResourceDefinitions
reviewers:
- deads2k
- jpbetz
- liggitt
- roycaihw
- sttts
content_type: task
min-kubernetes-server-version: 1.16
weight: 20
-->
<!-- overview -->
<!--
This page shows how to install a
[custom resource](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
into the Kubernetes API by creating a
[CustomResourceDefinition](/docs/reference/generated/kubernetes-api/v1.22/#customresourcedefinition-v1-apiextensions-k8s-io).
-->
<p>本页展示如何使用
<a href="/docs/reference/generated/kubernetes-api/v1.22/#customresourcedefinition-v1-apiextensions-k8s-io">CustomResourceDefinition</a>
将
<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源（Custom Resource）</a>
安装到 Kubernetes API 上。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 1.16.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
If you are using an older version of Kubernetes that is still supported, switch to
the documentation for that version to see advice that is relevant for your cluster.
-->
<p>如果你在使用较老的、仍处于被支持范围的 Kubernetes 版本，请切换到该版本的
文档查看对于的集群而言有用的建议。</p>
<!-- steps -->
<!--
## Create a CustomResourceDefinition

When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server
creates a new RESTful resource path for each version you specify. The CRD can be
either namespaced or cluster-scoped, as specified in the CRD's `scope` field. As
with existing built-in objects, deleting a namespace deletes all custom objects
in that namespace. CustomResourceDefinitions themselves are non-namespaced and
are available to all namespaces.

For example, if you save the following CustomResourceDefinition to `resourcedefinition.yaml`:
-->
<h2 id="create-a-customresourcedefinition">创建 CustomResourceDefinition </h2>
<p>当你创建新的 CustomResourceDefinition（CRD）时，Kubernetes API 服务器会为你所
指定的每一个版本生成一个 RESTful 的 资源路径。CRD 可以是名字空间作用域的，也可以
是集群作用域的，取决于 CRD 的 <code>scope</code> 字段设置。和其他现有的内置对象一样，删除
一个名字空间时，该名字空间下的所有定制对象也会被删除。CustomResourceDefinition
本身是不受名字空间限制的，对所有名字空间可用。</p>
<p>例如，如果你将下面的 CustomResourceDefinition 保存到 <code>resourcedefinition.yaml</code>
文件：</p>
<!--
```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # name must match the spec fields below, and be in the form: <plural>.<group>
  name: crontabs.stable.example.com
spec:
  # group name to use for REST API: /apis/<group>/<version>
  group: stable.example.com
  # list of versions supported by this CustomResourceDefinition
  versions:
    - name: v1
      # Each version can be enabled/disabled by Served flag.
      served: true
      # One and only one version must be marked as the storage version.
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                cronSpec:
                  type: string
                image:
                  type: string
                replicas:
                  type: integer
  # either Namespaced or Cluster
  scope: Namespaced
  names:
    # plural name to be used in the URL: /apis/<group>/<version>/<plural>
    plural: crontabs
    # singular name to be used as an alias on the CLI and for display
    singular: crontab
    # kind is normally the CamelCased singular type. Your resource manifests use this.
    kind: CronTab
    # shortNames allow shorter string to match your resource on the CLI
    shortNames:
    - ct
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 名字必需与下面的 spec 字段匹配，并且格式为 &#39;&lt;名称的复数形式&gt;.&lt;组名&gt;&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 组名称，用于 REST API: /apis/&lt;组&gt;/&lt;版本&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 列举此 CustomResourceDefinition 所支持的版本</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 每个版本都可以通过 served 标志来独立启用或禁止</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># 其中一个且只有一个版本必需被标记为存储版本</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 可以是 Namespaced 或 Cluster</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的复数形式，用于 URL：/apis/&lt;组&gt;/&lt;版本&gt;/&lt;名称的复数形式&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的单数形式，作为命令行使用时和显示时的别名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># kind 通常是单数形式的驼峰编码（CamelCased）形式。你的资源清单会使用这一形式。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># shortNames 允许你在命令行使用较短的字符串来匹配资源</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div><!--
and create it:
-->
<p>之后创建它：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f resourcedefinition.yaml
</code></pre></div><!--
Then a new namespaced RESTful API endpoint is created at:
-->
<p>这样一个新的受名字空间约束的 RESTful API 端点会被创建在：</p>
<pre tabindex="0"><code>/apis/stable.example.com/v1/namespaces/*/crontabs/...
</code></pre><!--
This endpoint URL can then be used to create and manage custom objects.
The `kind` of these objects will be `CronTab` from the spec of the
CustomResourceDefinition object you created above.

It might take a few seconds for the endpoint to be created.
You can watch the `Established` condition of your CustomResourceDefinition
to be true or watch the discovery information of the API server for your
resource to show up.
-->
<p>此端点 URL 自此可以用来创建和管理定制对象。对象的 <code>kind</code> 将是来自你上面创建时
所用的 spec 中指定的 <code>CronTab</code>。</p>
<p>创建端点的操作可能需要几秒钟。你可以监测你的 CustomResourceDefinition 的
<code>Established</code> 状况变为 true，或者监测 API 服务器的发现信息等待你的资源出现在
那里。</p>
<!--
## Create custom objects

After the CustomResourceDefinition object has been created, you can create
custom objects. Custom objects can contain custom fields. These fields can
contain arbitrary JSON.
In the following example, the `cronSpec` and `image` custom fields are set in a
custom object of kind `CronTab`.  The kind `CronTab` comes from the spec of the
CustomResourceDefinition object you created above.

If you save the following YAML to `my-crontab.yaml`:
-->
<h2 id="create-custom-objects">创建定制对象  </h2>
<p>在创建了 CustomResourceDefinition 对象之后，你可以创建定制对象（Custom
Objects）。定制对象可以包含定制字段。这些字段可以包含任意的 JSON 数据。
在下面的例子中，在类别为 <code>CrontTab</code> 的定制对象中，设置了<code>cronSpec</code> 和 <code>image</code>
定制字段。类别 <code>CronTab</code> 来自你在上面所创建的 CRD 的规约。</p>
<p>如果你将下面的 YAML 保存到 <code>my-crontab.yaml</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * * */5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span></code></pre></div><!--
and create it:
-->
<p>并执行创建命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-crontab.yaml
</code></pre></div><!--
You can then manage your CronTab objects using kubectl. For example:
-->
<p>你就可以使用 kubectl 来管理你的 CronTab 对象了。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get crontab
</code></pre></div><!--
Should print a list like this:
-->
<p>应该会输出如下列表：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">NAME                 AGE
my-new-cron-object   6s
</code></pre><!--
Resource names are not case-sensitive when using kubectl, and you can use either
the singular or plural forms defined in the CRD, as well as any short names.

You can also view the raw YAML data:
-->
<p>使用 kubectl 时，资源名称是大小写不敏感的，而且你既可以使用 CRD 中所定义的单数
形式或复数形式，也可以使用其短名称：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ct -o yaml
</code></pre></div><!--
You should see that it contains the custom `cronSpec` and `image` fields
from the YAML you used to create it:
-->
<p>你可以看到输出中包含了你创建定制对象时在 YAML 文件中指定的定制字段 <code>cronSpec</code>
和 <code>image</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>stable.example.com/v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">kubectl.kubernetes.io/last-applied-configuration</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic">
</span><span style="color:#b44;font-style:italic">        </span><span style="color:#bbb">        </span>{<span style="color:#b44">&#34;apiVersion&#34;</span>:<span style="color:#b44">&#34;stable.example.com/v1&#34;</span>,<span style="color:#b44">&#34;kind&#34;</span>:<span style="color:#b44">&#34;CronTab&#34;</span>,<span style="color:#b44">&#34;metadata&#34;</span>:{<span style="color:#b44">&#34;annotations&#34;</span>:{},<span style="color:#b44">&#34;name&#34;</span>:<span style="color:#b44">&#34;my-new-cron-object&#34;</span>,<span style="color:#b44">&#34;namespace&#34;</span>:<span style="color:#b44">&#34;default&#34;</span>},<span style="color:#b44">&#34;spec&#34;</span>:{<span style="color:#b44">&#34;cronSpec&#34;</span>:<span style="color:#b44">&#34;* * * * */5&#34;</span>,<span style="color:#b44">&#34;image&#34;</span>:<span style="color:#b44">&#34;my-awesome-cron-image&#34;</span>}}<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2021-06-20T07:35:27Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">generation</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1326&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>9aab1d66-628e-41bb-a422-57b8b3b1f5a9<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;* * * * */5&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>List<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selfLink</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
## Delete a CustomResourceDefinition

When you delete a CustomResourceDefinition, the server will uninstall the RESTful API endpoint
and delete all custom objects stored in it.
-->
<h2 id="delete-a-customresourcedefinition">删除 CustomResourceDefinition   </h2>
<p>当你删除某 CustomResourceDefinition 时，服务器会卸载其 RESTful API
端点，并删除服务器上存储的所有定制对象。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete -f resourcedefinition.yaml
kubectl get crontabs
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Error from server (NotFound): Unable to list {&quot;stable.example.com&quot; &quot;v1&quot; &quot;crontabs&quot;}: the server could not find the requested resource (get crontabs.stable.example.com)
</code></pre><!--
If you later recreate the same CustomResourceDefinition, it will start out empty.
-->
<p>如果你在以后创建相同的 CustomResourceDefinition 时，该 CRD 会是一个空的结构。</p>
<!--
## Specifying a structural schema

CustomResources store structured data in custom fiels (alongside the built-in
fields `apiVersion`, `kind` and `metadata`, which the API server validates
implicitly). With [OpenAPI v3.0 validation](#validation) a schema can be
specified, which is validated during creation and updates, compare below for
details and limits of such a schema.

With `apiextensions.k8s.io/v1` the definition of a structural schema is
mandatory for CustomResourceDefinitions. In the beta version of
CustomResourceDefinition, structural schemas were optional.
-->
<h2 id="specifying-a-structural-schema">设置结构化的模式  </h2>
<p>CustomResource 对象在定制字段中保存结构化的数据，这些字段和内置的字段
<code>apiVersion</code>、<code>kind</code> 和 <code>metadata</code> 等一起存储，不过内置的字段都会被 API
服务器隐式完成合法性检查。有了 <a href="#validation">OpenAPI v3.0 检查</a>
能力之后，你可以设置一个模式（Schema），在创建和更新定制对象时，这一模式会被用来
对对象内容进行合法性检查。参阅下文了解这类模式的细节和局限性。</p>
<p>在 <code>apiextensions.k8s.io/v1</code> 版本中，CustomResourceDefinition 的这一结构化模式
定义是必需的。
在 CustomResourceDefinition 的 beta 版本中，结构化模式定义是可选的。</p>
<!--
A structural schema is an [OpenAPI v3.0 validation schema](#validation) which:

1. specifies a non-empty type (via `type` in OpenAPI) for the root, for each specified field of an object node (via `properties` or `additionalProperties` in OpenAPI) and for each item in an array node (via `items` in OpenAPI), with the exception of:
   * a node with `x-kubernetes-int-or-string: true`
   * a node with `x-kubernetes-preserve-unknown-fields: true`
2. for each field in an object and each item in an array which is specified within any of `allOf`, `anyOf`, `oneOf` or `not`, the schema also specifies the field/item outside of those logical junctors (compare example 1 and 2).
3. does not set `description`, `type`, `default`, `additionalProperties`, `nullable` within an `allOf`, `anyOf`, `oneOf` or `not`, with the exception of the two pattern for `x-kubernetes-int-or-string: true` (see below).
4. if `metadata` is specified, then only restrictions on `metadata.name` and `metadata.generateName` are allowed.
-->
<p>结构化模式本身是一个 <a href="#validation">OpenAPI v3.0 验证模式</a>，其中：</p>
<ol>
<li>为对象根（root）设置一个非空的 type 值（藉由 OpenAPI 中的 <code>type</code>），对每个
object 节点的每个字段（藉由 OpenAPI 中的 <code>properties</code> 或 <code>additionalProperties</code>）以及
array 节点的每个条目（藉由 OpenAPI 中的 <code>items</code>）也要设置非空的 type 值，
除非：
<ul>
<li>节点包含属性 <code>x-kubernetes-int-or-string: true</code></li>
<li>节点包含属性 <code>x-kubernetes-preserve-unknown-fields: true</code></li>
</ul>
</li>
<li>对于 object 的每个字段或 array 中的每个条目，如果其定义中包含 <code>allOf</code>、<code>anyOf</code>、<code>oneOf</code>
或 <code>not</code>，则模式也要指定这些逻辑组合之外的字段或条目（试比较例 1 和例 2)。</li>
<li>在 <code>allOf</code>、<code>anyOf</code>、<code>oneOf</code> 或 <code>not</code> 上下文内不设置 <code>description</code>、<code>type</code>、<code>default</code>、
<code>additionalProperties</code> 或者 <code>nullable</code>。此规则的例外是
<code>x-kubernetes-int-or-string</code> 的两种模式（见下文）。</li>
<li>如果 <code>metadata</code> 被设置，则只允许对 <code>metadata.name</code> 和 <code>metadata.generateName</code> 设置约束。</li>
</ol>
<!--
Non-structural example 1:
-->
<p>非结构化的例 1:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">allOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>...<span style="color:#bbb">
</span></code></pre></div><!--
conflicts with rule 2. The following would be correct:
-->
<p>违反了第 2 条规则。下面的是正确的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">allOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>...<span style="color:#bbb">
</span></code></pre></div><!--
Non-structural example 2:
-->
<p>非结构化的例 2：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">allOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>...<span style="color:#bbb">
</span></code></pre></div><!--
conflicts with rule 2. The following would be correct:
-->
<p>违反了第 2 条规则。下面的是正确的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>...<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">allOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>...<span style="color:#bbb">
</span></code></pre></div><!--
Non-structural example 3:
-->
<p>非结构化的例 3：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pattern</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;abc&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pattern</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;^a&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">finalizers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>array<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">pattern</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;my-finalizer&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">anyOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">minimum</span>:<span style="color:#bbb"> </span><span style="color:#666">42</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">required</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;bar&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;foo bar object&#34;</span><span style="color:#bbb">
</span></code></pre></div><!--
is not a structural schema because of the following violations:

* the type at the root is missing (rule 1).
* the type of `foo` is missing (rule 1).
* `bar` inside of `anyOf` is not specified outside (rule 2).
* `bar`'s `type` is within `anyOf` (rule 3).
* the description is set within `anyOf` (rule 3).
* `metadata.finalizers` might not be restricted (rule 4).
-->
<p>不是一个结构化的模式，因为其中存在以下违例：</p>
<ul>
<li>根节点缺失 type 设置（规则 1）</li>
<li><code>foo</code> 的 type 缺失（规则 1）</li>
<li><code>anyOf</code> 中的 <code>bar</code> 未在外部指定（规则 2）</li>
<li><code>bar</code> 的 <code>type</code> 位于 <code>anyOf</code> 中（规则 3）</li>
<li><code>anyOf</code> 中设置了 <code>description</code> （规则 3）</li>
<li><code>metadata.finalizers</code> 不可以被限制 (规则 4）</li>
</ul>
<!--
In contrast, the following, corresponding schema is structural:
-->
<p>作为对比，下面的 YAML 所对应的模式则是结构化的：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;foo bar object&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">pattern</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;abc&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">pattern</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;^a&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">anyOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">minimum</span>:<span style="color:#bbb"> </span><span style="color:#666">42</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">required</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;bar&#34;</span>]<span style="color:#bbb">
</span></code></pre></div><!--
Violations of the structural schema rules are reported in the `NonStructural` condition in the CustomResourceDefinition.
-->
<p>如果违反了结构化模式规则，CustomResourceDefinition 的 <code>NonStructural</code> 状况中
会包含报告信息。</p>
<!--
### Field pruning

CustomResourceDefinitions store validated resource data in the cluster's persistence store, <a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a>. As with native Kubernetes resources such as <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>, if you specify a field that the API server does not recognize, the unknown field  is _pruned_ (removed) before being persisted.
-->
<h3 id="field-pruning">字段剪裁    </h3>
<p>CustomResourceDefinition 在集群的持久性存储
<a class='glossary-tooltip' title='etcd 是兼具一致性和高可用性的键值数据库，用作保存 Kubernetes 所有集群数据的后台数据库。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/administer-cluster/configure-upgrade-etcd/' target='_blank' aria-label='etcd'>etcd</a>
中保存经过合法性检查的资源数据。
就像原生的 Kubernetes 资源，例如 <a class='glossary-tooltip' title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/configure-pod-configmap/' target='_blank' aria-label='ConfigMap'>ConfigMap</a>，
如果你指定了 API 服务器所无法识别的字段，则该未知字段会在保存资源之前
被 <em>剪裁（Pruned）</em> 掉（删除）。</p>
<!--
CRDs converted from `apiextensions.k8s.io/v1beta1` to
`apiextensions.k8s.io/v1` might lack structural schemas, and
`spec.preserveUnknownFields` might be `true`.

For legacy CustomResourceDefinition objects created as
`apiextensions.k8s.io/v1beta1` with `spec.preserveUnknownFields` set to
`true`, the following is also true:

* Pruning is not enabled.
* You can store arbitrary data.

For compatibility with `apiextensions.k8s.io/v1`, update your custom
resource definitions to:

1. Use a structural OpenAPI schema.
2. Set `spec.preserveUnknownFields` to `false`.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>从 <code>apiextensions.k8s.io/v1beta1</code> 转换到 <code>apiextensions.k8s.io/v1</code> 的 CRD
可能没有结构化的模式定义，因此其 <code>spec.preserveUnknownFields</code> 可能为 <code>true</code>。</p>
<p>对于使用 <code>apiextensions.k8s.io/v1beta1</code> 且将 <code>spec.preserveUnknownFields</code> 设置为 <code>true</code>
创建的旧 CustomResourceDefinition 对象，有以下表现：</p>
<ul>
<li>裁剪未启用。</li>
<li>可以存储任意数据。</li>
</ul>
<p>为了与 <code>apiextensions.k8s.io/v1</code> 兼容，将你的自定义资源定义更新为：</p>
<ol>
<li>使用结构化的 OpenAPI 模式。</li>
<li><code>spec.preserveUnknownFields</code> 设置为 <code>false</code>。</li>
</ol>
</div>
</blockquote>
<!--
If you save the following YAML to `my-crontab.yaml`:
-->
<p>如果你将下面的 YAML 保存到 <code>my-crontab.yaml</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * * */5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">someRandomField</span>:<span style="color:#bbb"> </span><span style="color:#666">42</span><span style="color:#bbb">
</span></code></pre></div><!--
and create it:
-->
<p>并创建之：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create --validate<span style="color:#666">=</span><span style="color:#a2f">false</span> -f my-crontab.yaml -o yaml
</code></pre></div><!--
your output is similar to:
-->
<p>输出类似于：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">apiVersion: stable.example.com/v1
kind: CronTab
metadata:
  creationTimestamp: 2017-05-31T12:56:35Z
  generation: 1
  name: my-new-cron-object
  namespace: default
  resourceVersion: &quot;285&quot;
  uid: 9423255b-4600-11e7-af6a-28d2447dc82b
spec:
  cronSpec: '* * * * */5'
  image: my-awesome-cron-image
</code></pre><!--
Notice that the field `someRandomField` was pruned.
-->
<p>注意其中的字段 <code>someRandomField</code> 已经被剪裁掉。</p>
<!--
This example turned off client-side validation to demonstrate the API server's behavior, by adding the `--validate=false` command line option.
Because the [OpenAPI validation schemas are also published](#publish-validation-schema-in-openapi-v2)
to clients, `kubectl` also checks for unknown fields and rejects those objects well before they would be sent to the API server.
-->
<p>本例中通过 <code>--validate=false</code> 命令行选项 关闭了客户端的合法性检查以展示 API 服务器的行为，
因为 <a href="#publish-validation-schema-in-openapi-v2">OpenAPI 合法性检查模式也会发布到</a>
客户端，<code>kubectl</code> 也会检查未知的字段并在对象被发送到 API
服务器之前就拒绝它们。</p>
<!--
#### Controlling pruning

By default, all unspecified fields for a custom resource, across all versions, are pruned. It is possible though to opt-out of that for specific sub-trees of fields by adding `x-kubernetes-preserve-unknown-fields: true` in the [structural OpenAPI v3 validation schema](#specifying-a-structural-schema).
For example:
-->
<h4 id="controlling-pruning">控制剪裁  </h4>
<p>默认情况下，定制资源的所有版本中的所有未规定的字段都会被剪裁掉。
通过在结构化的 OpenAPI v3 <a href="#specifying-a-structural-schema">检查模式定义</a>
中为特定字段的子树添加 <code>x-kubernetes-preserve-unknown-fields: true</code> 属性，可以
选择不对其执行剪裁操作。
例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">json</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">x-kubernetes-preserve-unknown-fields</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
The field `json` can store any JSON value, without anything being pruned.

You can also partially specify the permitted JSON; for example:
-->
<p>字段 <code>json</code> 可以保存任何 JSON 值，其中内容不会被剪裁掉。</p>
<p>你也可以部分地指定允许的 JSON 数据格式；例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">json</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">x-kubernetes-preserve-unknown-fields</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span>this is arbitrary JSON<span style="color:#bbb">
</span></code></pre></div><!--
With this, only `object` type values are allowed.

Pruning is enabled again for each specified property (or `additionalProperties`):
-->
<p>通过这样设置，JSON 中只能设置 <code>object</code> 类型的值。</p>
<p>对于所指定的每个属性（或 <code>additionalProperties</code>），剪裁会再次被启用。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">json</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">x-kubernetes-preserve-unknown-fields</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span></code></pre></div><!--
With this, the value:
-->
<p>对于上述定义，如果提供的数值如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">json</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>abc<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb"> </span>def<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">something</span>:<span style="color:#bbb"> </span>x<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">something</span>:<span style="color:#bbb"> </span>x<span style="color:#bbb">
</span></code></pre></div><!--
is pruned to:
-->
<p>则该值会被剪裁为：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">json</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span>abc<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb"> </span>def<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">something</span>:<span style="color:#bbb"> </span>x<span style="color:#bbb">
</span></code></pre></div><!--
This means that the `something` field in the specified `spec` object is pruned, but everything outside is not.
-->
<p>这意味着所指定的 <code>spec</code> 对象中的 <code>something</code> 字段被剪裁掉，而其外部的内容都被保留。</p>
<!--
### IntOrString

Nodes in a schema with `x-kubernetes-int-or-string: true` are excluded from rule 1, such that the following is structural:
-->
<h3 id="intorstring">IntOrString</h3>
<p>模式定义中标记了 <code>x-kubernetes-int-or-string: true</code> 的节点不受前述规则 1
约束，因此下面的定义是结构化的模式：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">x-kubernetes-int-or-string</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
Also those nodes are partially excluded from rule 3 in the sense that the
following two patterns are allowed (exactly those, without variations in order
to additional fields):
-->
<p>此外，所有这类节点也不再受规则 3 约束，也就是说，下面两种模式是被允许的
（注意，仅限于这两种模式，不支持添加新字段的任何其他变种）：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">x-kubernetes-int-or-string</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">anyOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><p>和</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">x-kubernetes-int-or-string</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">allOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">anyOf</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb"></span>- ...<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># zero or more</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><!--
With one of those specification, both an integer and a string validate.

In [Validation Schema Publishing](#publish-validation-schema-in-openapi-v2),
`x-kubernetes-int-or-string: true` is unfolded to one of the two patterns shown above.
-->
<p>在以上两种规约中，整数值和字符串值都会被认为是合法的。</p>
<p>在<a href="#publish-validation-schema-in-openapi-v2">合法性检查模式定义的发布时</a>，
<code>x-kubernetes-int-or-string: true</code> 会被展开为上述两种模式之一。</p>
<h3 id="rawextension">RawExtension</h3>
<!--
RawExtensions (as in `runtime.RawExtension` defined in
[k8s.io/apimachinery](https://github.com/kubernetes/apimachinery/blob/03ac7a9ade429d715a1a46ceaa3724c18ebae54f/pkg/runtime/types.go#L94))
holds complete Kubernetes objects, i.e. with `apiVersion` and `kind` fields.

It is possible to specify those embedded objects (both completely without constraints or partially specified) by setting `x-kubernetes-embedded-resource: true`. For example:
-->
<p>RawExtensions（就像在
<a href="https://github.com/kubernetes/apimachinery/blob/03ac7a9ade429d715a1a46ceaa3724c18ebae54f/pkg/runtime/types.go#L94">k8s.io/apimachinery</a>
项目中 <code>runtime.RawExtension</code> 所定义的那样）
可以保存完整的 Kubernetes 对象，也就是，其中会包含 <code>apiVersion</code> 和 <code>kind</code>
字段。</p>
<p>通过 <code>x-kubernetes-embedded-resource: true</code> 来设定这些嵌套对象的规约（无论是
完全无限制还是部分指定都可以）是可能的。例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">x-kubernetes-embedded-resource</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">x-kubernetes-preserve-unknown-fields</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div><!--
Here, the field `foo` holds a complete object, e.g.:
-->
<p>这里，字段 <code>foo</code> 包含一个完整的对象，例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></code></pre></div><!--
Because `x-kubernetes-preserve-unknown-fields: true` is specified alongside,
nothing is pruned. The use of `x-kubernetes-preserve-unknown-fields: true` is
optional though.

With `x-kubernetes-embedded-resource: true`, the `apiVersion`, `kind` and `metadata` are implicitly specified and validated.
-->
<p>由于字段上设置了 <code>x-kubernetes-preserve-unknown-fields: true</code>，其中的内容不会
被剪裁。不过，在这个语境中，<code>x-kubernetes-preserve-unknown-fields: true</code> 的
使用是可选的。</p>
<p>设置了 <code>x-kubernetes-embedded-resource: true</code> 之后，<code>apiVersion</code>、<code>kind</code> 和
<code>metadata</code> 都是隐式设定并隐式完成合法性验证。</p>
<!--
## Serving multiple versions of a CRD

See [Custom resource definition versioning](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/)
for more information about serving multiple versions of your
CustomResourceDefinition and migrating your objects from one version to another.
-->
<h2 id="serving-multiple-versions-of-a-crd">提供 CRD 的多个版本  </h2>
<p>关于如何为你的 CustomResourceDefinition 提供多个版本的支持，以及如何将你的对象
从一个版本迁移到另一个版本， 详细信息可参阅
<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">定制资源定义的版本</a>。</p>
<!-- discussion -->
<!--
## Advanced topics

### Finalizers
-->
<h2 id="advanced-topics">高级主题    </h2>
<h3 id="finalizers">Finalizers</h3>
<!--
*Finalizers* allow controllers to implement asynchronous pre-delete hooks.
Custom objects support finalizers similar to built-in objects.

You can add a finalizer to a custom object like this:
-->
<p><em>Finalizer</em> 能够让控制器实现异步的删除前（Pre-delete）回调。
与内置对象类似，定制对象也支持 Finalizer。</p>
<p>你可以像下面一样为定制对象添加 Finalizer：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">finalizers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- stable.example.com/finalizer<span style="color:#bbb">
</span></code></pre></div><!--
Identifiers of custom finalizers consist of a domain name, a forward slash and the name of
the finalizer. Any controller can add a finalizer to any object's list of finalizers.

The first delete request on an object with finalizers sets a value for the
`metadata.deletionTimestamp` field but does not delete it. Once this value is set,
entries in the `finalizers` list can only be removed. While any finalizers remain it is also
impossible to force the deletion of an object.

When the `metadata.deletionTimestamp` field is set, controllers watching the object execute any
finalizers they handle and remove the finalizer from the list after they are done. It is the
responsibility of each controller to remove its finalizer from the list.
-->
<p>自定义 Finalizer 的标识符包含一个域名、一个正向斜线和 finalizer 的名称。
任何控制器都可以在任何对象的 finalizer 列表中添加新的 finalizer。</p>
<p>对带有 Finalizer 的对象的第一个删除请求会为其 <code>metadata.deletionTimestamp</code>
设置一个值，但不会真的删除对象。一旦此值被设置，<code>finalizers</code> 列表中的表项
只能被移除。在列表中仍然包含 finalizer 时，无法强制删除对应的对象。</p>
<p>当 <code>metadata.deletionTimestamp</code> 字段被设置时，监视该对象的各个控制器会
执行它们所能处理的 finalizer，并在完成处理之后将其从列表中移除。
每个控制器负责将其 finalizer 从列表中删除。</p>
<!--
The value of `metadata.deletionGracePeriodSeconds` controls the interval between polling updates.

Once the list of finalizers is empty, meaning all finalizers have been executed, the resource is
deleted by Kubernetes.
-->
<p><code>metadata.deletionGracePeriodSeconds</code> 的取值控制对更新的轮询周期。</p>
<p>一旦 finalizers 列表为空时，就意味着所有 finalizer 都被执行过，
Kubernetes 会最终删除该资源，</p>
<!--
### Validation

Custom resources are validated via
[OpenAPI v3 schemas](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject)
and you can add additional validation using
[admission webhooks](/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook).
-->
<h3 id="validation">合法性检查   </h3>
<p>定制资源是通过
<a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject">OpenAPI v3 模式定义</a>
来执行合法性检查的，
你可以通过使用<a href="/zh/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">准入控制 Webhook</a>
来添加额外的合法性检查逻辑。</p>
<!--
Additionally, the following restrictions are applied to the schema:

- These fields cannot be set:
  - `definitions`,
  - `dependencies`,
  - `deprecated`,
  - `discriminator`,
  - `id`,
  - `patternProperties`,
  - `readOnly`,
  - `writeOnly`,
  - `xml`,
  - `$ref`.
- The field `uniqueItems` cannot be set to `true`.
- The field `additionalProperties` cannot be set to `false`.
- The field `additionalProperties` is mutually exclusive with `properties`.
-->
<p>此外，对模式定义存在以下限制：</p>
<ul>
<li>以下字段不可设置：
<ul>
<li><code>definitions</code></li>
<li><code>dependencies</code></li>
<li><code>deprecated</code></li>
<li><code>discriminator</code></li>
<li><code>id</code></li>
<li><code>patternProperties</code></li>
<li><code>readOnly</code></li>
<li><code>writeOnly</code></li>
<li><code>xml</code></li>
<li><code>$ref</code></li>
</ul>
</li>
<li>字段 <code>uniqueItems</code> 不可设置为 <code>true</code></li>
<li>字段 <code>additionalProperties</code> 不可设置为 <code>false</code></li>
<li>字段 <code>additionalProperties</code> 与 <code>properties</code> 互斥，不可同时使用</li>
</ul>
<!--
The `default` field can be set when the [Defaulting feature](#defaulting) is enabled,
which is the case with `apiextensions.k8s.io/v1` CustomResourceDefinitions.
Defaulting is in GA since 1.17 (beta since 1.16 with the `CustomResourceDefaulting`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
enabled, which is the case automatically for many clusters for beta features).
-->
<p>当<a href="#defaulting">设置默认值特性</a>被启用时，可以设置字段 <code>default</code>。
就 <code>apiextensions.k8s.io/v1</code> 组的 CustomResourceDefinitions，这一条件是满足的。
设置默认值的功能特性从 1.17 开始正式发布。该特性在 1.16 版本中处于
Beta 状态，要求 <code>CustomResourceDefaulting</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
被启用。对于大多数集群而言，Beta 状态的特性门控默认都是自动启用的。</p>
<!--
Refer to the [structural schemas](#specifying-a-structural-schema) section for other
restrictions and CustomResourceDefinition features.
-->
<p>关于对某些 CustomResourceDefinition 特性所必需的限制，可参见
<a href="#specifying-a-structural-schema">结构化的模式定义</a>小节。</p>
<!--
The schema is defined in the CustomResourceDefinition. In the following example, the
CustomResourceDefinition applies the following validations on the custom object:

- `spec.cronSpec` must be a string and must be of the form described by the regular expression.
- `spec.replicas` must be an integer and must have a minimum value of 1 and a maximum value of 10.

Save the CustomResourceDefinition to `resourcedefinition.yaml`:
-->
<p>模式定义是在 CustomResourceDefinition 中设置的。在下面的例子中，
CustomResourceDefinition 对定制对象执行以下合法性检查：</p>
<ul>
<li><code>spec.cronSpec</code> 必须是一个字符串，必须是正则表达式所描述的形式；</li>
<li><code>spec.replicas</code> 必须是一个整数，且其最小值为 1、最大值为 10。</li>
</ul>
<p>将此 CustomResourceDefinition 保存到 <code>resourcedefinition.yaml</code> 文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># openAPIV3Schema is the schema for validating custom objects.</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">pattern</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">minimum</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">maximum</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div><!--
and create it:
-->
<p>并创建 CustomResourceDefinition：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f resourcedefinition.yaml
</code></pre></div><!--
A request to create a custom object of kind CronTab is rejected if there are invalid values in its fields.
In the following example, the custom object contains fields with invalid values:

- `spec.cronSpec` does not match the regular expression.
- `spec.replicas` is greater than 10.

If you save the following YAML to `my-crontab.yaml`:
-->
<p>对于一个创建 CronTab 类别对象的定制对象的请求而言，如果其字段中包含非法值，则
该请求会被拒绝。
在下面的例子中，定制对象中包含带非法值的字段：</p>
<ul>
<li><code>spec.cronSpec</code> 与正则表达式不匹配</li>
<li><code>spec.replicas</code> 数值大于 10。</li>
</ul>
<p>如果你将下面的 YAML 保存到 <code>my-crontab.yaml</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * *&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span></code></pre></div><!--
and attempt to create it:
-->
<p>并尝试创建定制对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-crontab.yaml
</code></pre></div><!--
then you get an error:
-->
<p>你会看到下面的错误信息：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">The CronTab &quot;my-new-cron-object&quot; is invalid: []: Invalid value: map[string]interface {}{&quot;apiVersion&quot;:&quot;stable.example.com/v1&quot;, &quot;kind&quot;:&quot;CronTab&quot;, &quot;metadata&quot;:map[string]interface {}{&quot;name&quot;:&quot;my-new-cron-object&quot;, &quot;namespace&quot;:&quot;default&quot;, &quot;deletionTimestamp&quot;:interface {}(nil), &quot;deletionGracePeriodSeconds&quot;:(*int64)(nil), &quot;creationTimestamp&quot;:&quot;2017-09-05T05:20:07Z&quot;, &quot;uid&quot;:&quot;e14d79e7-91f9-11e7-a598-f0761cb232d1&quot;, &quot;clusterName&quot;:&quot;&quot;}, &quot;spec&quot;:map[string]interface {}{&quot;cronSpec&quot;:&quot;* * * *&quot;, &quot;image&quot;:&quot;my-awesome-cron-image&quot;, &quot;replicas&quot;:15}}:
validation failure list:
spec.cronSpec in body should match '^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'
spec.replicas in body should be less than or equal to 10
</code></pre><!--
If the fields contain valid values, the object creation request is accepted.

Save the following YAML to `my-crontab.yaml`:
-->
<p>如果所有字段都包含合法值，则对象创建的请求会被接受。</p>
<p>将下面的 YAML 保存到 <code>my-crontab.yaml</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * * */5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">5</span><span style="color:#bbb">
</span></code></pre></div><!--
And create it:
-->
<p>并创建定制对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-crontab.yaml
crontab <span style="color:#b44">&#34;my-new-cron-object&#34;</span> created
</code></pre></div><!--
### Defaulting
-->
<h3 id="efaulting">设置默认值  </h3>
<!--
To use defaulting, your CustomResourceDefinition must use API version `apiextensions.k8s.io/v1`.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 要使用设置默认值功能，你的 CustomResourceDefinition 必须使用 API 版本 <code>apiextensions.k8s.io/v1</code>。</div>
</blockquote>
<!--
Defaulting allows to specify default values in the [OpenAPI v3 validation schema](#validation):
-->
<p>设置默认值的功能允许在 <a href="#validation">OpenAPI v3 合法性检查模式定义</a>中设置默认值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># openAPIV3Schema 是用来检查定制对象的模式定义</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">pattern</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5 0 * * *&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">minimum</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">maximum</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div><!--
With this both `cronSpec` and `replicas` are defaulted:
-->
<p>使用此 CRD 定义时，<code>cronSpec</code> 和 <code>replicas</code> 都会被设置默认值：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span></code></pre></div><!--
leads to
-->
<p>会生成：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;5 0 * * *&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span></code></pre></div><!--
Defaulting happens on the object

* in the request to the API server using the request version defaults,
* when reading from etcd using the storage version defaults,
* after mutating admission plugins with non-empty patches using the admission webhook object version defaults.

Defaults applied when reading data from etcd are not automatically written back to etcd. An update request via the API is required to persist those defaults back into etcd.
-->
<p>默认值设定的行为发生在定制对象上：</p>
<ul>
<li>在向 API 服务器发送的请求中，基于请求版本的设定设置默认值；</li>
<li>在从 etcd 读取对象时，使用存储版本来设置默认值；</li>
<li>在 Mutating 准入控制插件执行非空的补丁操作时，基于准入 Webhook 对象
版本设置默认值。</li>
</ul>
<p>从 etcd 中读取数据时所应用的默认值设置不会被写回到 etcd 中。
需要通过 API 执行更新请求才能将这种方式设置的默认值写回到 etcd。</p>
<!--
Default values must be pruned (with the exception of defaults for `metadata` fields) and must validate against a provided schema.

Default values for `metadata` fields of `x-kubernetes-embedded-resources: true` nodes (or parts of a default value covering `metadata`) are not pruned during CustomResourceDefinition creation, but through the pruning step during handling of requests.
-->
<p>默认值一定会被剪裁（除了 <code>metadata</code> 字段的默认值设置），且必须通过所提供
的模式定义的检查。</p>
<p>针对 <code>x-kubernetes-embedded-resource: true</code> 节点（或者包含 <code>metadata</code> 字段的结构的默认值）
的 <code>metadata</code> 字段的默认值设置不会在 CustomResourceDefinition 创建时被剪裁，
而是在处理请求的字段剪裁阶段被删除。</p>
<!--
#### Defaulting and Nullable

**New in 1.20:** null values for fields that either don't specify the nullable flag, or give it a `false` value, will be pruned before defaulting happens. If a default is present, it will be applied. When nullable is `true`, null values will be conserved and won't be defaulted.

For example, given the OpenAPI schema below:
-->
<h4 id="defaulting-and-nullable">设置默认值和字段是否可为空（Nullable）  </h4>
<p><strong>1.20 版本新增:</strong> 对于未设置其 nullable 标志的字段或者将该标志设置为
<code>false</code> 的字段，其空值（Null）会在设置默认值之前被剪裁掉。如果对应字段
存在默认值，则默认值会被赋予该字段。当 <code>nullable</code> 被设置为 <code>true</code> 时，
字段的空值会被保留，且不会在设置默认值时被覆盖。</p>
<p>例如，给定下面的 OpenAPI 模式定义：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">nullable</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;default&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">nullable</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">baz</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span></code></pre></div><!--
creating an object with null values for `foo` and `bar` and `baz`
-->
<p>像下面这样创建一个为 <code>foo</code>、<code>bar</code> 和 <code>baz</code> 设置空值的对象时：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">baz</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span></code></pre></div><!--
leads to
-->
<p>其结果会是这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">foo</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;default&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">bar</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">null</span><span style="color:#bbb">
</span></code></pre></div><!--
with `foo` pruned and defaulted because the field is non-nullable, `bar` maintaining the null value due to `nullable: true`, and `baz` pruned because the field is non-nullable and has no default.
-->
<p>其中的 <code>foo</code> 字段被剪裁掉并重新设置默认值，因为该字段是不可为空的。
<code>bar</code> 字段的 <code>nullable: true</code> 使得其能够保有其空值。
<code>baz</code> 字段则被完全剪裁掉，因为该字段是不可为空的，并且没有默认值设置。</p>
<!--
### Publish Validation Schema in OpenAPI v2

CustomResourceDefinition [OpenAPI v3 validation schemas](#validation) which are [structural](#specifying-a-structural-schema) and [enable pruning](#field-pruning) are published as part of the [OpenAPI v2 spec](/docs/concepts/overview/kubernetes-api/#openapi-and-swagger-definitions) from Kubernetes API server.

The [kubectl](/docs/reference/kubectl/overview) command-line tool consumes the published schema to perform client-side validation (`kubectl create` and `kubectl apply`), schema explanation (`kubectl explain`) on custom resources. The published schema can be consumed for other purposes as well, like client generation or documentation.
-->
<h3 id="publish-validation-schema-in-openapi-v2">以 OpenAPI v2 形式发布合法性检查模式     </h3>
<p>CustomResourceDefinition 的<a href="#specifying-a-structural-schema">结构化的</a>、
<a href="#preserving-unknown-fields">启用了剪裁的</a> <a href="#validation">OpenAPI v3 合法性检查模式</a>
会在 Kubernetes API 服务器上作为
<a href="/zh/docs/concepts/overview/kubernetes-api/#openapi-and-swagger-definitions">OpenAPI v2 规约</a>
的一部分发布出来。</p>
<p><a href="/zh/docs/reference/kubectl/overview">kubectl</a> 命令行工具会基于所发布的模式定义来执行
客户端的合法性检查（<code>kubectl create</code> 和 <code>kubectl apply</code>），为定制资源的模式定义
提供解释（<code>kubectl explain</code>）。
所发布的模式还可被用于其他目的，例如生成客户端或者生成文档。</p>
<!--
The OpenAPI v3 validation schema is converted to OpenAPI v2 schema, and
show up in `definitions` and `paths` fields in the [OpenAPI v2 spec](/docs/concepts/overview/kubernetes-api/#openapi-and-swagger-definitions).

The following modifications are applied during the conversion to keep backwards compatibility with
kubectl in previous 1.13 version. These modifications prevent kubectl from being over-strict and rejecting
valid OpenAPI schemas that it doesn't understand. The conversion won't modify the validation schema defined in CRD,
and therefore won't affect [validation](#validation) in the API server.
-->
<p>OpenAPI v3 合法性检查模式定义会被转换为 OpenAPI v2 模式定义，并出现在
<a href="/zh/docs/concepts/overview/kubernetes-api/#openapi-and-swagger-definitions">OpenAPI v2 规范</a>
的 <code>definitions</code> 和 <code>paths</code> 字段中。</p>
<p>在转换过程中会发生以下修改，目的是保持与 1.13 版本以前的 kubectl 工具兼容。
这些修改可以避免 kubectl 过于严格，以至于拒绝它无法理解的 OpenAPI 模式定义。
转换过程不会更改 CRD 中定义的合法性检查模式定义，因此不会影响到 API 服务器中
的<a href="#validation">合法性检查</a>。</p>
<!--
1. The following fields are removed as they aren't supported by OpenAPI v2 (in future versions OpenAPI v3 will be used without these restrictions)
   - The fields `allOf`, `anyOf`, `oneOf` and `not` are removed
2. If `nullable: true` is set, we drop `type`, `nullable`, `items` and `properties` because OpenAPI v2 is not able to express nullable. To avoid kubectl to reject good objects, this is necessary.
-->
<ol>
<li>以下字段会被移除，因为它们在 OpenAPI v2 中不支持（在将来版本中将使用 OpenAPI v3，
因而不会有这些限制）
<ul>
<li>字段 <code>allOf</code>、<code>anyOf</code>、<code>oneOf</code> 和 <code>not</code> 会被删除</li>
</ul>
</li>
<li>如果设置了 <code>nullable: true</code>，我们会丢弃 <code>type</code>、<code>nullable</code>、<code>items</code> 和 <code>properties</code>
OpenAPI v2 无法表达 Nullable。为了避免 kubectl 拒绝正常的对象，这一转换是必要的。</li>
</ol>
<!--
### Additional printer columns

The kubectl tool relies on server-side output formatting. Your cluster's API server decides which
columns are shown by the `kubectl get` command. You can customize these columns for a
CustomResourceDefinition. The following example adds the `Spec`, `Replicas`, and `Age`
columns.

Save the CustomResourceDefinition to `resourcedefinition.yaml`:
-->
<h3 id="additional-printer-columns">额外的打印列   </h3>
<p><code>kubectl</code> 工具依赖服务器端的输出格式化。你的集群的 API 服务器决定 <code>kubectl get</code> 命令要显示的列有哪些。
你可以为 CustomResourceDefinition 定制这些要打印的列。
下面的例子添加了 <code>Spec</code>、<code>Replicas</code> 和 <code>Age</code> 列：</p>
<p>将此 CustomResourceDefinition 保存到 <code>resourcedefinition.yaml</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">additionalPrinterColumns</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>Spec<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span>The cron spec defining the interval a CronJob is run<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">jsonPath</span>:<span style="color:#bbb"> </span>.spec.cronSpec<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>Replicas<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span>The number of jobs launched by the CronJob<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">jsonPath</span>:<span style="color:#bbb"> </span>.spec.replicas<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>Age<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>date<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">jsonPath</span>:<span style="color:#bbb"> </span>.metadata.creationTimestamp<span style="color:#bbb">
</span></code></pre></div><!--
Create the CustomResourceDefinition:
-->
<p>创建 CustomResourceDefinition：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f resourcedefinition.yaml
</code></pre></div><!--
Create an instance using the `my-crontab.yaml` from the previous section.
-->
<p>使用前文中的 <code>my-crontab.yaml</code> 创建一个实例。</p>
<!--
Invoke the server-side printing:
-->
<p>启用服务器端打印输出：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get crontab my-new-cron-object
</code></pre></div><!--
Notice the `NAME`, `SPEC`, `REPLICAS`, and `AGE` columns in the output:
-->
<p>注意输出中的 <code>NAME</code>、<code>SPEC</code>、<code>REPLICAS</code> 和 <code>AGE</code> 列：</p>
<pre tabindex="0"><code>NAME                 SPEC        REPLICAS   AGE
my-new-cron-object   * * * * *   1          7s
</code></pre><!--
The `NAME` column is implicit and does not need to be defined in the CustomResourceDefinition.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <code>NAME</code> 列是隐含的，不需要在 CustomResourceDefinition 中定义。</div>
</blockquote>
<!--
#### Priority

Each column includes a `priority` field. Currently, the priority
differentiates between columns shown in standard view or wide view (using the `-o wide` flag).

- Columns with priority `0` are shown in standard view.
- Columns with priority greater than `0` are shown only in wide view.
-->
<h4 id="priority">优先级   </h4>
<p>每个列都包含一个 <code>priority</code>（优先级）字段。当前，优先级用来区分标准视图（Standard
View）和宽视图（Wide View）（使用 <code>-o wide</code> 标志）中显示的列：</p>
<ul>
<li>优先级为 <code>0</code> 的列会在标准视图中显示。</li>
<li>优先级大于 <code>0</code> 的列只会在宽视图中显示。</li>
</ul>
<!--
#### Type

A column's `type` field can be any of the following (compare [OpenAPI v3 data types](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#dataTypes)):

- `integer` – non-floating-point numbers
- `number` – floating point numbers
- `string` – strings
- `boolean` – `true` or `false`
- `date` – rendered differentially as time since this timestamp.
-->
<h4 id="type">类型   </h4>
<p>列的 <code>type</code> 字段可以是以下值之一
（比较 <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#dataTypes">OpenAPI v3 数据类型</a>）：</p>
<ul>
<li><code>integer</code> – 非浮点数字</li>
<li><code>number</code> – 浮点数字</li>
<li><code>string</code> – 字符串</li>
<li><code>boolean</code> – <code>true</code> 或 <code>false</code></li>
<li><code>date</code> – 显示为以自此时间戳以来经过的时长</li>
</ul>
<!--
If the value inside a CustomResource does not match the type specified for the column,
the value is omitted. Use CustomResource validation to ensure that the value
types are correct.
-->
<p>如果定制资源中的值与列中指定的类型不匹配，该值会被忽略。
你可以通过定制资源的合法性检查来确保取值类型是正确的。</p>
<!--
#### Format

A column's `format` field can be any of the following:
-->
<h4 id="format">格式   </h4>
<p>列的 <code>format</code> 字段可以是以下值之一：</p>
<ul>
<li><code>int32</code></li>
<li><code>int64</code></li>
<li><code>float</code></li>
<li><code>double</code></li>
<li><code>byte</code></li>
<li><code>date</code></li>
<li><code>date-time</code></li>
<li><code>password</code></li>
</ul>
<!--
The column's `format` controls the style used when `kubectl` prints the value.
-->
<p>列的 <code>format</code> 字段控制 <code>kubectl</code> 打印对应取值时采用的风格。</p>
<!--
### Subresources

Custom resources support `/status` and `/scale` subresources.

The status and scale subresources can be optionally enabled by
defining them in the CustomResourceDefinition.
-->
<h3 id="subresources">子资源    </h3>
<p>定制资源支持 <code>/status</code> 和 <code>/scale</code> 子资源。</p>
<p>通过在 CustomResourceDefinition 中定义 <code>status</code> 和 <code>scale</code>，
可以有选择地启用这些子资源。</p>
<!--
#### Status subresource

When the status subresource is enabled, the `/status` subresource for the custom resource is exposed.

- The status and the spec stanzas are represented by the `.status` and `.spec` JSONPaths respectively inside of a custom resource.
- `PUT` requests to the `/status` subresource take a custom resource object and ignore changes to anything except the status stanza.
- `PUT` requests to the `/status` subresource only validate the status stanza of the custom resource.
- `PUT`/`POST`/`PATCH` requests to the custom resource ignore changes to the status stanza.
- The `.metadata.generation` value is incremented for all changes, except for changes to `.metadata` or `.status`.
-->
<h4 id="status-subresource">Status 子资源 </h4>
<p>当启用了 status 子资源时，对应定制资源的 <code>/status</code> 子资源会被暴露出来。</p>
<ul>
<li>status 和 spec 内容分别用定制资源内的 <code>.status</code> 和 <code>.spec</code> JSON 路径来表达；</li>
<li>对 <code>/status</code> 子资源的 <code>PUT</code> 请求要求使用定制资源对象作为其输入，但会忽略
status 之外的所有内容。</li>
<li>对 <code>/status</code> 子资源的 <code>PUT</code> 请求仅对定制资源的 status 内容进行合法性检查。</li>
<li>对定制资源的 <code>PUT</code>、<code>POST</code>、<code>PATCH</code> 请求会忽略 status 内容的改变。</li>
<li>对所有变更请求，除非改变是针对 <code>.metadata</code> 或 <code>.status</code>，<code>.metadata.generation</code>
的取值都会增加。</li>
</ul>
<!--
- Only the following constructs are allowed at the root of the CRD OpenAPI validation schema:
-->
<ul>
<li>
<p>在 CRD OpenAPI 合法性检查模式定义的根节点，只允许存在以下结构：</p>
<ul>
<li><code>description</code></li>
<li><code>example</code></li>
<li><code>exclusiveMaximum</code></li>
<li><code>exclusiveMinimum</code></li>
<li><code>externalDocs</code></li>
<li><code>format</code></li>
<li><code>items</code></li>
<li><code>maximum</code></li>
<li><code>maxItems</code></li>
<li><code>maxLength</code></li>
<li><code>minimum</code></li>
<li><code>minItems</code></li>
<li><code>minLength</code></li>
<li><code>multipleOf</code></li>
<li><code>pattern</code></li>
<li><code>properties</code></li>
<li><code>required</code></li>
<li><code>title</code></li>
<li><code>type</code></li>
<li><code>uniqueItems</code></li>
</ul>
</li>
</ul>
<!--
#### Scale subresource

When the scale subresource is enabled, the `/scale` subresource for the custom resource is exposed.
The `autoscaling/v1.Scale` object is sent as the payload for `/scale`.

To enable the scale subresource, the following fields are defined in the CustomResourceDefinition.
-->
<h4 id="scale-subresource">Scale 子资源  </h4>
<p>当启用了 scale 子资源时，定制资源的 <code>/scale</code> 子资源就被暴露出来。
针对 <code>/scale</code> 所发送的对象是 <code>autoscaling/v1.Scale</code>。</p>
<p>为了启用 scale 子资源，CustomResourceDefinition 定义了以下字段：</p>
<!--
- `specReplicasPath` defines the JSONPath inside of a custom resource that corresponds to `scale.spec.replicas`.

  - It is a required value.
  - Only JSONPaths under `.spec` and with the dot notation are allowed.
  - If there is no value under the `specReplicasPath` in the custom resource,
the `/scale` subresource will return an error on GET.
-->
<ul>
<li>
<p><code>specReplicasPath</code> 指定定制资源内与 <code>scale.spec.replicas</code> 对应的 JSON 路径。</p>
<ul>
<li>此字段为必需值。</li>
<li>只可以使用 <code>.spec</code> 下的 JSON 路径，只可使用带句点的路径。</li>
<li>如果定制资源的 <code>specReplicasPath</code> 下没有取值，则针对 <code>/scale</code> 子资源执行 GET
操作时会返回错误。</li>
</ul>
</li>
</ul>
<!--
- `statusReplicasPath` defines the JSONPath inside of a custom resource that corresponds to `scale.status.replicas`.

  - It is a required value.
  - Only JSONPaths under `.status` and with the dot notation are allowed.
  - If there is no value under the `statusReplicasPath` in the custom resource,
the status replica value in the `/scale` subresource will default to 0.
-->
<ul>
<li>
<p><code>statusReplicasPath</code> 指定定制资源内与 <code>scale.status.replicas</code> 对应的 JSON 路径。</p>
<ul>
<li>此字段为必需值。</li>
<li>只可以使用 <code>.status</code> 下的 JSON 路径，只可使用带句点的路径。</li>
<li>如果定制资源的 <code>statusReplicasPath</code> 下没有取值，则针对 <code>/scale</code> 子资源的
副本个数状态值默认为 0。</li>
</ul>
</li>
</ul>
<!--
- `labelSelectorPath` defines the JSONPath inside of a custom resource that corresponds to `scale.status.selector`.

  - It is an optional value.
  - It must be set to work with HPA.
  - Only JSONPaths under `.status` or `.spec` and with the dot notation are allowed.
  - If there is no value under the `labelSelectorPath` in the custom resource,
the status selector value in the `/scale` subresource will default to the empty string.
  - The field pointed by this JSON path must be a string field (not a complex selector struct) which contains a serialized label selector in string form.
-->
<ul>
<li>
<p><code>labelSelectorPath</code> 指定定制资源内与 <code>scale.status.selector</code> 对应的 JSON 路径。</p>
<ul>
<li>此字段为可选值。</li>
<li>此字段必须设置才能使用 HPA。</li>
<li>只可以使用 <code>.status</code> 或 <code>.spec</code> 下的 JSON 路径，只可使用带句点的路径。</li>
<li>如果定制资源的 <code>labelSelectorPath</code> 下没有取值，则针对 <code>/scale</code> 子资源的
选择算符状态值默认为空字符串。</li>
<li>此 JSON 路径所指向的字段必须是一个字符串字段（而不是复合的选择算符结构），
其中包含标签选择算符串行化的字符串形式。</li>
</ul>
</li>
</ul>
<!--
In the following example, both status and scale subresources are enabled.

Save the CustomResourceDefinition to `resourcedefinition.yaml`:
-->
<p>在下面的例子中，<code>status</code> 和 <code>scale</code> 子资源都被启用。</p>
<p>将此 CustomResourceDefinition 保存到 <code>resourcedefinition.yaml</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">labelSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># subresources 描述定制资源的子资源</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">subresources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># status 启用 status 子资源</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#080;font-style:italic"># scale 启用 scale 子资源</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scale</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># specReplicasPath 定义定制资源中对应 scale.spec.replicas 的 JSON 路径</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">specReplicasPath</span>:<span style="color:#bbb"> </span>.spec.replicas<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># statusReplicasPath 定义定制资源中对应 scale.status.replicas 的 JSON 路径 </span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">statusReplicasPath</span>:<span style="color:#bbb"> </span>.status.replicas<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#080;font-style:italic"># labelSelectorPath  定义定制资源中对应 scale.status.selector 的 JSON 路径 </span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">labelSelectorPath</span>:<span style="color:#bbb"> </span>.status.labelSelector<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div><!--
And create it:
-->
<p>之后创建此 CustomResourceDefinition：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f resourcedefinition.yaml
</code></pre></div><!--
After the CustomResourceDefinition object has been created, you can create custom objects.

If you save the following YAML to `my-crontab.yaml`:
-->
<p>CustomResourceDefinition 对象创建完毕之后，你可以创建定制对象，。</p>
<p>如果你将下面的 YAML 保存到 <code>my-crontab.yaml</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * * */5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb">
</span></code></pre></div><!--
and create it:
-->
<p>并创建定制对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-crontab.yaml
</code></pre></div><!--
Then new namespaced RESTful API endpoints are created at:
-->
<p>那么会创建新的、命名空间作用域的 RESTful  API 端点：</p>
<pre tabindex="0"><code>/apis/stable.example.com/v1/namespaces/*/crontabs/status
</code></pre><!-- and -->
<p>和</p>
<pre tabindex="0"><code>/apis/stable.example.com/v1/namespaces/*/crontabs/scale
</code></pre><!--
A custom resource can be scaled using the `kubectl scale` command.
For example, the following command sets `.spec.replicas` of the
custom resource created above to 5:
-->
<p>定制资源可以使用 <code>kubectl scale</code> 命令来扩缩其规模。
例如下面的命令将前面创建的定制资源的 <code>.spec.replicas</code> 设置为 5：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl scale --replicas<span style="color:#666">=</span><span style="color:#666">5</span> crontabs/my-new-cron-object
</code></pre></div><pre tabindex="0"><code>crontabs &quot;my-new-cron-object&quot; scaled
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get crontabs my-new-cron-object -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.spec.replicas}&#39;</span>
</code></pre></div><pre tabindex="0"><code>5
</code></pre><!--
You can use a [PodDisruptionBudget](/docs/tasks/run-application/configure-pdb/) to protect custom
resources that have the scale subresource enabled.
-->
<p>你可以使用 <a href="/zh/docs/tasks/run-application/configure-pdb/">PodDisruptionBudget</a>
来保护启用了 scale 子资源的定制资源。</p>
<!--
### Categories
-->
<h3 id="categories">分类  </h3>
<!--
Categories is a list of grouped resources the custom resource belongs to (eg. `all`).
You can use `kubectl get <category-name>` to list the resources belonging to the category.

The following example adds `all` in the list of categories in the CustomResourceDefinition
and illustrates how to output the custom resource using `kubectl get all`.

Save the following CustomResourceDefinition to `resourcedefinition.yaml`:
-->
<p>分类（Categories）是定制资源所归属的分组资源列表（例如，<code>all</code>）。
你可以使用 <code>kubectl get &lt;分类名称&gt;</code> 来列举属于某分类的所有资源。</p>
<p>下面的示例在 CustomResourceDefinition 中将 <code>all</code> 添加到分类列表中，
并展示了如何使用 <code>kubectl get all</code> 来输出定制资源：</p>
<p>将下面的 CustomResourceDefinition 保存到 <code>resourcedefinition.yaml</code> 文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>stable.example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">                </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># categories 是定制资源所归属的分类资源列表</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">categories</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- all<span style="color:#bbb">
</span></code></pre></div><!--
and create it:
-->
<p>之后创建此 CRD：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f resourcedefinition.yaml
</code></pre></div><!--
After the CustomResourceDefinition object has been created, you can create custom objects.

Save the following YAML to `my-crontab.yaml`:
-->
<p>创建了 CustomResourceDefinition 对象之后，你可以创建定制对象。</p>
<p>将下面的 YAML 保存到 <code>my-crontab.yaml</code> 中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;stable.example.com/v1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-new-cron-object<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">cronSpec</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;* * * * */5&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>my-awesome-cron-image<span style="color:#bbb">
</span></code></pre></div><!--
and create it:
-->
<p>并创建定制对象：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-crontab.yaml
</code></pre></div><!--
You can specify the category when using `kubectl get`:
-->
<p>你可以在使用 <code>kubectl get</code> 时指定分类：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get all
</code></pre></div><!--
The output will include the custom resources of kind `CronTab`:
-->
<p>输出中会包含类别为 <code>CronTab</code> 的定制资源：</p>
<pre tabindex="0"><code class="language-console" data-lang="console">NAME                          AGE
crontabs/my-new-cron-object   3s
</code></pre><h2 id="接下来">接下来</h2>
<!--
* Read about [custom resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/).

* See [CustomResourceDefinition](/docs/reference/generated/kubernetes-api/v1.22/#customresourcedefinition-v1-apiextensions-k8s-io).

* Serve [multiple versions](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/) of a
  CustomResourceDefinition.
-->
<ul>
<li>阅读了解<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源</a></li>
<li>参阅 <a href="/docs/reference/generated/kubernetes-api/v1.22/#customresourcedefinition-v1-apiextensions-k8s-io">CustomResourceDefinition</a></li>
<li>参阅支持 CustomResourceDefinition 的<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/">多个版本</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7d2e2400f208b1637530752794e5a3bd">11.1.2 - CustomResourceDefinition 的版本</h1>
    
	<!--
title: Versions in CustomResourceDefinitions
reviewers:
- sttts
- liggitt
content_type: task
weight: 30
min-kubernetes-server-version: v1.16
-->
<!-- overview -->
<!--
This page explains how to add versioning information to
[CustomResourceDefinitions](/docs/reference/generated/kubernetes-api/v1.22/#customresourcedefinition-v1beta1-apiextensions), to indicate the stability
level of your CustomResourceDefinitions or advance your API to a new version with conversion between API representations. It also describes how to upgrade an object from one version to another.
-->
<p>本页介绍如何添加版本信息到
<a href="/docs/reference/generated/kubernetes-api/v1.22/#customresourcedefinition-v1beta1-apiextensions">CustomResourceDefinitions</a>。
目的是标明 CustomResourceDefinitions 的稳定级别或者服务于 API 升级。
API 升级时需要在不同 API 表示形式之间进行转换。
本页还描述如何将对象从一个版本升级到另一个版本。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!--
You should have a initial understanding of [custom resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/).
-->
<p>你应该对<a href="/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/">定制资源</a>
有一些初步了解。</p>


您的 Kubernetes 服务器版本必须不低于版本 v1.16.
 要获知版本信息，请输入 <code>kubectl version</code>.

<!-- steps -->
<!--
## Overview

The CustomResourceDefinition API provides a workflow for introducing and upgrading
to new versions of a CustomResourceDefinition.

When a CustomResourceDefinition is created, the first version is set in the
CustomResourceDefinition `spec.versions` list to an appropriate stability level
and a version number. For example `v1beta1` would indicate that the first
version is not yet stable. All custom resource objects will initially be stored
at this version.

Once the CustomResourceDefinition is created, clients may begin using the
`v1beta1` API.

Later it might be necessary to add new version such as `v1`.
-->
<h2 id="概览">概览</h2>
<p>CustomResourceDefinition API 提供了用于引入和升级的工作流程到 CustomResourceDefinition
的新版本。</p>
<p>创建 CustomResourceDefinition 时，会在 CustomResourceDefinition <code>spec.versions</code>
列表设置适当的稳定级别和版本号。例如，<code>v1beta1</code> 表示第一个版本尚未稳定。
所有定制资源对象将首先用这个版本保存。</p>
<p>创建 CustomResourceDefinition 后，客户端可以开始使用 <code>v1beta1</code> API。</p>
<p>稍后可能需要添加新版本，例如 <code>v1</code>。</p>
<!--
Adding a new version:

1. Pick a conversion strategy. Since custom resource objects need to be able to
   be served at both versions, that means they will sometimes be served at a
   different version than their storage version. In order for this to be
   possible, the custom resource objects must sometimes be converted between the
   version they are stored at and the version they are served at. If the
   conversion involves schema changes and requires custom logic, a conversion
   webhook should be used. If there are no schema changes, the default `None`
   conversion strategy may be used and only the `apiVersion` field will be
   modified when serving different versions.
2. If using conversion webhooks, create and deploy the conversion webhook. See
   the [Webhook conversion](#webhook-conversion) for more details.
3. Update the CustomResourceDefinition to include the new version in the
   `spec.versions` list with `served:true`.  Also, set `spec.conversion` field
   to the selected conversion strategy. If using a conversion webhook, configure
   `spec.conversion.webhookClientConfig` field to call the webhook.
-->
<p>添加新版本：</p>
<ol>
<li>选择一种转化策略。由于定制资源对象需要能够两种版本都可用，
这意味着它们有时会以与存储版本不同的版本来提供服务。为了能够做到这一点，
有时必须在它们存储的版本和提供的版本之间进行转换。如果转换涉及模式变更，
并且需要自定义逻辑，则应该使用 Webhook 来完成。如果没有模式变更，
则可使用默认的 <code>None</code> 转换策略，为不同版本提供服务时只有 <code>apiVersion</code> 字段
会被改变。</li>
<li>如果使用转换 Webhook，请创建并部署转换 Webhook。更多详细信息请参见
<a href="#webhook-conversion">Webhook conversion</a>。</li>
<li>更新 CustomResourceDefinition，将新版本设置为 <code>served：true</code>，加入到
<code>spec.versions</code> 列表。另外，还要设置 <code>spec.conversion</code> 字段
为所选的转换策略。如果使用转换 Webhook，请配置
<code>spec.conversion.webhookClientConfig</code> 来调用 Webhook。</li>
</ol>
<!--
Once the new version is added, clients may incrementally migrate to the new
version. It is perfectly safe for some clients to use the old version while
others use the new version.

Migrate stored objects to the new version:
-->
<p>添加新版本后，客户端可以逐步迁移到新版本。让某些客户使用旧版本的同时
支持其他人使用新版本是相当安全的。</p>
<p>将存储的对象迁移到新版本：</p>
<!--
1. See the [upgrade existing objects to a new stored version](#upgrade-existing-objects-to-a-new-stored-version) section.

It is safe for clients to use both the old and new version before, during and
after upgrading the objects to a new stored version.
-->
<ol>
<li>请参阅<a href="#upgrade-existing-objects-to-a-new-stored-version">将现有对象升级到新的存储版本</a>节。</li>
</ol>
<p>对于客户来说，在将对象升级到新的存储版本之前、期间和之后使用旧版本和新版本都是安全的。</p>
<!--
Removing an old version:

1. Ensure all clients are fully migrated to the new version. The kube-apiserver
   logs can be reviewed to help identify any clients that are still accessing via
   the old version.
1. Set `served` to `false` for the old version in the `spec.versions` list. If
   any clients are still unexpectedly using the old version they may begin reporting
   errors attempting to access the custom resource objects at the old version.
   If this occurs, switch back to using `served:true` on the old version, migrate the 
   remaining clients to the new version and repeat this step.
1. Ensure the [upgrade of existing objects to the new stored version](#upgrade-existing-objects-to-a-new-stored-version) step has been completed.
    1. Verify that the `storage` is set to `true` for the new version in the `spec.versions` list in the CustomResourceDefinition.
    1. Verify that the old version is no longer listed in the CustomResourceDefinition `status.storedVersions`.
1. Remove the old version from the CustomResourceDefinition `spec.versions` list.
1. Drop conversion support for the old version in conversion webhooks.
-->
<p>删除旧版本：</p>
<ol>
<li>确保所有客户端都已完全迁移到新版本。
可以查看 kube-apiserver 的日志以识别仍通过旧版本进行访问的所有客户端。</li>
<li>在 <code>spec.versions</code> 列表中将旧版本的 <code>served</code> 设置为 <code>false</code>。
如果仍有客户端意外地使用旧版本，他们可能开始会报告采用旧版本尝试访
定制资源的错误消息。
如果发生这种情况，请将旧版本的<code>served：true</code> 恢复，然后迁移余下的客户端
使用新版本，然后重复此步骤。</li>
<li>确保已完成<a href="#upgrade-existing-objects-to-a-new-stored-version">将现有对象升级到新存储版本</a>
的步骤。
<ol>
<li>在 CustomResourceDefinition 的 <code>spec.versions</code> 列表中，确认新版本的
<code>storage</code> 已被设置为 <code>true</code>。</li>
<li>确认旧版本不在 CustomResourceDefinition <code>status.storedVersions</code> 中。</li>
</ol>
</li>
<li>从 CustomResourceDefinition <code>spec.versions</code> 列表中删除旧版本。</li>
<li>在转换 Webhooks 中放弃对旧版本的转换支持。</li>
</ol>
<!--
## Specify multiple versions

The CustomResourceDefinition API `versions` field can be used to support multiple versions of custom resources that you
have developed. Versions can have different schemas, and conversion webhooks can convert custom resources between versions.
Webhook conversions should follow the [Kubernetes API conventions](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md) wherever applicable.
Specifically, See the [API change documentation](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md) for a set of useful gotchas and suggestions.
-->
<h2 id="specify-multiple-versions">指定多个版本 </h2>
<p>CustomResourceDefinition API 的 <code>versions</code> 字段可用于支持你所开发的
定制资源的多个版本。版本可以具有不同的模式，并且转换 Webhooks
可以在多个版本之间转换定制资源。
在适当的情况下，Webhook 转换应遵循
<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">Kubernetes API 约定</a>。
尤其是，请查阅
<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md">API 变更文档</a>
以了解一些有用的常见错误和建议。</p>
<!--
In `apiextensions.k8s.io/v1beta1`, there was a `version` field instead of `versions`. The
`version` field is deprecated and optional, but if it is not empty, it must
match the first item in the `versions` field.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 在 <code>apiextensions.k8s.io/v1beta1</code> 版本中曾经有一个 <code>version</code> 字段，
名字不叫做 <code>versions</code>。该 <code>version</code> 字段已经被废弃，成为可选项。
不过如果该字段不是空，则必须与 <code>versions</code> 字段中的第一个条目匹配。</div>
</blockquote>
<!--
This example shows a CustomResourceDefinition with two versions. For the first
example, the assumption is all versions share the same schema with no conversion
between them. The comments in the YAML provide more context.
-->
<p>下面的示例显示了两个版本的 CustomResourceDefinition。
第一个例子中假设所有的版本使用相同的模式而它们之间没有转换。
YAML 中的注释提供了更多背景信息。</p>
<ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-1" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#customresourcedefinition-versioning-example-1-0" role="tab" aria-controls="customresourcedefinition-versioning-example-1-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#customresourcedefinition-versioning-example-1-1" role="tab" aria-controls="customresourcedefinition-versioning-example-1-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="customresourcedefinition-versioning-example-1"><div id="customresourcedefinition-versioning-example-1-0" class="tab-pane show active" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-1-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># name 必须匹配后面 spec 中的字段，且使用格式 &lt;plural&gt;.&lt;group&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 组名，用于 REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此 CustomResourceDefinition 所支持的版本列表</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 每个 version 可以通过 served 标志启用或禁止</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 有且只能有一个 version 必须被标记为存储版本</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># schema 是必需字段</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># conversion 节是 Kubernetes 1.13+ 版本引入的，其默认值为无转换，即</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># strategy 子字段设置为 None。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># None 转换假定所有版本采用相同的模式定义，仅仅将定制资源的 apiVersion</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 设置为合适的值.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 可以是 Namespaced 或 Cluster</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的复数形式，用于 URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的单数形式，用于在命令行接口和显示时作为其别名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># kind 通常是驼峰编码（CamelCased）的单数形式，用于资源清单中</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># shortNames 允许你在命令行接口中使用更短的字符串来匹配你的资源</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="customresourcedefinition-versioning-example-1-1" class="tab-pane" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-1-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 在 v1.16 中被弃用以推荐使用 apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># name 必须匹配后面 spec 中的字段，且使用格式 &lt;plural&gt;.&lt;group&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 组名，用于 REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此 CustomResourceDefinition 所支持的版本列表</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 每个 version 可以通过 served 标志启用或禁止</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 有且只能有一个 version 必须被标记为存储版本</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">validation</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># conversion 节是 Kubernetes 1.13+ 版本引入的，其默认值为无转换，即</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># strategy 子字段设置为 None。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># None 转换假定所有版本采用相同的模式定义，仅仅将定制资源的 apiVersion</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 设置为合适的值.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 可以是 Namespaced 或 Cluster</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的复数形式，用于 URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的单数形式，用于在命令行接口和显示时作为其别名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># kind 通常是大驼峰编码（PascalCased）的单数形式，用于资源清单中</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># shortNames 允许你在命令行接口中使用更短的字符串来匹配你的资源</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
You can save the CustomResourceDefinition in a YAML file, then use
`kubectl apply` to create it.
-->
<p>你可以将 CustomResourceDefinition 存储在 YAML 文件中，然后使用
<code>kubectl apply</code> 来创建它。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-versioned-crontab.yaml
</code></pre></div><!--
After creation, the API server starts to serve each enabled version at an HTTP
REST endpoint. In the above example, the API versions are available at
`/apis/example.com/v1beta1` and `/apis/example.com/v1`.
-->
<p>在创建之后，API 服务器开始在 HTTP REST 端点上为每个已启用的版本提供服务。
在上面的示例中，API 版本可以在 <code>/apis/example.com/v1beta1</code> 和
<code>/apis/example.com/v1</code> 处获得。</p>
<!--
### Version priority

Regardless of the order in which versions are defined in a
CustomResourceDefinition, the version with the highest priority is used by
kubectl as the default version to access objects. The priority is determined
by parsing the _name_ field to determine the version number, the stability
(GA, Beta, or Alpha), and the sequence within that stability level.
-->
<h3 id="版本优先级">版本优先级</h3>
<p>不考虑 CustomResourceDefinition 中版本被定义的顺序，kubectl 使用
具有最高优先级的版本作为访问对象的默认版本。
通过解析 <em>name</em> 字段确定优先级来决定版本号，稳定性（GA、Beta 或 Alpha）
级别及该稳定性级别内的序列。</p>
<!--
The algorithm used for sorting the versions is designed to sort versions in the
same way that the Kubernetes project sorts Kubernetes versions. Versions start with a
`v` followed by a number, an optional `beta` or `alpha` designation, and
optional additional numeric versioning information. Broadly, a version string might look
like `v2` or `v2beta1`. Versions are sorted using the following algorithm:
-->
<p>用于对版本进行排序的算法在设计上与 Kubernetes 项目对 Kubernetes 版本进行排序的方式相同。
版本以 <code>v</code> 开头跟一个数字，一个可选的 <code>beta</code> 或者 <code>alpha</code> 和一个可选的附加数字
作为版本信息。
从广义上讲，版本字符串可能看起来像 <code>v2</code> 或者 <code>v2beta1</code>。
使用以下算法对版本进行排序：</p>
<!--
- Entries that follow Kubernetes version patterns are sorted before those that
  do not.
- For entries that follow Kubernetes version patterns, the numeric portions of
  the version string is sorted largest to smallest.
- If the strings `beta` or `alpha` follow the first numeric portion, they sorted
  in that order, after the equivalent string without the `beta` or `alpha`
  suffix (which is presumed to be the GA version).
- If another number follows the `beta`, or `alpha`, those numbers are also
  sorted from largest to smallest.
- Strings that don't fit the above format are sorted alphabetically and the
  numeric portions are not treated specially. Notice that in the example below,
  `foo1` is sorted above `foo10`. This is different from the sorting of the
  numeric portion of entries that do follow the Kubernetes version patterns.
-->
<ul>
<li>遵循 Kubernetes 版本模式的条目在不符合条件的条目之前进行排序。</li>
<li>对于遵循 Kubernetes 版本模式的条目，版本字符串的数字部分从最大到最小排序。</li>
<li>如果第一个数字后面有字符串 <code>beta</code> 或 <code>alpha</code>，它们首先按去掉 <code>beta</code> 或
<code>alpha</code> 之后的版本号排序（相当于 GA 版本），之后按 <code>beta</code> 先、<code>alpha</code> 后的顺序排序，</li>
<li>如果 <code>beta</code> 或 <code>alpha</code> 之后还有另一个数字，那么也会针对这些数字
从大到小排序。</li>
<li>不符合上述格式的字符串按字母顺序排序，数字部分不经过特殊处理。
请注意，在下面的示例中，<code>foo1</code> 排在 <code>foo10</code> 之前。
这与遵循 Kubernetes 版本模式的条目的数字部分排序不同。</li>
</ul>
<!--
This might make sense if you look at the following sorted version list:
-->
<p>如果查看以下版本排序列表，这些规则就容易懂了：</p>
<pre tabindex="0"><code class="language-none" data-lang="none">- v10
- v2
- v1
- v11beta2
- v10beta3
- v3beta1
- v12alpha1
- v11alpha2
- foo1
- foo10
</code></pre><!--
For the example in [Specify multiple versions](#specify-multiple-versions), the
version sort order is `v1`, followed by `v1beta1`. This causes the kubectl
command to use `v1` as the default version unless the provided object specifies
the version.
-->
<p>对于<a href="#specify-multiple-versions">指定多个版本</a>中的示例，版本排序顺序为
<code>v1</code>，后跟着 <code>v1beta1</code>。
这导致了 kubectl 命令使用 <code>v1</code> 作为默认版本，除非所提供的对象指定了版本。</p>
<!--
### Version deprecation
-->
<h3 id="版本废弃">版本废弃</h3>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code>
</div>

<!--
Starting in v1.19, a CustomResourceDefinition can indicate a particular version of the resource it defines is deprecated.
When API requests to a deprecated version of that resource are made, a warning message is returned in the API response as a header.
The warning message for each deprecated version of the resource can be customized if desired.
-->
<p>从 v1.19 开始，CustomResourceDefinition 可用来标明所定义的资源的特定版本
被废弃。当发起对已废弃的版本的 API 请求时，会在 API 响应中以 HTTP 头部
的形式返回警告消息。
如果需要，可以对资源的每个废弃版本定制该警告消息。</p>
<!--
A customized warning message should indicate the deprecated API group, version, and kind,
and should indicate what API group, version, and kind should be used instead, if applicable.
-->
<p>定制的警告消息应该标明废弃的 API 组、版本和类别（kind），并且应该标明
应该使用（如果有的话）哪个 API 组、版本和类别作为替代。</p>
<ul class="nav nav-tabs" id="customresourcedefinition-versioning-deprecated" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#customresourcedefinition-versioning-deprecated-0" role="tab" aria-controls="customresourcedefinition-versioning-deprecated-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#customresourcedefinition-versioning-deprecated-1" role="tab" aria-controls="customresourcedefinition-versioning-deprecated-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="customresourcedefinition-versioning-deprecated"><div id="customresourcedefinition-versioning-deprecated-0" class="tab-pane show active" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-deprecated-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此属性标明此定制资源的 v1alpha1 版本已被弃用。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 发给此版本的 API 请求会在服务器响应中收到警告消息头。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">deprecated</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此属性设置用来覆盖返回给发送 v1alpha1 API 请求的客户端的默认警告信息。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">deprecationWarning</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;example.com/v1alpha1 CronTab is deprecated; see http://example.com/v1alpha1-v1 for instructions to migrate to example.com/v1 CronTab&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此属性标明该定制资源的 v1beta1 版本已被弃用。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 发给此版本的 API 请求会在服务器响应中收到警告消息头。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 针对此版本的请求所返回的是默认的警告消息。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">deprecated</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="customresourcedefinition-versioning-deprecated-1" class="tab-pane" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-deprecated-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 在 v1.16 中弃用以推荐使用  apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">validation</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此属性标明此定制资源的 v1alpha1 版本已被弃用。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 发给此版本的 API 请求会在服务器响应中收到警告消息头。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">deprecated</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此属性设置用来覆盖返回给发送 v1alpha1 API 请求的客户端的默认警告信息。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">deprecationWarning</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;example.com/v1alpha1 CronTab is deprecated; see http://example.com/v1alpha1-v1 for instructions to migrate to example.com/v1 CronTab&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 此属性标明该定制资源的 v1beta1 版本已被弃用。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 发给此版本的 API 请求会在服务器响应中收到警告消息头。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 针对此版本的请求所返回的是默认的警告消息。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">deprecated</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
## Webhook conversion

Webhook conversion is available as beta since 1.15, and as alpha since Kubernetes 1.13. The
`CustomResourceWebhookConversion` feature should be enabled. Please refer to the [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) documentation for more information.
-->
<h2 id="webhook-conversion">Webhook 转换  </h2>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.16 [stable]</code>
</div>

<blockquote class="note callout">
  <div><strong>说明：</strong> Webhook 转换在 Kubernetes 1.13 版本引入，在 Kubernetes 1.15 中成为 Beta 功能。
要使用此功能，应启用 <code>CustomResourceWebhookConversion</code> 特性。
在大多数集群上，这类 Beta 特性应该是自动启用的。
请参阅<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
文档以获得更多信息。</div>
</blockquote>
<!--
The above example has a None conversion between versions which only sets the `apiVersion` field
on conversion and does not change the rest of the object. The API server also supports webhook
conversions that call an external service in case a conversion is required. For example when:
-->
<p>上面的例子在版本之间有一个 None 转换，它只在转换时设置 <code>apiVersion</code> 字段
而不改变对象的其余部分。API 服务器还支持在需要转换时调用外部服务的 webhook 转换。
例如：</p>
<!--
* custom resource is requested in a different version than stored version.
* Watch is created in one version but the changed object is stored in another version.
* custom resource PUT request is in a different version than storage version.
-->
<ul>
<li>定制资源的请求版本与其存储版本不同。</li>
<li>使用某版本创建了 Watch 请求，但所更改对象以另一版本存储。</li>
<li>定制资源的 PUT 请求所针对版本与存储版本不同。</li>
</ul>
<!--
To cover all of these cases and to optimize conversion by the API server,
the conversion requests may contain multiple objects in order to minimize the external calls.
The webhook should perform these conversions independently.
-->
<p>为了涵盖所有这些情况并优化 API 服务器所作的转换，转换请求可以包含多个对象，
以便减少外部调用。Webhook 应该独立执行各个转换。</p>
<!--
### Write a conversion webhook server

Please refer to the implementation of the [custom resource conversion webhook
server](https://github.com/kubernetes/kubernetes/tree/v1.15.0/test/images/crd-conversion-webhook/main.go)
that is validated in a Kubernetes e2e test. The webhook handles the
`ConversionReview` requests sent by the API servers, and sends back conversion
results wrapped in `ConversionResponse`. Note that the request
contains a list of custom resources that need to be converted independently without
changing the order of objects.
The example server is organized in a way to be reused for other conversions.
Most of the common code are located in the
[framework file](https://github.com/kubernetes/kubernetes/tree/v1.15.0/test/images/crd-conversion-webhook/converter/framework.go)
that leaves only
[one function](https://github.com/kubernetes/kubernetes/blob/v1.15.0/test/images/crd-conversion-webhook/converter/example_converter.go#L29-L80)
to be implemented for different conversions.
-->
<h3 id="编写一个转换-webhook-服务器">编写一个转换 Webhook 服务器</h3>
<p>请参考<a href="https://github.com/kubernetes/kubernetes/tree/v1.15.0/test/images/crd-conversion-webhook/main.go">定制资源转换 Webhook 服务器</a>
的实现；该实现在 Kubernetes e2e 测试中得到验证。
Webhook 处理由 API 服务器发送的 <code>ConversionReview</code> 请求，并在
<code>ConversionResponse</code> 中封装发回转换结果。
请注意，请求包含需要独立转换的定制资源列表，这些对象在被转换之后不能改变其
在列表中的顺序。该示例服务器的组织方式使其可以复用于其他转换。
大多数常见代码都位于
<a href="https://github.com/kubernetes/kubernetes/tree/v1.15.0/test/images/crd-conversion-webhook/converter/framework.go">framework 文件</a>
中，只留下
<a href="https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/crd-conversion-webhook/converter/example_converter.go#L29-L80">一个函数</a>
用于实现不同的转换。</p>
<!--
The example conversion webhook server leaves the `ClientAuth` field
[empty](https://github.com/kubernetes/kubernetes/tree/v1.13.0/test/images/crd-conversion-webhook/config.go#L47-L48),
which defaults to `NoClientCert`. This means that the webhook server does not
authenticate the identity of the clients, supposedly API servers. If you need
mutual TLS or other ways to authenticate the clients, see
how to [authenticate API servers](/docs/reference/access-authn-authz/extensible-admission-controllers/#authenticate-apiservers).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 转换 Webhook 服务器示例中将 <code>ClientAuth</code> 字段设置为
<a href="https://github.com/kubernetes/kubernetes/tree/v1.13.0/test/images/crd-conversion-webhook/config.go#L47-L48">空</a>，
默认为 <code>NoClientCert</code>。
这意味着 webhook 服务器没有验证客户端（也就是 API 服务器）的身份。
如果你需要双向 TLS 或者其他方式来验证客户端，请参阅如何
<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#authenticate-apiservers">验证 API 服务</a>。</div>
</blockquote>
<!--
#### Permissible mutations

A conversion webhook must not mutate anything inside of `metadata` of the converted object
other than `labels` and `annotations`.
Attempted changes to `name`, `UID` and `namespace` are rejected and fail the request
which caused the conversion. All other changes are ignored. 
-->
<h4 id="被允许的变更">被允许的变更</h4>
<p>转换 Webhook 不可以更改被转换对象的 <code>metadata</code> 中除 <code>labels</code> 和 <code>annotations</code>
之外的任何属性。
尝试更改 <code>name</code>、<code>UID</code> 和 <code>namespace</code> 时都会导致引起转换的请求失败。
所有其他变更都被忽略。</p>
<!--
### Deploy the conversion webhook service

Documentation for deploying the conversion webhook is the same as for the [admission webhook example service](/docs/reference/access-authn-authz/extensible-admission-controllers/#deploy_the_admission_webhook_service).
The assumption for next sections is that the conversion webhook server is deployed to a service named `example-conversion-webhook-server` in `default` namespace and serving traffic on path `/crdconvert`.
-->
<h3 id="部署转换-webhook-服务">部署转换 Webhook 服务</h3>
<p>用于部署转换 webhook 的文档与
<a href="/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#deploy_the_admission_webhook_service">准入 Webhook 服务示例</a>相同。
这里的假设是转换 Webhook 服务器被部署为 <code>default</code> 名字空间中名为
<code>example-conversion-webhook-server</code> 的服务，并在路径 <code>/crdconvert</code>
上处理请求。</p>
<!--
When the webhook server is deployed into the Kubernetes cluster as a
service, it has to be exposed via a service on port 443 (The server
itself can have an arbitrary port but the service object should map it to port 443).
The communication between the API server and the webhook service may fail
if a different port is used for the service.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 当 Webhook 服务器作为一个服务被部署到 Kubernetes 集群中时，它必须
通过端口 443 公开其服务（服务器本身可以使用任意端口，但是服务对象
应该将它映射到端口 443）。
如果为服务器使用不同的端口，则 API 服务器和 Webhook 服务器之间的通信
可能会失败。</div>
</blockquote>
<!--
### Configure CustomResourceDefinition to use conversion webhooks

The `None` conversion example can be extended to use the conversion webhook by modifying `conversion`
section of the `spec`:
-->
<h3 id="配置-customresourcedefinition-以使用转换-webhook">配置 CustomResourceDefinition 以使用转换 Webhook</h3>
<p>通过修改 <code>spec</code> 中的 <code>conversion</code> 部分，可以扩展 <code>None</code> 转换示例来
使用转换 Webhook。</p>
<ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-2" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#customresourcedefinition-versioning-example-2-0" role="tab" aria-controls="customresourcedefinition-versioning-example-2-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#customresourcedefinition-versioning-example-2-1" role="tab" aria-controls="customresourcedefinition-versioning-example-2-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="customresourcedefinition-versioning-example-2"><div id="customresourcedefinition-versioning-example-2-0" class="tab-pane show active" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-2-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># name 必须匹配后面 spec 中的字段，且使用格式 &lt;plural&gt;.&lt;group&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 组名，用于 REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此 CustomResourceDefinition 所支持的版本列表</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 每个 version 可以通过 served 标志启用或禁止</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 有且只能有一个 version 必须被标记为存储版本</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 当不存在顶级模式定义时，每个版本（version）可以定义其自身的模式</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Webhook strategy 告诉 API 服务器调用外部 Webhook 来完成定制资源</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 之间的转换</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 当 strategy 为 &#34;Webhook&#34; 时，webhook 属性是必需的</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 该属性配置将被 API 服务器调用的 Webhook 端点</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">webhook</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># conversionReviewVersions 标明 Webhook 所能理解或偏好使用的</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># ConversionReview 对象版本。</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># API 服务器所能理解的列表中的第一个版本会被发送到 Webhook</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Webhook 必须按所接收到的版本响应一个 ConversionReview 对象</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">conversionReviewVersions</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;v1&#34;</span>,<span style="color:#b44">&#34;v1beta1&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">clientConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-conversion-webhook-server<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/crdconvert<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">caBundle</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 可以是 Namespaced 或 Cluster</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的复数形式，用于 URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的单数形式，用于在命令行接口和显示时作为其别名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># kind 通常是驼峰编码（CamelCased）的单数形式，用于资源清单中</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># shortNames 允许你在命令行接口中使用更短的字符串来匹配你的资源</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="customresourcedefinition-versioning-example-2-1" class="tab-pane" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-2-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 在 v1.16 中被弃用以推荐使用 apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># name 必须匹配后面 spec 中的字段，且使用格式 &lt;plural&gt;.&lt;group&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>crontabs.example.com<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 组名，用于 REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 裁剪掉下面的 OpenAPI 模式中未曾定义的对象字段</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">preserveUnknownFields</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 此 CustomResourceDefinition 所支持的版本列表</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versions</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 每个 version 可以通过 served 标志启用或禁止</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 有且只能有一个 version 必须被标记为存储版本</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 当不存在顶级模式定义时，每个版本（version）可以定义其自身的模式</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">served</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Webhook strategy 告诉 API 服务器调用外部 Webhook 来完成定制资源</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 当 strategy 为 &#34;Webhook&#34; 时，webhookClientConfig 属性是必需的</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 该属性配置将被 API 服务器调用的 Webhook 端点</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">webhookClientConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-conversion-webhook-server<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/crdconvert<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">caBundle</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 可以是 Namespaced 或 Cluster</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">scope</span>:<span style="color:#bbb"> </span>Namespaced<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">names</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的复数形式，用于 URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">plural</span>:<span style="color:#bbb"> </span>crontabs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 名称的单数形式，用于在命令行接口和显示时作为其别名</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">singular</span>:<span style="color:#bbb"> </span>crontab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># kind 通常是驼峰编码（CamelCased）的单数形式，用于资源清单中</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># shortNames 允许你在命令行接口中使用更短的字符串来匹配你的资源</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">shortNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- ct<span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
You can save the CustomResourceDefinition in a YAML file, then use
`kubectl apply` to apply it.
-->
<p>你可以将 CustomResourceDefinition 保存在 YAML 文件中，然后使用
<code>kubectl apply</code> 来应用它。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f my-versioned-crontab-with-conversion.yaml
</code></pre></div><!--
Make sure the conversion service is up and running before applying new changes.
-->
<p>在应用新更改之前，请确保转换服务器已启动并正在运行。</p>
<!--
### Contacting the webhook

Once the API server has determined a request should be sent to a conversion webhook,
it needs to know how to contact the webhook. This is specified in the `webhookClientConfig`
stanza of the webhook configuration.

Conversion webhooks can either be called via a URL or a service reference,
and can optionally include a custom CA bundle to use to verify the TLS connection.
-->
<h3 id="调用-webhook">调用 Webhook</h3>
<p>API 服务器一旦确定请求应发送到转换 Webhook，它需要知道如何调用 Webhook。
这是在 <code>webhookClientConfig</code> 中指定的 Webhook 配置。</p>
<p>转换 Webhook 可以通过 URL 或服务引用来调用，并且可以选择包含自定义 CA 包，
以用于验证 TLS 连接。</p>
<h3 id="url">URL</h3>
<!--
`url` gives the location of the webhook, in standard URL form
(`scheme://host:port/path`). 

The `host` should not refer to a service running in the cluster; use
a service reference by specifying the `service` field instead.
The host might be resolved via external DNS in some apiservers
(i.e., `kube-apiserver` cannot resolve in-cluster DNS as that would
be a layering violation). `host` may also be an IP address.

Please note that using `localhost` or `127.0.0.1` as a `host` is
risky unless you take great care to run this webhook on all hosts
which run an apiserver which might need to make calls to this
webhook. Such installations are likely to be non-portable or not readily run in a new cluster.
-->
<p>url 以标准 URL 形式给出 Webhook 的位置（<code>scheme://host:port/path</code>）。
<code>host</code> 不应引用集群中运行的服务，而应通过指定 <code>service</code> 字段来提供
服务引用。
在某些 API 服务器中，<code>host</code> 可以通过外部 DNS 进行解析（即
<code>kube-apiserver</code> 无法解析集群内 DNS，那样会违反分层规则）。
<code>host</code> 也可以是 IP 地址。</p>
<p>请注意，除非你非常小心地在所有运行着可能调用 Webhook 的 API 服务器的
主机上运行此 Webhook，否则将 <code>localhost</code> 或 <code>127.0.0.1</code> 用作 <code>host</code>
是风险很大的。这样的安装可能是不可移植的，或者不容易在一个新的集群中运行。</p>
<!--
The scheme must be "https"; the URL must begin with "https://".

Attempting to use a user or basic auth e.g. "user:password@" is not allowed.
Fragments ("#...") and query parameters ("?...") are also not allowed.

Here is an example of a conversion webhook configured to call a URL
(and expects the TLS certificate to be verified using system trust roots, so does not specify a caBundle):
-->
<p>HTTP 协议必须为 <code>https</code>；URL 必须以 <code>https://</code> 开头。</p>
<p>尝试使用用户或基本身份验证（例如，使用<code>user:password@</code>）是不允许的。
URL 片段（<code>#...</code>）和查询参数（<code>?...</code>）也是不允许的。</p>
<p>下面是为调用 URL 来执行转换 Webhook 的示例，其中期望使用系统信任根
来验证 TLS 证书，因此未指定 caBundle：</p>
<ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-3" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#customresourcedefinition-versioning-example-3-0" role="tab" aria-controls="customresourcedefinition-versioning-example-3-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#customresourcedefinition-versioning-example-3-1" role="tab" aria-controls="customresourcedefinition-versioning-example-3-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="customresourcedefinition-versioning-example-3"><div id="customresourcedefinition-versioning-example-3-0" class="tab-pane show active" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-3-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">webhook</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">clientConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">url</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;https://my-webhook.example.com:9443/my-webhook-path&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div></div>
  <div id="customresourcedefinition-versioning-example-3-1" class="tab-pane" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-3-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># 在 v1.16 中已弃用以推荐使用 apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">webhookClientConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">url</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;https://my-webhook.example.com:9443/my-webhook-path&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
### Service Reference

The `service` stanza inside `webhookClientConfig` is a reference to the service for a conversion webhook.
If the webhook is running within the cluster, then you should use `service` instead of `url`.
The service namespace and name are required. The port is optional and defaults to 443.
The path is optional and defaults to "/".

Here is an example of a webhook that is configured to call a service on port "1234"
at the subpath "/my-path", and to verify the TLS connection against the ServerName
`my-service-name.my-service-namespace.svc` using a custom CA bundle.
-->
<h3 id="服务引用">服务引用</h3>
<p><code>webhookClientConfig</code> 内部的 <code>service</code> 段是对转换 Webhook 服务的引用。
如果 Webhook 在集群中运行，则应使用 <code>service</code> 而不是 <code>url</code>。
服务的名字空间和名称是必需的。端口是可选的，默认为 443。
路径是可选的，默认为<code>/</code>。</p>
<p>下面配置中，服务配置为在端口 <code>1234</code>、子路径 <code>/my-path</code> 上被调用。
例子中针对 ServerName <code>my-service-name.my-service-namespace.svc</code>，
使用自定义 CA 包验证 TLS 连接。</p>
<ul class="nav nav-tabs" id="customresourcedefinition-versioning-example-4" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#customresourcedefinition-versioning-example-4-0" role="tab" aria-controls="customresourcedefinition-versioning-example-4-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#customresourcedefinition-versioning-example-4-1" role="tab" aria-controls="customresourcedefinition-versioning-example-4-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="customresourcedefinition-versioning-example-4"><div id="customresourcedefinition-versioning-example-4-0" class="tab-pane show active" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-4-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">webhook</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">clientConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>my-service-namespace<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service-name<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/my-path<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">1234</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">caBundle</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div></div>
  <div id="customresourcedefinition-versioning-example-4-1" class="tab-pane" role="tabpanel" aria-labelledby="customresourcedefinition-versioning-example-4-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic">#  v1.16 中被弃用以推荐使用 apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">webhookClientConfig</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>my-service-namespace<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service-name<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/my-path<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">1234</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">caBundle</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
## Webhook request and response

### Request

Webhooks are sent a POST request, with `Content-Type: application/json`,
with a `ConversionReview` API object in the `apiextensions.k8s.io` API group
serialized to JSON as the body.

Webhooks can specify what versions of `ConversionReview` objects they accept
with the `conversionReviewVersions` field in their CustomResourceDefinition:
-->
<h2 id="webhook-请求和响应">Webhook 请求和响应</h2>
<h3 id="请求">请求</h3>
<p>向 Webhooks 发起请求的动词是 POST，请求的 <code>Content-Type</code> 为 <code>application/json</code>。
请求的主题为 JSON 序列化形式的
apiextensions.k8s.io API 组的 ConversionReview API 对象。</p>
<p>Webhooks 可以在其 CustomResourceDefinition 中使用<code>conversionReviewVersions</code> 字段
设置它们接受的 <code>ConversionReview</code> 对象的版本：</p>
<ul class="nav nav-tabs" id="conversionreviewversions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#conversionreviewversions-0" role="tab" aria-controls="conversionreviewversions-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#conversionreviewversions-1" role="tab" aria-controls="conversionreviewversions-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="conversionreviewversions"><div id="conversionreviewversions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="conversionreviewversions-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">webhook</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">conversionReviewVersions</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;v1&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;v1beta1&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">      </span>...<span style="color:#bbb">
</span></code></pre></div><!--
`conversionReviewVersions` is a required field when creating 
`apiextensions.k8s.io/v1` custom resource definitions.
Webhooks are required to support at least one `ConversionReview`
version understood by the current and previous API server.
-->
<p>创建 <code>apiextensions.k8s.io/v1</code> 版本的自定义资源定义时，
<code>conversionReviewVersions</code>是必填字段。
Webhooks 要求支持至少一个 <code>ConversionReview</code> 当前和以前的 API 服务器
可以理解的版本。</p>
</div>
  <div id="conversionreviewversions-1" class="tab-pane" role="tabpanel" aria-labelledby="conversionreviewversions-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># v1.16 已弃用以推荐使用 apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">conversion</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">strategy</span>:<span style="color:#bbb"> </span>Webhook<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">conversionReviewVersions</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;v1&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;v1beta1&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span>...<span style="color:#bbb">
</span></code></pre></div><!--
If no `conversionReviewVersions` are specified, the default when creating 
`apiextensions.k8s.io/v1beta1` custom resource definitions is `v1beta1`.
-->
<p>创建 apiextensions.k8s.io/v1beta1 定制资源定义时若未指定
<code>conversionReviewVersions</code>，则默认值为 v1beta1。</p>
</div></div>

<!--
API servers send the first `ConversionReview` version in the `conversionReviewVersions` list they support.
If none of the versions in the list are supported by the API server, the custom resource definition will not be allowed to be created.
If an API server encounters a conversion webhook configuration that was previously created and does not support any of the `ConversionReview`
versions the API server knows how to send, attempts to call to the webhook will fail.
-->
<p>API 服务器将 <code>conversionReviewVersions</code> 列表中他们所支持的第一个
<code>ConversionReview</code> 资源版本发送给 Webhook。
如果列表中的版本都不被 API 服务器支持，则无法创建自定义资源定义。
如果某 API 服务器遇到之前创建的转换 Webhook 配置，并且该配置不支持
API 服务器知道如何发送的任何 <code>ConversionReview</code> 版本，调用 Webhook
的尝试会失败。</p>
<!--
This example shows the data contained in an `ConversionReview` object
for a request to convert `CronTab` objects to `example.com/v1`:
-->
<p>下面的示例显示了包含在 <code>ConversionReview</code> 对象中的数据，
该请求意在将 <code>CronTab</code> 对象转换为 <code>example.com/v1</code>：</p>
<ul class="nav nav-tabs" id="conversionreview-request" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#conversionreview-request-0" role="tab" aria-controls="conversionreview-request-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#conversionreview-request-1" role="tab" aria-controls="conversionreview-request-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="conversionreview-request"><div id="conversionreview-request-0" class="tab-pane show active" role="tabpanel" aria-labelledby="conversionreview-request-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConversionReview<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">request</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 用来唯一标识此转换调用的随机 UID</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>705ab4f5-6393-11e8-b7cc-42010a800002<span style="color:#bbb">
</span><span style="color:#bbb">  
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 对象要转换到的目标 API 组和版本</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">desiredAPIVersion</span>:<span style="color:#bbb"> </span>example.com/v1<span style="color:#bbb">
</span><span style="color:#bbb">  
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 要转换的对象列表</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 其中可能包含一个或多个对象，版本可能相同也可能不同</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">objects</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-04T14:03:02Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>local-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;143&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;3415a7fc-162b-4300-b5da-fd6083580d66&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;localhost:1234&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-03T13:02:01Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>remote-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;12893&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;359a83ec-b575-460d-b553-d859cedde8a0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span>example.com:2345<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="conversionreview-request-1" class="tab-pane" role="tabpanel" aria-labelledby="conversionreview-request-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># v1.16 中已废弃以推荐使用 apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConversionReview<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">request</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 用来唯一标识此转换调用的随机 UID</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>705ab4f5-6393-11e8-b7cc-42010a800002<span style="color:#bbb">
</span><span style="color:#bbb">  
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 对象要转换到的目标 API 组和版本</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">desiredAPIVersion</span>:<span style="color:#bbb"> </span>example.com/v1<span style="color:#bbb">
</span><span style="color:#bbb">  
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 要转换的对象列表</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 其中可能包含一个或多个对象，版本可能相同也可能不同</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">objects</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-04T14:03:02Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>local-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;143&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;3415a7fc-162b-4300-b5da-fd6083580d66&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;localhost:1234&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-03T13:02:01Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>remote-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;12893&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;359a83ec-b575-460d-b553-d859cedde8a0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span>example.com:2345<span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
### Response

Webhooks respond with a 200 HTTP status code, `Content-Type: application/json`,
and a body containing a `ConversionReview` object (in the same version they were sent),
with the `response` stanza populated, serialized to JSON.

If conversion succeeds, a webhook should return a `response` stanza containing the following fields:
* `uid`, copied from the `request.uid` sent to the webhook
* `result`, set to `{"status":"Success"}`
* `convertedObjects`, containing all of the objects from `request.objects`, converted to `request.desiredVersion`

Example of a minimal successful response from a webhook:
-->
<h3 id="响应">响应</h3>
<p>Webhooks 响应包含 200 HTTP 状态代码、<code>Content-Type: application/json</code>，
在主体中包含 JSON 序列化形式的数据，在 <code>response</code> 节中给出
ConversionReview 对象（与发送的版本相同）。</p>
<p>如果转换成功，则 Webhook 应该返回包含以下字段的 <code>response</code> 节：</p>
<ul>
<li><code>uid</code>，从发送到 webhook 的 <code>request.uid</code> 复制而来</li>
<li><code>result</code>，设置为 <code>{&quot;status&quot;:&quot;Success&quot;}}</code></li>
<li><code>convertedObjects</code>，包含来自 <code>request.objects</code> 的所有对象，均已转换为
<code>request.desiredVersion</code></li>
</ul>
<p>Webhook 的最简单成功响应示例：</p>
<ul class="nav nav-tabs" id="conversionreview-response-success" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#conversionreview-response-success-0" role="tab" aria-controls="conversionreview-response-success-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#conversionreview-response-success-1" role="tab" aria-controls="conversionreview-response-success-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="conversionreview-response-success"><div id="conversionreview-response-success-0" class="tab-pane show active" role="tabpanel" aria-labelledby="conversionreview-response-success-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConversionReview<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">response</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 必须与 &lt;request.uid&gt; 匹配</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">result</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span>Success<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 这里的对象必须与 request.objects 中的对象顺序相同并且其 apiVersion</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 被设置为 &lt;request.desiredAPIVersion&gt;。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># kind、metadata.uid、metadata.name 和 metadata.namespace 等字段都不可</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 被 Webhook 修改。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Webhook 可以更改 metadata.labels 和 metadata.annotations 字段值</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Webhook 对 metadata 中其他字段的更改都会被忽略</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">convertedObjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-04T14:03:02Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>local-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;143&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;3415a7fc-162b-4300-b5da-fd6083580d66&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>localhost<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1234&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-03T13:02:01Z&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>remote-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;12893&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;359a83ec-b575-460d-b553-d859cedde8a0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2345&#34;</span><span style="color:#bbb">
</span></code></pre></div></div>
  <div id="conversionreview-response-success-1" class="tab-pane" role="tabpanel" aria-labelledby="conversionreview-response-success-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#080;font-style:italic"># v1.16 中已弃用以推荐使用  apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConversionReview<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">response</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 必须与 &lt;request.uid&gt; 匹配</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;705ab4f5-6393-11e8-b7cc-42010a800002&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">result</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span>Failed<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 这里的对象必须与 request.objects 中的对象顺序相同并且其 apiVersion</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 被设置为 &lt;request.desiredAPIVersion&gt;。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># kind、metadata.uid、metadata.name 和 metadata.namespace 等字段都不可</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 被 Webhook 修改。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Webhook 可以更改 metadata.labels 和 metadata.annotations 字段值</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># Webhook 对 metadata 中其他字段的更改都会被忽略</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">convertedObjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-04T14:03:02Z&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>local-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;143&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;3415a7fc-162b-4300-b5da-fd6083580d66&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>localhost<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;1234&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CronTab<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>example.com/v1<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2019-09-03T13:02:01Z&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>remote-crontab<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;12893&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;359a83ec-b575-460d-b553-d859cedde8a0&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span>example.com<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2345&#34;</span><span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
If conversion fails, a webhook should return a `response` stanza containing the following fields:
* `uid`, copied from the `request.uid` sent to the webhook
* `result`, set to `{"status":"Failed"}`
-->
<p>如果转换失败，则 Webhook 应该返回包含以下字段的 <code>response</code> 节：</p>
<ul>
<li><code>uid</code>，从发送到 Webhook 的 <code>request.uid</code> 复制而来</li>
<li><code>result</code>，设置为 <code>{&quot;status&quot;: &quot;Failed&quot;}</code></li>
</ul>
<blockquote class="warning callout">
  <div><strong>警告：</strong> <!--
Failing conversion can disrupt read and write access to the custom resources,
including the ability to update or delete the resources. Conversion failures 
should be avoided whenever possible, and should not be used to enforce validation
 constraints (use validation schemas or webhook admission instead).
-->
<p>转换失败会破坏对定制资源的读写访问，包括更新或删除资源的能力。
转换失败应尽可能避免，并且不可用于实施合法性检查约束
（应改用验证模式或 Webhook 准入插件）。</div>
</blockquote>

<!--
Example of a response from a webhook indicating a conversion request failed, with an optional message:
-->
<p>来自 Webhook 的响应示例，指示转换请求失败，并带有可选消息：</p>
<ul class="nav nav-tabs" id="conversionreview-response-failure" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#conversionreview-response-failure-0" role="tab" aria-controls="conversionreview-response-failure-0" aria-selected="true">apiextensions.k8s.io/v1</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#conversionreview-response-failure-1" role="tab" aria-controls="conversionreview-response-failure-1">apiextensions.k8s.io/v1beta1</a></li></ul>
<div class="tab-content" id="conversionreview-response-failure"><div id="conversionreview-response-failure-0" class="tab-pane show active" role="tabpanel" aria-labelledby="conversionreview-response-failure-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConversionReview<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">response</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>&lt;value from request.uid&gt;<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">result</span>:<span style="color:#bbb"> </span>{<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span>Failed<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>hostPort could not be parsed into a separate host and port<span style="color:#bbb">
</span></code></pre></div></div>
  <div id="conversionreview-response-failure-1" class="tab-pane" role="tabpanel" aria-labelledby="conversionreview-response-failure-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># v1.16 中弃用以推荐使用 apiextensions.k8s.io/v1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ConversionReview<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">response</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>&lt;value from request.uid&gt;<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">result</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span>Failed<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>hostPort could not be parsed into a separate host and port<span style="color:#bbb">
</span></code></pre></div></div></div>

<!--
## Writing, reading, and updating versioned CustomResourceDefinition objects
-->
<h2 id="编写-读取和更新版本化的-customresourcedefinition-对象">编写、读取和更新版本化的 CustomResourceDefinition 对象</h2>
<!--
When an object is written, it is persisted at the version designated as the
storage version at the time of the write. If the storage version changes,
existing objects are never converted automatically. However, newly-created
or updated objects are written at the new storage version. It is possible for an
object to have been written at a version that is no longer served.
-->
<p>写入对象时，将使用写入时指定的存储版本来存储。如果存储版本发生变化，
现有对象永远不会被自动转换。然而，新创建或被更新的对象将以新的存储版本写入。
对象写入的版本不再被支持是有可能的。</p>
<!--
When you read an object, you specify the version as part of the path. If you
specify a version that is different from the object's persisted version,
Kubernetes returns the object to you at the version you requested, but the
persisted object is neither changed on disk, nor converted in any way
(other than changing the `apiVersion` string) while serving the request.
You can request an object at any version that is currently served.
-->
<p>当读取对象时，作为路径的一部分，你需要指定版本。
如果所指定的版本与对象的持久版本不同，Kubernetes 会按所请求的版本将对象返回，
但是在满足服务请求时，被持久化的对象既不会在磁盘上更改，也不会以任何方式进行
转换（除了 <code>apiVersion</code> 字符串被更改之外）。你可以以当前提供的任何版本
来请求对象。</p>
<!--
If you update an existing object, it is rewritten at the version that is
currently the storage version. This is the only way that objects can change from
one version to another.
-->
<p>如果你更新一个现有对象，它将以当前的存储版本被重写。
这是可以将对象从一个版本改到另一个版本的唯一办法。</p>
<!--
To illustrate this, consider the following hypothetical series of events:
-->
<p>为了说明这一点，请考虑以下假设的一系列事件：</p>
<!--
1.  The storage version is `v1beta1`. You create an object. It is persisted in
    storage at version `v1beta1`
2.  You add version `v1` to your CustomResourceDefinition and designate it as
    the storage version.
3.  You read your object at version `v1beta1`, then you read the object again at
    version `v1`. Both returned objects are identical except for the apiVersion
    field.
4.  You create a new object. It is persisted in storage at version `v1`. You now
    have two objects, one of which is at `v1beta1`, and the other of which is at
    `v1`.
5.  You update the first object. It is now persisted at version `v1` since that
    is the current storage version.
-->
<ol>
<li>存储版本是 <code>v1beta1</code>。你创建一个对象。该对象以版本 <code>v1beta1</code> 存储。</li>
<li>你将为 CustomResourceDefinition 添加版本 <code>v1</code>，并将其指定为存储版本。</li>
<li>你使用版本 <code>v1beta1</code> 来读取你的对象，然后你再次用版本 <code>v1</code> 读取对象。
除了 apiVersion 字段之外，返回的两个对象是完全相同的。</li>
<li>你创建一个新对象。对象以版本 <code>v1</code> 保存在存储中。
你现在有两个对象，其中一个是 <code>v1beta1</code>，另一个是 <code>v1</code>。</li>
<li>你更新第一个对象。该对象现在以版本 <code>v1</code> 保存，因为 <code>v1</code> 是当前的存储版本。</li>
</ol>
<!--
### Previous storage versions
-->
<h3 id="以前的存储版本">以前的存储版本</h3>
<!--
The API server records each version which has ever been marked as the storage
version in the status field `storedVersions`. Objects may have been persisted
at any version that has ever been designated as a storage version. No objects
can exist in storage at a version that has never been a storage version.
-->
<p>API 服务器在状态字段 <code>storedVersions</code> 中记录曾被标记为存储版本的每个版本。
对象可能以任何曾被指定为存储版本的版本保存。
存储中不会出现从未成为存储版本的版本的对象。</p>
<!--
## Upgrade existing objects to a new stored version
-->
<h2 id="upgrade-existing-objects-to-a-new-stored-version">将现有对象升级到新的存储版本    </h2>
<!--
When deprecating versions and dropping support, devise a storage upgrade
procedure.
-->
<p>弃用版本并删除其支持时，请设计存储升级过程。</p>
<!--
*Option 1:* Use the Storage Version Migrator

1. Run the [storage Version migrator](https://github.com/kubernetes-sigs/kube-storage-version-migrator)
2. Remove the old version from the CustomResourceDefinition `status.storedVersions` field.
-->
<p><em>选项 1：</em> 使用存储版本迁移程序（Storage Version Migrator）</p>
<ol>
<li>运行<a href="https://github.com/kubernetes-sigs/kube-storage-version-migrator">存储版本迁移程序</a></li>
<li>从 CustomResourceDefinition 的 <code>status.storedVersions</code> 字段中去掉
老的版本。</li>
</ol>
<!--
*Option 2:* Manually upgrade the existing objects to a new stored version

The following is an example procedure to upgrade from `v1beta1` to `v1`.
-->
<p><em>选项 2：</em> 手动将现有对象升级到新的存储版本</p>
<p>以下是从 <code>v1beta1</code> 升级到 <code>v1</code> 的示例过程。</p>
<!--
1.  Set `v1` as the storage in the CustomResourceDefinition file and apply it
    using kubectl. The `storedVersions` is now `v1beta1, v1`.
2.  Write an upgrade procedure to list all existing objects and write them with
    the same content. This forces the backend to write objects in the current
    storage version, which is `v1`.
3.  Update the CustomResourceDefinition `Status` by removing `v1beta1` from
    `storedVersions` field.
-->
<ol>
<li>在 CustomResourceDefinition 文件中将 <code>v1</code> 设置为存储版本，并使用 kubectl 应用它。
<code>storedVersions</code>现在是<code>v1beta1, v1</code>。</li>
<li>编写升级过程以列出所有现有对象并使用相同内容将其写回存储。
这会强制后端使用当前存储版本（即 <code>v1</code>）写入对象。</li>
<li>通过从 <code>storedVersions</code> 字段中删除 <code>v1beta1</code> 来更新 CustomResourceDefinition
的<code>Status</code>。</li>
</ol>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2bd28753e62a14a597073fa8ea18a5d8">11.2 - 配置聚合层</h1>
    
	<!--
title: Configure the Aggregation Layer
reviewers:
- lavalamp
- cheftako
- chenopis
content_type: task
weight: 10
-->
<!-- overview -->
<!--
Configuring the [aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) allows the Kubernetes apiserver to be extended with additional APIs, which are not part of the core Kubernetes APIs.
-->
<p>配置<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">聚合层</a>
可以允许 Kubernetes apiserver 使用其它 API 扩展，这些 API 不是核心
Kubernetes API 的一部分。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
There are a few setup requirements for getting the aggregation layer working in your environment to support mutual TLS auth between the proxy and extension apiservers. Kubernetes and the kube-apiserver have multiple CAs, so make sure that the proxy is signed by the aggregation layer CA and not by something else, like the Kubernetes general CA.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 要使聚合层在你的环境中正常工作以支持代理服务器和扩展 apiserver 之间的相互 TLS 身份验证，
需要满足一些设置要求。Kubernetes 和 kube-apiserver 具有多个 CA，
因此请确保代理是由聚合层 CA 签名的，而不是由 Kubernetes 通用 CA 签名的。</div>
</blockquote>
<!--
Reusing the same CA for different client types can negatively impact the cluster's ability to function. For more information, see [CA Reusage and Conflicts](#ca-reusage-and-conflicts).
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> 对不同的客户端类型重复使用相同的 CA 会对群集的功能产生负面影响。
有关更多信息，请参见 <a href="#ca-reusage-and-conflicts">CA 重用和冲突</a>。</div>
</blockquote>

<!-- steps -->
<!--
## Authentication Flow

Unlike Custom Resource Definitions (CRDs), the Aggregation API involves another server - your Extension apiserver - in addition to the standard Kubernetes apiserver. The Kubernetes apiserver will need to communicate with your extension apiserver, and your extension apiserver will need to communicate with the Kubernetes apiserver. In order for this communication to be secured, the Kubernetes apiserver uses x509 certificates to authenticate itself to the extension apiserver.

This section describes how the authentication and authorization flows work, and how to configure them.
-->
<h2 id="身份认证流程">身份认证流程</h2>
<p>与自定义资源定义（CRD）不同，除标准的 Kubernetes apiserver 外，Aggregation API
还涉及另一个服务器：扩展 apiserver。
Kubernetes apiserver 将需要与你的扩展 apiserver 通信，并且你的扩展 apiserver
也需要与 Kubernetes apiserver 通信。
为了确保此通信的安全，Kubernetes apiserver 使用 x509 证书向扩展 apiserver 认证。</p>
<p>本节介绍身份认证和鉴权流程的工作方式以及如何配置它们。</p>
<!--
The high-level flow is as follows:

1. Kubernetes apiserver: authenticate the requesting user and authorize their rights to the requested API path.
2. Kubernetes apiserver: proxy the request to the extension apiserver
3. Extension apiserver: authenticate the request from the Kubernetes apiserver
4. Extension apiserver: authorize the request from the original user
5. Extension apiserver: execute
-->
<p>大致流程如下：</p>
<ol>
<li>Kubernetes apiserver：对发出请求的用户身份认证，并对请求的 API 路径执行鉴权。</li>
<li>Kubernetes apiserver：将请求转发到扩展 apiserver</li>
<li>扩展 apiserver：认证来自 Kubernetes apiserver 的请求</li>
<li>扩展 apiserver：对来自原始用户的请求鉴权</li>
<li>扩展 apiserver：执行</li>
</ol>
<!--
The rest of this section describes these steps in detail.

The flow can be seen in the following diagram.

The source for the above swimlanes can be found in the source of this document.
-->
<p>本节的其余部分详细描述了这些步骤。</p>
<p>该流程可以在下图中看到。</p>
<p><img src="/images/docs/aggregation-api-auth-flow.png" alt="聚合层认证流程">.</p>
<p>以上泳道的来源可以在本文档的源码中找到。</p>
<!--
Swimlanes generated at https://swimlanes.io with the source as follows:

-----BEGIN-----
title: Welcome to swimlanes.io


User -> kube-apiserver / aggregator:

note:
1. The user makes a request to the Kube API server using any recognized credential (e.g. OIDC or client certs)

kube-apiserver / aggregator -> kube-apiserver / aggregator: authentication

note:
2. The Kube API server authenticates the incoming request using any configured authentication methods (e.g. OIDC or client certs)

kube-apiserver / aggregator -> kube-apiserver / aggregator: authorization

note:
3. The Kube API server authorizes the requested URL using any configured authorization method (e.g. RBAC)

kube-apiserver / aggregator -> aggregated apiserver:

note:
4.The aggregator opens a connection to the aggregated API server using `--proxy-client-cert-file`/`--proxy-client-key-file` client certificate/key to secure the channel
5.The aggregator sends the user info from step 1 to the aggregated API server as http headers, as defined by the following flags:
  * `--requestheader-username-headers`
  * `--requestheader-group-headers`
  * `--requestheader-extra-headers-prefix`

aggregated apiserver -> aggregated apiserver: authentication

note:
6. The aggregated apiserver authenticates the incoming request using the auth proxy authentication method:
  * verifies the request has a recognized auth proxy client certificate
  * pulls user info from the incoming request's http headers

By default, it pulls the configuration information for this from a configmap in the kube-system namespace that is published by the kube-apiserver, containing the info from the `--requestheader-...` flags provided to the kube-apiserver (CA bundle to use, auth proxy client certificate names to allow, http header names to use, etc)

aggregated apiserver -> kube-apiserver / aggregator: authorization

note:
7. The aggregated apiserver authorizes the incoming request by making a SubjectAccessReview call to the kube-apiserver

aggregated apiserver -> aggregated apiserver: admission

note:
8. For mutating requests, the aggregated apiserver runs admission checks. by default, the namespace lifecycle admission plugin ensures namespaced resources are created in a namespace that exists in the kube-apiserver
-----END-----

-->
<!--
在 https://swimlanes.io 生成的泳道，其源码如下：

-----BEGIN-----
title: 认证流程

User -> kube-apiserver / aggregator:

note:
1.用户使用任何公认的凭证（例如 OIDC 或客户端证书）向 Kube Apiserver 发出请求

kube-apiserver / aggregator -> kube-apiserver / aggregator: 认证

note:
2.Kube Apiserver 使用任何配置的身份验证方法（例如 OIDC 或客户端证书）对传入请求认证

kube-apiserver / aggregator -> kube-apiserver / aggregator: 鉴权

note:
3.Kube Apiserver 使用任何配置的鉴权方法（例如 RBAC）对请求的 URL 鉴权

kube-apiserver / aggregator -> 聚合的 apiserver:

note:
4.aggregator 使用 `--proxy-client-cert-file`，`--proxy-client-key-file`
  客户端证书/密钥打开与聚合 Apiserver 的连接以保护通道

5.aggregator 将步骤 1 中的用户信息作为 http 标头发送到聚合的 Apiserver，
  如以下标志所定义：

  * `--requestheader-username-headers`
  * `--requestheader-group-headers`
  * `--requestheader-extra-headers-prefix`

kube-apiserver / aggregator -> 聚合的 apiserver: 认证

note:
6.聚合的 apiserver 使用代理身份验证方法对传入的请求认证：

  * 验证请求是否具有公认的身份验证代理客户端证书
  * 从传入请求的 HTTP 标头中提取用户信息

默认情况下，它从 kube-apiserver 发布的 kube-system 命名空间中的 configmap
中获取配置信息，其中包含提供给 kube-apiserver 的`--requestheader-...`
标志中的信息（要使用的 CA 包，要允许的身份验证代理客户端证书名称，
要使用的 HTTP 标头名称等）

kube-apiserver / aggregator -> 聚合的 apiserver: 鉴权

note:
7.聚合的 apiserver 通过 SubjectAccessReview 请求 kube-apiserver 鉴权

kube-apiserver / aggregator -> 聚合的 apiserver: 准入

note:
8.对于可变请求，聚合的 apiserver 运行准入检查。
  默认情况下，namespace 生命周期准入插件可确保在 kube-apiserver
  中存在的 namespace 中创建指定 namespace 下的资源
-----END-----

-->
<!--
### Kubernetes Apiserver Authentication and Authorization

A request to an API path that is served by an extension apiserver begins the same way as all API requests: communication to the Kubernetes apiserver. This path already has been registered with the Kubernetes apiserver by the extension apiserver.

The user communicates with the Kubernetes apiserver, requesting access to the path. The Kubernetes apiserver uses standard authentication and authorization configured with the Kubernetes apiserver to authenticate the user and authorize access to the specific path.

For an overview of authenticating to a Kubernetes cluster, see ["Authenticating to a Cluster"](/docs/reference/access-authn-authz/authentication/). For an overview of authorization of access to Kubernetes cluster resources, see ["Authorization Overview"](/docs/reference/access-authn-authz/authorization/).

Everything to this point has been standard Kubernetes API requests, authentication and authorization.

The Kubernetes apiserver now is prepared to send the request to the extension apiserver.
-->
<h3 id="kubernetes-apiserver-认证和授权">Kubernetes Apiserver 认证和授权</h3>
<p>由扩展 apiserver 服务的对 API 路径的请求以与所有 API 请求相同的方式开始：
与 Kubernetes apiserver 的通信。该路径已通过扩展 apiserver 在
Kubernetes apiserver 中注册。</p>
<p>用户与 Kubernetes apiserver 通信，请求访问路径。
Kubernetes apiserver 使用它的标准认证和授权配置来对用户认证，以及对特定路径的鉴权。</p>
<p>有关对 Kubernetes 集群认证的概述，请参见
<a href="/zh/docs/reference/access-authn-authz/authentication/">对集群认证</a>。
有关对Kubernetes群集资源的访问鉴权的概述，请参见
<a href="/zh/docs/reference/access-authn-authz/authorization/">鉴权概述</a>。</p>
<p>到目前为止，所有内容都是标准的 Kubernetes API 请求，认证与鉴权。</p>
<p>Kubernetes apiserver 现在准备将请求发送到扩展 apiserver。</p>
<!--
### Kubernetes Apiserver Proxies the Request

The Kubernetes apiserver now will send, or proxy, the request to the extension apiserver that registered to handle the request. In order to do so, it needs to know several things:

1. How should the Kubernetes apiserver authenticate to the extension apiserver, informing the extension apiserver that the request, which comes over the network, is coming from a valid Kubernetes apiserver?
2. How should the Kubernetes apiserver inform the extension apiserver of the username and group for which the original request was authenticated?

In order to provide for these two, you must configure the Kubernetes apiserver using several flags.
-->
<h3 id="kubernetes-apiserver-代理请求">Kubernetes Apiserver 代理请求</h3>
<p>Kubernetes apiserver 现在将请求发送或代理到注册以处理该请求的扩展 apiserver。
为此，它需要了解几件事：</p>
<ol>
<li>
<p>Kubernetes apiserver 应该如何向扩展 apiserver 认证，以通知扩展
apiserver 通过网络发出的请求来自有效的 Kubernetes apiserver？</p>
</li>
<li>
<p>Kubernetes apiserver 应该如何通知扩展 apiserver 原始请求
已通过认证的用户名和组？</p>
</li>
</ol>
<p>为提供这两条信息，你必须使用若干标志来配置 Kubernetes apiserver。</p>
<!--
#### Kubernetes Apiserver Client Authentication

The Kubernetes apiserver connects to the extension apiserver over TLS, authenticating itself using a client certificate. You must provide the following to the Kubernetes apiserver upon startup, using the provided flags:

* private key file via `--proxy-client-key-file`
* signed client certificate file via `--proxy-client-cert-file`
* certificate of the CA that signed the client certificate file via `--requestheader-client-ca-file`
* valid Common Names (CN) in the signed client certificate via `--requestheader-allowed-names`
-->
<h4 id="kubernetes-apiserver-客户端认证">Kubernetes Apiserver 客户端认证</h4>
<p>Kubernetes apiserver 通过 TLS 连接到扩展 apiserver，并使用客户端证书认证。
你必须在启动时使用提供的标志向 Kubernetes apiserver 提供以下内容：</p>
<ul>
<li>通过 <code>--proxy-client-key-file</code> 指定私钥文件</li>
<li>通过 <code>--proxy-client-cert-file</code> 签名的客户端证书文件</li>
<li>通过 <code>--requestheader-client-ca-file</code> 签署客户端证书文件的 CA 证书</li>
<li>通过 <code>--requestheader-allowed-names</code> 在签署的客户证书中有效的公用名（CN）</li>
</ul>
<!--
The Kubernetes apiserver will use the files indicated by `--proxy-client-*-file` to authenticate to the extension apiserver. In order for the request to be considered valid by a compliant extension apiserver, the following conditions must be met:

1. The connection must be made using a client certificate that is signed by the CA whose certificate is in `--requestheader-client-ca-file`.
2. The connection must be made using a client certificate whose CN is one of those listed in `--requestheader-allowed-names`. **Note:** You can set this option to blank as `--requestheader-allowed-names=""`. This will indicate to an extension apiserver that _any_ CN is acceptable.
-->
<p>Kubernetes apiserver 将使用由 <code>--proxy-client-*-file</code> 指示的文件来验证扩展 apiserver。
为了使合规的扩展 apiserver 能够将该请求视为有效，必须满足以下条件：</p>
<ol>
<li>连接必须使用由 CA 签署的客户端证书，该证书的证书位于 <code>--requestheader-client-ca-file</code> 中。</li>
<li>连接必须使用客户端证书，该客户端证书的 CN 是 <code>--requestheader-allowed-names</code> 中列出的证书之一。</li>
</ol>
<blockquote class="note callout">
  <div><strong>说明：</strong> 你可以将此选项设置为空白，即为<code>--requestheader-allowed-names</code>。
这将向扩展 apiserver 指示任何 CN 是可接受的。</div>
</blockquote>
<!--
When started with these options, the Kubernetes apiserver will:

1. Use them to authenticate to the extension apiserver.
2. Create a configmap in the `kube-system` namespace called `extension-apiserver-authentication`, in which it will place the CA certificate and the allowed CNs. These in turn can be retrieved by extension apiservers to validate requests.

Note that the same client certificate is used by the Kubernetes apiserver to authenticate against _all_ extension apiservers. It does not create a client certificate per extension apiserver, but rather a single one to authenticate as the Kubernetes apiserver. This same one is reused for all extension apiserver requests.
-->
<p>使用这些选项启动时，Kubernetes apiserver 将：</p>
<ol>
<li>使用它们向扩展 apiserver 认证。</li>
<li>在 <code>kube-system</code> 命名空间中
创建一个名为 <code>extension-apiserver-authentication</code> 的 ConfigMap，
它将在其中放置 CA 证书和允许的 CN。
反过来，扩展 apiserver 可以检索这些内容以验证请求。</li>
</ol>
<p>请注意，Kubernetes apiserver 使用相同的客户端证书对所有扩展 apiserver 认证。
它不会为每个扩展 apiserver 创建一个客户端证书，而是创建一个证书作为
Kubernetes apiserver 认证。所有扩展 apiserver 请求都重复使用相同的请求。</p>
<!--
#### Original Request Username and Group

When the Kubernetes apiserver proxies the request to the extension apiserver, it informs the extension apiserver of the username and group with which the original request successfully authenticated. It provides these in http headers of its proxied request. You must inform the Kubernetes apiserver of the names of the headers to be used.

* the header in which to store the username via `-requestheader-username-headers`
* the header in which to store the group via `-requestheader-group-headers`
* the prefix to append to all extra headers via `-requestheader-extra-headers-prefix`

These header names are also placed in the `extension-apiserver-authentication` configmap, so they can be retrieved and used by extension apiservers.
-->
<h4 id="原始请求用户名和组">原始请求用户名和组</h4>
<p>当 Kubernetes apiserver 将请求代理到扩展 apiserver 时，
它将向扩展 apiserver 通知原始请求已成功通过其验证的用户名和组。
它在其代理请求的 HTTP 头部中提供这些。你必须将要使用的标头名称告知
Kubernetes apiserver。</p>
<ul>
<li>通过<code>--requestheader-username-headers</code> 标明用来保存用户名的头部</li>
<li>通过<code>--requestheader-group-headers</code> 标明用来保存 group 的头部</li>
<li>通过<code>--requestheader-extra-headers-prefix</code> 标明用来保存拓展信息前缀的头部</li>
</ul>
<p>这些头部名称也放置在 <code>extension-apiserver-authentication</code> ConfigMap 中，
因此扩展 apiserver 可以检索和使用它们。</p>
<!--
### Extension Apiserver Authenticates the Request

The extension apiserver, upon receiving a proxied request from the Kubernetes apiserver, must validate that the request actually did come from a valid authenticating proxy, which role the Kubernetes apiserver is fulfilling. The extension apiserver validates it via:

1. Retrieve the following from the configmap in `kube-system`, as described above:
    * Client CA certificate
    * List of allowed names (CNs)
    * Header names for username, group and extra info

2. Check that the TLS connection was authenticated using a client certificate which:
    * Was signed by the CA whose certificate matches the retrieved CA certificate.
    * Has a CN in the list of allowed CNs, unless the list is blank, in which case all CNs are allowed.
    * Extract the username and group from the appropriate headers
-->
<h3 id="扩展-apiserver-认证">扩展 Apiserver 认证</h3>
<p>扩展 apiserver 在收到来自 Kubernetes apiserver 的代理请求后，
必须验证该请求确实确实来自有效的身份验证代理，
该认证代理由 Kubernetes apiserver 履行。扩展 apiserver 通过以下方式对其认证：</p>
<ol>
<li>
<p>如上所述，从<code>kube-system</code>中的 configmap 中检索以下内容：</p>
<ul>
<li>客户端 CA 证书</li>
<li>允许名称（CN）列表</li>
<li>用户名，组和其他信息的头部</li>
</ul>
</li>
<li>
<p>使用以下证书检查 TLS 连接是否已通过认证：</p>
<ul>
<li>由其证书与检索到的 CA 证书匹配的 CA 签名。</li>
<li>在允许的 CN 列表中有一个 CN，除非列表为空，在这种情况下允许所有 CN。</li>
<li>从适当的头部中提取用户名和组</li>
</ul>
</li>
</ol>
<!--
If the above passes, then the request is a valid proxied request from a legitimate authenticating proxy, in this case the Kubernetes apiserver.

Note that it is the responsibility of the extension apiserver implementation to provide the above. Many do it by default, leveraging the `k8s.io/apiserver/` package. Others may provide options to override it using command-line options.

In order to have permission to retrieve the configmap, an extension apiserver requires the appropriate role. There is a default role named `extension-apiserver-authentication-reader` in the `kube-system` namespace which can be assigned.
-->
<p>如果以上均通过，则该请求是来自合法认证代理（在本例中为 Kubernetes apiserver）
的有效代理请求。</p>
<p>请注意，扩展 apiserver 实现负责提供上述内容。
默认情况下，许多扩展 apiserver 实现利用 <code>k8s.io/apiserver/</code> 软件包来做到这一点。
也有一些实现可能支持使用命令行选项来覆盖这些配置。</p>
<p>为了具有检索 configmap 的权限，扩展 apiserver 需要适当的角色。
在 <code>kube-system</code> 名字空间中有一个默认角色
<code>extension-apiserver-authentication-reader</code> 可用于设置。</p>
<!--
### Extension Apiserver Authorizes the Request

The extension apiserver now can validate that the user/group retrieved from the headers are authorized to execute the given request. It does so by sending a standard [SubjectAccessReview](/docs/reference/access-authn-authz/authorization/) request to the Kubernetes apiserver.

In order for the extension apiserver to be authorized itself to submit the `SubjectAccessReview` request to the Kubernetes apiserver, it needs the correct permissions. Kubernetes includes a default `ClusterRole` named `system:auth-delegator` that has the appropriate permissions. It can be granted to the extension apiserver's service account.
-->
<h3 id="扩展-apiserver-对请求鉴权">扩展 Apiserver 对请求鉴权</h3>
<p>扩展 apiserver 现在可以验证从标头检索的<code>user/group</code>是否有权执行给定请求。
通过向 Kubernetes apiserver 发送标准
<a href="/zh/docs/reference/access-authn-authz/authorization/">SubjectAccessReview</a> 请求来实现。</p>
<p>为了使扩展 apiserver 本身被鉴权可以向 Kubernetes apiserver 提交 SubjectAccessReview 请求，
它需要正确的权限。
Kubernetes 包含一个具有相应权限的名为 <code>system:auth-delegator</code> 的默认 <code>ClusterRole</code>，
可以将其授予扩展 apiserver 的服务帐户。</p>
<!--
### Extension Apiserver Executes

If the `SubjectAccessReview` passes, the extension apiserver executes the request.

## Enable Kubernetes Apiserver flags

Enable the aggregation layer via the following kube-apiserver flags. They may have already been taken care of by your provider.
-->
<h3 id="扩展-apiserver-执行">扩展 Apiserver 执行</h3>
<p>如果 <code>SubjectAccessReview</code> 通过，则扩展 apiserver 执行请求。</p>
<h2 id="启用-kubernetes-apiserver-标志">启用 Kubernetes Apiserver 标志</h2>
<p>通过以下 kube-apiserver 标志启用聚合层。
你的服务提供商可能已经为你完成了这些工作：</p>
<pre tabindex="0"><code>    --requestheader-client-ca-file=&lt;path to aggregator CA cert&gt;
    --requestheader-allowed-names=front-proxy-client
    --requestheader-extra-headers-prefix=X-Remote-Extra-
    --requestheader-group-headers=X-Remote-Group
    --requestheader-username-headers=X-Remote-User
    --proxy-client-cert-file=&lt;path to aggregator proxy cert&gt;
    --proxy-client-key-file=&lt;path to aggregator proxy key&gt;
</code></pre><!--
### CA Reusage and Conflicts

The Kubernetes apiserver has two client CA options:
-->
<h3 id="ca-reusage-and-conflicts">CA-重用和冲突 </h3>
<p>Kubernetes apiserver 有两个客户端 CA 选项：</p>
<ul>
<li><code>--client-ca-file</code></li>
<li><code>--requestheader-client-ca-file</code></li>
</ul>
<!--
Each of these functions independently and can conflict with each other, if not used correctly.

* `--client-ca-file`: When a request arrives to the Kubernetes apiserver, if this option is enabled, the Kubernetes apiserver checks the certificate of the request. If it is signed by one of the CA certificates in the file referenced by `--client-ca-file`, then the request is treated as a legitimate request, and the user is the value of the common name `CN=`, while the group is the organization `O=`. See the [documentaton on TLS authentication](/docs/reference/access-authn-authz/authentication/#x509-client-certs).
* `--requestheader-client-ca-file`: When a request arrives to the Kubernetes apiserver, if this option is enabled, the Kubernetes apiserver checks the certificate of the request. If it is signed by one of the CA certificates in the file reference by `--requestheader-client-ca-file`, then the request is treated as a potentially legitimate request. The Kubernetes apiserver then checks if the common name `CN=` is one of the names in the list provided by `--requestheader-allowed-names`. If the name is allowed, the request is approved; if it is not, the request is not.
-->
<p>这些功能中的每个功能都是独立的；如果使用不正确，可能彼此冲突。</p>
<ul>
<li>
<p><code>--client-ca-file</code>：当请求到达 Kubernetes apiserver 时，如果启用了此选项，
则 Kubernetes apiserver 会检查请求的证书。
如果它是由 <code>--client-ca-file</code> 引用的文件中的 CA 证书之一签名的，
并且用户是公用名<code>CN=</code>的值，而组是组织<code>O=</code> 的取值，则该请求被视为合法请求。
请参阅<a href="/zh/docs/reference/access-authn-authz/authentication/#x509-client-certs">关于 TLS 身份验证的文档</a>。</p>
</li>
<li>
<p><code>--requestheader-client-ca-file</code>：当请求到达 Kubernetes apiserver 时，
如果启用此选项，则 Kubernetes apiserver 会检查请求的证书。
如果它是由文件引用中的 --requestheader-client-ca-file 所签署的 CA 证书之一签名的，
则该请求将被视为潜在的合法请求。
然后，Kubernetes apiserver 检查通用名称 <code>CN=</code> 是否是
<code>--requestheader-allowed-names</code> 提供的列表中的名称之一。
如果名称允许，则请求被批准；如果不是，则请求被拒绝。</p>
</li>
</ul>
<!--
If _both_ `--client-ca-file` and `--requestheader-client-ca-file` are provided, then the request first checks the `--requestheader-client-ca-file` CA and then the `--client-ca-file`. Normally, different CAs, either root CAs or intermediate CAs, are used for each of these options; regular client requests match against `--client-ca-file`, while aggregation requests match against `--requestheader-client-ca-file`. However, if both use the _same_ CA, then client requests that normally would pass via `--client-ca-file` will fail, because the CA will match the CA in `--requestheader-client-ca-file`, but the common name `CN=` will **not** match one of the acceptable common names in `--requestheader-allowed-names`. This can cause your kubelets and other control plane components, as well as end-users, to be unable to authenticate to the Kubernetes apiserver.

For this reason, use different CA certs for the `--client-ca-file` option - to authorize control plane components and end-users - and the `--requestheader-client-ca-file` option - to authorize aggregation apiserver requests.
-->
<p>如果同时提供了 <code>--client-ca-file</code> 和 <code>--requestheader-client-ca-file</code>，
则首先检查 <code>--requestheader-client-ca-file</code> CA，然后再检查<code>--client-ca-file</code>。
通常，这些选项中的每一个都使用不同的 CA（根 CA 或中间 CA）。
常规客户端请求与 <code>--client-ca-file</code> 相匹配，而聚合请求要与
<code>--requestheader-client-ca-file</code> 相匹配。
但是，如果两者都使用同一个 CA，则通常会通过 <code>--client-ca-file</code>
传递的客户端请求将失败，因为 CA 将与 <code>--requestheader-client-ca-file</code>
中的 CA 匹配，但是通用名称 <code>CN=</code> 将不匹配 <code>--requestheader-allowed-names</code>
中可接受的通用名称之一。
这可能导致你的 kubelet 和其他控制平面组件以及最终用户无法向 Kubernetes
apiserver 认证。</p>
<p>因此，请对用于控制平面组件和最终用户鉴权的 <code>--client-ca-file</code> 选项和
用于聚合 apiserver 鉴权的 <code>--requestheader-client-ca-file</code> 选项使用
不同的 CA 证书。</p>
<!--
Do **not** reuse a CA that is used in a different context unless you understand the risks and the mechanisms to protect the CA's usage.
-->
<blockquote class="warning callout">
  <div><strong>警告：</strong> 除非你了解风险和保护 CA 用法的机制，否则 <em>不要</em> 重用在不同上下文中使用的 CA。</div>
</blockquote>

<!--
If you are not running kube-proxy on a host running the API server, then you must make sure that the system is enabled with the following `kube-apiserver` flag:
-->
<p>如果你未在运行 API 服务器的主机上运行 kube-proxy，则必须确保使用以下
<code>kube-apiserver</code> 标志启用系统：</p>
<pre tabindex="0"><code>--enable-aggregator-routing=true
</code></pre><!--
### Register APIService objects

You can dynamically configure what client requests are proxied to extension
apiserver. The following is an example registration:
-->
<h3 id="注册-apiservice-对象">注册 APIService 对象</h3>
<p>你可以动态配置将哪些客户端请求代理到扩展 apiserver。以下是注册示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiregistration.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>APIService<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>&lt;注释对象名称&gt;<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">group</span>:<span style="color:#bbb"> </span>&lt;扩展 Apiserver 的 API 组名&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>&lt;扩展 Apiserver 的 API 版本&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">groupPriorityMinimum</span>:<span style="color:#bbb"> </span>&lt;APIService 对应组的优先级, 参考 API 文档&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">versionPriority</span>:<span style="color:#bbb"> </span>&lt;版本在组中的优先排序, 参考 API 文档&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>&lt;拓展 Apiserver 服务的名字空间&gt;<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>&lt;拓展 Apiserver 服务的名称&gt;<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">caBundle</span>:<span style="color:#bbb"> </span>&lt;PEM 编码的 CA 证书，用于对 Webhook 服务器的证书签名&gt;<span style="color:#bbb">
</span></code></pre></div><!--
The name of an APIService object must be a valid
[path segment name](/docs/concepts/overview/working-with-objects/names#path-segment-names).
-->
<p>APIService 对象的名称必须是合法的
<a href="/zh/docs/concepts/overview/working-with-objects/names#path-segment-names">路径片段名称</a>。</p>
<!--
#### Contacting the extension apiserver

Once the Kubernetes apiserver has determined a request should be sent to an extension apiserver,
it needs to know how to contact it.

The `service` stanza is a reference to the service for an extension apiserver.
The service namespace and name are required. The port is optional and defaults to 443.
The path is optional and defaults to "/".

Here is an example of an extension apiserver that is configured to be called on port "1234"
at the subpath "/my-path", and to verify the TLS connection against the ServerName
`my-service-name.my-service-namespace.svc` using a custom CA bundle.
-->
<h4 id="调用扩展-apiserver">调用扩展 apiserver</h4>
<p>一旦 Kubernetes apiserver 确定应将请求发送到扩展 apiserver，
它需要知道如何调用它。</p>
<p><code>service</code> 部分是对扩展 apiserver 的服务的引用。
服务的名字空间和名字是必需的。端口是可选的，默认为 443。
路径配置是可选的，默认为 <code>/</code>。</p>
<p>下面是为可在端口 <code>1234</code> 上调用的扩展 apiserver 的配置示例
服务位于子路径 <code>/my-path</code> 下，并针对 ServerName
<code>my-service-name.my-service-namespace.svc</code>
使用自定义的 CA 包来验证 TLS 连接
使用自定义 CA 捆绑包的<code>my-service-name.my-service-namespace.svc</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiregistration.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>APIService<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>...<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">service</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>my-service-namespace<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service-name<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">1234</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">caBundle</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Ci0tLS0tQk...&lt;base64-encoded PEM bundle&gt;...tLS0K&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb">
</span></code></pre></div><h2 id="接下来">接下来</h2>
<!--
* [Setup an extension api-server](/docs/tasks/access-kubernetes-api/setup-extension-api-server/) to work with the aggregation layer.
* For a high level overview, see [Extending the Kubernetes API with the aggregation layer](/docs/concepts/api-extension/apiserver-aggregation/).
* Learn how to [Extend the Kubernetes API Using Custom Resource Definitions](/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/).
-->
<ul>
<li>使用聚合层<a href="/zh/docs/tasks/extend-kubernetes/setup-extension-api-server/">安装扩展 API 服务器</a>。</li>
<li>有关高级概述，请参阅<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">使用聚合层扩展 Kubernetes API</a>。</li>
<li>了解如何<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">使用自定义资源扩展 Kubernetes API</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c4798e42eaccc051e396542befb3c57b">11.3 - 安装一个扩展的 API server</h1>
    
	<!--
title: Setup an extension API server
reviewers:
- lavalamp
- cheftako
- chenopis
content_type: task
weight: 15
-->
<!-- overview -->
<!--
Setting up an extension API server to work the aggregation layer allows the Kubernetes apiserver to be extended with additional APIs, which are not part of the core Kubernetes APIs.
-->
<p>安装扩展的 API 服务器来使用聚合层以让 Kubernetes API 服务器使用
其它 API 进行扩展，
这些 API 不是核心 Kubernetes API 的一部分。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
* You must [configure the aggregation layer](/docs/tasks/access-kubernetes-api/configure-aggregation-layer/) and enable the apiserver flags.
-->
<ul>
<li>你必须<a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">配置聚合层</a>
并且启用 API 服务器的相关参数。</li>
</ul>
<!-- steps -->
<!--
## Setup an extension api-server to work with the aggregation layer

The following steps describe how to set up an extension-apiserver *at a high level*. These steps apply regardless if you're using YAML configs or using APIs. An attempt is made to specifically identify any differences between the two. For a concrete example of how they can be implemented using YAML configs, you can look at the [sample-apiserver](https://github.com/kubernetes/sample-apiserver/blob/master/README.md) in the Kubernetes repo.

Alternatively, you can use an existing 3rd party solution, such as [apiserver-builder](https://github.com/Kubernetes-incubator/apiserver-builder/blob/master/README.md), which should generate a skeleton and automate all of the following steps for you.
-->
<h2 id="安装一个扩展的-api-服务器来使用聚合层">安装一个扩展的 API 服务器来使用聚合层</h2>
<p>以下步骤描述如何 <em>在一个高层次</em> 设置一个扩展的 apiserver。无论你使用的是 YAML 配置还是使用 API，这些步骤都适用。
目前我们正在尝试区分出两者的区别。有关使用 YAML 配置的具体示例，你可以在 Kubernetes 库中查看
<a href="https://github.com/kubernetes/sample-apiserver/blob/master/README.md">sample-apiserver</a>。</p>
<p>或者，你可以使用现有的第三方解决方案，例如
<a href="https://github.com/Kubernetes-incubator/apiserver-builder/blob/master/README.md">apiserver-builder</a>，
它将生成框架并自动执行以下所有步骤。</p>
<!--
1. Make sure the APIService API is enabled (check `-runtime-config`). It should be on by default, unless it's been deliberately turned off in your cluster.
1. You may need to make an RBAC rule allowing you to add APIService objects, or get your cluster administrator to make one. (Since API extensions affect the entire cluster, it is not recommended to do testing/development/debug of an API extension in a live cluster.)
1. Create the Kubernetes namespace you want to run your extension api-service in.
1. Create/get a CA cert to be used to sign the server cert the extension api-server uses for HTTPS.
1. Create a server cert/key for the api-server to use for HTTPS. This cert should be signed by the above CA. It should also have a CN of the Kube DNS name. This is derived from the Kubernetes service and be of the form `<service name>.<service name namespace>.svc`
1. Create a Kubernetes secret with the server cert/key in your namespace.
1. Create a Kubernetes deployment for the extension api-server and make sure you are loading the secret as a volume. It should contain a reference to a working image of your extension api-server. The deployment should also be in your namespace.
-->
<ol>
<li>确保启用了 APIService API（检查 <code>--runtime-config</code>）。默认应该是启用的，除非被特意关闭了。</li>
<li>你可能需要制定一个 RBAC 规则，以允许你添加 APIService 对象，或让你的集群管理员创建一个。
（由于 API 扩展会影响整个集群，因此不建议在实时集群中对 API 扩展进行测试/开发/调试）</li>
<li>创建 Kubernetes 命名空间，扩展的 api-service 将运行在该命名空间中。</li>
<li>创建（或获取）用来签署服务器证书的 CA 证书，扩展 api-server 中将使用该证书做 HTTPS 连接。</li>
<li>为 api-server 创建一个服务端的证书（或秘钥）以使用 HTTPS。这个证书应该由上述的 CA 签署。
同时应该还要有一个 Kube DNS 名称的 CN，这是从 Kubernetes 服务派生而来的，
格式为 <code>&lt;service name&gt;.&lt;service name namespace&gt;.svc</code>。</li>
<li>使用命名空间中的证书（或秘钥）创建一个 Kubernetes secret。</li>
<li>为扩展 api-server 创建一个 Kubernetes Deployment，并确保以卷的方式挂载了 Secret。
它应该包含对扩展 api-server 镜像的引用。Deployment 也应该在同一个命名空间中。</li>
</ol>
<!--
1. Make sure that your extension-apiserver loads those certs from that volume and that they are used in the HTTPS handshake.
1. Create a Kubernetes service account in your namespace.
1. Create a Kubernetes cluster role for the operations you want to allow on your resources.
1. Create a Kubernetes cluster role binding from the service account in your namespace to the cluster role you created.
1. Create a Kubernetes cluster role binding from the service account in your namespace to the `system:auth-delegator` cluster role to delegate auth decisions to the Kubernetes core API server.
1. Create a Kubernetes role binding from the service account in your namespace to the `extension-apiserver-authentication-reader` role. This allows your extension api-server to access the `extension-apiserver-authentication` configmap.
-->
<ol start="8">
<li>确保你的扩展 apiserver 从该卷中加载了那些证书，并在 HTTPS 握手过程中使用它们。</li>
<li>在你的命名空间中创建一个 Kubernetes 服务账号。</li>
<li>为资源允许的操作创建 Kubernetes 集群角色。</li>
<li>用你命名空间中的服务账号创建一个 Kubernetes 集群角色绑定，绑定到你创建的角色上。</li>
<li>用你命名空间中的服务账号创建一个 Kubernetes 集群角色绑定，绑定到 <code>system:auth-delegator</code>
集群角色，以将 auth 决策委派给 Kubernetes 核心 API 服务器。</li>
<li>以你命名空间中的服务账号创建一个 Kubernetes 集群角色绑定，绑定到
<code>extension-apiserver-authentication-reader</code> 角色。
这将让你的扩展 api-server 能够访问 <code>extension-apiserver-authentication</code> configmap。</li>
</ol>
<!--
1. Create a Kubernetes apiservice. The CA cert above should be base64 encoded, stripped of new lines and used as the spec.caBundle in the apiservice. This should not be namespaced. If using the [kube-aggregator API](https://github.com/kubernetes/kube-aggregator/), only pass in the PEM encoded CA bundle because the base 64 encoding is done for you.
1. Use kubectl to get your resource. When run, kubectl should return "No resources found.". This message indicates that everything worked but you currently have no objects of that resource type created.
-->
<ol start="14">
<li>创建一个 Kubernetes apiservice。
上述的 CA 证书应该使用 base64 编码，剥离新行并用作 apiservice 中的 spec.caBundle。
该资源不应放到任何名字空间。如果使用了
<a href="https://github.com/kubernetes/kube-aggregator/">kube-aggregator API</a>，那么只需要传入
PEM 编码的 CA 绑定，因为 base 64 编码已经完成了。</li>
<li>使用 kubectl 来获得你的资源。
它应该返回 &quot;找不到资源&quot;。此消息表示一切正常，但你目前还没有创建该资源类型的对象。</li>
</ol>
<h2 id="接下来">接下来</h2>
<!--
* If you haven't already, [configure the aggregation layer](/docs/tasks/access-kubernetes-api/configure-aggregation-layer/) and enable the apiserver flags.
* For a high level overview, see [Extending the Kubernetes API with the aggregation layer](/docs/concepts/api-extension/apiserver-aggregation).
* Learn how to [Extend the Kubernetes API Using Custom Resource Definitions](/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/).
-->
<ul>
<li>如果你还未配置，请<a href="/zh/docs/tasks/extend-kubernetes/configure-aggregation-layer/">配置聚合层</a>
并启用 apiserver 的相关参数。</li>
<li>高级概述，请参阅<a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation">使用聚合层扩展 Kubernetes API</a>。</li>
<li>了解如何<a href="/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">使用 Custom Resource Definition 扩展 Kubernetes API</a>。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c00a2767fac9dbfafce583cf489cc423">11.4 - 配置多个调度器</h1>
    
	<!--
reviewers:
- davidopp
- madhusudancs
title: Configure Multiple Schedulers
content_type: task
weight: 20
-->
<!-- overview -->
<!--
Kubernetes ships with a default scheduler that is described
[here](/docs/reference/command-line-tools-reference/kube-scheduler/).
If the default scheduler does not suit your needs you can implement your own scheduler.
Moreover, you can even run multiple schedulers simultaneously alongside the default
scheduler and instruct Kubernetes what scheduler to use for each of your pods. Let's
learn how to run multiple schedulers in Kubernetes with an example.
-->
<p>Kubernetes 自带了一个默认调度器，其详细描述请查阅
<a href="/zh/docs/reference/command-line-tools-reference/kube-scheduler/">这里</a>。
如果默认调度器不适合你的需求，你可以实现自己的调度器。
而且，你甚至可以和默认调度器一起同时运行多个调度器，并告诉 Kubernetes 为每个
Pod 使用哪个调度器。
让我们通过一个例子讲述如何在 Kubernetes 中运行多个调度器。</p>
<!--
A detailed description of how to implement a scheduler is outside the scope of this
document. Please refer to the kube-scheduler implementation in
[pkg/scheduler](https://github.com/kubernetes/kubernetes/tree/main/pkg/scheduler)
in the Kubernetes source directory for a canonical example.
-->
<p>关于实现调度器的具体细节描述超出了本文范围。
请参考 kube-scheduler 的实现，规范示例代码位于
<a href="https://github.com/kubernetes/kubernetes/tree/main/pkg/scheduler">pkg/scheduler</a>。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Package the scheduler

Package your scheduler binary into a container image. For the purposes of this example,
you can use the default scheduler (kube-scheduler) as your second scheduler.
Clone the [Kubernetes source code from GitHub](https://github.com/kubernetes/kubernetes)
and build the source.
-->
<h2 id="打包调度器">打包调度器</h2>
<p>将调度器可执行文件打包到容器镜像中。出于示例目的，可以使用默认调度器
（kube-scheduler）作为第二个调度器。
克隆 <a href="https://github.com/kubernetes/kubernetes">GitHub 上 Kubernetes 源代码</a>，
并编译构建源代码。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">git clone https://github.com/kubernetes/kubernetes.git
<span style="color:#a2f">cd</span> kubernetes
make
</code></pre></div><!--
Create a container image containing the kube-scheduler binary. Here is the `Dockerfile`
to build the image:
-->
<p>创建一个包含 kube-scheduler 二进制文件的容器镜像。用于构建镜像的 <code>Dockerfile</code> 内容如下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-docker" data-lang="docker"><span style="color:#a2f;font-weight:bold">FROM</span><span style="color:#b44"> busybox</span><span style="">
</span><span style=""></span><span style="color:#a2f;font-weight:bold">ADD</span> ./_output/local/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler<span style="">
</span></code></pre></div><!--
Save the file as `Dockerfile`, build the image and push it to a registry. This example
pushes the image to
[Google Container Registry (GCR)](https://cloud.google.com/container-registry/).
For more details, please read the GCR
[documentation](https://cloud.google.com/container-registry/docs/).
-->
<p>将文件保存为 <code>Dockerfile</code>，构建镜像并将其推送到镜像仓库。
此示例将镜像推送到 <a href="https://cloud.google.com/container-registry/">Google 容器镜像仓库（GCR）</a>。
有关详细信息，请阅读 GCR <a href="https://cloud.google.com/container-registry/docs/">文档</a>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 .
gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0
</code></pre></div><!--
## Define a Kubernetes Deployment for the scheduler

Now that you have your scheduler in a container image, create a pod
configuration for it and run it in your Kubernetes cluster. But instead of creating a pod
directly in the cluster, you can use a [Deployment](/docs/concepts/workloads/controllers/deployment/)
for this example. A [Deployment](/docs/concepts/workloads/controllers/deployment/) manages a
[Replica Set](/docs/concepts/workloads/controllers/replicaset/) which in turn manages the pods,
thereby making the scheduler resilient to failures. Here is the deployment
config. Save it as `my-scheduler.yaml`:
-->
<h2 id="为调度器定义-kubernetes-deployment">为调度器定义 Kubernetes Deployment</h2>
<p>现在将调度器放在容器镜像中，为它创建一个 Pod 配置，并在 Kubernetes 集群中
运行它。但是与其在集群中直接创建一个 Pod，不如使用
<a href="/zh/docs/concepts/workloads/controllers/deployment/">Deployment</a>。
Deployment 管理一个 <a href="/zh/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>，
ReplicaSet 再管理 Pod，从而使调度器能够免受一些故障的影响。
以下是 Deployment 配置，将其保存为 <code>my-scheduler.yaml</code>：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/sched/my-scheduler.yaml" download="admin/sched/my-scheduler.yaml"><code>admin/sched/my-scheduler.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-sched-my-scheduler-yaml')" title="Copy admin/sched/my-scheduler.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-sched-my-scheduler-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-scheduler-as-kube-scheduler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">roleRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:kube-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-scheduler-as-volume-scheduler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">roleRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:volume-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Deployment<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>control-plane<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>scheduler<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>control-plane<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">component</span>:<span style="color:#bbb"> </span>scheduler<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">tier</span>:<span style="color:#bbb"> </span>control-plane<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">version</span>:<span style="color:#bbb"> </span>second<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- /usr/local/bin/kube-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --address=0.0.0.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --leader-elect=false<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- --scheduler-name=my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>gcr.io/my-gcp-project/my-kube-scheduler:1.0<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">10251</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kube-second-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">10251</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;0.1&#39;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">securityContext</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">privileged</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb"> </span>[]<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPID</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb"> </span>[]<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
An important thing to note here is that the name of the scheduler specified as an
argument to the scheduler command in the container spec should be unique.
This is the name that is matched against the value of the optional `spec.schedulerName`
on pods, to determine whether this scheduler is responsible for scheduling a particular pod.
-->
<p>这里需要注意的是，在容器规约中配置的调度器启动命令参数（--scheduler-name）所指定的
调度器名称应该是唯一的。
这个名称应该与 Pod 上的可选参数 <code>spec.schedulerName</code> 的值相匹配，也就是说调度器名称的匹配
关系决定了 Pods 的调度任务由哪个调度器负责。</p>
<!--
Note also that we created a dedicated service account `my-scheduler` and bind the cluster role
`system:kube-scheduler` to it so that it can acquire the same privileges as `kube-scheduler`.
-->
<p>还要注意，我们创建了一个专用服务账号 <code>my-scheduler</code> 并将集群角色 <code>system:kube-scheduler</code>
绑定到它，以便它可以获得与 <code>kube-scheduler</code> 相同的权限。</p>
<!--
Please see the
[kube-scheduler documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for
detailed description of other command line arguments.
-->
<p>请参阅 <a href="/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler 文档</a>
以获取其他命令行参数的详细说明。</p>
<!--
## Run the second scheduler in the cluster

In order to run your scheduler in a Kubernetes cluster, create the deployment
specified in the config above in a Kubernetes cluster:
-->
<h2 id="在集群中运行第二个调度器">在集群中运行第二个调度器</h2>
<p>为了在 Kubernetes 集群中运行我们的第二个调度器，在 Kubernetes 集群中创建上面配置中指定的 Deployment：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f my-scheduler.yaml
</code></pre></div><!--
Verify that the scheduler pod is running:
-->
<p>验证调度器 Pod 正在运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace<span style="color:#666">=</span>kube-system
</code></pre></div><p>输出类似于：</p>
<pre tabindex="0"><code>NAME                                           READY     STATUS    RESTARTS   AGE
....
my-scheduler-lnf4s-4744f                       1/1       Running   0          2m
...
</code></pre><!--
You should see a "Running" my-scheduler pod, in addition to the default kube-scheduler
pod in this list.
-->
<p>此列表中，除了默认的 <code>kube-scheduler</code> Pod 之外，你应该还能看到处于 “Running” 状态的
<code>my-scheduler</code> Pod。</p>
<!--
### Enable leader election

To run multiple-scheduler with leader election enabled, you must do the following:

First, update the following fields in your YAML file:
-->
<h3 id="启用领导者选举">启用领导者选举</h3>
<p>要在启用了 leader 选举的情况下运行多调度器，你必须执行以下操作：</p>
<p>首先，更新上述 Deployment YAML（my-scheduler.yaml）文件中的以下字段：</p>
<ul>
<li><code>--leader-elect=true</code></li>
<li><code>--lock-object-namespace=&lt;lock-object-namespace&gt;</code></li>
<li><code>--lock-object-name=&lt;lock-object-name&gt;</code></li>
</ul>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
The control plane creates the lock objects for you, but the namespace must already exist.
You can use the `kube-system` namespace.
-->
<p>控制平面会为你创建锁对象，但是命名空间必须已经存在。
你可以使用 <code>kube-system</code> 命名空间。</div>
</blockquote>
<!--
If RBAC is enabled on your cluster, you must update the `system:kube-scheduler` cluster role.
Add your scheduler name to the resourceNames of the rule applied for `endpoints` and `leases` resources, as in the following example:
-->
<p>如果在集群上启用了 RBAC，则必须更新 <code>system：kube-scheduler</code> 集群角色。
将调度器名称添加到应用了 <code>endpoints</code> 和 <code>leases</code> 资源的规则的 resourceNames 中，如以下示例所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit clusterrole system:kube-scheduler
</code></pre></div>

 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/sched/clusterrole.yaml" download="admin/sched/clusterrole.yaml"><code>admin/sched/clusterrole.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-sched-clusterrole-yaml')" title="Copy admin/sched/clusterrole.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-sched-clusterrole-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rbac.authorization.kubernetes.io/autoupdate</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/bootstrapping</span>:<span style="color:#bbb"> </span>rbac-defaults<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:kube-scheduler<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">rules</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- coordination.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- leases<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- create<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- coordination.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resourceNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- kube-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- leases<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- get<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- update<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroups</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#b44">&#34;&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resourceNames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- kube-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- endpoints<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">verbs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- delete<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- get<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- patch<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- update<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
## Specify schedulers for pods
-->
<h2 id="为-pod-指定调度器">为 Pod 指定调度器</h2>
<!--
Now that your second scheduler is running, create some pods, and direct them
to be scheduled by either the default scheduler or the one you deployed.
In order to schedule a given pod using a specific scheduler, specify the name of the
scheduler in that pod spec. Let's look at three examples.
-->
<p>现在第二个调度器正在运行，创建一些 Pod，并指定它们由默认调度器或部署的调度器进行调度。
为了使用特定的调度器调度给定的 Pod，在那个 Pod 的 spec 中指定调度器的名称。让我们看看三个例子。</p>
<!--
- Pod spec without any scheduler name
-->
<ul>
<li>
<p>Pod spec 没有任何调度器名称</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/sched/pod1.yaml" download="admin/sched/pod1.yaml"><code>admin/sched/pod1.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-sched-pod1-yaml')" title="Copy admin/sched/pod1.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-sched-pod1-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">no</span>-annotation<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>multischeduler-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-with-no-annotation-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0</code></pre></div>
    </div>
</div>


<!--
When no scheduler name is supplied, the pod is automatically scheduled using the
default-scheduler.
-->
<p>如果未提供调度器名称，则会使用 default-scheduler 自动调度 pod。</p>
<!--
Save this file as `pod1.yaml` and submit it to the Kubernetes cluster.
-->
<p>将此文件另存为 <code>pod1.yaml</code>，并将其提交给 Kubernetes 集群。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f pod1.yaml
</code></pre></div></li>
</ul>
<!--
- Pod spec with `default-scheduler`
-->
<ul>
<li>
<p>Pod spec 设置为 <code>default-scheduler</code></p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/sched/pod2.yaml" download="admin/sched/pod2.yaml"><code>admin/sched/pod2.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-sched-pod2-yaml')" title="Copy admin/sched/pod2.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-sched-pod2-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>annotation-default-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>multischeduler-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>default-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-with-default-annotation-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
A scheduler is specified by supplying the scheduler name as a value to `spec.schedulerName`. In this case, we supply the name of the
default scheduler which is `default-scheduler`.
-->
<p>通过将调度器名称作为 <code>spec.schedulerName</code> 参数的值来指定调度器。
在这种情况下，我们提供默认调度器的名称，即 <code>default-scheduler</code>。</p>
<!--
Save this file as `pod2.yaml` and submit it to the Kubernetes cluster.
-->
<p>将此文件另存为 <code>pod2.yaml</code>，并将其提交给 Kubernetes 集群。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f pod2.yaml
</code></pre></div></li>
</ul>
<!--
- Pod spec with `my-scheduler`
-->
<ul>
<li>
<p>Pod spec 设置为 <code>my-scheduler</code></p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/sched/pod3.yaml" download="admin/sched/pod3.yaml"><code>admin/sched/pod3.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-sched-pod3-yaml')" title="Copy admin/sched/pod3.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-sched-pod3-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>annotation-second-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>multischeduler-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">schedulerName</span>:<span style="color:#bbb"> </span>my-scheduler<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pod-with-second-annotation-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/pause:2.0<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
In this case, we specify that this pod should be scheduled using the scheduler that we
deployed - `my-scheduler`. Note that the value of `spec.schedulerName` should match the name supplied to the scheduler
command as an argument in the deployment config for the scheduler.
-->
<p>在这种情况下，我们指定此 pod 使用我们部署的 <code>my-scheduler</code> 来调度。
请注意，<code>spec.schedulerName</code> 参数的值应该与 Deployment 中配置的提供给
scheduler 命令的参数名称匹配。</p>
<!--
Save this file as `pod3.yaml` and submit it to the Kubernetes cluster.
-->
<p>将此文件另存为 <code>pod3.yaml</code>，并将其提交给 Kubernetes 集群。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f pod3.yaml
</code></pre></div></li>
</ul>
<!--
  Verify that all three pods are running.
-->
<p>确认所有三个 pod 都在运行。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods
</code></pre></div><!-- discussion -->
<!--
### Verifying that the pods were scheduled using the desired schedulers
-->
<h3 id="验证是否使用所需的调度器调度了-pod">验证是否使用所需的调度器调度了 pod</h3>
<!--
In order to make it easier to work through these examples, we did not verify that the
pods were actually scheduled using the desired schedulers. We can verify that by
changing the order of pod and deployment config submissions above. If we submit all the
pod configs to a Kubernetes cluster before submitting the scheduler deployment config,
we see that the pod `annotation-second-scheduler` remains in "Pending" state forever
while the other two pods get scheduled. Once we submit the scheduler deployment config
and our new scheduler starts running, the `annotation-second-scheduler` pod gets
scheduled as well.
-->
<p>为了更容易地完成这些示例，我们没有验证 Pod 实际上是使用所需的调度程序调度的。
我们可以通过更改 Pod 的顺序和上面的部署配置提交来验证这一点。
如果我们在提交调度器部署配置之前将所有 Pod 配置提交给 Kubernetes 集群，
我们将看到注解了 <code>annotation-second-scheduler</code> 的 Pod 始终处于 “Pending” 状态，
而其他两个 Pod 被调度。
一旦我们提交调度器部署配置并且我们的新调度器开始运行，注解了
<code>annotation-second-scheduler</code> 的 pod 就能被调度。</p>
<!--
Alternatively, you can look at the "Scheduled" entries in the event logs to
verify that the pods were scheduled by the desired schedulers.
-->
<p>或者，可以查看事件日志中的 “Scheduled” 条目，以验证是否由所需的调度器调度了 Pod。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get events
</code></pre></div><!--
You can also use a [custom scheduler configuration](/docs/reference/scheduling/config/#multiple-profiles)
or a custom container image for the cluster's main scheduler by modifying its static pod manifest
on the relevant control plane nodes.
-->
<p>你也可以使用<a href="/zh/docs/reference/scheduling/config/#multiple-profiles">自定义调度器配置</a>
或自定义容器镜像，用于集群的主调度器，方法是在相关控制平面节点上修改其静态 pod 清单。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1707517970dd390995f760308c2e2de6">11.5 - 使用 HTTP 代理访问 Kubernetes API</h1>
    
	<!--
---
title: Use an HTTP Proxy to Access the Kubernetes API
content_type: task
weight: 40
---
-->
<!-- overview -->
<!--
This page shows how to use an HTTP proxy to access the Kubernetes API.
-->
<p>本文说明如何使用 HTTP 代理访问 Kubernetes API。</p>
<h2 id="准备开始">准备开始</h2>
<ul>
<li><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</li>
</ul>
<!--
* If you do not already have an application running in your cluster, start
  a Hello world application by entering this command:
-->
<ul>
<li>如果您的集群中还没有任何应用，使用如下命令启动一个 Hello World 应用：</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create deployment node-hello --image<span style="color:#666">=</span>gcr.io/google-samples/node-hello:1.0 --port<span style="color:#666">=</span><span style="color:#666">8080</span>
</code></pre></div><!-- steps -->
<!--
## Using kubectl to start a proxy server
-->
<h2 id="使用-kubectl-启动代理服务器">使用 kubectl 启动代理服务器</h2>
<!--
This command starts a proxy to the Kubernetes API server:
-->
<p>使用如下命令启动 Kubernetes API 服务器的代理：</p>
<pre><code>kubectl proxy --port=8080
</code></pre>
<!--
## Exploring the Kubernetes API
-->
<h2 id="探究-kubernetes-api">探究 Kubernetes API</h2>
<!--
When the proxy server is running, you can explore the API using `curl`, `wget`,
or a browser.
-->
<p>当代理服务器在运行时，你可以通过 <code>curl</code>、<code>wget</code> 或者浏览器访问 API。</p>
<!--
Get the API versions:
 -->
<p>获取 API 版本：</p>
<pre><code>curl http://localhost:8080/api/
</code></pre>
<!--
The output should look similar to this:
-->
<p>输出应该类似这样：</p>
<pre><code>{
  &quot;kind&quot;: &quot;APIVersions&quot;,
  &quot;versions&quot;: [
    &quot;v1&quot;
  ],
  &quot;serverAddressByClientCIDRs&quot;: [
    {
      &quot;clientCIDR&quot;: &quot;0.0.0.0/0&quot;,
      &quot;serverAddress&quot;: &quot;10.0.2.15:8443&quot;
    }
  ]
}
</code></pre>
<!--
Get a list of pods:
 -->
<p>获取 Pod 列表：</p>
<pre><code>curl http://localhost:8080/api/v1/namespaces/default/pods

{
  &quot;kind&quot;: &quot;PodList&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {
    &quot;resourceVersion&quot;: &quot;33074&quot;
  },
  &quot;items&quot;: [
    {
      &quot;metadata&quot;: {
        &quot;name&quot;: &quot;kubernetes-bootcamp-2321272333-ix8pt&quot;,
        &quot;generateName&quot;: &quot;kubernetes-bootcamp-2321272333-&quot;,
        &quot;namespace&quot;: &quot;default&quot;,
        &quot;uid&quot;: &quot;ba21457c-6b1d-11e6-85f7-1ef9f1dab92b&quot;,
        &quot;resourceVersion&quot;: &quot;33003&quot;,
        &quot;creationTimestamp&quot;: &quot;2016-08-25T23:43:30Z&quot;,
        &quot;labels&quot;: {
          &quot;pod-template-hash&quot;: &quot;2321272333&quot;,
          &quot;run&quot;: &quot;kubernetes-bootcamp&quot;
        },
        ...
}
</code></pre>
<h2 id="接下来">接下来</h2>
<!--
Learn more about [kubectl proxy](/docs/reference/generated/kubectl/kubectl-commands#proxy).
-->
<p>想了解更多信息，请参阅 <a href="/docs/reference/generated/kubectl/kubectl-commands#proxy">kubectl 代理</a>。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-61cf1f2f0fbe98e7635fce65f04a775f">11.6 - 设置 Konnectivity 服务</h1>
    
	<!-- overview -->
<!--
The Konnectivity service provides a TCP level proxy for the control plane to cluster
communication.
-->
<p>Konnectivity 服务为控制平面提供集群通信的 TCP 级别代理。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!--
## Configure the Konnectivity service

The following steps require an egress configuration, for example:
-->
<h2 id="配置-konnectivity-服务">配置 Konnectivity 服务</h2>
<p>接下来的步骤需要出口配置，比如：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/konnectivity/egress-selector-configuration.yaml" download="admin/konnectivity/egress-selector-configuration.yaml"><code>admin/konnectivity/egress-selector-configuration.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-konnectivity-egress-selector-configuration-yaml')" title="Copy admin/konnectivity/egress-selector-configuration.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-konnectivity-egress-selector-configuration-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiserver.k8s.io/v1beta1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>EgressSelectorConfiguration<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">egressSelections</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># Since we want to control the egress traffic to the cluster, we use the</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># &#34;cluster&#34; as the name. Other supported values are &#34;etcd&#34;, and &#34;master&#34;.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cluster<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">connection</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># This controls the protocol between the API Server and the Konnectivity</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># server. Supported values are &#34;GRPC&#34; and &#34;HTTPConnect&#34;. There is no</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># end user visible difference between the two modes. You need to set the</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># Konnectivity server to work in the same mode.</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">proxyProtocol</span>:<span style="color:#bbb"> </span>GRPC<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">transport</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># This controls what transport the API Server uses to communicate with the</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Konnectivity server. UDS is recommended if the Konnectivity server</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># locates on the same machine as the API Server. You need to configure the</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># Konnectivity server to listen on the same UDS socket.</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># The other supported transport is &#34;tcp&#34;. You will need to set up TLS </span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># config to secure the TCP transport.</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">uds</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">udsName</span>:<span style="color:#bbb"> </span>/etc/kubernetes/konnectivity-server/konnectivity-server.socket<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
You need to configure the API Server to use the Konnectivity service
and direct the network traffic to the cluster nodes:

1. Make sure that
the `ServiceAccountTokenVolumeProjection` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
is enabled. You can enable
[service account token volume protection](/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection)
by providing the following flags to the kube-apiserver:
   ```
   --service-account-issuer=api
   --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
   --api-audiences=system:konnectivity-server
   ```
1. Create an egress configuration file such as `admin/konnectivity/egress-selector-configuration.yaml`.
1. Set the `--egress-selector-config-file` flag of the API Server to the path of
your API Server egress configuration file.
1. If you use UDS connection, add volumes config to the kube-apiserver:
   ```yaml
   spec:
     containers:
       volumeMounts:
       - name: konnectivity-uds
         mountPath: /etc/kubernetes/konnectivity-server
         readOnly: false
     volumes:
     - name: konnectivity-uds
       hostPath:
         path: /etc/kubernetes/konnectivity-server
         type: DirectoryOrCreate
   ```
-->
<p>你需要配置 API 服务器来使用 Konnectivity 服务，并将网络流量定向到集群节点：</p>
<ol>
<li>
<p>确保 <code>ServiceAccountTokenVolumeProjection</code>
<a href="/zh/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>
被启用。你可以通过为 kube-apiserver 提供以下标志启用
<a href="/zh/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">服务账号令牌卷保护</a>：</p>
<pre tabindex="0"><code>--service-account-issuer=api
--service-account-signing-key-file=/etc/kubernetes/pki/sa.key
--api-audiences=system:konnectivity-server
</code></pre></li>
<li>
<p>创建一个出站流量配置文件，比如 <code>admin/konnectivity/egress-selector-configuration.yaml</code>。</p>
</li>
<li>
<p>将 API 服务器的 <code>--egress-selector-config-file</code> 参数设置为你的 API 服务器的
离站流量配置文件路径。</p>
</li>
<li>
<p>如果你在使用 UDS 连接，须将卷配置添加到 kube-apiserver：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-uds<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/kubernetes/konnectivity-server<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-uds<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/etc/kubernetes/konnectivity-server<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>DirectoryOrCreate<span style="color:#bbb">
</span></code></pre></div></li>
</ol>
<!--
Generate or obtain a certificate and kubeconfig for konnectivity-server.
For example, you can use the OpenSSL command line tool to issue a X.509 certificate,
using the cluster CA certificate `/etc/kubernetes/pki/ca.crt` from a control-plane host.
-->
<p>为 konnectivity-server 生成或者取得证书和 kubeconfig 文件。
例如，你可以使用 OpenSSL 命令行工具，基于存放在某控制面主机上
<code>/etc/kubernetes/pki/ca.crt</code> 文件中的集群 CA 证书来
发放一个 X.509 证书，</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">openssl req -subj <span style="color:#b44">&#34;/CN=system:konnectivity-server&#34;</span> -new -newkey rsa:2048 -nodes -out konnectivity.csr -keyout konnectivity.key -out konnectivity.csr
openssl x509 -req -in konnectivity.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out konnectivity.crt -days <span style="color:#666">375</span> -sha256
<span style="color:#b8860b">SERVER</span><span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">$(</span>kubectl config view -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.clusters..server}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>
kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-credentials system:konnectivity-server --client-certificate konnectivity.crt --client-key konnectivity.key --embed-certs<span style="color:#666">=</span><span style="color:#a2f">true</span>
kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-cluster kubernetes --server <span style="color:#b44">&#34;</span><span style="color:#b8860b">$SERVER</span><span style="color:#b44">&#34;</span> --certificate-authority /etc/kubernetes/pki/ca.crt --embed-certs<span style="color:#666">=</span><span style="color:#a2f">true</span>
kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-context system:konnectivity-server@kubernetes --cluster kubernetes --user system:konnectivity-server
kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config use-context system:konnectivity-server@kubernetes
rm -f konnectivity.crt konnectivity.key konnectivity.csr
</code></pre></div><!--
Next, you need to deploy the Konnectivity server and agents.
[kubernetes-sigs/apiserver-network-proxy](https://github.com/kubernetes-sigs/apiserver-network-proxy)
is a reference implementation.

Deploy the Konnectivity server on your control plane node. The provided
`konnectivity-server.yaml` manifest assumes
that the Kubernetes components are deployed as a <a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='static Pod'>static Pod</a> in your cluster. If not, you can deploy the Konnectivity
server as a DaemonSet.
-->
<p>接下来，你需要部署 Konnectivity 服务器和代理。
<a href="https://github.com/kubernetes-sigs/apiserver-network-proxy">kubernetes-sigs/apiserver-network-proxy</a>
是一个参考实现。</p>
<p>在控制面节点上部署 Konnectivity 服务。
下面提供的 <code>konnectivity-server.yaml</code> 配置清单假定在你的集群中
Kubernetes 组件都是部署为<a class='glossary-tooltip' title='静态Pod（Static Pod）是指由特定节点上的 kubelet 守护进程直接管理的 Pod。' data-toggle='tooltip' data-placement='top' href='/zh/docs/tasks/configure-pod-container/static-pod/' target='_blank' aria-label='静态 Pod'>静态 Pod</a> 的。
如果不是，你可以将 Konnectivity 服务部署为 DaemonSet。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/konnectivity/konnectivity-server.yaml" download="admin/konnectivity/konnectivity-server.yaml"><code>admin/konnectivity/konnectivity-server.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-konnectivity-konnectivity-server-yaml')" title="Copy admin/konnectivity/konnectivity-server.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-konnectivity-konnectivity-server-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-server<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">priorityClassName</span>:<span style="color:#bbb"> </span>system-cluster-critical<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostNetwork</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-server-container<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-server:v0.0.16<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/proxy-server&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--logtostderr=true&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># This needs to be consistent with the value set in egressSelectorConfiguration.</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--uds-name=/etc/kubernetes/konnectivity-server/konnectivity-server.socket&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># The following two lines assume the Konnectivity server is</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># deployed on the same machine as the apiserver, and the certs and</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># key of the API Server are at the specified location.</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--cluster-cert=/etc/kubernetes/pki/apiserver.crt&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--cluster-key=/etc/kubernetes/pki/apiserver.key&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#080;font-style:italic"># This needs to be consistent with the value set in egressSelectorConfiguration.</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--mode=grpc&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--server-port=0&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--agent-port=8132&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--admin-port=8133&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--health-port=8134&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--agent-namespace=kube-system&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--agent-service-account=konnectivity-agent&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--kubeconfig=/etc/kubernetes/konnectivity-server.conf&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#b44">&#34;--authentication-audience=system:konnectivity-server&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span>]<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">scheme</span>:<span style="color:#bbb"> </span>HTTP<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">host</span>:<span style="color:#bbb"> </span><span style="color:#666">127.0.0.1</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8134</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">60</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>agentport<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8132</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8132</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>adminport<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8133</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8133</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>healthport<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8134</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">hostPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8134</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>k8s-certs<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kubeconfig<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/kubernetes/konnectivity-server.conf<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-uds<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/etc/kubernetes/konnectivity-server<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">false</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>k8s-certs<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/etc/kubernetes/pki<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kubeconfig<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/etc/kubernetes/konnectivity-server.conf<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>FileOrCreate<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-uds<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/etc/kubernetes/konnectivity-server<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>DirectoryOrCreate<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Then deploy the Konnectivity agents in your cluster:
-->
<p>在你的集群中部署 Konnectivity 代理：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/konnectivity/konnectivity-agent.yaml" download="admin/konnectivity/konnectivity-agent.yaml"><code>admin/konnectivity/konnectivity-agent.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-konnectivity-konnectivity-agent-yaml')" title="Copy admin/konnectivity/konnectivity-agent.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-konnectivity-konnectivity-agent-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># Alternatively, you can deploy the agents as Deployments. It is not necessary</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># to have an agent on each node.</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">addonmanager.kubernetes.io/mode</span>:<span style="color:#bbb"> </span>Reconcile<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>konnectivity-agent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-agent<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>konnectivity-agent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>konnectivity-agent<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">priorityClassName</span>:<span style="color:#bbb"> </span>system-cluster-critical<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;CriticalAddonsOnly&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Exists&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent:v0.0.16<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-agent<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/proxy-agent&#34;</span>]<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#b44">&#34;--logtostderr=true&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#b44">&#34;--ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#080;font-style:italic"># Since the konnectivity server runs with hostNetwork=true,</span><span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#080;font-style:italic"># this is the IP address of the master machine.</span><span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#b44">&#34;--proxy-server-host=35.225.206.7&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#b44">&#34;--proxy-server-port=8132&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#b44">&#34;--admin-server-port=8133&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#b44">&#34;--health-server-port=8134&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#b44">&#34;--service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">                  </span>]<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/run/secrets/tokens<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-agent-token<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">livenessProbe</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">httpGet</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">8134</span><span style="color:#bbb">
</span><span style="color:#bbb">              </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/healthz<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">initialDelaySeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">timeoutSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">15</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">serviceAccountName</span>:<span style="color:#bbb"> </span>konnectivity-agent<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-agent-token<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">projected</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">sources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">              </span>- <span style="color:#008000;font-weight:bold">serviceAccountToken</span>:<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>konnectivity-agent-token<span style="color:#bbb">
</span><span style="color:#bbb">                  </span><span style="color:#008000;font-weight:bold">audience</span>:<span style="color:#bbb"> </span>system:konnectivity-server<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Last, if RBAC is enabled in your cluster, create the relevant RBAC rules:
-->
<p>最后，如果你的集群启用了 RBAC，请创建相关的 RBAC 规则：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/admin/konnectivity/konnectivity-rbac.yaml" download="admin/konnectivity/konnectivity-rbac.yaml"><code>admin/konnectivity/konnectivity-rbac.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('admin-konnectivity-konnectivity-rbac-yaml')" title="Copy admin/konnectivity/konnectivity-rbac.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="admin-konnectivity-konnectivity-rbac-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRoleBinding<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:konnectivity-server<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">addonmanager.kubernetes.io/mode</span>:<span style="color:#bbb"> </span>Reconcile<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">roleRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterRole<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:auth-delegator<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">subjects</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>rbac.authorization.k8s.io<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>User<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>system:konnectivity-server<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ServiceAccount<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>konnectivity-agent<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">addonmanager.kubernetes.io/mode</span>:<span style="color:#bbb"> </span>Reconcile<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>



</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d3c88a8663f58e9ec0bed73faff5b670">12 - TLS</h1>
    <div class="lead">了解如何使用传输层安全性（ TLS ）保护集群中的流量。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-1272b18ac0c008f6ffc2c62a29fa929f">12.1 - 为 kubelet 配置证书轮换</h1>
    
	<!--
reviewers:
- jcbsmpsn
- mikedanese
title: Configure Certificate Rotation for the Kubelet
content_type: task
-->
<!-- overview -->
<!--
This page shows how to enable and configure certificate rotation for the kubelet.
-->
<p>本文展示如何在 kubelet 中启用并配置证书轮换。</p>





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code>
</div>

<h2 id="准备开始">准备开始</h2>
<!--
* Kubernetes version 1.8.0 or later is required
-->
<ul>
<li>要求 Kubernetes 1.8.0 或更高的版本</li>
</ul>
<!-- steps -->
<!--
## Overview

The kubelet uses certificates for authenticating to the Kubernetes API.  By
default, these certificates are issued with one year expiration so that they do
not need to be renewed too frequently.
-->
<h2 id="概述">概述</h2>
<p>Kubelet 使用证书进行 Kubernetes API 的认证。
默认情况下，这些证书的签发期限为一年，所以不需要太频繁地进行更新。</p>
<!--
Kubernetes contains [kubelet certificate
rotation](/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/),
that will automatically generate a new key and request a new certificate from
the Kubernetes API as the current certificate approaches expiration. Once the
new certificate is available, it will be used for authenticating connections to
the Kubernetes API.
-->
<p>Kubernetes 包含特性
<a href="/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">kubelet 证书轮换</a>，
在当前证书即将过期时，
将自动生成新的秘钥，并从 Kubernetes API 申请新的证书。 一旦新的证书可用，它将被用于与
Kubernetes API 间的连接认证。</p>
<!--
## Enabling client certificate rotation

The `kubelet` process accepts an argument `--rotate-certificates` that controls
if the kubelet will automatically request a new certificate as the expiration of
the certificate currently in use approaches.
-->
<h2 id="启用客户端证书轮换">启用客户端证书轮换</h2>
<p><code>kubelet</code> 进程接收 <code>--rotate-certificates</code> 参数，该参数决定 kubelet 在当前使用的
证书即将到期时，是否会自动申请新的证书。</p>
<!--
The `kube-controller-manager` process accepts an argument
`--cluster-signing-duration`  (`--experimental-cluster-signing-duration` prior to 1.19)
that controls how long certificates will be issued for.
-->
<p><code>kube-controller-manager</code> 进程接收 <code>--cluster-signing-duration</code> 参数
（在 1.19 版本之前为 <code>--experimental-cluster-signing-duration</code>），用来
控制签发证书的有效期限。</p>
<!--
## Understanding the certificate rotation configuration

When a kubelet starts up, if it is configured to bootstrap (using the
`--bootstrap-kubeconfig` flag), it will use its initial certificate to connect
to the Kubernetes API and issue a certificate signing request. You can view the
status of certificate signing requests using:
-->
<h2 id="理解证书轮换配置">理解证书轮换配置</h2>
<p>当 kubelet 启动时，如被配置为自举（使用<code>--bootstrap-kubeconfig</code> 参数），kubelet
会使用其初始证书连接到 Kubernetes API ，并发送证书签名的请求。
可以通过以下方式查看证书签名请求的状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get csr
</code></pre></div><!--
Initially a certificate signing request from the kubelet on a node will have a
status of `Pending`. If the certificate signing requests meets specific
criteria, it will be auto approved by the controller manager, then it will have
a status of `Approved`. Next, the controller manager will sign a certificate,
issued for the duration specified by the
`--cluster-signing-duration` parameter, and the signed certificate
will be attached to the certificate signing request.
-->
<p>最初，来自节点上 kubelet 的证书签名请求处于 <code>Pending</code> 状态。 如果证书签名请求满足特定条件，
控制器管理器会自动批准，此时请求会处于 <code>Approved</code> 状态。 接下来，控制器管理器会签署证书，
证书的有效期限由 <code>--cluster-signing-duration</code> 参数指定，签署的证书会被附加到证书签名请求中。</p>
<!--
The kubelet will retrieve the signed certificate from the Kubernetes API and
write that to disk, in the location specified by `--cert-dir`. Then the kubelet
will use the new certificate to connect to the Kubernetes API.
-->
<p>Kubelet 会从 Kubernetes API 取回签署的证书，并将其写入磁盘，存储位置通过 <code>--cert-dir</code>
参数指定。
然后 kubelet 会使用新的证书连接到 Kubernetes API。</p>
<!--
As the expiration of the signed certificate approaches, the kubelet will
automatically issue a new certificate signing request, using the Kubernetes API. 
This can happen at any point between 30% and 10% of the time remaining on the 
certificate. Again, the controller manager will automatically approve the certificate
request and attach a signed certificate to the certificate signing request. The
kubelet will retrieve the new signed certificate from the Kubernetes API and
write that to disk. Then it will update the connections it has to the
Kubernetes API to reconnect using the new certificate.
-->
<p>当签署的证书即将到期时，kubelet 会使用 Kubernetes API，自动发起新的证书签名请求。
该请求会发生在证书的有效时间剩下 30% 到 10% 之间的任意时间点。
同样地，控制器管理器会自动批准证书请求，并将签署的证书附加到证书签名请求中。 Kubelet
会从 Kubernetes API 取回签署的证书，并将其写入磁盘。 然后它会更新与 Kubernetes API
的连接，使用新的证书重新连接到 Kubernetes API。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-43d5e2b1fc2a7e104e66d481d08578dc">12.2 - 手动轮换 CA 证书</h1>
    
	<!--
title: Manual Rotation of CA Certificates
min-kubernetes-server-version: v1.13
content_type: task
-->
<!-- overview -->
<!--
This page shows how to manually rotate the certificate authority (CA) certificates.
-->
<p>本页展示如何手动轮换证书机构（CA）证书。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 v1.13.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
- For more information about authentication in Kubernetes, see [Authenticating](/docs/reference/access-authn-authz/authentication).
- For more information about best practices for CA certificates, see [Single root CA](/docs/setup/best-practices/certificates/#single-root-ca).
-->
<ul>
<li>要了解 Kubernetes 中用户认证的更多信息，参阅
<a href="/zh/docs/reference/access-authn-authz/authentication">认证</a>；</li>
<li>要了解与 CA 证书最佳实践有关的更多信息，参阅<a href="/zh/docs/setup/best-practices/certificates/#single-root-ca">单根 CA</a>。</li>
</ul>
<!-- steps -->
<!--
## Rotate the CA certificates manually
-->
<h2 id="rotate-the-ca-certificates-manually">手动轮换 CA 证书 </h2>
<!--
Make sure to back up your certificate directory along with configuration files and any other necessary files.

This approach assumes operation of the Kubernetes control plane in a HA configuration with multiple API servers.
Graceful termination of the API server is also assumed so clients can cleanly disconnect from one API server and reconnect to another.

Configurations with a single API server will experience unavailability while the API server is being restarted.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> <p>确保备份你的证书目录、配置文件以及其他必要文件。</p>
<p>这里的方法假定 Kubernetes 的控制面通过运行多个 API 服务器以高可用配置模式运行。
另一假定是 API 服务器可体面地终止，因而客户端可以彻底地与一个 API 服务器断开
连接并连接到另一个 API 服务器。</p>
<p>如果集群中只有一个 API 服务器，则在 API 服务器重启期间会经历服务中断期。</p>
</div>
</blockquote>

<!--
1. Distribute the new CA certificates and private keys
   (ex: `ca.crt`, `ca.key`, `front-proxy-ca.crt`, and `front-proxy-ca.key`)
   to all your control plane nodes in the Kubernetes certificates directory.
-->
<ol>
<li>将新的 CA 证书和私钥（例如：<code>ca.crt</code>、<code>ca.key</code>、<code>front-proxy-ca.crt</code> 和
<code>front-proxy-client.key</code>）分发到所有控制面节点，放在其 Kubernetes 证书目录下。</li>
</ol>
<!--
1. Update <a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a>'s `--root-ca-file` to
   include both old and new CA. Then restart the component.

   Any service account created after this point will get secrets that include both old and new CAs.
   
   <blockquote class="note callout">
  <div><strong>说明：</strong> The files specified by the kube-controller-manager flags <code>--client-ca-file</code> and <code>--cluster-signing-cert-file</code>
cannot be CA bundles. If these flags and <code>--root-ca-file</code> point to the same <code>ca.crt</code> file which is now a
bundle (includes both old and new CA) you will face an error. To workaround this problem you can copy the new CA to a separate
file and make the flags <code>--client-ca-file</code> and <code>--cluster-signing-cert-file</code> point to the copy. Once <code>ca.crt</code> is no longer
a bundle you can restore the problem flags to point to <code>ca.crt</code> and delete the copy.</div>
</blockquote>
-->
<ol start="2">
<li>
<p>更新 <a class='glossary-tooltip' title='主节点上运行控制器的组件。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kube-controller-manager/' target='_blank' aria-label='kube-controller-manager'>kube-controller-manager</a> 的
<code>--root-ca-file</code> 标志，使之同时包含老的和新的 CA，之后重启组件。</p>
<p>自此刻起，所创建的所有服务账号都会获得同时包含老的 CA 和新的 CA 的 Secret。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> kube-controller-manager 标志 <code>--client-ca-file</code> 和 <code>--cluster-signing-cert-file</code> 所引用的文件
不能是 CA 证书包。如果这些标志和 <code>--root-ca-file</code> 指向同一个 <code>ca.crt</code> 包文件（包含老的和新的 CA 证书），
你将会收到出错信息。
要解决这个问题，可以将新的 CA 证书复制到单独的文件中，并将 <code>--client-ca-file</code> 和 <code>--cluster-signing-cert-file</code>
标志指向该副本。一旦 <code>ca.crt</code> 不再是证书包文件，就可以恢复有问题的标志指向  <code>ca.crt</code> 并删除该副本。</div>
</blockquote>
</li>
</ol>
<!--
   1. Update all service account tokens to include both old and new CA certificates.

   If any pods are started before new CA is used by API servers, they will get this update and trust both old and new CAs.
-->
<ol start="3">
<li>
<p>更新所有服务账号令牌，使之同时包含老的和新的 CA 证书。</p>
<p>如果在 API 服务器使用新的 CA 之前启动了新的 Pod，这些 Pod
也会获得此更新并且同时信任老的和新的 CA 证书。</p>
<!--
```shell
base64_encoded_ca="$(base64 -w0 <path to file containing both old and new CAs>)"

for namespace in $(kubectl get ns --no-headers | awk '{print $1}'); do
    for token in $(kubectl get secrets --namespace "$namespace" --field-selector type=kubernetes.io/service-account-token -o name); do
        kubectl get $token --namespace "$namespace" -o yaml | \
          /bin/sed "s/\(ca.crt:\).*/\1 ${base64_encoded_ca}/" | \
          kubectl apply -f -
    done
done
```
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">base64_encoded_ca</span><span style="color:#666">=</span><span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>base64 -w0 &lt;path to file containing both old and new CAs&gt;<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>

<span style="color:#a2f;font-weight:bold">for</span> namespace in <span style="color:#a2f;font-weight:bold">$(</span>kubectl get ns --no-headers | awk <span style="color:#b44">&#39;{print $1}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>; <span style="color:#a2f;font-weight:bold">do</span>
    <span style="color:#a2f;font-weight:bold">for</span> token in <span style="color:#a2f;font-weight:bold">$(</span>kubectl get secrets --namespace <span style="color:#b44">&#34;</span><span style="color:#b8860b">$namespace</span><span style="color:#b44">&#34;</span> --field-selector <span style="color:#b8860b">type</span><span style="color:#666">=</span>kubernetes.io/service-account-token -o name<span style="color:#a2f;font-weight:bold">)</span>; <span style="color:#a2f;font-weight:bold">do</span>
        kubectl get <span style="color:#b8860b">$token</span> --namespace <span style="color:#b44">&#34;</span><span style="color:#b8860b">$namespace</span><span style="color:#b44">&#34;</span> -o yaml | <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>          /bin/sed <span style="color:#b44">&#34;s/\(ca.crt:\).*/\1 </span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">base64_encoded_ca</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/&#34;</span> | <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>          kubectl apply -f -
    <span style="color:#a2f;font-weight:bold">done</span>
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div></li>
</ol>
<!--
1. Restart all pods using in-cluster configs (ex: kube-proxy, coredns, etc) so they can use the updated certificate authority data from *ServiceAccount* secrets.

   * Make sure coredns, kube-proxy and other pods using in-cluster configs are working as expected.

1. Append the both old and new CA to the file against `-client-ca-file` and `-kubelet-certificate-authority` flag in the `kube-apiserver` configuration.

1. Append the both old and new CA to the file against `-client-ca-file` flag in the `kube-scheduler` configuration.
-->
<ol start="4">
<li>
<p>重启所有使用集群内配置的 Pods（例如：<code>kube-proxy</code>、<code>coredns</code> 等），以便这些 Pod 能够使用
来自 <em>ServiceAccount</em> Secret 中的、已更新的证书机构数据。</p>
<ul>
<li>确保 <code>coredns</code>、<code>kube-proxy</code> 和其他使用集群内配置的 Pod 都正按预期方式工作。</li>
</ul>
</li>
<li>
<p>将老的和新的 CA 都追加到 <code>kube-apiserver</code> 配置的 <code>--client-ca-file</code> 和 <code>--kubelet-certificate-authority</code> 标志所指的文件。</p>
</li>
<li>
<p>将老的和新的 CA 都追加到 <code>kube-scheduler</code> 配置的 <code>--client-ca-file</code> 标志所指的文件。</p>
</li>
</ol>
<!--
1. Update certificates for user accounts by replacing the content of `client-certificate-data` and `client-key-data` respectively.

   For information about creating certificates for individual user accounts, see
   [Configure certificates for user accounts](/docs/setup/best-practices/certificates/#configure-certificates-for-user-accounts).

   Additionally, update the `certificate-authority-data` section in the kubeconfig files,
   respectively with Base64-encoded old and new certificate authority data
-->
<ol start="7">
<li>
<p>通过替换 <code>client-certificate-data</code> 和 <code>client-key-data</code>
中的内容，更新用户账号的证书。</p>
<p>有关为独立用户账号创建证书的更多信息，可参阅
<a href="/zh/docs/setup/best-practices/certificates/#configure-certificates-for-user-accounts">为用户帐号配置证书</a>。</p>
<p>另外，还要更新 kubeconfig 文件中的 <code>certificate-authority-data</code>
节，使之包含 Base64 编码的老的和新的证书机构数据。</p>
</li>
</ol>
<!--
1. Follow below steps in a rolling fashion.

   1. Restart any other *[aggregated api servers](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)*
      or *webhook handlers* to trust the new CA certificates.

   1. Restart the kubelet by update the file against `clientCAFile` in kubelet configuration and
      `certificate-authority-data` in kubelet.conf to use both the old and new CA on all nodes.

      If your kubelet is not using client certificate rotation update `client-certificate-data` and
      `client-key-data` in kubelet.conf on all nodes along with the kubelet client certificate file
      usually found in `/var/lib/kubelet/pki`.
-->
<ol start="8">
<li>
<p>遵循下列步骤执行滚动更新</p>
<ol>
<li>
<p>重新启动所有其他 <em><a href="/zh/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">被聚合的 API 服务器</a></em>
或者 <em>Webhook 处理程序</em>，使之信任新的 CA 证书。</p>
</li>
<li>
<p>在所有节点上更新 kubelet 配置中的 <code>clientCAFile</code> 所指文件以及 kubelet.conf 中的
<code>certificate-authority-data</code> 并重启 kubelet 以同时使用老的和新的 CA 证书。</p>
<p>如果你的 kubelet 并未使用客户端证书轮换，则在所有节点上更新 kubelet.conf 中
<code>client-certificate-data</code> 和 <code>client-key-data</code> 以及 kubelet
客户端证书文件（通常位于 <code>/var/lib/kubelet/pki</code> 目录下）</p>
</li>
</ol>
<!--
1. Restart API servers with the certificates (`apiserver.crt`, `apiserver-kubelet-client.crt` and
   `front-proxy-client.crt`) signed by new CA.
   You can use the existing private keys or new private keys.
   If you changed the private keys then update these in the Kubernetes certificates directory as well.
-->
<ol start="3">
<li>
<p>使用用新的 CA 签名的证书
（<code>apiserver.crt</code>、<code>apiserver-kubelet-client.crt</code> 和 <code>front-proxy-client.crt</code>）
来重启 API 服务器。
你可以使用现有的私钥，也可以使用新的私钥。
如果你改变了私钥，则要将更新的私钥也放到 Kubernetes 证书目录下。</p>
<p>由于 Pod 既信任老的 CA 也信任新的 CA，Pod 中的客户端会经历短暂的连接断开状态，
之后再连接到使用新的 CA 所签名的证书的新的 API 服务器。</p>
<!--
* Restart Scheduler to use the new CAs.
* Make sure control plane components logs no TLS errors.
-->
<ul>
<li>重启调度器以使用新的 CA 证书。</li>
<li>确保控制面组件的日志中没有 TLS 相关的错误信息。</li>
</ul>
<!--
To generate certificates and private keys for your cluster using the `openssl`
command line tool, see [Certificates (`openssl`)](/docs/tasks/administer-cluster/certificates/#openssl).
You can also use [`cfssl`](/docs/tasks/administer-cluster/certificates/#cfssl).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 要使用 <code>openssl</code> 命令行为集群生成新的证书和私钥，可参阅
<a href="/zh/docs/tasks/administer-cluster/certificates/#openssl">证书（<code>openssl</code>）</a>。
你也可以使用<a href="/zh/docs/tasks/administer-cluster/certificates/#cfssl"><code>cfssl</code></a>.</div>
</blockquote>
</li>
</ol>
<!--
1. Annotate any Daemonsets and Deployments to trigger pod replacement in a safer rolling fashion.

   Example:
-->
<ol start="4">
<li>
<p>为 Daemonset 和 Deployment 添加注解，从而触发较安全的滚动更新，替换 Pod。</p>
<p>示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f;font-weight:bold">for</span> namespace in <span style="color:#a2f;font-weight:bold">$(</span>kubectl get namespace -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.items[*].metadata.name}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>; <span style="color:#a2f;font-weight:bold">do</span>
    <span style="color:#a2f;font-weight:bold">for</span> name in <span style="color:#a2f;font-weight:bold">$(</span>kubectl get deployments -n <span style="color:#b8860b">$namespace</span> -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.items[*].metadata.name}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>; <span style="color:#a2f;font-weight:bold">do</span>
        kubectl patch deployment -n <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">namespace</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">name</span><span style="color:#b68;font-weight:bold">}</span> -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;template&#34;:{&#34;metadata&#34;:{&#34;annotations&#34;:{&#34;ca-rotation&#34;: &#34;1&#34;}}}}}&#39;</span>;
    <span style="color:#a2f;font-weight:bold">done</span>
    <span style="color:#a2f;font-weight:bold">for</span> name in <span style="color:#a2f;font-weight:bold">$(</span>kubectl get daemonset -n <span style="color:#b8860b">$namespace</span> -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.items[*].metadata.name}&#39;</span><span style="color:#a2f;font-weight:bold">)</span>; <span style="color:#a2f;font-weight:bold">do</span>
        kubectl patch daemonset -n <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">namespace</span><span style="color:#b68;font-weight:bold">}</span> <span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">name</span><span style="color:#b68;font-weight:bold">}</span> -p <span style="color:#b44">&#39;{&#34;spec&#34;:{&#34;template&#34;:{&#34;metadata&#34;:{&#34;annotations&#34;:{&#34;ca-rotation&#34;: &#34;1&#34;}}}}}&#39;</span>;
    <span style="color:#a2f;font-weight:bold">done</span>
<span style="color:#a2f;font-weight:bold">done</span>
</code></pre></div><!--
To limit the number of concurrent disruptions that your application experiences,
see [configure pod disruption budget](/docs/tasks/run-application/configure-pdb/).
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 要限制应用可能受到的并发干扰数量，可以参阅
<a href="/zh/docs/tasks/run-application/configure-pdb/">配置 Pod 干扰预算</a>.</div>
</blockquote>
</li>
</ol>
</li>
</ol>
<!--
1. If your cluster is using bootstrap tokens to join nodes, update the ConfigMap `cluster-info` in the `kube-public` namespace with new CA.
-->
<ol start="9">
<li>
<p>如果你的集群使用启动引导令牌来添加节点，则需要更新 <code>kube-public</code> 名字空间下的
ConfigMap <code>cluster-info</code>，使之包含新的 CA 证书。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">base64_encoded_ca</span><span style="color:#666">=</span><span style="color:#b44">&#34;</span><span style="color:#a2f;font-weight:bold">$(</span>base64 -w0 /etc/kubernetes/pki/ca.crt<span style="color:#a2f;font-weight:bold">)</span><span style="color:#b44">&#34;</span>

kubectl get cm/cluster-info --namespace kube-public -o yaml | <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    /bin/sed <span style="color:#b44">&#34;s/\(certificate-authority-data:\).*/\1 </span><span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">base64_encoded_ca</span><span style="color:#b68;font-weight:bold">}</span><span style="color:#b44">/&#34;</span> | <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    kubectl apply -f -
</code></pre></div></li>
</ol>
<!--
1. Verify the cluster functionality.

   1. Validate the logs from control plane components, along with the kubelet and the
      kube-proxy are not throwing any tls errors, see
      [looking at the logs](/docs/tasks/debug-application-cluster/debug-cluster/#looking-at-logs).

   1. Validate logs from any aggregated api servers and pods using in-cluster config.
-->
<ol start="10">
<li>
<p>验证集群的功能正常</p>
<ol>
<li>
<p>验证控制面组件的日志，以及 <code>kubelet</code> 和 <code>kube-proxy</code> 的日志，确保其中没有
抛出 TLS 错误，参阅
<a href="/zh/docs/tasks/debug-application-cluster/debug-cluster/#looking-at-logs">查看日志</a>.</p>
</li>
<li>
<p>验证被聚合的 API 服务器的日志，以及所有使用集群内配置的 Pod 的日志。</p>
</li>
</ol>
</li>
</ol>
<!--
1. Once the cluster functionality is successfully verified:

   1. Update all service account tokens to include new CA certificate only.

      * All pods using an in-cluster kubeconfig will eventually need to be restarted to pick up the new SA secret for the old CA to be completely untrusted.

   1. Restart the control plane components by removing the old CA from the kubeconfig files and the files against `--client-ca-file`, `--root-ca-file` flags resp.

   1. Restart kubelet by removing the old CA from file against the `clientCAFile` flag and kubelet kubeconfig file.
-->
<ol start="11">
<li>
<p>完成集群功能的检查之后：</p>
<ol>
<li>
<p>更新所有的服务账号令牌，使之仅包含新的 CA 证书。</p>
<ul>
<li>使用集群内 kubeconfig 的 Pod 最终也需要被重启，以获得新的服务账号 Secret
数据，进而不再信任老的 CA 证书。</li>
</ul>
</li>
<li>
<p>从 kubeconfig 文件和 <code>--client-ca-file</code> 以及 <code>--root-ca-file</code> 标志所指向的文件
中去除老的 CA 数据，之后重启控制面组件。</p>
</li>
<li>
<p>重启 kubelet，移除 <code>clientCAFile</code> 标志所指向的文件以及 kubelet kubeconfig 文件中
的老的 CA 数据。</p>
</li>
</ol>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9a87de8ee8332cb487f34a05debb1125">12.3 - 管理集群中的 TLS 认证</h1>
    
	<!--
title: Manage TLS Certificates in a Cluster
content_type: task
reviewers:
- mikedanese
- beacham
- liggit
-->
<!-- overview -->
<!--
Kubernetes provides a `certificates.k8s.io` API, which lets you provision TLS
certificates signed by a Certificate Authority (CA) that you control. These CA
and certificates can be used by your workloads to establish trust.

`certificates.k8s.io` API uses a protocol that is similar to the [ACME
draft](https://github.com/ietf-wg-acme/acme/).
-->
<p>Kubernetes 提供 <code>certificates.k8s.io</code> API，可让你配置由你控制的证书颁发机构（CA）
签名的 TLS 证书。 你的工作负载可以使用这些 CA 和证书来建立信任。</p>
<p><code>certificates.k8s.io</code> API使用的协议类似于
<a href="https://github.com/ietf-wg-acme/acme/">ACME 草案</a>。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
Certificates created using the `certificates.k8s.io` API are signed by a
dedicated CA. It is possible to configure your cluster to use the cluster root
CA for this purpose, but you should never rely on this. Do not assume that
these certificates will validate against the cluster root CA.
-->
<p>使用 <code>certificates.k8s.io</code> API 创建的证书由指定 CA 颁发。将集群配置为使用集群根目录
CA 可以达到这个目的，但是你永远不要依赖这一假定。不要以为
这些证书将针对群根目录 CA 进行验证。</div>
</blockquote>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!-- steps -->
<!--
## Trusting TLS in a Cluster

Trusting the custom CA from an application running as a pod usually requires
some extra application configuration. You will need to add the CA certificate
bundle to the list of CA certificates that the TLS client or server trusts. For
example, you would do this with a golang TLS config by parsing the certificate
chain and adding the parsed certificates to the `RootCAs` field in the
[`tls.Config`](https://godoc.org/crypto/tls#Config) struct.

You can distribute the CA certificate as a
[ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap) that your
pods have access to use.
-->
<h2 id="集群中的-tls-信任">集群中的 TLS 信任</h2>
<p>信任 Pod 中运行的应用程序所提供的 CA 通常需要一些额外的应用程序配置。
你需要将 CA 证书包添加到 TLS 客户端或服务器信任的 CA 证书列表中。
例如，你可以使用 Golang TLS 配置通过解析证书链并将解析的证书添加到
<a href="https://godoc.org/crypto/tls#Config"><code>tls.Config</code></a> 结构中的 <code>RootCAs</code>
字段中。</p>
<p>你可以用你的应用能够访问到的
<a href="/zh/docs/tasks/configure-pod-container/configure-pod-configmap">ConfigMap</a>
的形式来发布 CA 证书。</p>
<!--
## Requesting a Certificate

The following section demonstrates how to create a TLS certificate for a
Kubernetes service accessed through DNS.

<blockquote class="note callout">
  <div><strong>说明：</strong> This tutorial uses CFSSL: Cloudflare's PKI and TLS toolkit <a href="https://blog.cloudflare.com/introducing-cfssl/">click here</a> to know more.</div>
</blockquote>
-->
<h2 id="请求证书">请求证书</h2>
<p>以下部分演示如何为通过 DNS 访问的 Kubernetes 服务创建 TLS 证书。</p>
<blockquote class="note callout">
  <div><strong>说明：</strong> 本教程使用 CFSSL：Cloudflare's PKI 和 TLS 工具包
<a href="https://blog.cloudflare.com/introducing-cfssl/">点击此处</a>了解更多信息。</div>
</blockquote>
<!--
## Download and install CFSSL

The cfssl tools used in this example can be downloaded at
[https://github.com/cloudflare/cfssl/releases](https://github.com/cloudflare/cfssl/releases).
-->
<h2 id="下载并安装-cfssl">下载并安装 CFSSL</h2>
<p>本例中使用的 cfssl 工具可以在 <a href="https://github.com/cloudflare/cfssl/releases">github.com/cloudflare/cfssl/releases</a> 下载。</p>
<!--
## Create a Certificate Signing Request

Generate a private key and certificate signing request (or CSR) by running
the following command:
-->
<h2 id="创建证书签名请求">创建证书签名请求</h2>
<p>通过运行以下命令生成私钥和证书签名请求（或 CSR）:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF | cfssl genkey - | cfssljson -bare server
</span><span style="color:#b44">{
</span><span style="color:#b44">  &#34;hosts&#34;: [
</span><span style="color:#b44">    &#34;my-svc.my-namespace.svc.cluster.local&#34;,
</span><span style="color:#b44">    &#34;my-pod.my-namespace.pod.cluster.local&#34;,
</span><span style="color:#b44">    &#34;192.0.2.24&#34;,
</span><span style="color:#b44">    &#34;10.0.34.2&#34;
</span><span style="color:#b44">  ],
</span><span style="color:#b44">  &#34;CN&#34;: &#34;system:node:my-pod.my-namespace.pod.cluster.local&#34;,
</span><span style="color:#b44">  &#34;key&#34;: {
</span><span style="color:#b44">    &#34;algo&#34;: &#34;ecdsa&#34;,
</span><span style="color:#b44">    &#34;size&#34;: 256
</span><span style="color:#b44">  },
</span><span style="color:#b44">  &#34;names&#34;: [
</span><span style="color:#b44">    {
</span><span style="color:#b44">      &#34;O&#34;: &#34;system:nodes&#34;
</span><span style="color:#b44">    }
</span><span style="color:#b44">  ]
</span><span style="color:#b44">}
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Where `192.0.2.24` is the service's cluster IP,
`my-svc.my-namespace.svc.cluster.local` is the service's DNS name,
`10.0.34.2` is the pod's IP and `my-pod.my-namespace.pod.cluster.local`
is the pod's DNS name. You should see the following output:
-->
<p>其中 <code>192.0.2.24</code> 是服务的集群 IP，<code>my-svc.my-namespace.svc.cluster.local</code>
是服务的 DNS 名称，<code>10.0.34.2</code> 是 Pod 的 IP，而
<code>my-pod.my-namespace.pod.cluster.local</code> 是 Pod 的 DNS 名称。
你能看到以下的输出：</p>
<pre tabindex="0"><code>2017/03/21 06:48:17 [INFO] generate received request
2017/03/21 06:48:17 [INFO] received CSR
2017/03/21 06:48:17 [INFO] generating key: ecdsa-256
2017/03/21 06:48:17 [INFO] encoded CSR
</code></pre><!--
This command generates two files; it generates `server.csr` containing the PEM
encoded [pkcs#10](https://tools.ietf.org/html/rfc2986) certification request,
and `server-key.pem` containing the PEM encoded key to the certificate that
is still to be created.
-->
<p>此命令生成两个文件；它生成包含 PEM 编码
<a href="https://tools.ietf.org/html/rfc2986">pkcs#10</a> 证书请求的 <code>server.csr</code>，
以及 PEM 编码密钥的 <code>server-key.pem</code>，用于待生成的证书。</p>
<!--
## Create a Certificate Signing Request object to send to the Kubernetes API

Generate a CSR yaml blob and send it to the apiserver by running the following
command:
-->
<h2 id="创建证书签名请求对象发送到-kubernetes-api">创建证书签名请求对象发送到 Kubernetes API</h2>
<p>使用以下命令创建 CSR YAML 文件，并发送到 API 服务器：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat <span style="color:#b44">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#b44">apiVersion: certificates.k8s.io/v1
</span><span style="color:#b44">kind: CertificateSigningRequest
</span><span style="color:#b44">metadata:
</span><span style="color:#b44">  name: my-svc.my-namespace
</span><span style="color:#b44">spec:
</span><span style="color:#b44">  request: $(cat server.csr | base64 | tr -d &#39;\n&#39;)
</span><span style="color:#b44">  signerName: kubernetes.io/kubelet-serving
</span><span style="color:#b44">  usages:
</span><span style="color:#b44">  - digital signature
</span><span style="color:#b44">  - key encipherment
</span><span style="color:#b44">  - server auth
</span><span style="color:#b44">EOF</span>
</code></pre></div><!--
Notice that the `server.csr` file created in step 1 is base64 encoded
and stashed in the `.spec.request` field. We are also requesting a
certificate with the "digital signature", "key encipherment", and "server
auth" key usages, signed by the `kubernetes.io/kubelet-serving` signer.
A specific `signerName` must be requested.
View documentation for [supported signer names](/docs/reference/access-authn-authz/certificate-signing-requests/#signers)
for more information.

The CSR should now be visible from the API in a Pending state. You can see
it by running:
-->
<p>请注意，在步骤 1 中创建的 <code>server.csr</code> 文件是 base64 编码并存储在
<code>.spec.request</code> 字段中的。我们还要求提供 “digital signature（数字签名）”，
“密钥加密（key encipherment）” 和 “服务器身份验证（server auth）” 密钥用途，
由 <code>kubernetes.io/kubelet-serving</code> 签名程序签名的证书。
你也可以要求使用特定的 <code>signerName</code>。更多信息可参阅
<a href="/zh/docs/reference/access-authn-authz/certificate-signing-requests/#signers">支持的签署者名称</a>。</p>
<p>在 API server 中可以看到这些 CSR 处于 Pending 状态。执行下面的命令你将可以看到：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe csr my-svc.my-namespace
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">Name:                   my-svc.my-namespace
Labels:                 &lt;none&gt;
Annotations:            &lt;none&gt;
CreationTimestamp:      Tue, 21 Mar 2017 07:03:51 -0700
Requesting User:        yourname@example.com
Status:                 Pending
Subject:
        Common Name:    my-svc.my-namespace.svc.cluster.local
        Serial Number:
Subject Alternative Names:
        DNS Names:      my-svc.my-namespace.svc.cluster.local
        IP Addresses:   192.0.2.24
                        10.0.34.2
Events: &lt;none&gt;
</code></pre><!--
## Get the Certificate Signing Request Approved

Approving the certificate signing request is either done by an automated
approval process or on a one off basis by a cluster administrator. More
information on what this involves is covered below.
-->
<h2 id="批准证书签名请求">批准证书签名请求</h2>
<p>批准证书签名请求是通过自动批准过程完成的，或由集群管理员一次性完成。
有关这方面涉及的更多信息，请参见下文。</p>
<!--
## Download the Certificate and Use It

Once the CSR is signed and approved you should see the following:
-->
<h2 id="下载证书并使用它">下载证书并使用它</h2>
<p>CSR 被签署并获得批准后，你应该看到以下内容：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get csr
</code></pre></div><pre tabindex="0"><code class="language-none" data-lang="none">NAME                  AGE       REQUESTOR               CONDITION
my-svc.my-namespace   10m       yourname@example.com    Approved,Issued
</code></pre><!--
You can download the issued certificate and save it to a `server.crt` file
by running the following:
-->
<p>你可以通过运行以下命令下载颁发的证书并将其保存到 <code>server.crt</code> 文件中：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get csr my-svc.my-namespace -o <span style="color:#b8860b">jsonpath</span><span style="color:#666">=</span><span style="color:#b44">&#39;{.status.certificate}&#39;</span> <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    | base64 --decode &gt; server.crt
</code></pre></div><!--
Now you can use `server.crt` and `server-key.pem` as the keypair to start
your HTTPS server.
-->
<p>现在你可以将 <code>server.crt</code> 和 <code>server-key.pem</code> 作为键值对来启动 HTTPS 服务器。</p>
<!--
## Approving Certificate Signing Requests

A Kubernetes administrator (with appropriate permissions) can manually approve
(or deny) Certificate Signing Requests by using the `kubectl certificate
approve` and `kubectl certificate deny` commands. However if you intend
to make heavy usage of this API, you might consider writing an automated
certificates controller.
-->
<h2 id="批准证书签名请求-1">批准证书签名请求</h2>
<p>Kubernetes 管理员（具有适当权限）可以使用 <code>kubectl certificate approve</code> 和
<code>kubectl certificate deny</code> 命令手动批准（或拒绝）证书签名请求。
但是，如果你打算大量使用此 API，则可以考虑编写自动化的证书控制器。</p>
<!--
Whether a machine or a human using kubectl as above, the role of the approver is
to verify that the CSR satisfies two requirements:
-->
<p>无论上述机器或人使用 kubectl，批准者的作用是验证 CSR 满足如下两个要求：</p>
<!--
1. The subject of the CSR controls the private key used to sign the CSR. This
   addresses the threat of a third party masquerading as an authorized subject.
   In the above example, this step would be to verify that the pod controls the
   private key used to generate the CSR.
2. The subject of the CSR is authorized to act in the requested context. This
   addresses the threat of an undesired subject joining the cluster. In the
   above example, this step would be to verify that the pod is allowed to
   participate in the requested service.
-->
<ol>
<li>CSR 的 subject 控制用于签署 CSR 的私钥。这解决了伪装成授权主体的第三方的威胁。
在上述示例中，此步骤将验证该 Pod 控制了用于生成 CSR 的私钥。</li>
<li>CSR 的 subject 被授权在请求的上下文中执行。
这点用于处理不期望的主体被加入集群的威胁。
在上述示例中，此步骤将是验证该 Pod 是否被允许加入到所请求的服务中。</li>
</ol>
<!--
If and only if these two requirements are met, the approver should approve
the CSR and otherwise should deny the CSR.
-->
<p>当且仅当满足这两个要求时，审批者应该批准 CSR，否则拒绝 CSR。</p>
<!--
## A Word of Warning on the Approval Permission

The ability to approve CSRs decides who trusts who within the cluster. This
includes who the Kubernetes API trusts. The ability to approve CSRs should
not be granted broadly or lightly. The requirements of the challenge
noted in the previous section and the repercussions of issuing a specific
certificate should be fully understood before granting this permission.
-->
<h2 id="关于批准权限的警告">关于批准权限的警告</h2>
<p>批准 CSR 的能力决定了群集中的信任关系。这也包括 Kubernetes API 所信任的人。
批准 CSR 的能力不能过于广泛和轻率。
在给予本许可之前，应充分了解上一节中提到的挑战和发布特定证书的后果。</p>
<!--
## A Note to Cluster Administrators

This tutorial assumes that a signer is setup to serve the certificates API. The
Kubernetes controller manager provides a default implementation of a signer. To
enable it, pass the `--cluster-signing-cert-file` and
`--cluster-signing-key-file` parameters to the controller manager with paths to
your Certificate Authority's keypair.
-->
<h2 id="给集群管理员的一个建议">给集群管理员的一个建议</h2>
<p>本教程假设已经为 certificates API 配置了签名者。Kubernetes 控制器管理器
提供了一个签名者的默认实现。要启用它，请为控制器管理器设置
<code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数，
使之取值为你的证书机构的密钥对的路径。</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ba58efa15c6d46f10e34d799be220965">13 - 管理集群守护进程</h1>
    <div class="lead">执行 DaemonSet 管理的常见任务，例如执行滚动更新。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-bcfd795e4b59420f7db275a0482af37c">13.1 - 对 DaemonSet 执行滚动更新</h1>
    
	<!--
reviewers:
- janetkuo
title: Perform a Rolling Update on a DaemonSet
content_type: task
-->
<!-- overview -->
<!--
This page shows how to perform a rolling update on a DaemonSet.
-->
<p>本文介绍了如何对 DaemonSet 执行滚动更新。</p>
<h2 id="准备开始">准备开始</h2>
<p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>

<!-- steps -->
<!--
## DaemonSet Update Strategy

DaemonSet has two update strategy types:
-->
<h2 id="daemonset-更新策略">DaemonSet 更新策略</h2>
<p>DaemonSet 有两种更新策略：</p>
<!--
* `OnDelete`: With `OnDelete` update strategy, after you update a DaemonSet template, new
  DaemonSet pods will *only* be created when you manually delete old DaemonSet
  pods. This is the same behavior of DaemonSet in Kubernetes version 1.5 or
  before.
* `RollingUpdate`: This is the default update strategy.  
  With `RollingUpdate` update strategy, after you update a
  DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods
  will be created automatically, in a controlled fashion.
  At most one pod of the DaemonSet will be running on each node during the whole update process.
-->
<ul>
<li><code>OnDelete</code>: 使用 <code>OnDelete</code> 更新策略时，在更新 DaemonSet 模板后，只有当你手动删除老的
DaemonSet pods 之后，新的 DaemonSet Pod <em>才会</em>被自动创建。跟 Kubernetes 1.6 以前的版本类似。</li>
<li><code>RollingUpdate</code>: 这是默认的更新策略。使用 <code>RollingUpdate</code> 更新策略时，在更新 DaemonSet 模板后，
老的 DaemonSet pods 将被终止，并且将以受控方式自动创建新的 DaemonSet pods。
更新期间，最多只能有 DaemonSet 的一个 Pod 运行于每个节点上。</li>
</ul>
<!--
## Performing a Rolling Update

To enable the rolling update feature of a DaemonSet, you must set its
`.spec.updateStrategy.type` to `RollingUpdate`.
-->
<h2 id="执行滚动更新">执行滚动更新</h2>
<p>要启用 DaemonSet 的滚动更新功能，必须设置 <code>.spec.updateStrategy.type</code> 为 <code>RollingUpdate</code>。</p>
<!--
You may want to set 
[`.spec.updateStrategy.rollingUpdate.maxUnavailable`](/docs/concepts/workloads/controllers/deployment/#max-unavailable) 
(default to 1),
[`.spec.minReadySeconds`](/docs/concepts/workloads/controllers/deployment/#min-ready-seconds) 
(default to 0) and 
[`.spec.maxSurge`](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-surge) 
(a beta feature and defaults to 25%) as well.
-->
<p>你可能想设置
<a href="/zh/docs/concepts/workloads/controllers/deployment/#max-unavailable"><code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code></a> (默认为 1)，
<a href="/zh/docs/concepts/workloads/controllers/deployment/#min-ready-seconds"><code>.spec.minReadySeconds</code></a> (默认为 0) 和
<a href="/zh/docs/concepts/workloads/controllers/deployment/#max-surge"><code>.spec.maxSurge</code></a> (一种 Beta 阶段的特性，默认为 25%)</p>
<!--
### Creating a DaemonSet with `RollingUpdate` update strategy

This YAML file specifies a DaemonSet with an update strategy as 'RollingUpdate'
-->
<h3 id="创建带有-rollingupdate-更新策略的-daemonset">创建带有 <code>RollingUpdate</code> 更新策略的 DaemonSet</h3>
<p>下面的 YAML 包含一个 DaemonSet，其更新策略为 'RollingUpdate'：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/fluentd-daemonset.yaml" download="controllers/fluentd-daemonset.yaml"><code>controllers/fluentd-daemonset.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-fluentd-daemonset-yaml')" title="Copy controllers/fluentd-daemonset.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-fluentd-daemonset-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>fluentd-logging<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">updateStrategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rollingUpdate</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this toleration is to have the daemonset runnable on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># remove it if your masters can&#39;t run pods</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>quay.io/fluentd_elasticsearch/fluentd:v2.5.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
After verifying the update strategy of the DaemonSet manifest, create the DaemonSet:
-->
<p>检查了 DaemonSet 清单中更新策略的设置之后，创建 DaemonSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml
</code></pre></div><!--
Alternatively, use `kubectl apply` to create the same DaemonSet if you plan to
update the DaemonSet with `kubectl apply`.
-->
<p>另一种方式是如果你希望使用 <code>kubectl apply</code> 来更新 DaemonSet 的话，也可以
使用 <code>kubectl apply</code> 来创建 DaemonSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml
</code></pre></div><!--
### Checking DaemonSet `RollingUpdate` update strategy

Check the update strategy of your DaemonSet, and make sure it's set to
`RollingUpdate`:
-->
<h3 id="检查-daemonset-的滚动更新策略">检查 DaemonSet 的滚动更新策略</h3>
<p>首先，检查 DaemonSet 的更新策略，确保已经将其设置为 <code>RollingUpdate</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get ds/fluentd-elasticsearch -o go-template<span style="color:#666">=</span><span style="color:#b44">&#39;{{.spec.updateStrategy.type}}{{&#34;\n&#34;}}&#39;</span> -n kube-system
</code></pre></div><!--
If you haven't created the DaemonSet in the system, check your DaemonSet
manifest with the following command instead:
-->
<p>如果还没在系统中创建 DaemonSet，请使用以下命令检查 DaemonSet 的清单：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml --dry-run<span style="color:#666">=</span>client -o go-template<span style="color:#666">=</span><span style="color:#b44">&#39;{{.spec.updateStrategy.type}}{{&#34;\n&#34;}}&#39;</span>
</code></pre></div><!--
The output from both commands should be:
-->
<p>两个命令的输出都应该为：</p>
<pre tabindex="0"><code>RollingUpdate
</code></pre><!--
If the output isn't `RollingUpdate`, go back and modify the DaemonSet object or
manifest accordingly.
-->
<p>如果输出不是 <code>RollingUpdate</code>，请返回并相应地修改 DaemonSet 对象或者清单。</p>
<!--
### Updating a DaemonSet template

Any updates to a `RollingUpdate` DaemonSet `.spec.template` will trigger a rolling
update. Let's update the DaemonSet by applying a new YAML file. This can be done with several different `kubectl` commands.
-->
<h3 id="更新-daemonset-模板">更新 DaemonSet 模板</h3>
<p>对 <code>RollingUpdate</code> DaemonSet 的 <code>.spec.template</code> 的任何更新都将触发滚动更新。
这可以通过几个不同的 <code>kubectl</code> 命令来完成。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/controllers/fluentd-daemonset-update.yaml" download="controllers/fluentd-daemonset-update.yaml"><code>controllers/fluentd-daemonset-update.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('controllers-fluentd-daemonset-update-yaml')" title="Copy controllers/fluentd-daemonset-update.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="controllers-fluentd-daemonset-update-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apps/v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>DaemonSet<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>fluentd-logging<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchLabels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">updateStrategy</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>RollingUpdate<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">rollingUpdate</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">maxUnavailable</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># this toleration is to have the daemonset runnable on master nodes</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># remove it if your masters can&#39;t run pods</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>node-role.kubernetes.io/master<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>fluentd-elasticsearch<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>quay.io/fluentd_elasticsearch/fluentd:v2.5.2<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span>100m<span style="color:#bbb">
</span><span style="color:#bbb">            </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>200Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">        </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">readOnly</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">terminationGracePeriodSeconds</span>:<span style="color:#bbb"> </span><span style="color:#666">30</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlog<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/log<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>varlibdockercontainers<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hostPath</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">path</span>:<span style="color:#bbb"> </span>/var/lib/docker/containers<span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
#### Declarative commands

If you update DaemonSets using
[configuration files](/docs/tasks/manage-kubernetes-objects/declarative-config/),
use `kubectl apply`:
-->
<h4 id="声明式命令">声明式命令</h4>
<p>如果你使用
<a href="/zh/docs/tasks/manage-kubernetes-objects/declarative-config/">配置文件</a>
来更新 DaemonSet，请使用 <code>kubectl apply</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset-update.yaml
</code></pre></div><!--
#### Imperative commands

If you update DaemonSets using
[imperative commands](/docs/concepts/overview/object-management-kubectl/imperative-command/),
use `kubectl edit`:
-->
<h4 id="指令式命令">指令式命令</h4>
<p>如果你使用
<a href="/zh/docs/tasks/manage-kubernetes-objects/imperative-command/">指令式命令</a>
来更新 DaemonSets，请使用<code>kubectl edit</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl edit ds/fluentd-elasticsearch -n kube-system
</code></pre></div><!--
##### Updating only the container image

If you only need to update the container image in the DaemonSet template, i.e.
`.spec.template.spec.containers[*].image`, use `kubectl set image`:
--->
<h5 id="只更新容器镜像">只更新容器镜像</h5>
<p>如果你只需要更新 DaemonSet 模板里的容器镜像，比如，<code>.spec.template.spec.containers[*].image</code>,
请使用 <code>kubectl set image</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">set</span> image ds/fluentd-elasticsearch fluentd-elasticsearch<span style="color:#666">=</span>quay.io/fluentd_elasticsearch/fluentd:v2.6.0 -n kube-system
</code></pre></div><!--
### Step 4: Watching the rolling update status

Finally, watch the rollout status of the latest DaemonSet rolling update:
-->
<h3 id="监视滚动更新状态">监视滚动更新状态</h3>
<p>最后，观察 DaemonSet 最新滚动更新的进度：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status ds/fluentd-elasticsearch -n kube-system
</code></pre></div><!--
When the rollout is complete, the output is similar to this:
-->
<p>当滚动更新完成时，输出结果如下：</p>
<pre tabindex="0"><code>daemonset &quot;fluentd-elasticsearch&quot; successfully rolled out
</code></pre><!--
## Troubleshooting

### DaemonSet rolling update is stuck
-->
<h2 id="故障排查">故障排查</h2>
<h3 id="daemonset-滚动更新卡住">DaemonSet 滚动更新卡住</h3>
<!--
Sometimes, a DaemonSet rolling update may be stuck. Here are some possible
causes:

#### Some nodes run out of resources
-->
<p>有时，DaemonSet 滚动更新可能卡住，以下是一些可能的原因：</p>
<h4 id="一些节点可用资源耗尽">一些节点可用资源耗尽</h4>
<!--
The rollout is stuck because new DaemonSet pods can't be scheduled on at least one
node. This is possible when the node is
[running out of resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).

When this happens, find the nodes that don't have the DaemonSet pods scheduled on
by comparing the output of `kubectl get nodes` and the output of:
-->
<p>DaemonSet 滚动更新可能会卡住，其 Pod 至少在某个节点上无法调度运行。
当节点上<a href="/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/">可用资源耗尽</a>时，
这是可能的。</p>
<p>发生这种情况时，通过对 <code>kubectl get nodes</code> 和下面命令行的输出作比较，
找出没有调度部署 DaemonSet Pods 的节点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods -l <span style="color:#b8860b">name</span><span style="color:#666">=</span>fluentd-elasticsearch -o wide -n kube-system
</code></pre></div><!--
Once you've found those nodes, delete some non-DaemonSet pods from the node to
make room for new DaemonSet pods.
-->
<p>一旦找到这些节点，从节点上删除一些非 DaemonSet Pod，为新的 DaemonSet Pod 腾出空间。</p>
<!--
This will cause service disruption when deleted pods are not controlled by any controllers or pods are not
replicated. This does not respect [PodDisruptionBudget](/docs/tasks/run-application/configure-pdb/)
either.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 当所删除的 Pod 不受任何控制器管理，也不是多副本的 Pod时，上述操作将导致服务中断。
同时，上述操作也不会考虑
<a href="/zh/docs/tasks/run-application/configure-pdb/">PodDisruptionBudget</a>
所施加的约束。</div>
</blockquote>
<!--
#### Broken rollout

If the recent DaemonSet template update is broken, for example, the container is
crash looping, or the container image doesn't exist (often due to a typo),
DaemonSet rollout won't progress.
-->
<h4 id="不完整的滚动更新">不完整的滚动更新</h4>
<p>如果最近的 DaemonSet 模板更新被破坏了，比如，容器处于崩溃循环状态或者容器镜像不存在
（通常由于拼写错误），就会发生 DaemonSet 滚动更新中断。</p>
<!--
To fix this, update the DaemonSet template again. New rollout won't be
blocked by previous unhealthy rollouts.
-->
<p>要解决此问题，需再次更新 DaemonSet 模板。新的滚动更新不会被以前的不健康的滚动更新阻止。</p>
<!--
#### Clock skew

If `.spec.minReadySeconds` is specified in the DaemonSet, clock skew between
master and nodes will make DaemonSet unable to detect the right rollout
progress.
-->
<h4 id="时钟偏差">时钟偏差</h4>
<p>如果在 DaemonSet 中指定了 <code>.spec.minReadySeconds</code>，主控节点和工作节点之间的时钟偏差会使
DaemonSet 无法检测到正确的滚动更新进度。</p>
<!--
## Clean up

Delete DaemonSet from a namespace :
-->
<h2 id="清理">清理</h2>
<p>从名字空间中删除 DaemonSet：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete ds fluentd-elasticsearch -n kube-system
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* See [Performing a rollback on a DaemonSet](/docs/tasks/manage-daemon/rollback-daemon-set/)
* See [Creating a DaemonSet to adopt existing DaemonSet pods](/docs/concepts/workloads/controllers/daemonset/)
-->
<ul>
<li>查看<a href="/zh/docs/tasks/manage-daemon/rollback-daemon-set/">在 DaemonSet 上执行回滚</a></li>
<li>查看<a href="/zh/docs/concepts/workloads/controllers/daemonset/">创建 DaemonSet 以收养现有 DaemonSet Pod</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f1bf7e426f482a85e1a417d1fd9ea7b7">13.2 - 对 DaemonSet 执行回滚</h1>
    
	<!--
reviewers:
- janetkuo
title: Perform a Rollback on a DaemonSet
content_type: task
weight: 20
min-kubernetes-server-version: 1.7
-->
<!-- overview -->
<!--
This page shows how to perform a rollback on a <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a>.
-->
<p>本文展示了如何对 <a class='glossary-tooltip' title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/workloads/controllers/daemonset/' target='_blank' aria-label='DaemonSet'>DaemonSet</a> 执行回滚。</p>
<h2 id="准备开始">准备开始</h2>
<p><p>你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。
如果你还没有集群，你可以通过 <a href="/zh/docs/tasks/tools/#minikube">Minikube</a> 构建一
个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：</p>
<!--
You need to have a Kubernetes cluster, and the kubectl command-line tool must
be configured to communicate with your cluster. If you do not already have a
cluster, you can create one by using
[minikube](/docs/tasks/tools/#minikube)
or you can use one of these Kubernetes playgrounds:
-->
<ul>
<li><a href="https://www.katacoda.com/courses/kubernetes/playground">Katacoda</a></li>
<li><a href="http://labs.play-with-k8s.com/">玩转 Kubernetes</a></li>
</ul>
 

您的 Kubernetes 服务器版本必须不低于版本 1.7.
 要获知版本信息，请输入 <code>kubectl version</code>.
</p>
<!--
You should already know how to [perform a rolling update on a
 DaemonSet](/docs/tasks/manage-daemon/update-daemon-set/).
-->
<p>你应该已经了解如何<a href="/zh/docs/tasks/manage-daemon/update-daemon-set/">为 DaemonSet 执行滚东更新</a>。</p>
<!-- steps -->
<!--
## Performing a rollback on a DaemonSet

### Step 1: Find the DaemonSet revision you want to roll back to

You can skip this step if you only want to roll back to the last revision.

List all revisions of a DaemonSet:
-->
<h2 id="对-daemonset-执行回滚">对 DaemonSet 执行回滚</h2>
<h3 id="步骤-1-找到想要-daemonset-回滚到的历史修订版本-revision">步骤 1：找到想要 DaemonSet 回滚到的历史修订版本（revision）</h3>
<p>如果只想回滚到最后一个版本，可以跳过这一步。</p>
<p>列出 DaemonSet 的所有版本：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> daemonset &lt;daemonset-name&gt;
</code></pre></div><!--
This returns a list of DaemonSet revisions:
-->
<p>此命令返回 DaemonSet 版本列表：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">daemonsets <span style="color:#b44">&#34;&lt;daemonset-name&gt;&#34;</span>
REVISION        CHANGE-CAUSE
<span style="color:#666">1</span>               ...
<span style="color:#666">2</span>               ...
...
</code></pre></div><!--
* Change cause is copied from DaemonSet annotation `kubernetes.io/change-cause`
  to its revisions upon creation. You may specify `-record=true` in `kubectl`
  to record the command executed in the change cause annotation.

To see the details of a specific revision:
-->
<ul>
<li>在创建时，DaemonSet 的变化原因从 <code>kubernetes.io/change-cause</code> 注解（annotation）
复制到其修订版本中。用户可以在 <code>kubectl</code> 命令中设置 <code>--record=true</code>，
将执行的命令记录在变化原因注解中。</li>
</ul>
<p>执行以下命令，来查看指定版本的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout <span style="color:#a2f">history</span> daemonset &lt;daemonset-name&gt; --revision<span style="color:#666">=</span><span style="color:#666">1</span>
</code></pre></div><!--
This returns the details of that revision:
-->
<p>该命令返回相应修订版本的详细信息：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">daemonsets <span style="color:#b44">&#34;&lt;daemonset-name&gt;&#34;</span> with revision <span style="color:#080;font-style:italic">#1</span>
Pod Template:
Labels:       <span style="color:#b8860b">foo</span><span style="color:#666">=</span>bar
Containers:
app:
 Image:       ...
 Port:        ...
 Environment: ...
 Mounts:      ...
Volumes:       ...
</code></pre></div><!--
### Step 2: Roll back to a specific revision
-->
<h3 id="步骤-2-回滚到指定版本">步骤 2：回滚到指定版本</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 在 --to-revision 中指定你从步骤 1 中获取的修订版本</span>
kubectl rollout undo daemonset &lt;daemonset-name&gt; --to-revision<span style="color:#666">=</span>&lt;revision&gt;
</code></pre></div><!--
If it succeeds, the command returns:
-->
<p>如果成功，命令会返回：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">daemonset <span style="color:#b44">&#34;&lt;daemonset-name&gt;&#34;</span> rolled back
</code></pre></div><!--
If `--to-revision` flag is not specified, kubectl picks the most recent revision.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果 <code>--to-revision</code> 参数未指定，将选中最近的版本。</div>
</blockquote>
<!--
### Step 3: Watch the progress of the DaemonSet rollback

`kubectl rollout undo daemonset` tells the server to start rolling back the
DaemonSet. The real rollback is done asynchronously inside the cluster
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='control plane'>control plane</a>.
-->
<h3 id="步骤-3-监视-daemonset-回滚进度">步骤 3：监视 DaemonSet 回滚进度</h3>
<p><code>kubectl rollout undo daemonset</code> 向服务器表明启动 DaemonSet 回滚。
真正的回滚是在集群的
<a class='glossary-tooltip' title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-control-plane' target='_blank' aria-label='控制面'>控制面</a>
异步完成的。</p>
<!--
To watch the progress of the rollback:
-->
<p>执行以下命令，来监视 DaemonSet 回滚进度：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl rollout status ds/&lt;daemonset-name&gt;
</code></pre></div><!--
When the rollback is complete, the output is similar to:
-->
<p>回滚完成时，输出形如：</p>
<pre tabindex="0"><code>daemonset &quot;&lt;daemonset-name&gt;&quot; successfully rolled out
</code></pre><!-- discussion -->
<!--
## Understanding DaemonSet revisions

In the previous `kubectl rollout history` step, you got a list of DaemonSet
revisions. Each revision is stored in a resource named ControllerRevision.

To see what is stored in each revision, find the DaemonSet revision raw
resources:
-->
<h2 id="理解-daemonset-修订版本">理解 DaemonSet 修订版本</h2>
<p>在前面的 <code>kubectl rollout history</code> 步骤中，你获得了一个修订版本列表，每个修订版本都存储在名为
<code>ControllerRevision</code> 的资源中。</p>
<p>要查看每个修订版本中保存的内容，可以找到 DaemonSet 修订版本的原生资源：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get controllerrevision -l &lt;daemonset-selector-key&gt;<span style="color:#666">=</span>&lt;daemonset-selector-value&gt;
</code></pre></div><!--
This returns a list of ControllerRevisions:
-->
<p>该命令返回 <code>ControllerRevisions</code> 列表：</p>
<pre tabindex="0"><code>NAME                               CONTROLLER                     REVISION   AGE
&lt;daemonset-name&gt;-&lt;revision-hash&gt;   DaemonSet/&lt;daemonset-name&gt;     1          1h
&lt;daemonset-name&gt;-&lt;revision-hash&gt;   DaemonSet/&lt;daemonset-name&gt;     2          1h
</code></pre><!--
Each ControllerRevision stores the annotations and template of a DaemonSet
revision.
-->
<p>每个 <code>ControllerRevision</code> 中存储了相应 DaemonSet 版本的注解和模板。</p>
<!--
`kubectl rollout undo` takes a specific ControllerRevision and replaces
DaemonSet template with the template stored in the ControllerRevision.
`kubectl rollout undo` is equivalent to updating DaemonSet template to a
previous revision through other commands, such as `kubectl edit` or `kubectl
apply`.
-->
<p><code>kubectl rollout undo</code> 选择特定的 <code>ControllerRevision</code>，并用
<code>ControllerRevision</code> 中存储的模板代替 DaemonSet 的模板。
<code>kubectl rollout undo</code> 相当于通过其他命令（如 <code>kubectl edit</code> 或 <code>kubectl apply</code>）
将 DaemonSet 模板更新至先前的版本。</p>
<!--
DaemonSet revisions only roll forward. That is to say, after a
rollback completes, the revision number (`.revision` field) of the
ControllerRevision being rolled back to will advance. For example, if you
have revision 1 and 2 in the system, and roll back from revision 2 to revision
1, the ControllerRevision with `.revision: 1` will become `.revision: 3`.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 注意 DaemonSet 修订版本只会正向变化。也就是说，回滚完成后，所回滚到的
<code>ControllerRevision</code> 版本号 (<code>.revision</code> 字段) 会增加。
例如，如果用户在系统中有版本 1 和版本 2，并从版本 2 回滚到版本 1，
带有 <code>.revision: 1</code> 的<code>ControllerRevision</code> 将变为 <code>.revision: 3</code>。</div>
</blockquote>
<!--
## Troubleshooting

* See [troubleshooting DaemonSet rolling
  update](/docs/tasks/manage-daemon/update-daemon-set/#troubleshooting).
-->
<h2 id="故障排查">故障排查</h2>
<ul>
<li>参阅 <a href="/zh/docs/tasks/manage-daemon/update-daemon-set/#troubleshooting">DaemonSet 滚动升级故障排除</a>。</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5266308e17490aeee8b018316bf47e03">14 - 安装服务目录</h1>
    <div class="lead">安装服务目录扩展 API。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-f741b6fc36e27a6f79c1c3d02a40d8f9">14.1 - 使用 Helm 安装 Service Catalog</h1>
    
	<!--
title: Install Service Catalog using Helm
reviewers:
- chenopis
content_type: task
-->
<!-- overview -->
<!--
---
title: Service Catalog
id: service-catalog
date: 2018-04-12
full_link: 
short_description: >
  An extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.

aka: 
tags:
- extension
---
-->
<!--
 An extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.
-->
<p><p>服务目录（Service Catalog）是 服务目录是一种扩展 API，它能让 Kubernetes 集群中运行的应用易于使用外部托管的的软件服务，例如云供应商提供的数据仓库服务。</p></p>
<!--
It provides a way to list, provision, and bind with external <a class='glossary-tooltip' title='由第三方供应商负责维护的一种软件产品。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-managed-service' target='_blank' aria-label='Managed Services'>Managed Services</a> from <a class='glossary-tooltip' title='由第三方提供并维护的一组托管服务的访问端点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-service-broker' target='_blank' aria-label='Service Brokers'>Service Brokers</a> without needing detailed knowledge about how those services are created or managed.
-->
<p>服务目录可以检索、供应、和绑定由 <a class='glossary-tooltip' title='由第三方提供并维护的一组托管服务的访问端点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-service-broker' target='_blank' aria-label='服务代理人（Service Brokers）'>服务代理人（Service Brokers）</a>
提供的外部<a class='glossary-tooltip' title='由第三方供应商负责维护的一种软件产品。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-managed-service' target='_blank' aria-label='托管服务（Managed Services）'>托管服务（Managed Services）</a>，
而无需知道那些服务具体是怎样创建和托管的。</p>
<!--
Use [Helm](https://helm.sh/) to install Service Catalog on your Kubernetes cluster. Up to date information on this process can be found at the [kubernetes-incubator/service-catalog](https://github.com/kubernetes-incubator/service-catalog/blob/master/docs/install.md) repo.
-->
<p>使用 <a href="https://helm.sh/">Helm</a> 在 Kubernetes 集群上安装 Service Catalog。
要获取有关此过程的最新信息，请浏览 <a href="https://github.com/kubernetes-incubator/service-catalog/blob/master/docs/install.md">kubernetes-incubator/service-catalog</a> 仓库。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* Understand the key concepts of [Service Catalog](/docs/concepts/service-catalog/).
* Service Catalog requires a Kubernetes cluster running version 1.7 or higher.
* You must have a Kubernetes cluster with cluster DNS enabled.
    * If you are using a cloud-based Kubernetes cluster or <a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>, you may already have cluster DNS enabled.
    * If you are using `hack/local-up-cluster.sh`, ensure that the `KUBE_ENABLE_CLUSTER_DNS` environment variable is set, then run the install script.
* [Install and setup kubectl](/docs/tasks/tools/) v1.7 or higher. Make sure it is configured to connect to the Kubernetes cluster.
* Install [Helm](http://helm.sh/) v2.7.0 or newer.
    * Follow the [Helm install instructions](https://github.com/kubernetes/helm/blob/master/docs/install.md).
    * If you already have an appropriate version of Helm installed, execute `helm init` to install Tiller, the server-side component of Helm.
-->
<ul>
<li>理解<a href="/zh/docs/concepts/extend-kubernetes/service-catalog/">服务目录</a> 的关键概念。</li>
<li>Service Catalog 需要 Kubernetes 集群版本在 1.7 或更高版本。</li>
<li>你必须启用 Kubernetes 集群的 DNS 功能。
<ul>
<li>如果使用基于云的 Kubernetes 集群或 <a class='glossary-tooltip' title='Minikube 是用来在本地运行 Kubernetes 的一种工具。' data-toggle='tooltip' data-placement='top' href='/docs/getting-started-guides/minikube/' target='_blank' aria-label='Minikube'>Minikube</a>，则可能已经启用了集群 DNS。</li>
<li>如果你正在使用 <code>hack/local-up-cluster.sh</code>，请确保设置了 <code>KUBE_ENABLE_CLUSTER_DNS</code> 环境变量，然后运行安装脚本。</li>
</ul>
</li>
<li><a href="/zh/docs/tasks/tools/">安装和设置 v1.7 或更高版本的 kubectl</a>，确保将其配置为连接到 Kubernetes 集群。</li>
<li>安装 v2.7.0 或更高版本的 <a href="https://helm.sh/">Helm</a>。
<ul>
<li>遵照 <a href="https://helm.sh/docs/intro/install/">Helm 安装说明</a>。</li>
<li>如果已经安装了适当版本的 Helm，请执行 <code>helm init</code> 来安装 Helm 的服务器端组件 Tiller。</li>
</ul>
</li>
</ul>
<!-- steps -->
<!--
## Add the service-catalog Helm repository

Once Helm is installed, add the *service-catalog* Helm repository to your local machine by executing the following command:
-->
<h2 id="添加-service-catalog-helm-仓库">添加 service-catalog Helm 仓库</h2>
<p>安装 Helm 后，通过执行以下命令将 <em>service-catalog</em> Helm 存储库添加到本地计算机：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">helm repo add svc-cat https://kubernetes-sigs.github.io/service-catalog
</code></pre></div><!--
Check to make sure that it installed successfully by executing the following command:
-->
<p>通过执行以下命令进行检查，以确保安装成功：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">helm search service-catalog
</code></pre></div><!--
If the installation was successful, the command should output the following:
-->
<p>如果安装成功，该命令应输出以下内容：</p>
<pre tabindex="0"><code>NAME            VERSION DESCRIPTION
svc-cat/catalog 0.0.1   service-catalog API server and controller-manag...
</code></pre><!--
## Enable RBAC

Your Kubernetes cluster must have RBAC enabled, which requires your Tiller Pod(s) to have `cluster-admin` access.

If you are using Minikube, run the `minikube start` command with the following flag:
-->
<h2 id="启用-rbac">启用 RBAC</h2>
<p>你的 Kubernetes 集群必须启用 RBAC，这需要你的 Tiller Pod 具有 <code>cluster-admin</code> 访问权限。</p>
<p>如果你使用的是 Minikube，请使用以下参数运行 <code>minikube start</code> 命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">minikube start --extra-config<span style="color:#666">=</span>apiserver.Authorization.Mode<span style="color:#666">=</span>RBAC
</code></pre></div><!--
If you are using `hack/local-up-cluster.sh`, set the `AUTHORIZATION_MODE` environment variable with the following values:
-->
<p>如果你使用 <code>hack/local-up-cluster.sh</code>，请使用以下值设置 <code>AUTHORIZATION_MODE</code> 环境变量：</p>
<pre tabindex="0"><code>AUTHORIZATION_MODE=Node,RBAC hack/local-up-cluster.sh -O
</code></pre><!--
By default, `helm init` installs the Tiller Pod into the `kube-system` namespace, with Tiller configured to use the `default` service account.
-->
<p>默认情况下，<code>helm init</code> 将 Tiller Pod 安装到 <code>kube-system</code> 命名空间，Tiller 配置为使用 <code>default</code> 服务帐户。</p>
<!--
If you used the `--tiller-namespace` or `--service-account` flags when running `helm init`, the `--serviceaccount` flag in the following command needs to be adjusted to reference the appropriate namespace and ServiceAccount name.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> 如果在运行 <code>helm init</code> 时使用了 <code>--tiller-namespace</code> 或 <code>--service-account</code> 参数，
则需要调整以下命令中的 <code>--serviceaccount</code> 参数以引用相应的名字空间和服务账号名称。</div>
</blockquote>
<!--
Configure Tiller to have `cluster-admin` access:
-->
<p>配置 Tiller 以获得 <code>cluster-admin</code> 访问权限：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create clusterrolebinding tiller-cluster-admin <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --clusterrole<span style="color:#666">=</span>cluster-admin <span style="color:#b62;font-weight:bold">\
</span><span style="color:#b62;font-weight:bold"></span>    --serviceaccount<span style="color:#666">=</span>kube-system:default
</code></pre></div><!--
## Install Service Catalog in your Kubernetes cluster

Install Service Catalog from the root of the Helm repository using the following command:
-->
<h2 id="在-kubernetes-集群中安装-service-catalog">在 Kubernetes 集群中安装 Service Catalog</h2>
<p>使用以下命令从 Helm 存储库的根目录安装 Service Catalog：</p>
<ul class="nav nav-tabs" id="helm-versions" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#helm-versions-0" role="tab" aria-controls="helm-versions-0" aria-selected="true">Helm version 3</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#helm-versions-1" role="tab" aria-controls="helm-versions-1">Helm version 2</a></li></ul>
<div class="tab-content" id="helm-versions"><div id="helm-versions-0" class="tab-pane show active" role="tabpanel" aria-labelledby="helm-versions-0">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">helm install catalog svc-cat/catalog --namespace catalog
</code></pre></div></div>
  <div id="helm-versions-1" class="tab-pane" role="tabpanel" aria-labelledby="helm-versions-1">

<p><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">helm install svc-cat/catalog --name catalog --namespace catalog
</code></pre></div></div></div>

<h2 id="接下来">接下来</h2>
<!--
* View [sample service brokers](https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers).
* Explore the [kubernetes-incubator/service-catalog](https://github.com/kubernetes-incubator/service-catalog) project.
-->
<ul>
<li>查看<a href="https://github.com/openservicebrokerapi/servicebroker/blob/mastergettingStarted.md#sample-service-brokers">示例服务代理</a>。</li>
<li>探索 <a href="https://github.com/kubernetes-incubator/service-catalog">kubernetes-incubator/service-catalog</a> 项目。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d85a30635b5c3578487b9f6f214c07ea">14.2 - 使用 SC 安装服务目录</h1>
    
	<!--
title: Install Service Catalog using SC
reviewers:
- chenopis
content_type: task
-->
<!-- overview -->
<!--
---
title: Service Catalog
id: service-catalog
date: 2018-04-12
full_link: 
short_description: >
  An extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.

aka: 
tags:
- extension
---
-->
<!--
 An extension API that enables applications running in Kubernetes clusters to easily use external managed software offerings, such as a datastore service offered by a cloud provider.
-->
<p><p>服务目录（Service Catalog）是 服务目录是一种扩展 API，它能让 Kubernetes 集群中运行的应用易于使用外部托管的的软件服务，例如云供应商提供的数据仓库服务。</p></p>
<!--
It provides a way to list, provision, and bind with external <a class='glossary-tooltip' title='由第三方供应商负责维护的一种软件产品。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-managed-service' target='_blank' aria-label='Managed Services'>Managed Services</a> from <a class='glossary-tooltip' title='由第三方提供并维护的一组托管服务的访问端点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-service-broker' target='_blank' aria-label='Service Brokers'>Service Brokers</a> without needing detailed knowledge about how those services are created or managed.
-->
<p>服务目录可以检索、供应、和绑定由 <a class='glossary-tooltip' title='由第三方提供并维护的一组托管服务的访问端点。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-service-broker' target='_blank' aria-label='服务代理人（Service Brokers）'>服务代理人（Service Brokers）</a>
提供的外部<a class='glossary-tooltip' title='由第三方供应商负责维护的一种软件产品。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/glossary/?all=true#term-managed-service' target='_blank' aria-label='托管服务（Managed Services）'>托管服务（Managed Services）</a>，
而无需知道那些服务具体是怎样创建和托管的。</p>
<!--
You can use the GCP [Service Catalog Installer](https://github.com/GoogleCloudPlatform/k8s-service-catalog#installation)
tool to easily install or uninstall Service Catalog on your Kubernetes cluster, linking it to
Google Cloud projects.

Service Catalog can work with any kind of managed service, not only Google Cloud.
-->
<p>使用 GCP <a href="https://github.com/GoogleCloudPlatform/k8s-service-catalog#installation">服务目录安装程序</a>
工具可以轻松地在 Kubernetes 集群上安装或卸载服务目录，并将其链接到 Google Cloud 项目。</p>
<p>服务目录不仅可以与 Google Cloud 一起使用，还可以与任何类型的托管服务一起使用。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* Understand the key concepts of [Service Catalog](/docs/concepts/extend-kubernetes/service-catalog/).
* Install [Go 1.6+](https://golang.org/dl/) and set the `GOPATH`.
* Install the [cfssl](https://github.com/cloudflare/cfssl) tool needed for generating SSL artifacts.
* Service Catalog requires Kubernetes version 1.7+.
* [Install and setup kubectl](/docs/tasks/tools/) so that it is configured to connect to a Kubernetes v1.7+ cluster.
* The kubectl user must be bound to the *cluster-admin* role for it to install Service Catalog. To ensure that this is true, run the following command:

        kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=<user-name>
-->
<ul>
<li>
<p>了解<a href="/zh/docs/concepts/extend-kubernetes/service-catalog/">服务目录</a>
的主要概念。</p>
</li>
<li>
<p>安装 <a href="https://golang.org/dl/">Go 1.6+</a> 以及设置 <code>GOPATH</code>。</p>
</li>
<li>
<p>安装生成 SSL 工件所需的 <a href="https://github.com/cloudflare/cfssl">cfssl</a> 工具。</p>
</li>
<li>
<p>服务目录需要 Kubernetes 1.7+ 版本。</p>
</li>
<li>
<p><a href="/zh/docs/tasks/tools/">安装和设置 kubectl</a>，
以便将其配置为连接到 Kubernetes v1.7+ 集群。</p>
</li>
<li>
<p>要安装服务目录，kubectl 用户必须绑定到 <em>cluster-admin</em> 角色。
为了确保这是正确的，请运行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create clusterrolebinding cluster-admin-binding --clusterrole<span style="color:#666">=</span>cluster-admin --user<span style="color:#666">=</span>&lt;user-name&gt;
</code></pre></div></li>
</ul>
<!-- steps -->
<!--
## Install `sc` in your local environment

The installer runs on your local computer as a CLI tool named `sc`.

Install using `go get`:
-->
<h2 id="在本地环境中安装-sc">在本地环境中安装 <code>sc</code></h2>
<p>安装程序在你的本地计算机上以 CLI 工具的形式运行，名为 <code>sc</code>。</p>
<p>使用 <code>go get</code> 安装：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">go get github.com/GoogleCloudPlatform/k8s-service-catalog/installer/cmd/sc
</code></pre></div><!--
`sc` should now be installed in your `GOPATH/bin` directory.
-->
<p>现在，<code>sc</code> 应该已经被安装在 <code>GOPATH/bin</code> 目录中了。</p>
<!--
## Install Service Catalog in your Kubernetes cluster

First, verify that all dependencies have been installed. Run:
-->
<h2 id="在-kubernetes-集群中安装服务目录">在 Kubernetes 集群中安装服务目录</h2>
<p>首先，检查是否已经安装了所有依赖项。运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sc check
</code></pre></div><!--
If the check is successful, it should return:
-->
<p>如检查通过，应输出：</p>
<pre tabindex="0"><code>Dependency check passed. You are good to go.
</code></pre><!--
Next, run the install command and specify the `storageclass` that you want to use for the backup:
-->
<p>接下来，运行安装命令并指定要用于备份的 <code>storageclass</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sc install --etcd-backup-storageclass <span style="color:#b44">&#34;standard&#34;</span>
</code></pre></div><!--
## Uninstall Service Catalog

If you would like to uninstall Service Catalog from your Kubernetes cluster using the `sc` tool, run:
-->
<h2 id="卸载服务目录">卸载服务目录</h2>
<p>如果您想使用 <code>sc</code> 工具从 Kubernetes 集群卸载服务目录，请运行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sc uninstall
</code></pre></div><h2 id="接下来">接下来</h2>
<!--
* View [sample service brokers](https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers).
* Explore the [kubernetes-sigs/service-catalog](https://github.com/kubernetes-sigs/service-catalog) project.
-->
<ul>
<li>查看<a href="https://github.com/openservicebrokerapi/servicebroker/blob/master/gettingStarted.md#sample-service-brokers">服务代理示例</a>。</li>
<li>探索 <a href="https://github.com/kubernetes-sigs/service-catalog">kubernetes-sigs/service-catalog</a> 项目。</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a701e71f3b32dae474c63ae4c596c856">15 - 网络</h1>
    <div class="lead">了解如何为你的集群配置网络。</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-2edb5b02ea1e646c333c9fe4d5f02ff1">15.1 - 使用 HostAliases 向 Pod /etc/hosts 文件添加条目</h1>
    
	<!--
reviewers:
- rickypai
- thockin
title: Adding entries to Pod /etc/hosts with HostAliases
content_type: task
weight: 60
min-kubernetes-server-version: 1.7
-->
<!-- overview -->
<!--
Adding entries to a Pod's /etc/hosts file provides Pod-level override of hostname resolution when DNS and other options are not applicable. You can add these custom entries with the HostAliases field in PodSpec.

Modification not using HostAliases is not suggested because the file is managed by Kubelet and can be overwritten on during Pod creation/restart.
-->
<p>当 DNS 配置以及其它选项不合理的时候，通过向 Pod 的 /etc/hosts 文件中添加条目，
可以在 Pod 级别覆盖对主机名的解析。你可以通过 PodSpec 的 HostAliases
字段来添加这些自定义条目。</p>
<p>建议通过使用 HostAliases 来进行修改，因为该文件由 Kubelet 管理，并且
可以在 Pod 创建/重启过程中被重写。</p>
<!-- steps -->
<!--
## Default Hosts File Content

Start an Nginx Pod which is assigned a Pod IP:
-->
<h2 id="默认-hosts-文件内容">默认 hosts 文件内容</h2>
<p>让我们从一个 Nginx Pod 开始，该 Pod 被分配一个 IP：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl run nginx --image nginx --generator<span style="color:#666">=</span>run-pod/v1
</code></pre></div><pre tabindex="0"><code>pod/nginx created
</code></pre><!--
Examine a Pod IP:
-->
<p>检查 Pod IP：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --output<span style="color:#666">=</span>wide
</code></pre></div><pre tabindex="0"><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre><!--
The hosts file content would look like this:
-->
<p>主机文件的内容如下所示：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> nginx -- cat /etc/hosts
</code></pre></div><pre tabindex="0"><code># Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.4	nginx
</code></pre><!--
By default, the `hosts` file only includes IPv4 and IPv6 boilerplates like
`localhost` and its own hostname.
-->
<p>默认情况下，hosts 文件只包含 IPv4 和 IPv6 的样板内容，像 <code>localhost</code> 和主机名称。</p>
<!--
## Adding Additional Entries with HostAliases

In addition to the default boilerplate, we can add additional entries to the
`hosts` file.
For example: to resolve `foo.local`, `bar.local` to `127.0.0.1` and `foo.remote`,
`bar.remote` to `10.1.2.3`, we can configure HostAliases for a Pod under
`.spec.hostAliases`:
-->
<h2 id="通过-hostaliases-增加额外条目">通过 HostAliases 增加额外条目</h2>
<p>除了默认的样板内容，我们可以向 hosts 文件添加额外的条目。
例如，要将 <code>foo.local</code>、<code>bar.local</code> 解析为 <code>127.0.0.1</code>，
将 <code>foo.remote</code>、 <code>bar.remote</code> 解析为 <code>10.1.2.3</code>，我们可以在
<code>.spec.hostAliases</code> 下为 Pod 配置 HostAliases。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/hostaliases-pod.yaml" download="service/networking/hostaliases-pod.yaml"><code>service/networking/hostaliases-pod.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-hostaliases-pod-yaml')" title="Copy service/networking/hostaliases-pod.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-hostaliases-pod-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hostaliases-pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">hostAliases</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">ip</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;127.0.0.1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostnames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;foo.local&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;bar.local&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">ip</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10.1.2.3&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">hostnames</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;foo.remote&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;bar.remote&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cat-hosts<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- cat<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;/etc/hosts&#34;</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
You can start a Pod with that configuration by running:
-->
<p>你可以使用以下命令用此配置启动 Pod：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl apply -f hostaliases-pod.yaml
</code></pre></div><pre tabindex="0"><code>pod/hostaliases-pod created
</code></pre><!--
Examine a Pod's details to see its IPv4 address and its status:
-->
<p>检查 Pod 详情，查看其 IPv4 地址和状态：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod --output<span style="color:#666">=</span>wide
</code></pre></div><pre tabindex="0"><code>NAME                READY     STATUS      RESTARTS   AGE       IP              NODE
hostaliases-pod     0/1       Completed   0          6s        10.200.0.5      worker0
</code></pre><!--
The `hosts` file content looks like this:
-->
<p>hosts 文件的内容看起来类似如下这样：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl logs hostaliases-pod
</code></pre></div><pre tabindex="0"><code># Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.5	hostaliases-pod

# Entries added by HostAliases.
127.0.0.1	foo.local	bar.local
10.1.2.3	foo.remote	bar.remote
</code></pre><!--
With the additional entries specified at the bottom.
-->
<p>在最下面额外添加了一些条目。</p>
<!--
## Why Does Kubelet Manage the Hosts File?

Kubelet [manages](https://github.com/kubernetes/kubernetes/issues/14633) the
`hosts` file for each container of the Pod to prevent Docker from
[modifying](https://github.com/moby/moby/issues/17190) the file after the
containers have already been started.
-->
<h2 id="为什么-kubelet-管理-hosts-文件">为什么 kubelet 管理 hosts 文件？</h2>
<p>kubelet <a href="https://github.com/kubernetes/kubernetes/issues/14633">管理</a> Pod
中每个容器的 hosts 文件，避免 Docker 在容器已经启动之后去
<a href="https://github.com/moby/moby/issues/17190">修改</a> 该文件。</p>
<blockquote class="caution callout">
  <div><strong>注意：</strong> <!--
Avoid making manual changes to the hosts file inside a container.

If you make manual changes to the hosts file,
those changes are lost when the container exits.
-->
<p>请避免手工更改容器内的 hosts 文件内容。</p>
<p>如果你对 hosts 文件做了手工修改，这些修改都会在容器退出时丢失。</p>
</div>
</blockquote>


</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eebac062766222247063d6513f95c7b2">15.2 - 验证 IPv4/IPv6 双协议栈</h1>
    
	<!--
reviewers:
- lachie83
- khenidak
title: Validate IPv4/IPv6 dual-stack
content_type: task
-->
<!-- overview -->
<!--
This document shares how to validate IPv4/IPv6 dual-stack enabled Kubernetes clusters.
-->
<p>本文分享了如何验证 IPv4/IPv6 双协议栈的 Kubernetes 集群。</p>
<h2 id="准备开始">准备开始</h2>
<!--
* Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide Kubernetes nodes with routable IPv4/IPv6 network interfaces)
* Kubenet network plugin
* [Dual-stack enabled](/docs/concepts/services-networking/dual-stack/) cluster
-->
<ul>
<li>提供程序对双协议栈网络的支持 (云供应商或其他方式必须能够为 Kubernetes 节点
提供可路由的 IPv4/IPv6 网络接口)</li>
<li>一个能够支持双协议栈的
<a href="/zh/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件</a>，
（如 kubenet 或 Calico）。</li>
<li><a href="/zh/docs/concepts/services-networking/dual-stack/">启用双协议栈</a> 集群</li>
</ul>
<!-- steps -->
<!--
## Validate addressing

### Validate node addressing

Each dual-stack Node should have a single IPv4 block and a single IPv6 block allocated. Validate that IPv4/IPv6 Pod address ranges are configured by running the following command. Replace the sample node name with a valid dual-stack Node from your cluster. In this example, the Node's name is `k8s-linuxpool1-34450317-0`:
-->
<h2 id="验证寻址">验证寻址</h2>
<h3 id="验证节点寻址">验证节点寻址</h3>
<p>每个双协议栈节点应分配一个 IPv4 块和一个 IPv6 块。
通过运行以下命令来验证是否配置了 IPv4/IPv6 Pod 地址范围。
将示例节点名称替换为集群中的有效双协议栈节点。
在此示例中，节点的名称为 <code>k8s-linuxpool1-34450317-0</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template<span style="color:#666">=</span><span style="color:#b44">&#39;{{range .spec.podCIDRs}}{{printf &#34;%s\n&#34; .}}{{end}}&#39;</span>
</code></pre></div><pre tabindex="0"><code>10.244.1.0/24
a00:100::/24
</code></pre><!--
There should be one IPv4 block and one IPv6 block allocated.
-->
<p>应该分配一个 IPv4 块和一个 IPv6 块。</p>
<!--
Validate that the node has an IPv4 and IPv6 interface detected. Replace node name with a valid node from the cluster. In this example the node name is `k8s-linuxpool1-34450317-0`: 
-->
<p>验证节点是否检测到 IPv4 和 IPv6 接口。用集群中的有效节点替换节点名称。
在此示例中，节点名称为 <code>k8s-linuxpool1-34450317-0</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes k8s-linuxpool1-34450317-0 -o go-template --template<span style="color:#666">=</span><span style="color:#b44">&#39;{{range .status.addresses}}{{printf &#34;%s: %s \n&#34; .type .address}}{{end}}&#39;</span>
</code></pre></div><pre tabindex="0"><code>Hostname: k8s-linuxpool1-34450317-0
InternalIP: 10.240.0.5
InternalIP: 2001:1234:5678:9abc::5
</code></pre><!--
### Validate Pod addressing

Validate that a Pod has an IPv4 and IPv6 address assigned. Replace the Pod name with a valid Pod in your cluster. In this example the Pod name is `pod01`.
-->
<h3 id="验证-pod-寻址">验证 Pod 寻址</h3>
<p>验证 Pod 已分配了 IPv4 和 IPv6 地址。用集群中的有效 Pod 替换 Pod 名称。
在此示例中，Pod 名称为 <code>pod01</code>：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods pod01 -o go-template --template<span style="color:#666">=</span><span style="color:#b44">&#39;{{range .status.podIPs}}{{printf &#34;%s \n&#34; .ip}}{{end}}&#39;</span>
</code></pre></div><pre tabindex="0"><code>10.244.1.4
a00:100::4
</code></pre><!--
You can also validate Pod IPs using the Downward API via the `status.podIPs` fieldPath. The following snippet demonstrates how you can expose the Pod IPs via an environment variable called `MY_POD_IPS` within a container.
-->
<p>你也可以通过 <code>status.podIPs</code> 使用 Downward API 验证 Pod IP。
以下代码段演示了如何通过容器内称为 <code>MY_POD_IPS</code> 的环境变量公开 Pod 的 IP 地址。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>MY_POD_IPS<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">valueFrom</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">fieldRef</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">fieldPath</span>:<span style="color:#bbb"> </span>status.podIPs<span style="color:#bbb">
</span></code></pre></div><!--
The following command prints the value of the `MY_POD_IPS` environment variable from within a container. The value is a comma separated list that corresponds to the Pod's IPv4 and IPv6 addresses.
-->
<p>使用以下命令打印出容器内部 <code>MY_POD_IPS</code> 环境变量的值。
该值是一个逗号分隔的列表，与 Pod 的 IPv4 和 IPv6 地址相对应。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it pod01 -- <span style="color:#a2f">set</span> | grep MY_POD_IPS
</code></pre></div><pre tabindex="0"><code>MY_POD_IPS=10.244.1.4,a00:100::4
</code></pre><!--
The Pod's IP addresses will also be written to `/etc/hosts` within a container. The following command executes a cat on `/etc/hosts` on a dual stack Pod. From the output you can verify both the IPv4 and IPv6 IP address for the Pod.
-->
<p>Pod 的 IP 地址也将被写入容器内的 <code>/etc/hosts</code> 文件中。
在双栈 Pod 上执行 cat <code>/etc/hosts</code> 命令操作。
从输出结果中，你可以验证 Pod 的 IPv4 和 IPv6 地址。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl <span style="color:#a2f">exec</span> -it pod01 -- cat /etc/hosts
</code></pre></div><pre tabindex="0"><code># Kubernetes-managed hosts file.
127.0.0.1    localhost
::1    localhost ip6-localhost ip6-loopback
fe00::0    ip6-localnet
fe00::0    ip6-mcastprefix
fe00::1    ip6-allnodes
fe00::2    ip6-allrouters
10.244.1.4    pod01
a00:100::4    pod01
</code></pre><!--
## Validate Services

Create the following Service that does not explicitly define `.spec.ipFamilyPolicy`. Kubernetes will assign a cluster IP for the Service from the first configured `service-cluster-ip-range` and set the `.spec.ipFamilyPolicy` to `SingleStack`.
-->
<h2 id="验证服务">验证服务</h2>
<p>创建以下未显式定义 <code>.spec.ipFamilyPolicy</code> 的 Service。
Kubernetes 将从首个配置的 <code>service-cluster-ip-range</code> 给 Service 分配集群 IP，
并将 <code>.spec.ipFamilyPolicy</code> 设置为 <code>SingleStack</code>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-default-svc.yaml" download="service/networking/dual-stack-default-svc.yaml"><code>service/networking/dual-stack-default-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-default-svc-yaml')" title="Copy service/networking/dual-stack-default-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-default-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
Use `kubectl` to view the YAML for the Service.
-->
<p>使用 <code>kubectl</code> 查看 Service 的 YAML 定义。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc my-service -o yaml
</code></pre></div><!--
The Service has `.spec.ipFamilyPolicy` set to `SingleStack` and `.spec.clusterIP` set to an IPv4 address from the first configured range set via `--service-cluster-ip-range` flag on kube-controller-manager.
-->
<p>该 Service 通过在 kube-controller-manager 的 <code>--service-cluster-ip-range</code>
标志设置的第一个配置范围，将 <code>.spec.ipFamilyPolicy</code> 设置为 <code>SingleStack</code>，
将 <code>.spec.clusterIP</code> 设置为 IPv4 地址。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.0.217.164</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIPs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#666">10.0.217.164</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv4<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">9376</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">sessionAffinity</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>ClusterIP<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">loadBalancer</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div><!--
Create the following Service that explicitly defines `IPv6` as the first array element in `.spec.ipFamilies`. Kubernetes will assign a cluster IP for the Service from the IPv6 range configured `service-cluster-ip-range` and set the `.spec.ipFamilyPolicy` to `SingleStack`.
-->
<p>创建以下显示定义 <code>.spec.ipFamilies</code> 数组中的第一个元素为 IPv6 的 Service。
Kubernetes 将 <code>service-cluster-ip-range</code> 配置的 IPv6 地址范围给 Service 分配集群 IP，
并将 <code>.spec.ipFamilyPolicy</code> 设置为 <code>SingleStack</code>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-ipfamilies-ipv6.yaml" download="service/networking/dual-stack-ipfamilies-ipv6.yaml"><code>service/networking/dual-stack-ipfamilies-ipv6.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-ipfamilies-ipv6-yaml')" title="Copy service/networking/dual-stack-ipfamilies-ipv6.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-ipfamilies-ipv6-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv6<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!-- 
Use `kubectl` to view the YAML for the Service.
-->
<p>使用 <code>kubectl</code> 查看 Service 的 YAML 定义。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc my-service -o yaml
</code></pre></div><!-- 
The Service has `.spec.ipFamilyPolicy` set to `SingleStack` and `.spec.clusterIP` set to an IPv6 address from the IPv6 range set via `--service-cluster-ip-range` flag on kube-controller-manager.
-->
<p>该 Service 通过在 kube-controller-manager 的 <code>--service-cluster-ip-range</code>
标志设置的 IPv6 地址范围，将 <code>.spec.ipFamilyPolicy</code> 设置为 <code>SingleStack</code>，
将 <code>.spec.clusterIP</code> 设置为 IPv6 地址。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span>fd00::5118<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">clusterIPs</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- fd00::5118<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv6<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>SingleStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">sessionAffinity</span>:<span style="color:#bbb"> </span>None<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>ClusterIP<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">loadBalancer</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb">
</span></code></pre></div><!--
Create the following Service that explicitly defines `PreferDualStack` in `.spec.ipFamilyPolicy`. Kubernetes will assign both IPv4 and IPv6 addresses (as this cluster has dual-stack enabled) and select the `.spec.ClusterIP` from the list of `.spec.ClusterIPs` based on the address family of the first element in the `.spec.ipFamilies` array.
-->
<p>创建以下显式定义 <code>.spec.ipFamilyPolicy</code> 为 <code>PreferDualStack</code> 的 Service。
Kubernetes 将分配 IPv4 和 IPv6 地址（因为该集群启用了双栈），
并根据 <code>.spec.ipFamilies</code> 数组中第一个元素的地址族，
从 <code>.spec.ClusterIPs</code> 列表中选择 <code>.spec.ClusterIP</code>。</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-preferred-svc.yaml" download="service/networking/dual-stack-preferred-svc.yaml"><code>service/networking/dual-stack-preferred-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-preferred-svc-yaml')" title="Copy service/networking/dual-stack-preferred-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-preferred-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<blockquote class="note callout">
  <div><strong>说明：</strong> <!--
The `kubectl get svc` command will only show the primary IP in the `CLUSTER-IP` field.
-->
<p><code>kubectl get svc</code> 命令将仅在 <code>CLUSTER-IP</code> 字段中显示主 IP。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>MyApp

NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span style="color:#666">(</span>S<span style="color:#666">)</span>   AGE
my-service   ClusterIP   fe80:20d::d06b   &lt;none&gt;        80/TCP    9s
</code></pre></div></div>
</blockquote>
<!-- 
Validate that the Service gets cluster IPs from the IPv4 and IPv6 address blocks using `kubectl describe`. You may then validate access to the service via the IPs and ports.
-->
<p>使用 <code>kubectl describe</code> 验证服务是否从 IPv4 和 IPv6 地址块中获取了集群 IP。
然后你就可以通过 IP 和端口，验证对服务的访问。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe svc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>MyApp
</code></pre></div><pre tabindex="0"><code>Name:              my-service
Namespace:         default
Labels:            app=MyApp
Annotations:       &lt;none&gt;
Selector:          app=MyApp
Type:              ClusterIP
IP Family Policy:  PreferDualStack
IP Families:       IPv4,IPv6
IP:                10.0.216.242
IPs:               10.0.216.242,fd00::af55
Port:              &lt;unset&gt;  80/TCP
TargetPort:        9376/TCP
Endpoints:         &lt;none&gt;
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre><!--
### Create a dual-stack load balanced Service

If the cloud provider supports the provisioning of IPv6 enabled external load balancers, create the following Service with `PreferDualStack` in `.spec.ipFamilyPolicy`. `IPv6` as the first element of the `.spec.ipFamilies` array and the `type` field set to `LoadBalancer`.
-->
<h3 id="创建双协议栈负载均衡服务">创建双协议栈负载均衡服务</h3>
<p>如果云提供商支持配置启用 IPv6 的外部负载均衡器，则创建如下 Service 时将
<code>.spec.ipFamilyPolicy</code> 设置为 <code>PreferDualStack</code>, 并将 <code>spec.ipFamilies</code> 字段
的第一个元素设置为 <code>IPv6</code>，将 <code>type</code> 字段设置为 <code>LoadBalancer</code>：</p>


 













<div class="highlight">
    <div class="copy-code-icon" style="text-align:right">
    <a href="https://raw.githubusercontent.com/kubernetes/website/main/content/zh/examples/service/networking/dual-stack-prefer-ipv6-lb-svc.yaml" download="service/networking/dual-stack-prefer-ipv6-lb-svc.yaml"><code>service/networking/dual-stack-prefer-ipv6-lb-svc.yaml</code>
    </a>
    <img src="/images/copycode.svg" style="max-height:24px; cursor: pointer" onclick="copyCode('service-networking-dual-stack-prefer-ipv6-lb-svc-yaml')" title="Copy service/networking/dual-stack-prefer-ipv6-lb-svc.yaml to clipboard">
    </img>
    </div>
    <div class="includecode" id="service-networking-dual-stack-prefer-ipv6-lb-svc-yaml">
    <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>my-service<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilyPolicy</span>:<span style="color:#bbb"> </span>PreferDualStack<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ipFamilies</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- IPv6<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>LoadBalancer<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">app</span>:<span style="color:#bbb"> </span>MyApp<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">80</span><span style="color:#bbb">
</span></code></pre></div>
    </div>
</div>


<!--
Check the Service:
-->
<p>检查服务：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get svc -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>MyApp
</code></pre></div><!--
Validate that the Service receives a `CLUSTER-IP` address from the IPv6 address block along with an `EXTERNAL-IP`. You may then validate access to the service via the IP and port. 
-->
<p>验证服务是否从 IPv6 地址块中接收到 <code>CLUSTER-IP</code> 地址以及 <code>EXTERNAL-IP</code>。
然后，你可以通过 IP 和端口验证对服务的访问。</p>
<pre tabindex="0"><code>NAME         TYPE           CLUSTER-IP   EXTERNAL-IP        PORT(S)        AGE
my-service   LoadBalancer   fd00::7ebc   2603:1030:805::5   80:30790/TCP   35s
</code></pre>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f34d6e348a8e677d6c6eb155cd1a99aa">16 - 用插件扩展 kubectl</h1>
    <div class="lead">通过创建和安装 kubectl 插件扩展 kubectl。</div>
	<!--
title: Extend kubectl with plugins
reviewers:
- juanvallejo
- soltysh
description: Extend kubectl by creating and installing kubectl plugins.
content_type: task
-->
<!-- overview -->
<!--
This guide demonstrates how to install and write extensions for [kubectl](/docs/reference/kubectl/kubectl/). By thinking of core `kubectl` commands as essential building blocks for interacting with a Kubernetes cluster, a cluster administrator can think
of plugins as a means of utilizing these building blocks to create more complex behavior. Plugins extend `kubectl` with new sub-commands, allowing for new and custom features not included in the main distribution of `kubectl`.
-->
<p>本指南演示了如何为 <a href="/zh/docs/reference/kubectl/kubectl/">kubectl</a> 安装和编写扩展。
通过将核心 <code>kubectl</code> 命令看作与 Kubernetes 集群交互的基本构建块，
集群管理员可以将插件视为一种利用这些构建块创建更复杂行为的方法。
插件用新的子命令扩展了 <code>kubectl</code>，允许新的和自定义的特性不包括在 <code>kubectl</code> 的主要发行版中。</p>
<h2 id="准备开始">准备开始</h2>
<!--
You need to have a working `kubectl` binary installed.
-->
<p>你需要安装一个可用的 <code>kubectl</code> 可执行文件。</p>
<!-- steps -->
<!--
## Installing kubectl plugins

A plugin is a standalone executable file, whose name begins with `kubectl-`. To install a plugin, move its executable file to anywhere on your `PATH`.
-->
<h2 id="安装-kubectl-插件">安装 kubectl 插件</h2>
<p>插件是一个独立的可执行文件，名称以 <code>kubectl-</code> 开头。
要安装插件，将其可执行文件移动到 <code>PATH</code> 中的任何位置。</p>
<!--
You can also discover and install kubectl plugins available in the open source
using [Krew](https://krew.dev/). Krew is a plugin manager maintained by
the Kubernetes SIG CLI community.
-->
<p>你也可以使用 <a href="https://krew.dev/">Krew</a> 来发现和安装开源的 kubectl 插件。
Krew 是一个由 Kubernetes SIG CLI 社区维护的插件管理器。</p>
<!--
Kubectl plugins available via the Krew [plugin index](https://krew.sigs.k8s.io/plugins/)
are not audited for security. You should install and run third-party plugins at your
own risk, since they are arbitrary programs running on your machine.
-->
<blockquote class="caution callout">
  <div><strong>注意：</strong> Krew <a href="https://krew.sigs.k8s.io/plugins/">插件索引</a> 所维护的 kubectl 插件并未经过安全性审查。
你要了解安装和运行第三方插件的安全风险，因为它们本质上时是一些在你的机器上
运行的程序。</div>
</blockquote>

<!--
### Discovering plugins

`kubectl` provides a command `kubectl plugin list` that searches your PATH for valid plugin executables.
Executing this command causes a traversal of all files in your PATH. Any files that are executable, and begin with `kubectl-` will show up *in the order in which they are present in your PATH* in this command's output.
A warning will be included for any files beginning with `kubectl-` that are *not* executable.
A warning will also be included for any valid plugin files that overlap each other's name.

You can use [Krew](https://krew.dev/) to discover and install `kubectl`
plugins from a community-curated
[plugin index](https://krew.sigs.k8s.io/plugins/).
-->
<h3 id="发现插件">发现插件</h3>
<p><code>kubectl</code> 提供一个命令 <code>kubectl plugin list</code>，用于搜索路径查找有效的插件可执行文件。
执行此命令将遍历路径中的所有文件。任何以 <code>kubectl-</code> 开头的可执行文件都将在这个命令的输出中以它们在路径中出现的顺序显示。
任何以 <code>kubectl-</code> 开头的文件如果<code>不可执行</code>，都将包含一个警告。
对于任何相同的有效插件文件，都将包含一个警告。</p>
<p>你可以使用 <a href="https://krew.dev/">Krew</a> 从社区策划的<a href="https://krew.sigs.k8s.io/plugins/">插件索引</a>
中发现和安装 <code>kubectl</code> 插件。</p>
<!--
#### Limitations

It is currently not possible to create plugins that overwrite existing `kubectl` commands. For example, creating a plugin `kubectl-version` will cause that plugin to never be executed, as the existing `kubectl version` command will always take precedence over it. Due to this limitation, it is also *not* possible to use plugins to add new subcommands to existing `kubectl` commands. For example, adding a subcommand `kubectl create foo` by naming your plugin `kubectl-create-foo` will cause that plugin to be ignored.

`kubectl plugin list` shows warnings for any valid plugins that attempt to do this.
-->
<h4 id="限制">限制</h4>
<p>目前无法创建覆盖现有 <code>kubectl</code> 命令的插件。
例如，创建一个插件 <code>kubectl-version</code> 将导致该插件永远不会被执行，
因为现有的 <code>kubectl version</code> 命令总是优先于它执行。
由于这个限制，也不可能使用插件将新的子命令添加到现有的 <code>kubectl</code> 命令中。
例如，通过将插件命名为 <code>kubectl-create-foo</code> 来添加子命令 <code>kubectl create foo</code> 将导致该插件被忽略。</p>
<p>对于任何试图这样做的有效插件 <code>kubectl plugin list</code> 的输出中将显示警告。</p>
<!--
## Writing kubectl plugins

You can write a plugin in any programming language or script that allows you to write command-line commands.
-->
<h2 id="编写-kubectl-插件">编写 kubectl 插件</h2>
<p>你可以用任何编程语言或脚本编写插件，允许你编写命令行命令。</p>
<!--
There is no plugin installation or pre-loading required. Plugin executables receive
the inherited environment from the `kubectl` binary.
A plugin determines which command path it wishes to implement based on its name.
For example, a plugin named `kubectl-foo` provides a command `kubectl foo`. You must
install the plugin executable somewhere in your `PATH`.
-->
<p>不需要安装插件或预加载，插件可执行程序从 <code>kubectl</code> 二进制文件接收继承的环境，
插件根据其名称确定它希望实现的命令路径。
例如，名为 <code>kubectl-foo</code> 的插件提供了命令 <code>kubectl foo</code>。
必须将插件的可执行文件安装在 <code>PATH</code> 中的某个位置。</p>
<!--
### Example plugin
-->
<h3 id="示例插件">示例插件</h3>
<pre tabindex="0"><code>#!/bin/bash

# 可选的参数处理
if [[ &quot;$1&quot; == &quot;version&quot; ]]
then
    echo &quot;1.0.0&quot;
    exit 0
fi

# 可选的参数处理
if [[ &quot;$1&quot; == &quot;config&quot; ]]
then
    echo $KUBECONFIG
    exit 0
fi

echo &quot;I am a plugin named kubectl-foo&quot;
</code></pre><!--
### Using a plugin
-->
<h3 id="使用插件">使用插件</h3>
<!--
To use a plugin, make the plugin executable:
-->
<p>要使用某插件，先要使其可执行：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo chmod +x ./kubectl-foo
</code></pre></div><!--
and place it anywhere in your PATH:
-->
<p>并将它放在你的 PATH 中的任何地方：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo mv ./kubectl-foo /usr/local/bin
</code></pre></div><!--
You may now invoke your plugin as a `kubectl` command:
-->
<p>你现在可以调用你的插件作为 <code>kubectl</code> 命令：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl foo
</code></pre></div><pre tabindex="0"><code>I am a plugin named kubectl-foo
</code></pre><!--
All args and flags are passed as-is to the executable:
-->
<p>所有参数和标记按原样传递给可执行文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl foo version
</code></pre></div><pre tabindex="0"><code>1.0.0
</code></pre><!--
All environment variables are also passed as-is to the executable:
-->
<p>所有环境变量也按原样传递给可执行文件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#a2f">export</span> <span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>~/.kube/config
kubectl foo config
</code></pre></div><pre tabindex="0"><code>/home/&lt;user&gt;/.kube/config
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBECONFIG</span><span style="color:#666">=</span>/etc/kube/config kubectl foo config
</code></pre></div><pre tabindex="0"><code>/etc/kube/config
</code></pre><!--
Additionally, the first argument that is passed to a plugin will always be the full path to the location where it was invoked (`$0` would equal `/usr/local/bin/kubectl-foo` in our example above).
-->
<p>此外，传递给插件的第一个参数总是调用它的位置的绝对路径（在上面的例子中，<code>$0</code> 将等于 <code>/usr/local/bin/kubectl-foo</code>）。</p>
<!--
### Naming a plugin

As seen in the example above, a plugin determines the command path that it will implement based on its filename. Every sub-command in the command path that a plugin targets, is separated by a dash (`-`).
For example, a plugin that wishes to be invoked whenever the command `kubectl foo bar baz` is invoked by the user, would have the filename of `kubectl-foo-bar-baz`.
-->
<h3 id="命名插件">命名插件</h3>
<p>如上面的例子所示，插件根据文件名确定要实现的命令路径，插件所针对的命令路径中的每个子命令都由破折号（<code>-</code>）分隔。
例如，当用户调用命令 <code>kubectl foo bar baz</code> 时，希望调用该命令的插件的文件名为 <code>kubectl-foo-bar-baz</code>。</p>
<!--
#### Flags and argument handling
-->
<h4 id="参数和标记处理">参数和标记处理</h4>
<!--
The plugin mechanism doest _not_ create any custom, plugin-specific values or environment variables to a plugin process.

An older kubectl plugin mechanism provided environment variables such as `KUBECTL_PLUGINS_CURRENT_NAMESPACE`; that no longer happens.
-->
<blockquote class="note callout">
  <div><strong>说明：</strong> <p>插件机制不会为插件进程创建任何定制的、特定于插件的值或环境变量。</p>
<p>较老的插件机制会提供环境变量（例如 <code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>）；这种机制已被废弃。</p>
</div>
</blockquote>
<!--
kubectl plugins must parse and validate all of the arguments passed to them.
See [using the command line runtime package](#using-the-command-line-runtime-package) for details of a Go library aimed at plugin authors.

Here are some additional cases where users invoke your plugin while providing additional flags and arguments. This builds upon the `kubectl-foo-bar-baz` plugin from the scenario above.
-->
<p>kubectl 插件必须解析并检查传递给它们的所有参数。
参阅<a href="#using-the-command-line-runtime-package">使用命令行运行时包</a>了解针对
插件开发人员的 Go 库的细节。</p>
<!--
Here are some additional cases where users invoke your plugin while providing additional flags and arguments. This builds upon the `kubectl-foo-bar-baz` plugin from the scenario above.
-->
<p>这里是一些用户调用你的插件的时候提供额外标志和参数的场景。
这些场景时基于上述案例中的 <code>kubectl-foo-bar-baz</code> 插件的。</p>
<!--
If you run `kubectl foo bar baz arg1 -flag=value arg2`, kubectl's plugin mechanism will first try to find the plugin with the longest possible name, which in this case
would be `kubectl-foo-bar-baz-arg1`. Upon not finding that plugin, kubectl then treats the last dash-separated value as an argument (`arg1` in this case), and attempts to find the next longest possible name, `kubectl-foo-bar-baz`.
Upon having found a plugin with this name, kubectl then invokes that plugin, passing all args and flags after the plugin's name as arguments to the plugin process.
-->
<p>如果你运行 <code>kubectl foo bar baz arg1 --flag=value arg2</code>，kubectl 的插件机制将首先尝试找到
最长可能名称的插件，在本例中是 <code>kubectl-foo-bar-baz-arg1</code>。
当没有找到这个插件时，kubectl 就会将最后一个以破折号分隔的值视为参数（在本例中为 <code>arg1</code>），
并尝试找到下一个最长的名称 <code>kubectl-foo-bar-baz</code>。
在找到具有此名称的插件后，它将调用该插件，并在其名称之后将所有参数和标志传递给插件进程。</p>
<!-- Example: -->
<p>示例：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># 创建一个插件</span>
<span style="color:#a2f">echo</span> -e <span style="color:#b44">&#39;#!/bin/bash\n\necho &#34;My first command-line argument was $1&#34;&#39;</span> &gt; kubectl-foo-bar-baz
sudo chmod +x ./kubectl-foo-bar-baz

<span style="color:#080;font-style:italic"># 将插件放到 PATH 下完成&#34;安装&#34;</span>
sudo mv ./kubectl-foo-bar-baz /usr/local/bin

<span style="color:#080;font-style:italic"># 确保 kubectl 能够识别我们的插件</span>
kubectl plugin list
</code></pre></div><pre tabindex="0"><code>The following kubectl-compatible plugins are available:

/usr/local/bin/kubectl-foo-bar-baz
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 测试通过 &#34;kubectl&#34; 命令来调用我们的插件时可行的</span>
<span style="color:#080;font-style:italic"># 即使我们给插件传递一些额外的参数或标志</span>
kubectl foo bar baz arg1 --meaningless-flag<span style="color:#666">=</span><span style="color:#a2f">true</span>
</code></pre></div><pre tabindex="0"><code>My first command-line argument was arg1
</code></pre><!--
As you can see, our plugin was found based on the `kubectl` command specified by a user, and all extra arguments and flags were passed as-is to the plugin executable once it was found.
-->
<p>正如你所看到的，我们的插件是基于用户指定的 <code>kubectl</code> 命令找到的，
所有额外的参数和标记都是按原样传递给插件可执行文件的。</p>
<!--
#### Names with dashes and underscores

Although the `kubectl` plugin mechanism uses the dash (`-`) in plugin filenames to separate the sequence of sub-commands processed by the plugin, it is still possible to create a plugin
command containing dashes in its commandline invocation by using underscores (`_`) in its filename.
-->
<h4 id="带有破折号和下划线的名称">带有破折号和下划线的名称</h4>
<p>虽然 <code>kubectl</code> 插件机制在插件文件名中使用破折号（<code>-</code>）分隔插件处理的子命令序列，
但是仍然可以通过在文件名中使用下划线（<code>_</code>）来创建命令行中包含破折号的插件命令。</p>
<!-- Example: -->
<p>例子：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># 创建文件名中包含下划线的插件</span>
<span style="color:#a2f">echo</span> -e <span style="color:#b44">&#39;#!/bin/bash\n\necho &#34;I am a plugin with a dash in my name&#34;&#39;</span> &gt; ./kubectl-foo_bar
sudo chmod +x ./kubectl-foo_bar
  
<span style="color:#080;font-style:italic"># 将插件放到 PATH 下</span>
sudo mv ./kubectl-foo_bar /usr/local/bin

<span style="color:#080;font-style:italic"># 现在可以通过 kubectl 来调用插件</span>
kubectl foo-bar
</code></pre></div><pre tabindex="0"><code>I am a plugin with a dash in my name
</code></pre><!--
Note that the introduction of underscores to a plugin filename does not prevent us from having commands such as `kubectl foo_bar`.
The command from the above example, can be invoked using either a dash (`-`) or an underscore (`_`):
-->
<p>请注意，在插件文件名中引入下划线并不会阻止我们使用 <code>kubectl foo_bar</code> 之类的命令。
可以使用破折号（<code>-</code>）或下划线（<code>_</code>）调用上面示例中的命令:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 我们的插件也可以用破折号来调用</span>
kubectl foo-bar
</code></pre></div><pre tabindex="0"><code>I am a plugin with a dash in my name
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 你也可以使用下划线来调用我们的定制命令</span>
kubectl foo_bar
</code></pre></div><pre tabindex="0"><code>I am a plugin with a dash in my name
</code></pre><!--
#### Name conflicts and overshadowing

It is possible to have multiple plugins with the same filename in different locations throughout your PATH.
For example, given a PATH with the following value: `PATH=/usr/local/bin/plugins:/usr/local/bin/moreplugins`, a copy of plugin `kubectl-foo` could exist in `/usr/local/bin/plugins` and `/usr/local/bin/moreplugins`,
such that the output of the `kubectl plugin list` command is:
-->
<h4 id="命名冲突和弊端">命名冲突和弊端</h4>
<p>可以在 <code>PATH</code> 的不同位置提供多个文件名相同的插件，
例如，给定一个 <code>PATH</code> 为: <code>PATH=/usr/local/bin/plugins:/usr/local/bin/moreplugins</code>，
在 <code>/usr/local/bin/plugins</code> 和 <code>/usr/local/bin/moreplugins</code> 中可以存在一个插件
<code>kubectl-foo</code> 的副本，这样 <code>kubectl plugin list</code> 命令的输出就是:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b8860b">PATH</span><span style="color:#666">=</span>/usr/local/bin/plugins:/usr/local/bin/moreplugins kubectl plugin list
</code></pre></div><pre tabindex="0"><code>The following kubectl-compatible plugins are available:
  
/usr/local/bin/plugins/kubectl-foo
/usr/local/bin/moreplugins/kubectl-foo
  - warning: /usr/local/bin/moreplugins/kubectl-foo is overshadowed by a similarly named plugin: /usr/local/bin/plugins/kubectl-foo
  
error: one plugin warning was found
</code></pre><!--
In the above scenario, the warning under `/usr/local/bin/moreplugins/kubectl-foo` tells us that this plugin will never be executed. Instead, the executable that appears first in our PATH, `/usr/local/bin/plugins/kubectl-foo`, will always be found and executed first by the `kubectl` plugin mechanism.
-->
<p>在上面的场景中 <code>/usr/local/bin/moreplugins/kubectl-foo</code> 下的警告告诉我们这个插件永远不会被执行。
相反，首先出现在我们路径中的可执行文件 <code>/usr/local/bin/plugins/kubectl-foo</code>
总是首先被 <code>kubectl</code> 插件机制找到并执行。</p>
<!--
A way to resolve this issue is to ensure that the location of the plugin that you wish to use with `kubectl` always comes first in your PATH. For example, if we wanted to always use `/usr/local/bin/moreplugins/kubectl-foo` anytime that the `kubectl` command `kubectl foo` was invoked, we would simply change the value of our PATH to be `PATH=/usr/local/bin/moreplugins:/usr/local/bin/plugins`.
-->
<p>解决这个问题的一种方法是你确保你希望与 <code>kubectl</code> 一起使用的插件的位置总是在你的路径中首先出现。
例如，如果我们总是想使用 <code>/usr/local/bin/moreplugins/kubectl foo</code>，
那么在调用 <code>kubectl</code> 命令 <code>kubectl foo</code> 时，我们只需将路径的值更改为 <code>PATH=/usr/local/bin/moreplugins:/usr/local/bin/plugins</code>。</p>
<!--
#### Invocation of the longest executable filename

There is another kind of overshadowing that can occur with plugin filenames. Given two plugins present in a user's PATH `kubectl-foo-bar` and `kubectl-foo-bar-baz`, the `kubectl` plugin mechanism will always choose the longest possible plugin name for a given user command. Some examples below, clarify this further:
-->
<h4 id="调用最长的可执行文件名">调用最长的可执行文件名</h4>
<p>对于插件文件名而言还有另一种弊端，给定用户路径中的两个插件 <code>kubectl-foo-bar</code> 和 <code>kubectl-foo-bar-baz</code>
<code>kubectl</code> 插件机制总是为给定的用户命令选择尽可能长的插件名称。下面的一些例子进一步的说明了这一点：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#080;font-style:italic"># 对于给定的 kubectl 命令，最长可能文件名的插件是被优先选择的</span>
kubectl foo bar baz
</code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar-baz is executed
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl foo bar
</code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar is executed
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl foo bar baz buz
</code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar-baz is executed, with &quot;buz&quot; as its first argument
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl foo bar buz
</code></pre></div><pre tabindex="0"><code>Plugin kubectl-foo-bar is executed, with &quot;buz&quot; as its first argument
</code></pre><!--
This design choice ensures that plugin sub-commands can be implemented across multiple files, if needed, and that these sub-commands can be nested under a "parent" plugin command:
-->
<p>这种设计选择确保插件子命令可以跨多个文件实现，如果需要，这些子命令可以嵌套在&quot;父&quot;插件命令下：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ls ./plugin_command_tree
</code></pre></div><pre tabindex="0"><code>kubectl-parent
kubectl-parent-subcommand
kubectl-parent-subcommand-subsubcommand
</code></pre><!--
### Checking for plugin warnings

You can use the aforementioned `kubectl plugin list` command to ensure that your plugin is visible by `kubectl`, and verify that there are no warnings preventing it from being called as a `kubectl` command.
-->
<h3 id="检查插件警告">检查插件警告</h3>
<p>你可以使用前面提到的 <code>kubectl plugin list</code> 命令来确保你的插件可以被 <code>kubectl</code> 看到，
并且验证没有警告防止它被称为 <code>kubectl</code> 命令。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl plugin list
</code></pre></div><pre tabindex="0"><code>The following kubectl-compatible plugins are available:
  
test/fixtures/pkg/kubectl/plugins/kubectl-foo
/usr/local/bin/kubectl-foo
  - warning: /usr/local/bin/kubectl-foo is overshadowed by a similarly named plugin: test/fixtures/pkg/kubectl/plugins/kubectl-foo
plugins/kubectl-invalid
  - warning: plugins/kubectl-invalid identified as a kubectl plugin, but it is not executable

error: 2 plugin warnings were found
</code></pre><!--
### Using the command line runtime package

If you're writing a plugin for kubectl and you're using Go, you can make use
of the
[cli-runtime](https://github.com/kubernetes/cli-runtime) utility libraries.

These libraries provide helpers for parsing or updating a user's
[kubeconfig](/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
file, for making REST-style requests to the API server, or to bind flags
associated with configuration and printing.

See the [Sample CLI Plugin](https://github.com/kubernetes/sample-cli-plugin) for
an example usage of the tools provided in the CLI Runtime repo.
-->
<h3 id="using-the-command-line-runtime-package">使用命令行运行时包 </h3>
<p>如果你在编写 kubectl 插件，而且你选择使用 Go 语言，你可以利用
<a href="https://github.com/kubernetes/cli-runtime">cli-runtime</a> 工具库。</p>
<p>这些库提供了一些辅助函数，用来解析和更新用户的
<a href="/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig</a>
文件，向 API 服务器发起 REST 风格的请求，或者将参数绑定到某配置上，
抑或将其打印输出。</p>
<p>关于 CLI Runtime 仓库所提供的工具的使用实例，可参考
<a href="https://github.com/kubernetes/sample-cli-plugin">CLI 插件示例</a> 项目。</p>
<!--
## Distributing kubectl plugins

If you have developed a plugin for others to use, you should consider how you
package it, distribute it and deliver updates to your users.
-->
<h2 id="分发-kubectl-插件">分发 kubectl 插件</h2>
<p>如果你开发了一个插件给别人使用，你应该考虑如何为其封装打包、如何分发软件
以及将来的更新到用户。</p>
<!--
### Krew {#distributing-krew}

[Krew](https://krew.dev/) offers a cross-platform way to package and
distribute your plugins. This way, you use a single packaging format for all
target platforms (Linux, Windows, macOS etc) and deliver updates to your users.
Krew also maintains a [plugin
index](https://krew.sigs.k8s.io/plugins/) so that other people can
discover your plugin and install it.
-->
<h3 id="distributing-krew">Krew</h3>
<p><a href="https://krew.dev/">Krew</a> 提供了一种对插件进行打包和分发的跨平台方式。
基于这种方式，你会在所有的目标平台（Linux、Windows、macOS 等）使用同一
种打包形式，包括为用户提供更新。
Krew 也维护一个<a href="https://krew.sigs.k8s.io/plugins/">插件索引（plugin index）</a>
以便其他人能够发现你的插件并安装之。</p>
<!--
### Native / platform specific package management {#distributing-native}

Alternatively, you can use traditional package managers such as, `apt` or `yum`
on Linux, Chocolatey on Windows, and Homebrew on macOS. Any package
manager will be suitable if it can place new executables placed somewhere
in the user's `PATH`.
As a plugin author, if you pick this option then you also have the burden
of updating your kubectl plugin’s distribution package across multiple
platforms for each release.
-->
<h3 id="distributing-native">原生的与特定平台的包管理    </h3>
<p>另一种方式是，你可以使用传统的包管理器（例如 Linux 上 的 <code>apt</code> 或 <code>yum</code>，
Windows 上的 Chocolatey、macOs 上的 Homebrew）。
只要能够将新的可执行文件放到用户的 <code>PATH</code> 路径上某处，这种包管理器就符合需要。
作为一个插件作者，如果你选择这种方式来分发，你就需要自己来管理和更新
你的 kubectl 插件的分发包，包括所有平台和所有发行版本。</p>
<!--
### Source code {#distributing-source-code}

You can publish the source code; for example, as a Git repository. If you
choose this option, someone who wants to use that plugin must fetch the code,
set up a build environment (if it needs compiling), and deploy the plugin.
If you also make compiled packages available, or use Krew, that will make
installs easier.
-->
<h3 id="distributing-source-code">源代码  </h3>
<p>你也可以发布你的源代码，例如，发布为某个 Git 仓库。
如果你选择这条路线，希望使用该插件的用户必须取回代码、配置一个构造环境
（如果需要编译的话）并部署该插件。
如果你也提供编译后的软件包，或者使用 Krew，那就会大大简化安装过程了。</p>
<h2 id="接下来">接下来</h2>
<!--
* Check the Sample CLI Plugin repository for [a detailed example](https://github.com/kubernetes/sample-cli-plugin) of a plugin written in Go.
* In case of any questions, feel free to reach out to the [CLI SIG team](https://github.com/kubernetes/community/tree/master/sig-cli).
* Read about [Krew](https://krew.dev/), a package manager for kubectl plugins.
-->
<ul>
<li>查看 CLI 插件库示例，查看用 Go 编写的插件的<a href="https://github.com/kubernetes/sample-cli-plugin">详细示例</a></li>
<li>如有任何问题，请随时联系 <a href="https://github.com/kubernetes/community/tree/master/sig-cli">SIG CLI </a></li>
<li>了解 <a href="https://krew.dev/">Krew</a>，一个 kubectl 插件管理器。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-fdfb2a2cba62a1e624897eaebac0168e">17 - 管理巨页（HugePages）</h1>
    <div class="lead">将大页配置和管理为集群中的可调度资源。</div>
	<!--
---
reviewers:
- derekwaynecarr
title: Manage HugePages
content_type: task
---
--->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.22 [stable]</code>
</div>

<!--
Kubernetes supports the allocation and consumption of pre-allocated huge pages
by applications in a Pod. This page describes how users can consume huge pages.
--->
<p>Kubernetes 支持在 Pod 应用中使用预先分配的巨页。本文描述了用户如何使用巨页，以及当前的限制。</p>
<h2 id="准备开始">准备开始</h2>
<!--
1. Kubernetes nodes must pre-allocate huge pages in order for the node to report
   its huge page capacity. A node may only pre-allocate huge pages for a single
   size.

The nodes will automatically discover and report all huge page resources as a
schedulable resource.
--->
<ol>
<li>为了使节点能够上报巨页容量，Kubernetes 节点必须预先分配巨页。每个节点只能预先分配一种特定规格的巨页。</li>
</ol>
<p>节点会自动发现全部巨页资源，并作为可供调度的资源进行上报。</p>
<!-- steps -->
<h2 id="api">API</h2>
<!--
Huge pages can be consumed via container level resource requirements using the
resource name `hugepages-<size>`, where `<size>` is the most compact binary
notation using integer values supported on a particular node. For example, if a
node supports 2048KiB and 1048576KiB page sizes, it will expose a schedulable
resources `hugepages-2Mi` and `hugepages-1Gi`. Unlike CPU or memory, huge pages
do not support overcommit. Note that when requesting hugepage resources, either
memory or CPU resources must be requested as well.

A pod may consume multiple huge page sizes in a single pod spec. In this case it
must use `medium: HugePages-<hugepagesize>` notation for all volume mounts.
--->
<p>用户可以通过在容器级别的资源需求中使用资源名称 <code>hugepages-&lt;size&gt;</code> 来使用巨页，其中的 size 是特定节点上支持的以整数值表示的最小二进制单位。 例如，如果一个节点支持 2048KiB 和 1048576KiB 页面大小，它将公开可调度的资源 <code>hugepages-2Mi</code> 和 <code>hugepages-1Gi</code>。与 CPU 或内存不同，巨页不支持过量使用（overcommit）。注意，在请求巨页资源时，还必须请求内存或 CPU 资源。</p>
<p>同一 Pod 的 spec 中可能会消耗不同尺寸的巨页。在这种情况下，它必须对所有挂载卷使用 <code>medium: HugePages-&lt;hugepagesize&gt;</code> 标识。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>huge-pages-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>fedora:latest<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- inf<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/hugepages-2Mi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hugepage-2mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/hugepages-1Gi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hugepage-1gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hugepages-2Mi</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hugepages-1Gi</span>:<span style="color:#bbb"> </span>2Gi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hugepage-2mi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">medium</span>:<span style="color:#bbb"> </span>HugePages-2Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hugepage-1gi<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">medium</span>:<span style="color:#bbb"> </span>HugePages-1Gi<span style="color:#bbb">
</span></code></pre></div><!--
A pod may use `medium: HugePages` only if it requests huge pages of one size.
-->
<p>Pod 只有在请求同一大小的巨页时才使用 <code>medium：HugePages</code>。</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>huge-pages-example<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>fedora:latest<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- sleep<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- inf<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/hugepages<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hugepage<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">hugepages-2Mi</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">memory</span>:<span style="color:#bbb"> </span>100Mi<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>hugepage<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">medium</span>:<span style="color:#bbb"> </span>HugePages<span style="color:#bbb">
</span></code></pre></div><!--
- Huge page requests must equal the limits. This is the default if limits are
  specified, but requests are not.
- Huge pages are isolated at a container scope, so each container has own 
  limit on their cgroup sandbox as requested in a container spec.
- EmptyDir volumes backed by huge pages may not consume more huge page memory
  than the pod request.
- Applications that consume huge pages via `shmget()` with `SHM_HUGETLB` must
  run with a supplemental group that matches `proc/sys/vm/hugetlb_shm_group`.
- Huge page usage in a namespace is controllable via ResourceQuota similar
  to other compute resources like `cpu` or `memory` using the `hugepages-<size>`
  token.
- Support of multiple sizes huge pages is feature gated. It can be
  enabled with the `HugePageStorageMediumSize` [feature
gate](/docs/reference/command-line-tools-reference/feature-gates/) on the <a class='glossary-tooltip' title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle='tooltip' data-placement='top' href='/docs/reference/generated/kubelet' target='_blank' aria-label='kubelet'>kubelet</a> and <a class='glossary-tooltip' title='提供 Kubernetes API 服务的控制面组件。' data-toggle='tooltip' data-placement='top' href='/zh/docs/reference/command-line-tools-reference/kube-apiserver/' target='_blank' aria-label='kube-apiserver'>kube-apiserver</a> (`--feature-gates=HugePageStorageMediumSize=false`).
--->
<ul>
<li>巨页的资源请求值必须等于其限制值。该条件在指定了资源限制，而没有指定请求的情况下默认成立。</li>
<li>巨页是被隔离在 pod 作用域的，因此每个容器在 spec 中都对 cgroup 沙盒有自己的限制。</li>
<li>巨页可用于 EmptyDir 卷，不过 EmptyDir 卷所使用的巨页数量不能够超出 Pod 请求的巨页数量。</li>
<li>通过带有 <code>SHM_HUGETLB</code> 的 <code>shmget()</code> 使用巨页的应用，必须运行在一个与
<code>proc/sys/vm/hugetlb_shm_group</code> 匹配的补充组下。</li>
<li>通过 ResourceQuota 资源，可以使用 <code>hugepages-&lt;size&gt;</code> 标记控制每个命名空间下的巨页使用量，
类似于使用 <code>cpu</code> 或 <code>memory</code> 来控制其他计算资源。</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5ab7bc7f14942c5c4b29d19f4a87271c">18 - 调度 GPUs</h1>
    <div class="lead">配置和调度 GPU 成一类资源以供集群中节点使用。</div>
	<!--
reviewers:
- vishh
content_type: concept
title: Schedule GPUs
description: Configure and schedule GPUs for use as a resource by nodes in a cluster.
-->
<!-- overview -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code>
</div>

<!--
Kubernetes includes **experimental** support for managing AMD and NVIDIA GPUs
(graphical processing units) across several nodes.

This page describes how users can consume GPUs across different Kubernetes versions
and the current limitations.
-->
<p>Kubernetes 支持对节点上的 AMD 和 NVIDIA GPU （图形处理单元）进行管理，目前处于<strong>实验</strong>状态。</p>
<p>本页介绍用户如何在不同的 Kubernetes 版本中使用 GPU，以及当前存在的一些限制。</p>
<!-- body -->
<!--
## Using device plugins

Kubernetes implements <a class='glossary-tooltip' title='一种软件扩展，可以使 Pod 访问由特定厂商初始化或者安装的设备。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/' target='_blank' aria-label='Device Plugins'>Device Plugins</a>
to let Pods access specialized hardware features such as GPUs.

As an administrator, you have to install GPU drivers from the corresponding
hardware vendor on the nodes and run the corresponding device plugin from the
GPU vendor:
-->
<h2 id="using-device-plugins">使用设备插件 </h2>
<p>Kubernetes 实现了<a class='glossary-tooltip' title='一种软件扩展，可以使 Pod 访问由特定厂商初始化或者安装的设备。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/' target='_blank' aria-label='设备插件（Device Plugins）'>设备插件（Device Plugins）</a>
以允许 Pod 访问类似 GPU 这类特殊的硬件功能特性。</p>
<p>作为集群管理员，你要在节点上安装来自对应硬件厂商的 GPU 驱动程序，并运行
来自 GPU 厂商的对应的设备插件。</p>
<ul>
<li><a href="#deploying-amd-gpu-device-plugin">AMD</a></li>
<li><a href="#deploying-nvidia-gpu-device-plugin">NVIDIA</a></li>
</ul>
<!--
When the above conditions are true, Kubernetes will expose `amd.com/gpu` or
`nvidia.com/gpu` as a schedulable resource.

You can consume these GPUs from your containers by requesting
`<vendor>.com/gpu` the same way you request `cpu` or `memory`.
However, there are some limitations in how you specify the resource requirements
when using GPUs:
-->
<p>当以上条件满足时，Kubernetes 将暴露 <code>amd.com/gpu</code> 或 <code>nvidia.com/gpu</code> 为
可调度的资源。</p>
<p>你可以通过请求 <code>&lt;vendor&gt;.com/gpu</code> 资源来使用 GPU 设备，就像你为 CPU
和内存所做的那样。
不过，使用 GPU 时，在如何指定资源需求这个方面还是有一些限制的：</p>
<!--
- GPUs are only supposed to be specified in the `limits` section, which means:
  * You can specify GPU `limits` without specifying `requests` because
    Kubernetes will use the limit as the request value by default.
  * You can specify GPU in both `limits` and `requests` but these two values
    must be equal.
  * You cannot specify GPU `requests` without specifying `limits`.
- Containers (and Pods) do not share GPUs. There's no overcommitting of GPUs.
- Each container can request one or more GPUs. It is not possible to request a
  fraction of a GPU.
-->
<ul>
<li>GPUs 只能设置在 <code>limits</code> 部分，这意味着：
<ul>
<li>你可以指定 GPU 的 <code>limits</code> 而不指定其 <code>requests</code>，Kubernetes 将使用限制
值作为默认的请求值；</li>
<li>你可以同时指定 <code>limits</code> 和 <code>requests</code>，不过这两个值必须相等。</li>
<li>你不可以仅指定 <code>requests</code> 而不指定 <code>limits</code>。</li>
</ul>
</li>
<li>容器（以及 Pod）之间是不共享 GPU 的。GPU 也不可以过量分配（Overcommitting）。</li>
<li>每个容器可以请求一个或者多个 GPU，但是用小数值来请求部分 GPU 是不允许的。</li>
</ul>
<!--
Here's an example:
-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cuda-vector-add<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cuda-vector-add<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;k8s.gcr.io/cuda-vector-add:v0.1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">nvidia.com/gpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># requesting 1 GPU</span><span style="color:#bbb">
</span></code></pre></div><!--
### Deploying AMD GPU device plugin

The [official AMD GPU device plugin](https://github.com/RadeonOpenCompute/k8s-device-plugin)
has the following requirements:
-->
<h3 id="deploying-amd-gpu-device-plugin">部署 AMD GPU 设备插件  </h3>
<p><a href="https://github.com/RadeonOpenCompute/k8s-device-plugin">官方的 AMD GPU 设备插件</a> 有以下要求：</p>
<!--
- Kubernetes nodes have to be pre-installed with AMD GPU Linux driver.

To deploy the AMD device plugin once your cluster is running and the above
requirements are satisfied:
```
# For Kubernetes v1.9
kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.9/k8s-ds-amdgpu-dp.yaml

# For Kubernetes v1.10
kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.10/k8s-ds-amdgpu-dp.yaml
```
-->
<ul>
<li>Kubernetes 节点必须预先安装 AMD GPU 的 Linux 驱动。</li>
</ul>
<p>如果你的集群已经启动并且满足上述要求的话，可以这样部署 AMD 设备插件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://raw.githubusercontent.com/RadeonOpenCompute/k8s-device-plugin/r1.10/k8s-ds-amdgpu-dp.yaml
</code></pre></div><!--
You can report issues with this third-party device plugin by logging an issue in
[RadeonOpenCompute/k8s-device-plugin](https://github.com/RadeonOpenCompute/k8s-device-plugin).
-->
<p>你可以到 <a href="https://github.com/RadeonOpenCompute/k8s-device-plugin">RadeonOpenCompute/k8s-device-plugin</a>
项目报告有关此设备插件的问题。</p>
<!--
### Deploying NVIDIA GPU device plugin

There are currently two device plugin implementations for NVIDIA GPUs:
-->
<h3 id="deploying-nvidia-gpu-device-plugin">部署 NVIDIA GPU 设备插件 </h3>
<p>对于 NVIDIA GPUs，目前存在两种设备插件的实现：</p>
<!--
#### Official NVIDIA GPU device plugin

The [official NVIDIA GPU device plugin](https://github.com/NVIDIA/k8s-device-plugin)
has the following requirements:
-->
<h4 id="官方的-nvidia-gpu-设备插件">官方的 NVIDIA GPU 设备插件</h4>
<p><a href="https://github.com/NVIDIA/k8s-device-plugin">官方的 NVIDIA GPU 设备插件</a> 有以下要求:</p>
<!--
- Kubernetes nodes have to be pre-installed with NVIDIA drivers.
- Kubernetes nodes have to be pre-installed with [nvidia-docker 2.0](https://github.com/NVIDIA/nvidia-docker)
- nvidia-container-runtime must be configured as the [default runtime](https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes)
  for docker instead of runc.
- NVIDIA drivers ~= 361.93

To deploy the NVIDIA device plugin once your cluster is running and the above
requirements are satisfied:
-->
<ul>
<li>Kubernetes 的节点必须预先安装了 NVIDIA 驱动</li>
<li>Kubernetes 的节点必须预先安装 <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker 2.0</a></li>
<li>Docker 的<a href="https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes">默认运行时</a>必须设置为 nvidia-container-runtime，而不是 runc</li>
<li>NVIDIA 驱动版本 ~= 384.81</li>
</ul>
<p>如果你的集群已经启动并且满足上述要求的话，可以这样部署 NVIDIA 设备插件：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml
</code></pre></div><p>请到 <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA/k8s-device-plugin</a>项目报告有关此设备插件的问题。</p>
<!--
#### NVIDIA GPU device plugin used by GCE

The [NVIDIA GPU device plugin used by GCE](https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu)
doesn't require using nvidia-docker and should work with any container runtime
that is compatible with the Kubernetes Container Runtime Interface (CRI). It's tested
on [Container-Optimized OS](https://cloud.google.com/container-optimized-os/)
and has experimental code for Ubuntu from 1.9 onwards.
-->
<h4 id="gce-中使用的-nvidia-gpu-设备插件">GCE 中使用的 NVIDIA GPU 设备插件</h4>
<p><a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu">GCE 使用的 NVIDIA GPU 设备插件</a> 并不要求使用 nvidia-docker，并且对于任何实现了 Kubernetes CRI 的容器运行时，都应该能够使用。这一实现已经在 <a href="https://cloud.google.com/container-optimized-os/">Container-Optimized OS</a> 上进行了测试，并且在 1.9 版本之后会有对于 Ubuntu 的实验性代码。</p>
<p>你可以使用下面的命令来安装 NVIDIA 驱动以及设备插件：</p>
<pre tabindex="0"><code># 在 COntainer-Optimized OS 上安装 NVIDIA 驱动:
kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/daemonset.yaml

# 在 Ubuntu 上安装 NVIDIA 驱动 (实验性质):
kubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/stable/nvidia-driver-installer/ubuntu/daemonset.yaml

# 安装设备插件:
kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.12/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml
</code></pre><!--
Report issues with this device plugin and installation method to [GoogleCloudPlatform/container-engine-accelerators](https://github.com/GoogleCloudPlatform/container-engine-accelerators).

Google publishes its own [instructions](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus) for using NVIDIA GPUs on GKE .
-->
<p>请到 <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators">GoogleCloudPlatform/container-engine-accelerators</a> 报告有关此设备插件以及安装方法的问题。</p>
<p>关于如何在 GKE 上使用 NVIDIA GPUs，Google 也提供自己的<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/gpus">指令</a>。</p>
<!--
## Clusters containing different types of GPUs

If different nodes in your cluster have different types of GPUs, then you
can use [Node Labels and Node Selectors](/docs/tasks/configure-pod-container/assign-pods-nodes/)
to schedule pods to appropriate nodes.

For example:
-->
<h2 id="集群内存在不同类型的-gpu">集群内存在不同类型的 GPU</h2>
<p>如果集群内部的不同节点上有不同类型的 NVIDIA GPU，那么你可以使用
<a href="/zh/docs/tasks/configure-pod-container/assign-pods-nodes/">节点标签和节点选择器</a>
来将 pod 调度到合适的节点上。</p>
<p>例如：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#080;font-style:italic"># 为你的节点加上它们所拥有的加速器类型的标签</span>
kubectl label nodes &lt;node-with-k80&gt; <span style="color:#b8860b">accelerator</span><span style="color:#666">=</span>nvidia-tesla-k80
kubectl label nodes &lt;node-with-p100&gt; <span style="color:#b8860b">accelerator</span><span style="color:#666">=</span>nvidia-tesla-p100
</code></pre></div><!--
## Automatic node labelling {#node-labeller}
-->
<h2 id="node-labeller">自动节点标签 </h2>
<!--
If you're using AMD GPU devices, you can deploy
[Node Labeller](https://github.com/RadeonOpenCompute/k8s-device-plugin/tree/master/cmd/k8s-node-labeller).
Node Labeller is a <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='controller'>controller</a> that automatically
labels your nodes with GPU properties.

At the moment, that controller can add labels for:
-->
<p>如果你在使用 AMD GPUs，你可以部署
<a href="https://github.com/RadeonOpenCompute/k8s-device-plugin/tree/master/cmd/k8s-node-labeller">Node Labeller</a>，
它是一个 <a class='glossary-tooltip' title='控制器通过 apiserver 监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle='tooltip' data-placement='top' href='/zh/docs/concepts/architecture/controller/' target='_blank' aria-label='控制器'>控制器</a>，
会自动给节点打上 GPU 属性标签。目前支持的属性：</p>
<!--
* Device ID (-device-id)
* VRAM Size (-vram)
* Number of SIMD (-simd-count)
* Number of Compute Unit (-cu-count)
* Firmware and Feature Versions (-firmware)
* GPU Family, in two letters acronym (-family)
  * SI - Southern Islands
  * CI - Sea Islands
  * KV - Kaveri
  * VI - Volcanic Islands
  * CZ - Carrizo
  * AI - Arctic Islands
  * RV - Raven
Example result:
--->
<ul>
<li>设备 ID (-device-id)</li>
<li>VRAM 大小 (-vram)</li>
<li>SIMD 数量(-simd-count)</li>
<li>计算单位数量(-cu-count)</li>
<li>固件和特性版本 (-firmware)</li>
<li>GPU 系列，两个字母的首字母缩写(-family)
<ul>
<li>SI - Southern Islands</li>
<li>CI - Sea Islands</li>
<li>KV - Kaveri</li>
<li>VI - Volcanic Islands</li>
<li>CZ - Carrizo</li>
<li>AI - Arctic Islands</li>
<li>RV - Raven</li>
</ul>
</li>
</ul>
<p>示例:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe node cluster-node-23
</code></pre></div><pre tabindex="0"><code>    Name:               cluster-node-23
    Roles:              &lt;none&gt;
    Labels:             beta.amd.com/gpu.cu-count.64=1
                        beta.amd.com/gpu.device-id.6860=1
                        beta.amd.com/gpu.family.AI=1
                        beta.amd.com/gpu.simd-count.256=1
                        beta.amd.com/gpu.vram.16G=1
                        beta.kubernetes.io/arch=amd64
                        beta.kubernetes.io/os=linux
                        kubernetes.io/hostname=cluster-node-23
    Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                        node.alpha.kubernetes.io/ttl: 0
    ......
</code></pre><!--
With the Node Labeller in use, you can specify the GPU type in the Pod spec:
-->
<p>使用了 Node Labeller 的时候，你可以在 Pod 的规约中指定 GPU 的类型：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cuda-vector-add<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>OnFailure<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>cuda-vector-add<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#080;font-style:italic"># https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;k8s.gcr.io/cuda-vector-add:v0.1&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb">
</span><span style="color:#bbb">          </span><span style="color:#008000;font-weight:bold">nvidia.com/gpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#008000;font-weight:bold">nodeSelector</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">accelerator</span>:<span style="color:#bbb"> </span>nvidia-tesla-p100<span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># or nvidia-tesla-k80 etc.</span><span style="color:#bbb">
</span></code></pre></div><!--
This will ensure that the pod will be scheduled to a node that has the GPU type
you specified.
-->
<p>这能够保证 Pod 能够被调度到你所指定类型的 GPU 的节点上去。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0c4484d31ad3902880897e694bbd306f">19 - 配置 kubelet 镜像凭据提供程序</h1>
    <div class="lead">配置 kubelet 的镜像凭据提供程序插件</div>
	<!-- 
title: Configure a kubelet image credential provider
reviewers:
- liggitt
- cheftako
description: Configure the kubelet's image credential provider plugin
content_type: task
-->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code>
</div>

<!-- overview -->
<!-- 
Starting from Kubernetes v1.20, the kubelet can dynamically retrieve credentials for a container image registry
using exec plugins. The kubelet and the exec plugin communicate through stdio (stdin, stdout, and stderr) using
Kubernetes versioned APIs. These plugins allow the kubelet to request credentials for a container registry dynamically
as opposed to storing static credentials on disk. For example, the plugin may talk to a local metadata server to retrieve
short-lived credentials for an image that is being pulled by the kubelet.
-->
<p>从 Kubernetes v1.20 开始，kubelet 可以使用 exec 插件动态检索容器镜像注册中心的凭据。
kubelet 和 exec 插件使用 Kubernetes 版本化 API 通过标准输入输出（标准输入、标准输出和标准错误）通信。
这些插件允许 kubelet 动态请求容器注册中心的凭据，而不是将静态凭据存储在磁盘上。
例如，插件可能会与本地元数据通信，以检索 kubelet 正在拉取的镜像的短期凭据。</p>
<!-- 
You may be interested in using this capability if any of the below are true:

* API calls to a cloud provider service are required to retrieve authentication information for a registry.
* Credentials have short expiration times and requesting new credentials frequently is required.
* Storing registry credentials on disk or in imagePullSecrets is not acceptable.

This guide demonstrates how to configure the kubelet's image credential provider plugin mechanism.
-->
<p>如果以下任一情况属实，你可能对此功能感兴趣：</p>
<ul>
<li>需要调用云提供商的 API 来检索注册中心的身份验证信息。</li>
<li>凭据的到期时间很短，需要频繁请求新凭据。</li>
<li>将注册中心凭据存储在磁盘或者 imagePullSecret 是不可接受的。</li>
</ul>
<h2 id="准备开始">准备开始</h2>
<!-- 
* The kubelet image credential provider is introduced in v1.20 as an alpha feature. As with other alpha features,
a feature gate `KubeletCredentialProviders` must be enabled on only the kubelet for the feature to work.
* A working implementation of a credential provider exec plugin. You can build your own plugin or use one provided by cloud providers.
-->
<ul>
<li>kubelet 镜像凭证提供程序在 v1.20 版本作为 alpha 功能引入。
与其他 alpha 功能一样，当前仅当在 kubelet 启动 <code>KubeletCredentialProviders</code> 特性门禁才能使该功能正常工作。</li>
<li>凭据提供程序 exec 插件的工作实现。你可以构建自己的插件或使用云提供商提供的插件。</li>
</ul>
<!-- steps -->
<!-- 
## Installing Plugins on Nodes

A credential provider plugin is an executable binary that will be run by the kubelet. Ensure that the plugin binary exists on
every node in your cluster and stored in a known directory. The directory will be required later when configuring kubelet flags.
-->
<h2 id="installing-plugins-on-nodes">在节点上安装插件 </h2>
<p>凭据提供程序插件是将由 kubelet 运行的可执行二进制文件。
确保插件二进制存在于你的集群的每个节点上，并存储在已知目录中。
稍后配置 kubelet 标志需要该目录。</p>
<!-- 
## Configuring the Kubelet

In order to use this feature, the kubelet expects two flags to be set:
* `--image-credential-provider-config` - the path to the credential provider plugin config file.
* `--image-credential-provider-bin-dir` - the path to the directory where credential provider plugin binaries are located.
-->
<h2 id="configuring-the-kubelet">配置 kubelet </h2>
<p>为了使用这个特性，kubelet 需要设置以下两个标志：</p>
<ul>
<li><code>--image-credential-provider-config</code> —— 凭据提供程序插件配置文件的路径。</li>
<li><code>--image-credential-provider-bin-dir</code> —— 凭据提供程序插件二进制文件所在目录的路径。</li>
</ul>
<!-- 
### Configure a kubelet credential provider

The configuration file passed into `--image-credential-provider-config` is read by the kubelet to determine which exec plugins
should be invoked for which container images. Here's an example configuration file you may end up using if you are using the [ECR](https://aws.amazon.com/ecr/)-based plugin:
-->
<h3 id="configure-a-kubelet-credential-provider">配置 kubelet 凭据提供程序 </h3>
<p>kubelet 会读取传入 <code>--image-credential-provider-config</code> 的配置文件文件，
以确定应该为哪些容器镜像调用哪些 exec 插件。
如果你正在使用基于 <a href="https://aws.amazon.com/ecr/">ECR</a> 插件，
这里有个样例配置文件你可能最终会使用到：</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CredentialProviderConfig<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kubelet.config.k8s.io/v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># providers 是将由 kubelet 启用的凭证提供程序插件列表。</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 多个提供程序可能与单个镜像匹配，在这种情况下，来自所有提供程序的凭据将返回到 kubelet。</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 如果为单个镜像调用多个提供程序，则结果会合并。</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#080;font-style:italic"># 如果提供程序返回重叠的身份验证密钥，则使用提供程序列表中较早的值。</span><span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">providers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># name 是凭据提供程序的必需名称。 </span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 它必须与 kubelet 看到的提供程序可执行文件的名称相匹配。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># 可执行文件必须在 kubelet 的 bin 目录中</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#080;font-style:italic"># （由 --image-credential-provider-bin-dir 标志设置）。</span><span style="color:#bbb">
</span><span style="color:#bbb">  </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ecr<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># matchImages 是一个必需的字符串列表，用于匹配镜像以确定是否应调用此提供程序。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 如果其中一个字符串与 kubelet 请求的镜像相匹配，则该插件将被调用并有机会提供凭据。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 镜像应包含注册域和 URL 路径。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># matchImages 中的每个条目都是一个模式，可以选择包含端口和路径。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 通配符可以在域中使用，但不能在端口或路径中使用。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 支持通配符作为子域（例如“*.k8s.io”或“k8s.*.io”）和顶级域（例如“k8s.*”）。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 还支持匹配部分子域，如“app*.k8s.io”。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 每个通配符只能匹配一个子域段，因此 *.io 不匹配 *.k8s.io。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 当以下所有条件都为真时，镜像和 matchImage 之间存在匹配：</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - 两者都包含相同数量的域部分并且每个部分都匹配。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - imageMatch 的 URL 路径必须是目标镜像 URL 路径的前缀。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - 如果 imageMatch 包含端口，则该端口也必须在图像中匹配。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic">#</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># matchImages 的示例值：</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - 123456789.dkr.ecr.us-east-1.amazonaws.com</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - *.azurecr.io</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - gcr.io</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - *.*.registry.io</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - registry.io:8080/path</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">matchImages</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;*.dkr.ecr.*.amazonaws.com&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;*.dkr.ecr.*.amazonaws.cn&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;*.dkr.ecr-fips.*.amazonaws.com&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;*.dkr.ecr.us-iso-east-1.c2s.ic.gov&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#b44">&#34;*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># defaultCacheDuration 是插件将在内存中缓存凭据的默认持续时间</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 如果插件响应中未提供缓存持续时间。此字段是必需的。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">defaultCacheDuration</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;12h&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># exec CredentialProviderRequest 的必需输入版本。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 返回的 CredentialProviderResponse 必须使用与输入相同的编码版本。当前支持的值为：</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># - credentialprovider.kubelet.k8s.io/v1alpha1</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>credentialprovider.kubelet.k8s.io/v1alpha1<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 执行命令时传递给命令的参数。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># +可选</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- get-credentials<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># env 定义了额外的环境变量以暴露给进程。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># 这些与主机环境以及 client-go 用于将参数传递给插件的变量结合在一起。</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#080;font-style:italic"># +可选</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#008000;font-weight:bold">env</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>AWS_PROFILE<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span>example_profile<span style="color:#bbb">
</span></code></pre></div><!-- 
The `providers` field is a list of enabled plugins used by the kubelet. Each entry has a few required fields:
* `name`: the name of the plugin which MUST match the name of the executable binary that exists in the directory passed into `--image-credential-provider-bin-dir`.
* `matchImages`: a list of strings used to match against images in order to determine if this provider should be invoked. More on this below.
* `defaultCacheDuration`: the default duration the kubelet will cache credentials in-memory if a cache duration was not specified by the plugin.
* `apiVersion`: the api version that the kubelet and the exec plugin will use when communicating.

Each credential provider can also be given optional args and environment variables as well. Consult the plugin implementors to determine what set of arguments and environment variables are required for a given plugin.
-->
<p><code>providers</code> 字段是 kubelet 使用的已启用插件列表。每个条目都有几个必填字段：</p>
<ul>
<li><code>name</code>：插件的名称，必须与传入<code>--image-credential-provider-bin-dir</code>
的目录中存在的可执行二进制文件的名称相匹配。</li>
<li><code>matchImages</code>：用于匹配图像以确定是否应调用此提供程序的字符串列表。更多相关信息如下。</li>
<li><code>defaultCacheDuration</code>：如果插件未指定缓存持续时间，kubelet 将在内存中缓存凭据的默认持续时间。</li>
<li><code>apiVersion</code>：kubelet 和 exec 插件在通信时将使用的 api 版本。</li>
</ul>
<p>每个凭证提供程序也可以被赋予可选的参数和环境变量。
咨询插件实现者以确定给定插件需要哪些参数和环境变量集。</p>
<!-- 
#### Configure image matching

The `matchImages` field for each credential provider is used by the kubelet to determine whether a plugin should be invoked
for a given image that a Pod is using. Each entry in `matchImages` is an image pattern which can optionally contain a port and a path.
Globs can be used in the domain, but not in the port or the path. Globs are supported as subdomains like `*.k8s.io` or `k8s.*.io`,
and top-level domains such as `k8s.*`. Matching partial subdomains like `app*.k8s.io` is also supported. Each glob can only match
a single subdomain segment, so `*.io` does NOT match `*.k8s.io`.
-->
<h4 id="configure-image-matching">配置镜像匹配 </h4>
<p>kubelet 使用每个凭证提供程序的 <code>matchImages</code> 字段来确定是否应该为 Pod 正在使用的给定镜像调用插件。
<code>matchImages</code> 中的每个条目都是一个镜像模式，可以选择包含端口和路径。
通配符可以在域中使用，但不能在端口或路径中使用。
支持通配符作为子域，如 <code>*.k8s.io</code> 或 <code>k8s.*.io</code>，以及顶级域，如 <code>k8s.*</code>。
还支持匹配部分子域，如 <code>app*.k8s.io</code>。每个通配符只能匹配一个子域段，
因此 <code>*.io</code> 不匹配 <code>*.k8s.io</code>。</p>
<!-- 
A match exists between an image name and a `matchImage` entry when all of the below are true:

* Both contain the same number of domain parts and each part matches.
* The URL path of match image must be a prefix of the target image URL path.
* If the imageMatch contains a port, then the port must match in the image as well.

Some example values of `matchImages` patterns are:
-->
<p>当以下所有条件都为真时，镜像名称和 <code>matchImage</code> 条目之间存在匹配：</p>
<ul>
<li>两者都包含相同数量的域部分并且每个部分都匹配。</li>
<li>匹配图片的 URL 路径必须是目标图片 URL 路径的前缀。</li>
<li>如果 imageMatch 包含端口，则该端口也必须在镜像中匹配。</li>
</ul>
<p><code>matchImages</code> 模式的一些示例值：</p>
<ul>
<li><code>123456789.dkr.ecr.us-east-1.amazonaws.com</code></li>
<li><code>*.azurecr.io</code></li>
<li><code>gcr.io</code></li>
<li><code>*.*.registry.io</code></li>
<li><code>foo.registry.io:8080/path</code></li>
</ul>

</div>



    
	
  



          </main>
        </div>
      </div>
      
<footer class="d-print-none">
  <div class="footer__links">
    <nav>
      
      
      
      <a class="text-white" href="/zh/docs/home/">主页</a>
      
      <a class="text-white" href="/zh/blog/">博客</a>
      
      <a class="text-white" href="/zh/training/">培训</a>
      
      <a class="text-white" href="/zh/partners/">合作伙伴</a>
      
      <a class="text-white" href="/zh/community/">社区</a>
      
      <a class="text-white" href="/zh/case-studies/">案例分析</a>
      
    </nav>
  </div>
  <div class="container-fluid">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="User mailing list" aria-label="User mailing list">
    <a class="text-white" target="_blank" href="https://discuss.kubernetes.io">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" href="https://twitter.com/kubernetesio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Calendar" aria-label="Calendar">
    <a class="text-white" target="_blank" href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io">
      <i class="fas fa-calendar-alt"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Youtube" aria-label="Youtube">
    <a class="text-white" target="_blank" href="https://youtube.com/kubernetescommunity">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-2 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/kubernetes/kubernetes">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" href="https://slack.k8s.io">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Contribute" aria-label="Contribute">
    <a class="text-white" target="_blank" href="https://git.k8s.io/community/contributors/guide">
      <i class="fas fa-edit"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" href="https://stackoverflow.com/questions/tagged/kubernetes">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <small class="text-white">&copy; 2021 The Kubernetes 作者 | 文档发布基于 <a href="https://git.k8s.io/website/LICENSE" class="light-text">CC BY 4.0</a> 授权许可</small>
        <br/>
        <small class="text-white">Copyright &copy; 2021 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href="https://www.linuxfoundation.org/trademark-usage" class="light-text">商标使用页面</a></small>
        <br/>
        <small class="text-white">ICP license: 京ICP备17074266号-3</small>
        
        
          
        
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="/js/popper-1.14.3.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="/js/bootstrap-4.3.1.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>











<script src="/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js" integrity="sha256-QGFiUam25LaJ53ab4DQGYe&#43;k1&#43;u3P5V0BOlj4TW07VI=" crossorigin="anonymous"></script>






  </body>
</html>
