<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Orquestração de contêineres prontos para produção</title>
    <link>https://kubernetes.io/pt-br/</link>
    <description>The Kubernetes project blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <image>
      <url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url>
      <title>Kubernetes.io</title>
      <link>https://kubernetes.io/pt-br/</link>
    </image>
    
	<atom:link href="https://kubernetes.io/pt-br/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: Não entre em pânico: Kubernetes e Docker</title>
      <link>https://kubernetes.io/pt-br/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/pt-br/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Autores / Autoras&lt;/strong&gt;: Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tradução:&lt;/strong&gt; João Brito&lt;/p&gt;
&lt;p&gt;Kubernetes está &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation&#34;&gt;deixando de usar Docker&lt;/a&gt; como seu agente de execução após a versão v1.20.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Não entre em pânico. Não é tão dramático quanto parece.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TL;DR Docker como um agente de execução primário está sendo deixado de lado em favor de agentes de execução que utilizam a Interface de Agente de Execução de Containers (Container Runtime Interface &amp;quot;CRI&amp;quot;) criada para o Kubernetes. As imagens criadas com o Docker continuarão a funcionar em seu cluster com os agentes atuais, como sempre estiveram.&lt;/p&gt;
&lt;p&gt;Se você é um usuário final de Kubernetes, quase nada mudará para você. Isso não significa a morte do Docker, e isso não significa que você não pode, ou não deva, usar ferramentas Docker em desenvolvimento mais. Docker ainda é uma ferramenta útil para a construção de containers, e as imagens resultantes de executar &lt;code&gt;docker build&lt;/code&gt; ainda rodarão em seu cluster Kubernetes.&lt;/p&gt;
&lt;p&gt;Se você está usando um Kubernetes gerenciado como GKE, EKS, ou AKS (que usa como &lt;a href=&#34;https://github.com/Azure/AKS/releases/tag/2020-11-16&#34;&gt;padrão containerd&lt;/a&gt;) você precisará ter certeza que seus nós estão usando um agente de execução de container suportado antes que o suporte ao Docker seja removido nas versões futuras do Kubernetes. Se você tem mudanças em seus nós, talvez você precise atualizá-los baseado em seu ambiente e necessidades do agente de execução.&lt;/p&gt;
&lt;p&gt;Se você está rodando seus próprios clusters, você também precisa fazer mudanças para evitar quebras em seu cluster. Na versão v1.20, você terá o aviso de alerta da perda de suporte ao Docker. Quando o suporte ao agente de execução do Docker for removido em uma versão futura (atualmente planejado para a versão 1.22 no final de 2021) do Kubernetes ele não será mais suportado e você precisará trocar para um dos outros agentes de execução de container compatível, como o containerd ou CRI-O. Mas tenha certeza que esse agente de execução escolhido tenha suporte às configurações do daemon do Docker usadas atualmente (Ex.: logs)&lt;/p&gt;
&lt;h2 id=&#34;então-porque-a-confusão-e-toda-essa-turma-surtando&#34;&gt;Então porque a confusão e toda essa turma surtando?&lt;/h2&gt;
&lt;p&gt;Estamos falando aqui de dois ambientes diferentes, e isso está criando essa confusão. Dentro do seu cluster Kubernetes, existe uma coisa chamada de agente de execução de container que é responsável por baixar e executar as imagens de seu container. Docker é a escolha popular para esse agente de execução (outras escolhas comuns incluem containerd e CRI-O), mas Docker não foi projetado para ser embutido no Kubernetes, e isso causa problemas.&lt;/p&gt;
&lt;p&gt;Se liga, o que chamamos de &amp;quot;Docker&amp;quot; não é exatamente uma coisa - é uma stack tecnológica inteira, e uma parte disso é chamado de &amp;quot;containerd&amp;quot;, que é o agente de execução de container de alto-nível por si só. Docker é legal e útil porque ele possui muitas melhorias de experiência do usuário e isso o torna realmente fácil para humanos interagirem com ele enquanto estão desenvolvendo, mas essas melhorias para o usuário não são necessárias para o Kubernetes, pois ele não é humano.&lt;/p&gt;
&lt;p&gt;Como resultado dessa camada de abstração amigável aos humanos, seu cluster Kubernetes precisa usar outra ferramenta chamada Dockershim para ter o que ele realmente precisa, que é o containerd. Isso não é muito bom, porque adiciona outra coisa a ser mantida e que pode quebrar. O que está atualmente acontecendo aqui é que o Dockershim está sendo removido do Kubelet assim que que a versão v1.23 for lançada, que remove o suporte ao Docker como agente de execução de container como resultado. Você deve estar pensando, mas se o containerd está incluso na stack do Docker, porque o Kubernetes precisa do Dockershim?&lt;/p&gt;
&lt;p&gt;Docker não é compatível com CRI, a &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&#34;&gt;Container Runtime Interface&lt;/a&gt; (interface do agente de execução de container). Se fosse, nós não precisaríamos do shim, e isso não seria nenhum problema. Mas isso não é o fim do mundo, e você não precisa entrar em pânico - você só precisa mudar seu agente de execução de container do Docker para um outro suportado.&lt;/p&gt;
&lt;p&gt;Uma coisa a ser notada: Se você está contando com o socket do Docker (&lt;code&gt;/var/run/docker.sock&lt;/code&gt;) como parte do seu fluxo de trabalho em seu cluster hoje, mover para um agente de execução diferente acaba com sua habilidade de usá-lo. Esse modelo é conhecido como Docker em Docker. Existem diversas opções por aí para esse caso específico como o &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34;&gt;kaniko&lt;/a&gt;, &lt;a href=&#34;https://github.com/genuinetools/img&#34;&gt;img&lt;/a&gt;, e &lt;a href=&#34;https://github.com/containers/buildah&#34;&gt;buildah&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;o-que-essa-mudança-representa-para-os-desenvolvedores-ainda-escrevemos-dockerfiles-ainda-vamos-fazer-build-com-docker&#34;&gt;O que essa mudança representa para os desenvolvedores?  Ainda escrevemos Dockerfiles? Ainda vamos fazer build com Docker?&lt;/h2&gt;
&lt;p&gt;Essa mudança aborda um ambiente diferente do que a maioria das pessoas usa para interagir com Docker. A instalação do Docker que você está usando em desenvolvimento não tem relação com o agente de execução de Docker dentro de seu cluster Kubernetes. É confuso, dá pra entender.
Como desenvolvedor, Docker ainda é útil para você em todas as formas que era antes dessa mudança ser anunciada. A imagem que o Docker cria não é uma imagem específica para Docker e sim uma imagem que segue o padrão OCI (&lt;a href=&#34;https://opencontainers.org/&#34;&gt;Open Container Initiative&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Qualquer imagem compatível com OCI, independente da ferramenta usada para construí-la será vista da mesma forma pelo Kubernetes. Ambos &lt;a href=&#34;https://containerd.io/&#34;&gt;containerd&lt;/a&gt; e &lt;a href=&#34;https://cri-o.io/&#34;&gt;CRI-O&lt;/a&gt; sabem como baixar e executá-las. Esse é o porque temos um padrão para containers.&lt;/p&gt;
&lt;p&gt;Então, essa mudança está chegando. Isso irá causar problemas para alguns, mas nada catastrófico, no geral é uma boa coisa. Dependendo de como você interage com o Kubernetes, isso tornará as coisas mais fáceis. Se isso ainda é confuso para você, tudo bem, tem muita coisa rolando aqui; Kubernetes tem um monte de partes móveis, e ninguém é 100% especialista nisso. Nós encorajamos toda e qualquer tipo de questão independente do nível de experiência ou de complexidade! Nosso objetivo é ter certeza que todos estão entendendo o máximo possível as mudanças que estão chegando. Esperamos que isso tenha respondido a maioria de suas questões e acalmado algumas ansiedades! ❤️&lt;/p&gt;
&lt;p&gt;Procurando mais respostas? Dê uma olhada em nosso apanhado de &lt;a href=&#34;https://kubernetes.io/blog/2020/12/02/dockershim-faq/&#34;&gt;questões quanto ao desuso do Dockershim&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Escalando a rede do Kubernetes com EndpointSlices</title>
      <link>https://kubernetes.io/pt-br/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubernetes.io/pt-br/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Autor:&lt;/strong&gt; Rob Scott (Google)&lt;/p&gt;
&lt;p&gt;EndpointSlices é um novo tipo de API que provê uma alternativa escalável e extensível à API de Endpoints. EndpointSlices mantém o rastreio dos endereços IP, portas, informações de topologia e prontidão de Pods que compõem um serviço.&lt;/p&gt;
&lt;p&gt;No Kubernetes 1.19 essa funcionalidade está habilitada por padrão, com o kube-proxy lendo os  &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/&#34;&gt;EndpointSlices&lt;/a&gt; ao invés de Endpoints. Apesar de isso ser uma mudança praticamente transparente, resulta numa melhoria notável de escalabilidade em grandes clusters. Também permite a adição de novas funcionalidades em releases futuras do Kubernetes, como o &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service-topology/&#34;&gt;Roteamento baseado em topologia.&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;limitações-de-escalabilidade-da-api-de-endpoints&#34;&gt;Limitações de escalabilidade da API de Endpoints&lt;/h2&gt;
&lt;p&gt;Na API de Endpoints, existia apenas um recurso de Endpoint por serviço (Service). Isso significa que
era necessário ser possível armazenar endereços IPs e portas para cada Pod que compunha o serviço correspondente. Isso resultava em recursos imensos de API. Para piorar, o kube-proxy rodava em cada um dos nós e observava qualquer alteração nos recursos de Endpoint. Mesmo que fosse uma simples mudança em um Endpoint, todo o objeto precisava ser enviado para cada uma das instâncias do kube-proxy.&lt;/p&gt;
&lt;p&gt;Outra limitação da API de Endpoints era que ela limitava o número de objetos que podiam ser associados a um &lt;em&gt;Service&lt;/em&gt;. O tamanho padrão de um objeto armazenado no etcd é 1.5MB. Em alguns casos, isso poderia limitar um Endpoint a 5,000 IPs de Pod. Isso não chega a ser um problema para a maioria dos usuários, mas torna-se um problema significativo para serviços que se aproximem desse tamanho.&lt;/p&gt;
&lt;p&gt;Para demonstrar o quão significante se torna esse problema em grande escala, vamos usar de um simples exemplo: Imagine um &lt;em&gt;Service&lt;/em&gt; que possua 5,000 Pods, e que possa causar o Endpoint a ter 1.5Mb . Se apenas um Endpoint nessa lista sofra uma alteração, todo o objeto de Endpoint precisará ser redistribuído para cada um dos nós do cluster. Em um cluster com 3.000 nós, essa atualização causará o envio de 4.5Gb de dados (1.5Mb de Endpoints * 3,000 nós) para todo o cluster. Isso é quase que o suficiente para encher um DVD, e acontecerá para cada mudança de Endpoint. Agora imagine uma atualização gradual em um &lt;em&gt;Deployment&lt;/em&gt; que resulte nos 5,000 Pods serem substituídos - isso é mais que 22Tb (ou 5,000 DVDs) de dados transferidos.&lt;/p&gt;
&lt;h2 id=&#34;dividindo-os-endpoints-com-a-api-de-endpointslice&#34;&gt;Dividindo os endpoints com a API de EndpointSlice&lt;/h2&gt;
&lt;p&gt;A API de EndpointSlice foi desenhada para resolver esse problema com um modelo similar de &lt;em&gt;sharding&lt;/em&gt;. Ao invés de rastrar todos os IPs dos Pods para um &lt;em&gt;Service&lt;/em&gt;, com um único recurso de Endpoint, nós dividimos eles em múltiplos EndpointSlices menores.&lt;/p&gt;
&lt;p&gt;Usemos por exemplo um serviço com 15 pods. Nós teríamos um único recurso de Endpoints referente a todos eles. Se o EndpointSlices for configurado para armazenar 5 &lt;em&gt;endpoints&lt;/em&gt; cada, nós teríamos 3 EndpointSlices diferentes:
&lt;img src=&#34;https://kubernetes.io/images/blog/2020-09-02-scaling-kubernetes-networking-endpointslices/endpoint-slices.png&#34; alt=&#34;EndpointSlices&#34;&gt;&lt;/p&gt;
&lt;p&gt;Por padrão, o EndpointSlices armazena um máximo de 100 &lt;em&gt;endpoints&lt;/em&gt; cada, podendo isso ser configurado com a flag &lt;code&gt;--max-endpoints-per-slice&lt;/code&gt; no kube-controller-manager.&lt;/p&gt;
&lt;h2 id=&#34;endpointslices-provê-uma-melhoria-de-escalabilidade-em-10x&#34;&gt;EndpointSlices provê uma melhoria de escalabilidade em 10x&lt;/h2&gt;
&lt;p&gt;Essa API melhora dramaticamente a escalabilidade da rede. Agora quando um Pod é adicionado ou removido, apenas 1 pequeno EndpointSlice necessita ser atualizado. Essa diferença começa a ser notada quando centenas ou milhares de Pods compõem um único &lt;em&gt;Service&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Mais significativo, agora que todos os IPs de Pods para um &lt;em&gt;Service&lt;/em&gt; não precisam ser armazenados em um único recurso, nós não precisamos nos preocupar com o limite de tamanho para objetos armazendos no etcd. EndpointSlices já foram utilizados para escalar um serviço além de 100,000 endpoints de rede.&lt;/p&gt;
&lt;p&gt;Tudo isso é possível com uma melhoria significativa de performance feita no kube-proxy. Quando o EndpointSlices é usado em grande escala, muito menos dados serão transferidos para as atualizações de endpoints e o kube-proxy torna-se mais rápido para atualizar regras do iptables ou do ipvs. Além disso, os &lt;em&gt;Services&lt;/em&gt; podem escalar agora para pelo menos 10x mais além dos limites anteriores.&lt;/p&gt;
&lt;h2 id=&#34;endpointslices-permitem-novas-funcionalidades&#34;&gt;EndpointSlices permitem novas funcionalidades&lt;/h2&gt;
&lt;p&gt;Introduzido como uma funcionalidade alpha no Kubernetes v1.16, os EndpointSlices foram construídos para permitir algumas novas funcionalidades arrebatadoras em futuras versões do Kubernetes. Isso inclui serviços dual-stack, roteamento baseado em topologia e subconjuntos de &lt;em&gt;endpoints&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Serviços Dual-stack são uma nova funcionalidade que foi desenvolvida juntamente com o EndpointSlices. Eles irão utilizar simultâneamente endereços IPv4 e IPv6 para serviços, e dependem do campo addressType do Endpointslices para conter esses novos tipos de endereço por família de IP.&lt;/p&gt;
&lt;p&gt;O roteamento baseado por topologia irá atualizar o kube-proxy para dar preferência no roteamento de requisições para a mesma região ou zona, utilizando-se de campos de topologia armazenados em cada endpoint dentro de um EndpointSlice. Como uma melhoria futura disso, estamos explorando o potencial de subconjuntos de endpoint. Isso irá permitir o kube-proxy apenas observar um subconjunto de EndpointSlices. Por exemplo, isso pode ser combinado com o roteamento baseado em topologia e assim, o kube-proxy precisará observar apenas EndpointSlices contendo &lt;em&gt;endpoints&lt;/em&gt; na mesma zona. Isso irá permitir uma outra melhoria significativa de escalabilidade.&lt;/p&gt;
&lt;h2 id=&#34;o-que-isso-significa-para-a-api-de-endpoints&#34;&gt;O que isso significa para a API de Endpoints?&lt;/h2&gt;
&lt;p&gt;Apesar da API de EndpointSlice prover uma alternativa nova e escalável à API de Endpoints, a API de Endpoints continuará a ser considerada uma funcionalidade estável. A mudança mais significativa para a API de Endpoints envolve começar a truncar Endpoints que podem causar problemas de escalabilidade.&lt;/p&gt;
&lt;p&gt;A API de Endpoints não será removida, mas muitas novas funcionalidades irão depender da nova API EndpointSlice. Para obter vantágem da funcionalidade e escalabilidade que os EndpointSlices provém, aplicações que hoje consomem a API de Endpoints devem considerar suportar EndpointSlices no futuro.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
